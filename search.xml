<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大半年的流水账]]></title>
    <url>%2F2019%2F06%2F18%2F2019-06-18-note%2F</url>
    <content type="text"><![CDATA[生活总是不知道会在什么时候给你点“惊喜”。 过去的半年到一年间发生了挺多事情的，对我自己来说有好事也有坏事，上上下下大起大落，自己的计划被一次次地打乱，每一次都算是艰难的抉择吧。看了下博客也有半年没更了，更让我觉得惊讶的是上一次挂了“随笔” tag 的居然已经是2年前了(可见这两年读研生涯中我是多么正经…^_^)，准备来理一理过去的流水账。 To be continued.]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Release 了一个新的 VizGraph]]></title>
    <url>%2F2019%2F01%2F01%2F2019-01-01-vizgraph%2F</url>
    <content type="text"><![CDATA[2019 第一篇！ 也是去年年底的时候心血来潮写的这个小东西： 【写了一个画框图的轮子】 中间断断续续往里面补了一些功能，修修 bug 什么的，然后就很久没有再管过了。 虽说一开始想要的本来不是个能写 GraphViz 的文本编辑器，结果写到最后还是写成了个文本编辑器。 话说 vscode 里面的 GraphViz 插件其实也挺不错的， 结果我自己平时还是觉得用这个更顺手哈哈 有天忘了在找什么的时候路过发现这个叫 CS Academy 的 OJ 上提供了一个简易的图编辑工具： 左边是文字形式记录的图关系，右边呢可以通过点击和拖动等等直观对图进行编辑，然后修改一边另外一边也会跟着同步更新。 刚看到的时候觉得惊了！这不就是我最早想要的玩意嘛！ 然后果断 F12 开始研究起来。 嗯，这个网站本身应该是用某种框架做的，所有的动作都被包含在一个大概 10 万行左右的超大 js 文件里面。虽然从这里面直接扒出来基本上应该是不太可能的，不过看了下几个位置的 click、鼠标拖动事件等等差不多也可以知道实现的原理了，又在 d3.js 里找到个类似的东西，照着改改很快也山寨了一个功能差不多的出来。 然后我就发现更纠结了。 最初的想法是通过右边的图形界面操作生成左边的中间结果，然后手动细调之后再进一步生成 GraphViz 的 dot 代码，最后通过 GraphViz 生成 svg 的图。 结果等到这个 Graph Editor 写出来之后，却发现这样瞎搞还不如从一开始就老老实实直接写 dot 代码来的顺畅。 这大概就是花了好大的精力实现了最初的想法之后发现自己走了歪路的感觉了。 说起来这算是我第一个真正走到了实用阶段还一直在继续改的脑洞了，歪过一次之后现在决定这玩意还是老老实实就写成个编辑器吧。 目前是加了个多模版的启动页，然后用 PPT 花几分钟做了个图标。 对！你没看错，PPT！ 一直没有真正好好学过前端相关的东西，所有这些都是有想法之后现场在网上找的教程和资料。 等以后有空了再改大概得想办法照顾一下这个界面的美观性（目前负分……），然后把逻辑部分的代码也好好重构一下。然后虽说现在把 Ace Editor 的自动补全功能打开了，但是这里面似乎不自带补全规则，之后还得看看怎么加。再然后还想把 GraphViz 语法的文档功能加进去，我自己经常用的时候还得开 GraphViz 的网站查有的东西怎么写，生产力啊生产力！ Keep coding！ 2019 大家继续加油呀！]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Node.js</tag>
        <tag>GraphViz</tag>
        <tag>VizGraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（十）：Allreduce]]></title>
    <url>%2F2018%2F12%2F18%2F2018-12-18-tfunpacking10%2F</url>
    <content type="text"><![CDATA[前篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device TensorFlow 拆包（五）：Distributed TensorFlow 拆包（六）：RDMA TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning TensorFlow 拆包（九）：High Level APIs 不知不觉居然写了十篇了……写这个的初衷是觉得自己是个容易忘事的人，不找个地方做点笔记可能回过头就忘了自己看过什么了。 这篇要分析的是 TensorFlow 自带的 Allreduce 实现。 APIs outside of tf wrapperTensorFlow 中常规的使用操作是： 1import tensorflow as tf 但是实际上 TensorFlow 目录下的 __init__.py 里面并没有把全部的内容都包含进去，另外的内容需要通过： 1from tensorflow.xxx.xxx import xxx 这样的方式直接导入单独的包。 然后更奇怪的是有的 API 明明不在 contrib 中，按理说应该算是官方库中正式内容了，但是在官网的文档中却找不到。也可能是正要从 contrib 往官方库中移？ 举例来说前面提过的 StagingArea 就是这样，然后这里的 Allreduce 操作的 API 也是，这就让人很好奇是不是还有别的什么不常用到的 API 可能在某些场景下会有奇效？ 扯远了， collective op首先是 tensorflow.python.ops 下的 collective_ops.py 这个文件，一共包含了 3 个 API： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def all_reduce(t, group_size, group_key, instance_key, merge_op, final_op, subdiv_offsets=(0,)) """Reduces tensors collectively, across devices. Args: t: the tensor to be reduced. group_size: the total number of tensors to be collectively reduced. Each must reside on a different device. group_key: an integer identifying the group of devices. instance_key: an integer identifying the participating group of Ops. merge_op: string naming the binary Op to be applied to compute each partial reduction. final_op: string naming the unary Op to be applied to each fully reduced value. Can be 'Id' for no operation. subdiv_offsets: a list of integer offsets into the tensor at which each independent subdivision should begin. Use [0] if no subdivision should be done. Returns: An Op implementing the distributed reduction. Raises: ValueError: if any of the input parameter constraints are not met. """def broadcast_send(t, shape, dtype, group_size, group_key, instance_key) """Broadcasts one tensor to a group of others, across devices. Args: t: the tensor to be sent. shape: the shape of the tensor being sent, which must agree with t. dtype: the type of the tensor being sent, which must agree with t. group_size: one plus the number of receiving tensors, i.e. the total number of devices participating. Each tensor must reside on a different device. group_key: an integer identifying the group of devices. instance_key: an integer identifying the participating group of Ops. Returns: An Op implementing the distributed broadcast send. Raises: ValueError: if any of the input parameter constraints are not met. Note that the shape and dtype arguments appear redundant since they should be obtainable from t. The are two reasons for including them. First, the shape and type of tensors passed via broadcast must be known ahead of time in their most specific form so that the receive side can allocate memory for the operation and shape/type inference can carry forward from there. Including the same declarations on the send side clarifies a commitment already made. Secondly, having nearly identical use syntax for send and receive sides may simplify tool-driven generation of broadcast. """def broadcast_recv(shape, dtype, group_size, group_key, instance_key) """Receives a broadcasts tensor, across devices. Args: shape: Shape of the tensor to be received. dtype: Type of the tensor to be received. group_size: one plus the number of receiving tensors, i.e. the total number of devices participating. Each tensor must reside on a different device. group_key: an integer identifying the group of devices. instance_key: an integer identifying the participating group of Ops. Returns: An Op implementing the broadcast receive. Raises: ValueError: if any of the input parameter constraints are not met. """ 基本的用法示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344with tf.device('cpu:0'): v0 = tf.Variable([1, 1, 1], dtype=tf.float32)with tf.device('gpu:0'): v1 = tf.Variable([2, 2, 2], dtype=tf.float32)with tf.device('gpu:1'): v2 = tf.Variable([3, 3, 3], dtype=tf.float32)sess = tf.Session(config=CONFIG)sess.run(tf.global_variables_initializer())print(sess.run([v0, v1, v2]))sum_reduce = []# with tf.device('cpu:0'):# out.append(collective_ops.all_reduce(v0, 3, 1, 1, 'Add', 'Id'))with tf.device('gpu:0'): sum_reduce.append(collective_ops.all_reduce(v1, 2, 0, 1, 'Add', 'Id'))with tf.device('gpu:1'): sum_reduce.append(collective_ops.all_reduce(v2, 2, 0, 1, 'Add', 'Id'))print(sess.run(sum_reduce))average_reduce = []with tf.device('gpu:0'): average_reduce.append(collective_ops.all_reduce(v1, 2, 1, 1, 'Add', 'Div'))with tf.device('gpu:1'): average_reduce.append(collective_ops.all_reduce(v2, 2, 1, 1, 'Add', 'Div'))print(sess.run(average_reduce))print(sess.run([v0, v1, v2]))print('==========================')bcast = []# with tf.device('cpu:0'):# bcast.append(collective_ops.broadcast_send(v0, v0.shape, v0.dtype, 2, 3, 1))with tf.device('gpu:0'): bcast.append(collective_ops.broadcast_send(v0, v0.shape, v0.dtype, 2, 3, 2))with tf.device('gpu:1'): bcast.append(collective_ops.broadcast_recv(v0.shape, v0.dtype, 2, 3, 2))print(sess.run(bcast)) 参数中的 t 是每个 device 上要进行 reduce 的本地 Tensor，group_size 是一共需要参与的 Tensor 数量，merge_op 和 final_op 分别是 reduce 聚合时和聚合结束之后要做的事情。 然后比较坑的是 group_key 和 instance_key 这两个参数，注释写的太不明确了（还是我理解能力有问题？）。instance_key 应该是用于标识完整的一次 reduce 操作，而 group_key 具体是怎么来的还不是很清楚。测试的时候整体换成另外的数字都是可以正常工作的。 另外 all_reduce 和 broadcast 都似乎不能在 GPU 和 CPU 间进行工作？选择两块 GPU 卡之前是没有问题的，而再加上一个 CPU 就报错了。 其中的具体实现来自于 gen_collective_ops 这个包，那我们就知道这又是个用 C++ 写的然后封装到 python 下面用的操作了。 接口定义在 tensorflow/core/ops/collective_ops.cc 中，C++ 部分的实际实现在 tensorflow/core/kernels/collective_ops.cc 中，但是这里的 CollectiveReduceOpKernel、CollectiveBcastSendOpKernel、CollectiveBcastRecvOpKernel 三个类也主要还是用于设置输入输出的参数信息、检查结果等等，更进一步的实现要到 tensorflow/core/common_runtime/base_collective_executor.cc 中。 继续深挖下去之后，在 collective_param_resolver_local.cc 中看到了这样一段： 123cp-&gt;instance.impl_details.collective_name = (cp-&gt;instance.type == BROADCAST_COLLECTIVE) ? "HierarchicalTreeBroadcast" : "RingReduce"; …… 嗯，所以这里的实现又是注册、又是各种封装的，结果到最后只有两种算法。 目前这里的 all_reduce 只有环状通信的算法实现，broadcast 则只有二叉树广播方式的算法实现。 group_key要想搞清楚这个 group_key 是怎么回事，还得从 tensorflow/core/kernels/collective_ops.cc 这个接口开始。 CollectiveReduceOpKernel、CollectiveBcastSendOpKernel、CollectiveBcastRecvOpKernel 三个类的 ComputeAsync() 方法的基本结构都差不多： 准备输出用的 Tensor，由于 Reduce 操作本身带有输入，这里也会尝试是否可以重用输入的 Tensor 调用 CanProceedWithCompute() 方法检查各项参数，对 group_key 的检查也在这里完成 调用对应的实现算法完成计算 单机环境下的 CanProceedWithCompute() 最后会调用 CollectiveParamResolverLocal::CompleteGroupLocal() ，group_key 在这里只是完全作为一个 map 的关键字来使用： 123456789101112131415&#123; mutex_lock l(group_mu_); auto it = group_table_.find(cp-&gt;group.group_key); if (it == group_table_.end()) &#123; gr = new GroupRec; gr-&gt;group.group_key = cp-&gt;group.group_key; gr-&gt;group.group_size = cp-&gt;group.group_size; gr-&gt;group.device_type = cp-&gt;group.device_type; group_table_[gr-&gt;group.group_key].reset(gr); VLOG(2) &lt;&lt; "New group_key=" &lt;&lt; gr-&gt;group.group_key &lt;&lt; " group_size=" &lt;&lt; gr-&gt;group.group_size; &#125; else &#123; gr = it-&gt;second.get(); &#125;&#125; 若某个 group_key 是第一次被使用，则与之关联的 GroupRec 会用当前创建该 op 的默认设备类型来作为整个 GroupRec 的设备类型。 当下一个 collective_op 创建时，再对目标的设备类型和根据 group_key 找出来的 GroupRec 作对比，不一致则报错，因此这里确实是限制了参与 reduce 或者 broadcast 的所有 op 都要是同一种设备类型的。 另外需要注意的是，同一个 group 中参与 all_reduce 和 broadcast 的 op 必须要和设备独立一一对应，所以也不可以在一块卡上同时发起 broadcast 的 send 和 recv，或者在同一块卡上的两个变量间进行 allreduce。 这个设备限制我觉得还是比较奇怪的，从 CPU 向当前节点下的所有 GPU 设备广播我觉得是很常规的一种逻辑啊…… instance_key 的作用也与 group_key 类似，InstanceRec 在这里主要是用来维护几个 mutex，主要负责多个 op 之间的同步，同时发生的多个 collective_op 操作只要 instance 不一样相互之间是不影响的。 To be continued.]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster and Faster -- ImageNet]]></title>
    <url>%2F2018%2F11%2F26%2F2018-11-26-fasterimagenet%2F</url>
    <content type="text"><![CDATA[这个月十几号的时候索尼发了篇论文，224 秒跑完了 ImageNet……想想大概去年的这个时候还是 Facebook 和 UCBerkeley 在 ImageNet 的大规模方面你争我赶，不得不感叹时间过得真快。 去年毕业开题大概开的也是这个方向，然后前段时间面试的时候： 面试官：你最近做了什么？ 我：我 follow 了 FB 和 UCB 他们在 ImageNet 上做训练的工作，大概试了下他们论文里面的想法。 面试官：哦，那你最多用到多少卡？ 我：e……最多跑到 16 块 V100。 说完这句自己突然感到一阵尴尬，人家工作多少块卡我多少块卡？真好意思说复现……（捂脸） 然而我也没办法啊，实验室资源已经算不少了，我还想拿一块卡假装模拟两块卡用呢。0.0 Anyway，趁着正好要读大法的论文，把之前其他人的东西都理一理。 前面这几段笔记还是去年的时候写的，看了下过了这么长时间，有的已经正式发了论文了，有的是 arXiv 上有了新版，那就下新的重新看一遍吧。 还是留下 OneDrive 的链接： 【Large Scale Deep Learning】 要说其他更多的材料的话，其实这几年在 ImageNet 上面冲分的有很多，Stanford 做过一个 DAWNBench，上面也记录了好多研究者跑 ImageNet 的最短时间、最小开销等等： 【DAWNBench - An End-to-End Deep Learning Benchmark and Competition】 Papers2017 Facebook - Accurate, Large Minibatch SGD: Training ImageNet in 1 HourFacebook 做的大规模 GPU 集群训练，顺便推一波 Caffe2。 256 块 GPU，1 小时内跑完整个 ImageNet 的数据集训练，想想都可怕。 这篇文章主要是从一个“炼丹师”的角度来写的，主要关注在学习率、minibatch 等等之间的关系，以及它们对训练带来的影响。文章的主要工作是提出了一种根据 minibatch 大小调整学习率的扩放规则，以及一种在训练时预跑的机制。 这些可能大多都是训练中总结出来的一些 tricks 吧。 随着网络模型和数据规模的不断增长，训练时间也跟着增长了，为了保证训练时间能够保持在一个有意义的限度之内，需要一些额外的工作。 文章的立足点是通过增大训练时的 batch size 来加快训练速度。例如 ResNet-50 这个网络通常大家采用的 batch 大小是 256，在 8 块 P100 上大约需要 29 个小时才能跑完整个 ImageNet 数据集（应该指的是原始的 ResNet 论文）。 但是如果加大 batch size 呢？改变 batch size 会影响到最终的收敛和训练结果的正确率。 文章接下来的工作就是通过调学习率等等其他的一些方法，把加大 batch size 带来的影响控制在一个可接受的范围内，最终做到 8192 个 batch 训出来的结果跟原本的效果持平。当然最重要的，batch size 增大了 32 倍，总的训练时间也能够降下来，用 256 块 GPU 在 1 小时内训完了整个 ImageNet！ 为了维持训练结果的准确率，他们在过往积累的大量经验上得出了一个令人难以置信地简单却又有效的规则： Linear Scaling Rule：minibatch 扩大 k 倍，学习率也扩大 k 倍。 其他超参数维持不变就好。 多次测试之后，可以发现应用了上面这个规则时，不同 minibatch 的 SGD 不仅最终准确率非常接近，而且训练的收敛曲线都是非常相似的。 但是在两种情况下，前面讨论的一些假定会有问题： 训练初始的几个 epoch 可能会引起网络参数巨变 在某些网络中，如果一开始设的学习率太大，初始的几个 epoch 会产生过大的梯度，上面这条规则就可能不太适用了。为了解决这个问题，文章于是提出了预热训练的阶段。 minibatch 的大小不能无限增长 就拿前面 ResNet-50 为例，这个规则只够保证 minibatch 在增长到 8192 的时候能维持训练效果，再大就保证不了了。 接下来是训练预热的机制： Gradual warmup：预训练时先用一个比较小的学习率，然后爬坡增长，逐渐增加到目标需要的 k 倍 经过实践调整之后，他们的实现里面是经过 5 个 epoch，学习率逐渐从$\eta$增长到$k\eta$。 分布式 SGD 在具体实现上还有很多细节的地方，可能稍微变动一下就会影响到整个模型的超参数，这里提了几个注意点吧： 权重衰减：交叉熵 loss 的扩放并不等价于学习率的扩放（？） 动量修正：如果用了像 Momentum SGD 这种方法，需要在改变学习率之后再应用动量修正 梯度聚合：用整个 minibatch 的大小 kn 来均一化每个 worker 的 loss，而不是每个 worker 负责的 n 数据清洗：先对训练数据做 shuffle 再把他们放到不同的 worker 上 关于我最关心的梯度聚合的实现，他们没有采用 Parameter Server 的形式，而是所有 worker 直接做 allreduce。 Allreduce 分成 3 个阶段： 每个节点的 8 块 GPU 的数据合并到同一个 buffer 上 所有节点对数据进行汇总做 allreduce 每个节点的梯度更新完之后再广播到各自的 8 块 GPU 上 1、3 两个阶段的操作，如果数据量大于 256 KB 则用 NVIDIA 的 NCCL 库来完成，否则就简单地各自把数据传回 CPU 再做汇总。 阶段 2 用到了递归二分倍增算法以及令牌环算法……嗯，基本上也没有什么特别的，就是想办法高效实现 all_reduce 和 all_gather。 软件栈用的是 Facebook 自家开源的 Gloo，具体的通信方面都是靠这个通信库来完成，就不涉及到具体的实现了（……）。以及 Caffe2。 硬件是 Facebook 的 Big Basin GPU 集群。每个节点是 8 块 NVLINK 的 P100（DGX-1），50 Gbit 的 InfiniBand。 预估一下 ResNet-50 的数据量：大约参数量是 100MB 左右，单 P100 跑完一次大约 120ms，也就是峰值带宽大概要在 100MB * 2 / 0.125s = 12.8 Gbit/s 的程度，加上一些额外的 overhead 应该不会超过 15 Gbit/s，对这套 IB 来说完全不是问题。 后面的训练设置描述得很详细，包括很多超参数要怎么取值，卷积层和全连接层参数的初始化分别采用不同的方法等等。 单节点的 baseline 是 8 卡（k=8），每块卡的 BatchSize 是 32（n=32），相当于单机一轮的 BatchSize 是 256，90 个 epoch 之后得到了 23.6% 的错误率，表现可以说是非常不错了。 多机训练的时候上了 32 个节点，一共 256 块卡，总的 BatchSize 达到了 8k，相较单机扩大了 32 倍。如前面所描述的，单机训的时候学习率选在 0.1，这里则是从 0.1 开始经过 5 个 epoch 逐渐增长到 3.2，即原来的 32 倍。用了这套 Gradual warmup 的预热机制之后，最终网络收敛在了 23.74%，几乎与单机一致了，并且从测试结果中可以看到后期甚至是连 error 的下降曲线都是基本一致的。 效率方面，他们使用的这套 Allreduce 算法得到了非常好的效果，基本上已经可以看到线性的性能表现。 做大规模训练的话，这篇论文基本上可以说是教科书级别的示范了。 2017 - Extremely Large MinibatchSGD Training ResNet-50 on ImageNet in 15 Minutes上面 Facebook 用 256 块 P100 做了大 minibatch 的 ImageNet 训练之后，感觉军备竞赛就开始了。 这是日本的一个研究机构，用 1024 块 P100，更大的 BatchSize！！15分钟跑完了 ImageNet。 完成这个任务主要有两方面的挑战： 算法层面，使用更大的 BatchSize 之后，要想办法防止精度的损失； 系统层面，需要设计一套方案来有效地利用上可用的硬件和软件资源。 这里给了个表，对比了一下目前用 ResNet 跑 ImageNet 的几个工作： Team Hardware Software Minibatch size Time Accuracy MSRA（ResNet作者） Tesla P100 * 8 Caffe 256 29 h 75.3% Facebook Tesla P100 * 256 Caffe2 8192 1 h 76.3% SURFsara KNL 7250 * 720 Intel Caffe 11520 62 m 75.0% UC Berkeley Xeon 8160 * 1600 Intel Caffe 16000 31 m 75.3% This work Tesla P100 * 1024 Chainer 32768 15 m 74.9% 嗯…话说，本来以为 UCB 最发的文章是在 KNL 上跑的，刚刚才发现他们当时最新的工作是用居然是 8160（这可是通用处理器啊？？？害怕，下面那篇真得好好看看了） 具体的实现，这里说是参照 Facebook 的工作做的，比较额外的就是加大了 BatchSize，然后软件方面换了他们自己写的 Chainer 框架，因此这块写的比较简略。可能还有个差别是这里用了 RMSprop 吧，另外也提供了他们所使用的 RMSprop 的 Warm-up 方案。 NCCL 和 OpenMPI 意味着他们写的这个框架是基于 MPI 做的通信，而且还做了卡间数据传输。主要还是用的同步模型，然后做 Allreduce。 训练用的单精，然后划重点，为了减少通信的数据量，数据传输的时候用的半精浮点！！测试之后发现对结果影响不大（666666….）。 硬件方面，双路 E5-2677，8 块 P100 的配置，应该是没有 NVLINK 的，IB 用的 FDR（56 Gbps）。 2017 UC Berkeley - 2018 ICPP - ImageNet Training in Minutes 2018 年更新，这篇已经发在了 ICPP 上，也增加了很多内容，基本上已经是一篇新的文章了 接下来上台的是 UC Berkeley。 最早看到他们 arXiv 上的论文版本写的是 24 分钟跑完，可能被同期其他同行的工作刺激了一下，后来更新把标题里面的 24 去掉了，最终实现里面确实也是把这个时间缩短到了 10 分钟的量级。 他们家应该是跟 Intel 合作，用的是 Intel Caffe 和 Intel 的计算设备。 17 年的论文中使用了两种硬件方案： Xeon Platinum 8160（Skylake 的 Xeon 通用 CPU） Intel Xeon Phi 7250（KNL） 18 年的正式版本还额外增加了 P100 的测试内容。 基本的结果是：2048 颗 8160 用 11 分钟跑完 100 个 epoch 的 AlexNet，准确率收敛在 58.6%；2048 块 KNL 用 20 分钟跑完 90 个 epoch 的 ResNet-50，然后 14 分钟跑到 64 个 epoch 的时候就已经收敛到 74.9% 了。 他们不出意外地也用了同步的更新策略，这里给的理由是同步可以保证整个计算过程的确定性，并行实现与串行实现的行为是一致的，这一点在设计和调试以及优化网络的时候尤其重要。有一些前人的工作也证明了异步的方式是不稳定的（例如 Revisiting Distributed Synchronous SGD 以及前面 Facebook 的工作等等）。 异步策略会引入更多的影响因素，而且由于各个 worker 上的参数存在延迟，对整体的算法收敛也会有影响。话说前面几篇论文里面提出的优化方案也是，在异步训练的时候完全是不同的表现，不能直接适用过去。 Introduction 最后点了一下这篇论文的一些要点： 本文证明了大规模的 CPU 集群也能跑的很好，面对 GPU 并不是没有一战之力（Skylake 这一代的 Xeon 是真的强啊）。这篇文章应该也是当时使用 Intel 的硬件达到的最好结果。 本文使用的名为 LARS 的优化算法在 AlexNet 和 ResNet-50 上都表现得非常好，目前大部分效果比较好的新网络都是类似 ResNet 这种特征（言下之意 LARS 同样能在那些网络上跑的很好）。然后轻点一下同行 Facebook 的工作，说 LARS 比他们的要好！ 嗯……Related Work 里面把前面两篇文章都提了一下，还是很有趣的。 第三章分析了一下大 Batch 训练的好处。理论上改变 BatchSize 是不会影响到一个 epoch 的总浮点计算量的，但是随着单个 step 的 BatchSize 的增长，一个 epoch 需要的迭代次数就减少了，整体通信的次数也减少了。 总之整体的运行时间可以减少，最理想情况当然是可以做到线性。表中给的总时间应该是以 GPU 数量取 log 来作为一次通信的 overhead 影响因子，因为这里考虑的是 Allreduce 同步的通信方式，log 级的估算基本上还是可以接受的。 话说我觉得这个问题还得换个角度更深入考虑一下，一个 epoch 中通信的次数减少了，但是如果增加节点数就意味着单次通信的总数据量要增加，最后还是反映在带宽上。 另一方面 BatchSize 变大之后单个 step 的计算时间也增大了，说起来通信这部分的 overhead 可能也更容易被 overlap 在计算时间之后。这里还只是一些比较模糊的定性讨论，数据并行能不能达到线性增长还是得看通信带来的 overhead 有多大和实际跑的结果。 另外一个原因是说大 Batch 可以提高 GPU 的利用率。 我觉得本来跑满一块 GPU 应该是基本吧。 其实像前面 Facebook 的工作，单块 P100 能够支持的 ResNet-50 的 BatchSize 应该远不止 32，不知道他们最后训的时候 P100 的使用率大概是个什么情况。当然他们也有实际收敛效果方面的考虑，以他们当时的优化策略还并不能够保证 BatchSize 再大以后的收敛性能。 在模型的选择上，他们提出了一个扩放率的概念，即计算和通信的比率，对比 AlexNet 和 ResNet-50 这两个网络： Model Communication# parameters Computiation# flops per image Comp/CommScaling Ratio AlexNet 61 Million 1.5 Billion 24.6 ResNet-50 25 Million 7.7 Billion 308 ResNet-50 的扩放率大概是 AlexNet 的 12.5 倍，所以 ResNet-50 的可扩放性更好，能达到更高的弱扩放效率。 但同时大 Batch 也会带来一定的问题，比如 BatchSize 不能无限往上加，大到一定程度之后，准确率就降下来了。所以后面的重点就在于怎么调整学习率这些超参数来保证增大 BatchSize 之后的训练效果（LARS 登场预告）。 基本的原则跟 Facebook 提出的观点一致： Linear Scaling：BatchSize 增大多少倍，学习率也要增大到相应的倍率； Warmup Scheme：初始的学习率不能太大，要在开始的几个 epoch 里逐渐增长到目标的学习率为好。 但是比 Facebook 的工作有了更深入的研究： 他们发现不同层参数和梯度的二范数比值（$\frac{||w||_2}{||\nabla w||_2}$）差距可能会很大，例如 AlexNet 中第一个卷积层和第一个全连接层的比值可能会差上千倍，猜测这个会成为影响大 BatchSize 训练精度的一个重要原因。 于是他们提出了 LARS（Layer-wise Adaptive Rate Scaling）算法来分别控制每一层的学习率。 666666 整体算法如下： 分别计算每一个参与训练的参数的学习率：$\alpha = l * \frac{||w||_2}{||\nabla w||_2+\beta||\nabla w||_2}$，这里 $l$ 是一个缩放系数，$\beta$ 用于控制权重衰减； 分别计算每一层的学习率：$\eta = \gamma * \alpha$，$\gamma$ 是一个用户控制的超参数，按照线性缩放设置就好了； 对梯度应用权值衰减：$\nabla w = \nabla w + \beta w$； 最后的更新量：$a = \mu a + \eta\nabla w$，$\mu$ 是扰动量 更新参数：$w = w - a$ 把一个算法扩大规模到更多的处理器上时，通常通信都会成为最大的 overhead。 所以这里从两个角度分析了通信比计算慢这一点，首先是硬件运算力（每个 flop 需要花费的时间，例如 P100 的峰值性能是 11TFlops 左右，$\frac{1}{11TFlops}$大约是$0.9*10^{-13}s$左右）会远小于理论传输速度（这里用的是$\frac{1}{Bandwidth}$，例如 56Gb/s 的 FDR，大约是$0.2*10^{-9}s$的水平），远小于延迟： $$Time-Per-Flop &lt;&lt; \frac{1}{Bandwidth}&lt;&lt;Latency$$ 去年的论文中还分析了在 45nm 工艺的 CMOS 处理器上，通信消耗的能量也会比计算更大： Operation Type Energy(pJ) 32-bit Int Add Computation 0.1 32-bit Float Add Computation 0.9 32-bit Int Multiply Computation 3.1 32-bit Float Multiply Computation 3.7 32-bit Register Access Communication 1.0 32-bit SRAM Access Communication 5.0 32-bit DRAM Access Communication 640 能量这个角度比较神奇，感觉应该体现的不是很明确，正式论文中已经删掉了这一段。 设总的 epoch 数为 E，图片数为 n，BatchSize 为 B，网络参数为 |W|。 当 E 和 n 保持不变的时候，总的计算量是固定的，总的迭代次数是 $E* \frac{n}{B}$，通信量是$|W|*E*\frac{n}{B}$。当 BatchSize 增大之后，总迭代次数减少，那么通信次数也减少了，通信量减少。 感觉这里再加一个 worker 数量等等这样的参数会更好一点。 更大的 BatchSize 有利于增大计算通信比，因此更大的 BatchSize 有利于网络向大规模进行扩展。 后面的测试都得到了非常好的结果。 2018 - Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes这篇是腾讯和香港浸会大学合作的成果，话说 HKBU 的这几个作者之前还发过一个叫 DLBench 的各个深度学习框架性能测试的文章，没想到又看到了他们的名字。 这篇文章主要做了三件事： 他们在训练的时候采用了一种混合精度的方法，增大吞吐量的同时不影响最终的收敛效果； 比前人的工作用了更大的 BatchSize，达到了 64k，保证最终收敛精度； 提出了一套高效的 Allreduce 算法。 最好的成果是 2048 块 P40 在 6.6 分钟把 ResNet-50 跑到了 75.8% 的准确率，以及 4 分钟跑完 95 个 epoch 的 AlexNet。 他们搭了一套名叫 Jizhi 的训练系统（机智？）： Input Pipeline 负责准备下一个 step 的训练数据，在上一个 step 的运算阶段准备完成以尽可能地减少 CPU 和 GPU 之间的数据传输延迟； 训练模块包括了模型的搭建和参数管理等等，他们对这部分增加了 fp16/fp32 混合精度训练以及 LARS 的支持； 通信模块包含了 Tensor 聚合和他们设计的混合 Allreduce 算法来提高整体的数据传输效率。 他们整体采用 fp16 来完成整个训练以提高整体运算的吞吐量，在额外应用 LARS 进行优化时遇到了半精范围溢出的问题。于是最终做了这么一件事： fp16 做完前向 把参数和梯度转换成 fp32 做 LARS 用梯度更新参数 把参数和梯度重新转换成 fp16 用 fp16 做完后向 这…… 使用这套方案之后，最终在 64k 的超大 BatchSize 下 ResNet-50 能够收敛到 76.2% 的准确率。 通信方面，首先是 Tensor 聚合，由于某些卷积层的参数量特别小，直接应用 Ring-Allreduce 等等算法去发送大量的小参数可能很难把带宽性能发挥出来，所以采用一个 buffer 池来收集完成前向运算的参数，当池中的参数量达到某个阈值时，再一次性发送出去。 我倒是比较好奇这块是怎么在 TensorFlow 中实现的。 根据参数量的大小，也分别采用不同的 Allreduce，并且在多级硬件框架中采用分级的 Allreduce 方法。 基本上就是这些。 2018 Sony - ImageNet/ResNet-50 Training in 224 Seconds再是索尼的这篇文章。 他们的成果是 2176 块 V100 在 224 秒中达到 75.03% 的准确率，以及 1088 块 V100 下达到 91.62% 的扩展效率。 为什么是这么奇怪的两个数字… 基本的思路还是跟前面的工作大同小异，他们采用了动态调整 Batch Size 配合 LARS 来作为超参数的优化策略，并且同样配合使用了 fp16/fp32 混合精度，并且在通信方面采用了 2D 环网的 Ring-Allreduce 模式（日本的研究者真是喜欢在网络拓扑上做文章，之前他们那台超算 京 也是在网络方面有独特的设计）。 2018 - Second-order Optimization Method for Large Mini-batch: Training ResNet50 on ImageNet in 35 EpochsNIPS 2017 上有一篇论文从数学层面分析过神经网络收敛方面的问题，得出的基本结论就是训练时间更久才能填上收敛的 gap（…嗯，虽然这个从经验上早就能得出来了）。前面的各位都是在考虑怎么把随机梯度下降的 BatchSize 提上去并且同时又把训练精度控制住，比如说各种花式处理学习率，花式改 BatchNorm，动态调整 BatchSize 等等。 这篇 11 月底的新工作则是换了个思路：大 Batch 带来的问题会不会就是 SGD 本身的限制，无论是 Momentum 还是 RMSprop 本质上都只是对 SGD 的优化修正算法，那如果干脆就不用 SGD 呢？ 于是这里改用了 K-FAC（Kronecker-factored approximate curvature） 这个二阶的优化方法，配合 Allreduce 和半精来做分布式训练，测试结果是要比一阶的 SGD 收敛更快，仅仅 35 个 epoch 就可以把准确率收敛到 75%。同时，使用这种方法在 1024 块 V100 上也能够在 10 分钟就把 ResNet-50 收敛到 74.9%。更进一步的优化策略（针对 K-FAC）还能再减少计算量和内存的占用量。相关工作中也说这里更多的是对 ICLR 2017 上的一个 K-FAC 工作的改进。 K-FAC 是利用各层参数得到的费希尔信息矩阵（Fisher Information Matrix）计算一个系数，再乘上参数的梯度来进行更新。数学部分先跳过了，大致的过程跟 SGD 差不太多，只是 Kronecker 系数的计算涉及到了更多的东西，相对 SGD 来说，这部分计算是额外的 overhead。 软件方面，他们的工作是在 Chainer 上完成的，这个框架平时完全没接触过，但是前面 2017 年那篇日本的工作用的也是这个，猜测应该是作者是日本的研究者或者在日本这个框架更流行吧。 以 3 层网络为例，基本的流程如下： Stage 1、2 分别是网络的前向和后向计算，之后的 3、4、5 是 Allreduce 计算 Kronecker 系数，最后 Stage 6 用每层计算出来的系数进行梯度更新。 其他诸如学习率的调整和预热的方法这里也有用。 ConclusionFacebook 的工作是我看到的最早在大规模训练实现这块引路的了，之后 UC Berkeley 提出的优化方案又把整个效果往前推进了一步，其他人后来的文章都是集合了当时各种高效方案的实现，最近看到的索尼的那篇应该算是这个方向的集大成了。 总结下来，基本的参考点大概有几个： 想办法增大计算吞吐量、减少通信的 overhead 使用同步而不是异步 全系统采用 Allreduce 并且针对逻辑拓扑结构设计最适合的 Allreduce 策略 fp16/fp32 混合 应用各种调超参的 trick 加速收敛 SGD 算法相关的各种改进和调整 扩放学习率到 k 倍或者动态调整 BatchSize 到 k 倍 动态调整学习率，LARS 分层分别使用不同的学习率 其他一些加快收敛的方法例如改进 BatchNorm 等等]]></content>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（九）：High Level APIs]]></title>
    <url>%2F2018%2F10%2F21%2F2018-10-21-tfunpacking9%2F</url>
    <content type="text"><![CDATA[前篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device TensorFlow 拆包（五）：Distributed TensorFlow 拆包（六）：RDMA TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning 这篇来研究一下 TF 中的一些高级 API。 TensorFlow 由于一直是在开源的社区环境中发展起来的，早期的一些 API 都比较简单粗暴（更直白地说就是不那么好用），以至于在它之上封装的更友好的 Keras 可能在大部分的使用者群体中会有更高的出现率。后来的 TensorFlow 中也有吸收 Keras 里面一些比较好的结构，有出现像 tf.layers 这样的更高层封装，可以期待一下 2.0 以后会不会大幅优化上层的编码 API 吧。 那这里说的高级 API 是什么呢？ 官网的 guide 里面列了几个： Keras：一个神奇的包 tf.keras，官方提供的从 TensorFlow 本身向 Keras API 兼容的实现（感觉怪怪的，底层库中包含了一个对高层封装的兼容？？？） Eager Execution：TensorFlow 的动态计算图实现，类似普通的 Numpy 或者 Pytorch 的执行模式 Importing Data：对输入数据的流水线封装 API Estimator：用于把封装好的模型方便地扩展到多卡/多机的高级 API 其他的还有包括像 StagingArea（构建软件流水，神器！！！）等等目前还在 tf.contrib 中处于实验阶段的很多东西，开源的力量太强大了，每隔一段时间就有很多新功能被社区添加进库中。 Estimator像 tf.keras 和 Estimator 的设计都是为了让用户能够更方便地编写网络，话说简单看了下 Estimator 的用法，API 的设计方式应该大概率是从 Keras 里面借鉴的。 具体的使用这里就不多记了，这里 写了个很小的例子，直接开始拆 Estimator 的实现吧。 核心是 tf.estimator.Estimator 这个类，先看初始化参数： 1234567__init__( model_fn, model_dir=None, config=None, params=None, warm_start_from=None) 第一个是用来构建网络的模型函数，具体后面详细分析；model_dir 用于指定保存的参数、checkpoint、log 信息等等存放的目录；再后面的几个都是一些额外的配置选项。 让我觉得非常怪的是官网的介绍页面说 Estimator 的优势是不需要用户建图……我真是一脸懵逼。或许对于 TF 内置的一些事先建好的 Estimator 是这样吧，但是如果想自定义呢？……写文档的人吹的有点过了吧。 创建 Estimator 时，首先初始化各项配置参数信息（值得一提的是 model_dir 是允许被 config 中的选项覆盖的），设置训练或者验证时用的数据分布策略（DistributionStrategy，后面再详细分析），设置参数和图节点分布的 device_fn；之后简单检查 model_fn 的参数是否符合规范，然后完处理完 warm_start 的一些设置就结束了。 传入的 model_fn 是用于构建 Estimator 代表的模型网络的核心函数，它能够接受的参数名有严格的规定： features：网络的输入数据，即一个 batch 的数据； labels：网络的标签数据，即一个 batch 的目标标签； mode：可选，但是一般都必须要有，要不实现起来会很麻烦。这个值会根据执行的模式由 Estimator 传入，会有 3 种，tf.estimator.ModeKeys.PREDICT、tf.estimator.ModeKeys.TRAIN 和 tf.estimator.ModeKeys.EVALUATE； params：可选，对应的是 Estimator 的初始化参数； config：可选，对应的是 Estimator 的初始化参数 Run the Estimator接下来是 Estimator 类的三个调用方法 evaluate、predict 和 train，从字面上就能够看出来各自对应的是什么功能了（Keras 里面对应的 API 应该是 evaluate、predict 和 fit）。 1234567891011121314151617181920212223evaluate( input_fn, steps=None, hooks=None, checkpoint_path=None, name=None)predict( input_fn, predict_keys=None, hooks=None, checkpoint_path=None, yield_single_examples=True)train( input_fn, hooks=None, steps=None, max_steps=None, saving_listeners=None) 三个方法的共同参数是这个 input_fn，这是类似前面 model_fn 一样，也需要 Estimator 的创建者写好的输出数据产生函数。这个函数的返回值是一个二元组 (features, labels) 对应了 model_fn 的前两个输入参数。 train 中的 steps 表示从哪里开始训练，Estimator 将首先从保存的 checkpoint 中找到最接近的保存点，然后开始这次的训练，max_steps 则简单地就是训练的 batch 数了。 12345def _train_model(self, input_fn, hooks, saving_listeners): if self._train_distribution: return self._train_model_distributed(input_fn, hooks, saving_listeners) else: return self._train_model_default(input_fn, hooks, saving_listeners) 如果在初始化时没有配置 _train_distribution 项，则会使用默认的方式来执行 train 操作，最终把 model_fn 也绑定出来： 12estimator_spec = self._call_model_fn( features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) 向 model_fn 中传入输入数据以及 ModeKeys.TRAIN，接下来实际的执行函数是： 12def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners) 添加 TensorBoard 中的 Summary、创建参数保存点、如果有 saving_listeners 则额外添加到运行的 hooks 中，之后： 123456789101112131415with training.MonitoredTrainingSession( master=self._config.master, is_chief=self._config.is_chief, checkpoint_dir=self._model_dir, scaffold=estimator_spec.scaffold, hooks=worker_hooks, chief_only_hooks=( tuple(chief_hooks) + tuple(estimator_spec.training_chief_hooks)), save_checkpoint_secs=0, # Saving is handled by a hook. save_summaries_steps=self._config.save_summary_steps, config=self._session_config, log_step_count_steps=self._config.log_step_count_steps) as mon_sess: loss = None while not mon_sess.should_stop(): _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss]) 嗯……这段代码是不是很熟悉，没错，官方建议的常规 TensorFlow 训练代码就是要写成这个格式。 至此，train 部分基本上分析完了（带 DistributionStrategy 的版本后面再说），整个过程就是把一套常规的 TensorFlow 代码的各个部分做了几级封装，要说有什么特别的就是它把 Summary 和 Saver 都默认包含在内了。 如果按照这个格式解开成普通的 TensorFlow 代码的话，可以说是非常好的官方范例了。 EstimatorSpec然后再注意到 model_fn 的返回值，前面也提到了 evaluate、predict 和 train 这三个实际执行的方法其实最终都是把 input_fn 中产生的数据传给 model_fn 来跑，这里的控制差别就需要配合对不同的 mode 选项的分支判断来做，所以一个 model_fn 函数写出来大概是这个样子的： 123456789101112131415def model_fn(features, labels, mode): xxxxxx if (mode == tf.estimator.ModeKeys.PREDICT): ... return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) if (mode == tf.estimator.ModeKeys.EVAL): ... return tf.estimator.EstimatorSpec(mode=mode, loss=cross_entropy, eval_metric_ops=eval_metric_ops) if (mode == tf.estimator.ModeKeys.TRAIN): ... return tf.estimator.EstimatorSpec(mode=mode, loss=cross_entropy, train_op=train_step, eval_metric_ops=eval_metric_ops) 不管是哪种模式，model_fn 最终的返回值都需要通过 EstimatorSpec 这个结构来传出去，其属性有： 123456789101112131415@staticmethod__new__( cls, mode, predictions=None, loss=None, train_op=None, eval_metric_ops=None, export_outputs=None, training_chief_hooks=None, training_hooks=None, scaffold=None, evaluation_hooks=None, prediction_hooks=None) mode：对应三种不同的模式标识； predictions：预测结果，要是一个 Tensor 或者 Tensor 组成的 dict； loss：训练的损失函数值，必须是一个标量或者形状为 [1] 的 Tensor； train_op：训练 step 的 op，一般是某个 Optimizer 的 minimize() 方法返回的那个； eval_metric_ops：一个包含了验证结果的 dict，可以是 Metric 类，或者一个 (metric_tensor, update_op) 的元组； 其他…略了 要求 ModeKeys.TRAIN 模式返回的必须包含 loss 和 train_op，ModeKeys.EVAL 模式返回的必须包含 loss，ModeKeys.PREDICT 模式返回的必须包含 predictions。 DistributionStrategy前面说了，Estimator 这套 API 的出现是因为开发者希望能够方便用户快速搭网络，并且易于扩展到各种不同的计算结构上。 那么 Estimator 本体上面已经拆过了，没什么神秘的，就是个简单封装，距离实现单机到多卡/多机的这种扩展其实还差挺多的，而 DistributionStrategy 就是用来补上中间那个 Gap 的。 官方提供的资料是个 Github 上的 README：Distribution Strategy 里面给的使用的例子都非常简明易懂： 123456distribution = tf.contrib.distribute.MirroredStrategy()config = tf.estimator.RunConfig(train_distribute=distribution)classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)classifier.train(input_fn=input_fn)classifier.evaluate(input_fn=input_fn) 下面的三行是个普通的 Estimator 的使用，跟前面一样，唯一区别的就是在 tf.estimator.RunConfig 中创建一个 DistributionStrategy，然后作为 config 选项传递给 Estimator 即可。 确实很方便啊。 如果有看过官方的 benchmarks中对多卡/多机的写法的话，可以发现那个写法大致上跟 DistributionStrategy 的设计非常像。 _train_model_distributed 的大致结构是这个样子的： 1234567891011121314151617with self._train_distribution.scope(): ... features, labels = estimator_util.parse_iterator_result( iterator.get_next()) grouped_estimator_spec = self._train_distribution.call_for_each_tower( self._call_model_fn, features, labels, # although this will be None it seems model_fn_lib.ModeKeys.TRAIN, self.config) loss = self._train_distribution.unwrap( self._train_distribution.reduce( distribute_lib.get_loss_reduction(), grouped_estimator_spec.loss, destinations='/device:CPU:0'))[0] distributed_train_op = grouped_estimator_spec.train_op ... _train_distribution.scope 中封装的是一些 variable_scope 和 custom_getter，用于在用tf.get_variable() 创建的变量之上再套上一层额外的封装控制。 对普通的 tf.get_variable() 套上 variable_scope 之后可以控制这个变量创建的设备位置等等很多东西，第一次看到这种写法的时候还觉得这像是一种 hack 的方法，但是不用改变原本里面的代码，还是非常方便的。 以单机多卡为例，call_for_each_tower 是在每块 GPU 卡上跑一遍 Estimator 中给定的 model_fn，即在每一块卡上独立创建一份数据并行的网络。基类里面这个函数是留空的，要求具体的实现类来完成这个部分。 话说这里我有个疑问……model_fn 中的正常写法应该是对训练模式返回一个包含了 Optimizer.minimize() 的 EstimatorSpec，但是多卡并行的过程中不是应该需要做梯度的聚合平均之后再更新到每个变量上吗？而且不同的并行模式下，这部分的处理方式应该也是不一样的，不知道这套 API 要怎么把这些全都统一起来。 看一下 MirroredStrategy 的这个实例里面是怎么实现这个函数的吧，中间部分的代码是这样的： 12345678910111213# TODO(isaprykin): Create these threads once instead of during every run()# call.threads = []for index, d in enumerate(distribution.worker_devices): variable_creator_fn = shared_variable_creator.make_fn( shared_variable_store, index) t = MirroredStrategy._MirroredTowerThread( # pylint: disable=protected-access distribution, coord, d, variable_creator_fn, fn, *values.select_device(d, args), **values.select_device(d, kwargs)) threads.append(t)for t in threads: t.start() distribution.worker_devices 中保存了单机中的多块 GPU 卡设备，这里直接对每块卡挂一个 shared_variable_creator 并且开一些的一对一的线程去处理。 shared_variable_creator 用于处理多卡之间的参数共享，在 device_id 为 0 的设备上调用 get_variable() 函数是创建新变量，并且保存到给定的 shared_variable_store dict 中；在 device_id 大于 0 的设备上调用 get_variable() 则会尝试共用前面创建好的变量。 接下来看一下创建线程这部分的逻辑。 主线程和多个子线程之间的控制这里用了 should_run 和 has_paused 两个 threading.Event() 来控制。开始的时候，每个线程都调用 should_run.wait() 来等待，等待主线程调用对应的 should_run.set() 来唤醒它们。主线程随后阻塞在 has_paused.wait() 上，等到每个线程完成自己那部分图的构建之后再用 has_paused.set() 唤醒。 话说为啥一定要用多线程来实现这个部分呢……感觉就用普通的单线程循环一样可以做到这里想要的事情。 那么对梯度的聚合和最终的 apply 呢？似乎这部分代码里面根本没看到啊，每个线程的 run() 函数基本上就是跑完各自的网络部分就没了。唯一看上去非常让人介意的是 run() 中在执行 main_fn() 前套了一堆 Python 的 Context，难道又是用有点 hack 的方法完成的？ 其中 MirroredTowerContext 这个结构继承了 distribute_lib.TowerContext，只用于call_for_each_tower 中用于处理多块卡之间相同代表数据的同步。 问题是我还是没有看到处理 reduce 等等这些的代码。 然后……抱着一种怀疑的想法，我重新打开了 Optimizer 中关于梯度计算部分的代码！！发现这里已经跟当时拆包第二篇（Optimizer in TF）里看到的不一样了。 例如新的 apply_gradients 中增加了这样的一段： 1234if distribution_strategy_context.has_distribution_strategy(): grads_and_vars = get_filtered_grad_fn(lambda: grads_and_vars)() return distribution_strategy_context.get_tower_context().merge_call( self._distributed_apply, grads_and_vars, global_step, name) 那么 DistributionStrategy 是如何配合 Estimator 把原本单机的代码直接扩展开来就很明白了。 当时最早拆包开始时大概是 TensorFlow 的 1.6 版左右。 查了下 Optimizer 中增加部分的 git 记录，差不多是在今年 3 月底的时候加上的，应该是在 TensorFlow 的 1.7 版左右，然后后来又有过一次较大的改动。 Design Philosophy再看一下官方文档中对 DistributionStrategy 的设计思想。 首先是一些底层的概念： Wrapped values：跨设备相关的变量可以被封装为两种类别，PerDevice 对象表示的变量在每个设备上的值不同，Mirrored 对象表示的变量在每个设备上的值都相同 Unwrapping and merging：考虑前面提过的这个函数 call_for_each_tower(fn, w)，fn 是模型函数，w 代表一些 Wrapped values。这个函数的调用过程中就包含了变量的 unwrapping 和 merging，假定在设备 d0 上 fn(w0) 得到的结果是 (x, a, v0)，在设备 d1 上 fn(w1) 得到的结果是 (x, b, v1)。首先在调用函数之前，w 需要被解包变成 w0 和 w1 然后分别调用 fn 函数。返回的结果有三种情况，第一个值都返回了一个相同的对象 x，则最终 merge 之后还是对象 x；第二个值是每个设备不一样的，则 merge 之后是一个 PerDevice 对象（其实就是个设备和对应值的 map）；第三个值是每个设备返回的分别是一组 Mirrored 对象的成员，则 merge 之后是一个 Mirrored 对象。所以 call_for_each_tower(fn, w) 在这里返回得到的就是一组 (x, PerDevice{...}, Mirrored{...}) Tower context vs. Cross-tower context：Tower context 指的是对每个设备的封装上下文，通常对每个设备分别跑一遍模型函数就需要这种封装；Cross-tower context 指的是跨设备的封装上下文，比如说像 reduce() 这种所有设备共同参与的一个操作就需要这种封装 Worker devices vs. parameter devices：负责计算的设备和存参数的设备，没啥好说的。 更新一个变量的常规操作如下： 把输入数据集封装在 d.distribute_dataset() 中，然后创建一个 iterator 对每一个设备共同调用 d.call_for_each_tower() 来分别创建网络模型，并且最终各自得到一组梯度/变量对：d0 上有 {(g0, v0), (g1, v1), ...}，d1 上有 {(g&#39;0, v0), (g&#39;1, v1), ...} 等等这样 调用 d.reduce(VariableAggregation.SUM, t, v) 或者 d.batch_reduce() 来对梯度求和，并且对应到各自的变量上：{(Sum(g0, g&#39;0), v0), (Sum(g1, g&#39;1), v1), ...} 调用 d.update(v) 来对每一个变量进行更新 3、4 两步如果用 Optimizer 中的 apply_gradients() 方法可以自动完成（……这就是 Optimizer 后来加进去那部分代码的作用），或者在一个 Cross-tower context 中调用 _distributed_apply() 方法也可以。常规的网络层都应该在 Tower context 中被调用。 话说这个 _distributed_apply() 为什么前面带下划线啊喂，这个方法本来不打算直接给人调的吧？？？大概是 API 还没最终设计好。 嗯，所以 Estimator 本身一点都不神奇，真正这套机制麻烦的地方在 DistributionStrategy 里面，手写一个 DistributionStrategy 应该会是一件很麻烦的事情。 不知道未来这套机制会如何改进得更好用一些。 2018 12月更新 最近正在试图手动 DIY 一个 DistributionStrategy，发现到处都找不到相关的资料，官方的文档方面对这部分也是写的不明不白。 试了下自己继承一个 DistributionStrategy 类，但是发现这个基类的几乎所有功能都是交给另外一个 DistributionStrategyExtend 类来做的，而且直接从空类开始写缺的东西也有点太多了，还没下手成功。准备之后再试试直接继承一个现有的类比如 MirroredStrategy 然后重载掉里面的功能函数试试看。 Estimator 这套机制想要达到的目标是非常好的，但是似乎……由于 TensorFlow 本身过于庞大和复杂，不知道什么时候这两个东西才能真正成为方便用户使用的好接口。 StagingArea从名字上直译过来应该是用于暂存的区域，这套 API 用于跨 step 地把数据保存到网络 data path 之外的地方，然后可以在另外的 step 中把保存下来的数据取出来。 解释上看起来挺绕的，而实际上用这套 API 实现出来的效果就是——软件流水。 例如以下面这个由三个阶段组成的计算过程为例： 在 a/b 和 b/c 之间分别加入 StagingArea 即： 更重要的是加入了暂存结构之后，事实上 a、b、c 三个计算阶段的依赖就被解耦了： 对执行的流程稍微进行一些修改： 12345step1: A1step2: A2 B1step3: A3 B2 C1step4: A4 B3 C2... 原本必须按顺序执行的三个计算阶段现在就可以互不相关地并行执行了，在某些计算与 I/O、数据通信共同存在的环境中，原本可能存在的数据延迟、等待等等就有可能通过流水线的方式隐藏掉！（例如多机分布式训练的情况，实测效果非常好） 关于这套 API 如何使用的介绍这里就不多记了，直接来看 TensorFlow 是怎么实现它的。 ImplementationStagingArea 这个类在 tensorflow/python/ops/data_flow_ops.py 中，很早以前应该是在 tf.contrib 里面的，大概试验成熟之后移到正式的包部分了。 put() 和 get() 这两个方法的实现分别调用了 gen_data_flow_ops.stage() 和 gen_data_flow_ops.unstage()，然后你会发现虽然一开始是从 from tensorflow.python.ops import gen_data_flow_ops 中引入了这个包，但是源代码里面是找不到这个包的。 原因在于这里面的东西都是在 C++ 层代码中定义然后在编译过程中生成的，追到 tensorflow/core/ops/data_flow_ops.cc 中可以看到大量用 REGISTER_OP 宏注册的 op，其中就有 StagingArea 用到的 stage()、unstage() 等等函数。 当然到这里为止还是没办法找到它的实现，因为 REGISTER_OP 宏只是负责 Python 与 C++ 的接口部分的处理，具体 C++ 层调用的实际内容还要再往 Kernel 里面找：tensorflow/core/kernels/stage_op.cc 。这里才是真正最底层的实现内容了，然后还能看到很多 REGISTER_KERNEL_BUILDER 宏，用于把 C++ 部分编译成的库与上面的接口绑定起来。 在拆包第三篇简单记过 TensorFlow 中 Op 的创建方式，嗯，在这里用上了。 然后就发现这玩意的实现就是个双向队列的封装，没啥神奇的……╮(╯_╰)╭ 1234567891011121314std::deque&lt;Tuple&gt; buf_;void Get(Tuple* tuple) &#123; ... *tuple = std::move(buf_.front()); buf_.pop_front(); ...&#125;Status Put(Tuple* tuple) &#123; ... buf_.push_back(std::move(*tuple)); ...&#125;]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Estimator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSE 599W： Systems for ML]]></title>
    <url>%2F2018%2F10%2F04%2F2018-10-04-cse559w%2F</url>
    <content type="text"><![CDATA[国庆在家闲不住想干活系列…… 本篇的内容是陈天奇大佬今年春季在华盛顿大学开的一门课。 大佬是上交 ACM 班的本硕，虽然目前还在 UW 读博中，但是在机器学习圈子里面已经有了很高的名望了，他的 MXNet 和 XGBoost 相信很多人就算没用过也肯定听说过（比如我就没用过…）。前段时间他发布的 TVM 也算是开启了深度学习和系统方面探索的一条新道路。 课程介绍上讲的是这门课的目标是填上深度学习算法和系统的实现以及优化之间的 gap，粗略地翻了一下 PPT，后面也有比较多的篇幅是介绍 TVM 的，正是我想了解的！ 没找到课程的视频，但是 PPT 可以在上面的课程链接或者这里找到。 下面的内容主要按照每篇 PPT 的整理： Lecture 1: Introduction to Deep Learning回顾了一下基本原理和发展历史。 机器学习的过程基本上就是 数学模型+评价指标+参数训练，深度学习则是模型特指各种神经网络。 具体主要涉及到各种不同的模型架构（CNN、RNN、各种变种），目标函数的选择和训练技巧，正则化初始化等等。 这些就不多记了。 Lecture 2 是个实验课，实践怎么搭网络。 Lecture 3: Overview of Deep Learning System这一节差不多是大纲的性质，每一个小点后面都会分节细讲。 基本上所有的深度学习框架都是差不多这个结构，首先来看 User API 层： 这里举了个线性回归的例子来对比手写 Numpy 和框架代码的差别。基本上网络模型都可以比较方便地用一个计算图的结构来表达，节点表示运算操作，边代表数据依赖。 那为了方便用户写代码，一个框架也是一定要有自动求导的功能的（如果反向计算还需要手写那就太瓜皮了）。 然后是 System Components 层： 这里涉及到了首先是计算图的优化，比如一次运行的时候直接过滤掉用不到的图节点（Deadcode Elimination），内存分配方面的优化，图节点和实际计算设备的对应等等。 实际跑图的时候如果有多个设备或者多个工作线程，如何调度以及发挥出计算设备的并行性也是一个需要考虑的问题。 最下面的 Architecture 层： 目前用来支持 DL 的设备也有很多，典型的如 GPU，其他的加速芯片也是越来越多，不同的设备可能要写对应不同的代码，这部分要怎么优化？ 现在最常规的做法是每一种不同的计算设备会有开发厂商自己提供支持库，但是这个对框架的开发者来说还是有一个要整合的过程。另外，如果系统中存在多种不同的计算设备，计算任务在多设备上要怎么分配和调度也会是一个很大的麻烦。 为了解决最后的这个问题，目前有一种 Compiler Based Approach，即整个 Architecture 层由一个 High Level Operator Description 加上 Tensor Compiler Stack 来统一解决。这就是之后要提到的 TVM 的设计思路了。 Lecture 4: Backpropagation and Automatic Differentiation详细解释第三节中的自动求导。 计算机中实现求导这个操作主要有两种方式：基于符号的算术求导和直接用数值进行求导。 算术求导需要构建一棵符号表示树，然后根据各种算术上的求导规则来写公式。缺点在于：如果遇到特别复杂的函数，则需要推导的符号表示树也会很大；然后如果目标只是想要一个导数值，则保存一棵符号表示树就很浪费了；再然后就是这样做容易有误差（？为什么…按公式算不应该误差更小吗）。 数值求导则是按导数的定义做，直接对方程取极限： $$ \frac{\partial f(x)}{\partial x_i} \approx \lim \limits_{h \to 0} \frac{f(x+he_i)-f(x)}{h}$$ 实现起来特别简单，h 取个 1e-6 就差不多了，但是一般只用来检验求导结果用。 然后对于网络中每一层的反向部分，其实求导涉及到的都只是跟本层运算相关的内容： 上一层传下来的是 $\frac{\partial Error}{\partial z}$，再往下可以通过链式求导法则一直推导下去，而其他需要的则只是与本层运算有关的$\frac{\partial z}{\partial x}$ 和 $\frac{\partial z}{\partial y}$。 更详细的推导可见这里。 因此自动求导则是根据以上的规则来创建反向计算图的过程，伪代码以及结果如下： 自动求导构建完成反向计算图之后，完整的计算图可以接下来一起用作整体的图优化。 Lecture 5: GPU Programming在 CPU 上进行数据运算大致有几个过程（按多级流水分）：Fetch、Decode、ALU Compute、Write Back。由于 CPU 本来也并不是为了纯运算而设计，因而在 ALU Compute 以外的其他部分会有比较大的计算资源和能耗上的 overhead。 后来增加的向量化指令能够相当程度地改善这种 overhead 的问题，而 GPU 从这个角度来看更像是一种把 ALU 的向量化做的更极致的加速器。 从存储的层次结构上来对比： GPU 的大量寄存器就使得它能够以比 CPU 小的多的开销来切换线程，这也就能够支撑起大规模的 SIMT 了。 后面是一些 CUDA 编程的例子，以及如何根据 GPU 的微架构特性高效地发挥出性能来。 Lecture 6: Optimize for Hardware Backends这一节的内容大概在 System Components 和 Architecture 层之间，一份代码面对不同规模的数据（甚至是不同数据块尺寸的数据）往往不作针对性地调整是达不到最佳性能的。 深入下去需要实际考虑到例如 CPU 的 Cache、GPU 的寄存器等等这些方面，以及 GPU 的多级存储之间的数据搬移开销、数据重用等等，同样是 GPU 也有多种不同的后端。 不同的 Tiling Patterns、Fuse Patterns、Data Layout、Hardware Backend 合起来使得优化工作也变得相当复杂了。 为了解决前面说到的所有这些麻烦的问题，然后这里就引出了 TVM Stack。 Lecture 7: Automatic Code Generation - TVM Stack各种不同的框架和实际执行运算的各种各样的硬件后端之间其实存在着很大的 gap。 如果从编译器的视角来看待如何解决这个问题，各种框架写的网络可以根据特定的规则转化成某种统一的表示形式，在统一表示的基础上进行一些可重用的图优化，之后再用不同的后端来生成对应不同设备的代码，这就是目前各家都在尝试的设计思路了。 举例说：TensorFlow 的 XLA 会把高级代码抽象成 XLA HLO 的表示，做目标无关优化之后再用对应后端来生成更深一层的代码。 NVIDIA 的 TensorRT 的优化策略也是在图转化之后的统一表示上做，例如根据设定好的规则来做一些相邻计算单元的合并（Kernel Fusion）等等。 当然这种方式实现的时候会遇到一些同样非常麻烦的问题，一个 operator 需要针对不同的硬件平台、数据格式、精度、线程结构写一堆代码生成规则和优化规则。 到头来是把原本 op 实现的复杂度变成了编译规则的复杂度，绕了个圈以后好像还是很麻烦啊。 TVM 借助了一种叫 Tensor Expression Language 的表示方法，同样采用这种类似表示的还有 Halide（一种图像处理语言）、Loopy（基于 Python 的 kernel 生成器）、TACO（稀疏 Tensor 代码生成器）、Tensor Comprehension（类似 TVM）等等。 这种表示法最初的想法来源于 Halide，核心在于把代码的计算和调度分开。 例如一段最原始的 TVM 代码： 12C = tvm.compute((n,), lambda i: A[i] + B[i])s = tvm.create_schedule(C.op) 生成得到的 C 代码是： 1234for (int i = 0; i &lt; n; ++i)&#123; C[i] = A[i] + B[i];&#125; 加上额外的调度控制： 123C = tvm.compute((n,), lambda i: A[i] + B[i])s = tvm.create_schedule(C.op)xo, xi = s[C].split(s[C].axis[0], factor=32) 再生成的代码就变成了： 1234567891011for (int xo = 0; xo &lt; ceil(n / 32); ++xo)&#123; for (int xi = 0; xi &lt; 32; ++xi) &#123; int i = xo * 32 + xi; if (i &lt; n) &#123; C[i] = A[i] + B[i]; &#125; &#125;&#125; 甚至于还可以支持绑定中间的 xo 和 xi 到特定的变量上： 123456C = tvm.compute((n,), lambda i: A[i] + B[i])s = tvm.create_schedule(C.op)xo, xi = s[C].split(s[C].axis[0], factor=32)s[C].recorder(xi, xo)s[C].bind(xo, tvm.thread_axis(“blockIdx.x”)s[C].bind(xi, tvm.thread_axis(“threadIdx.x”) 话说这样出来的代码就可以用在 CUDA kernel 里面了： 12345int i = threadIdx.x * 32 + blockIdx.x;if (i &lt; n)&#123; C[i] = A[i] + B[i];&#125; 具体后续的调度部分的设计，首先需要保证生成的代码在逻辑上要能跑出正确的结果，常见的手工优化代码的方法也都要包含在内，并且要能够方便引入其他额外的新技术。 目前 TVM 的调度部分还在继续开发中，已经从像 Halide、Loopy 这种成熟的语言中吸取过来的方法有例如 Loop Transformations、Thread Bindings、Cache Locality 等，针对 GPU 还开发了一些方法例如 Thread Cooperation、Tensorization、Latency Hiding 等。 再额外的就是 TVM 还用了 Auto-tuning，由于 TVM 的论文还没看，不确定我的理解对不对。Schedule Space 模型的自动调优就是尝试不同的优化方法组合，然后在整个策略空间里面搜索哪一种优化效果最好最终就采用哪一种吗？ 末尾给的一些测试中，TVM 表现出了相当不错的性能结果。 当然，TVM 还刚刚开始发展，后面还有一大堆问题留待解决。 Lecture 8: Hardware Specialization in Deep Learning上一节的 TVM 是一个纯软件栈，这一节就来探索一下用于深度学习的专用硬件。 DL 的疯狂发展对计算硬件也有了越来越高的需求，而且不同应用场景的需求还可能会差很多，例如数据中心和移动终端上面的 AI 设备就完全要往两个极端去考虑。 上面这张图讲的是在 DL 的发展过程中，对数据的尺寸以及存储精度的需求也在不断变化，低精度可以节省空间以及加速运算，但是这也要在硬件本身可以直接支持低精运算的前提下才能有效果（硬件是64位双精的，你要那它跑 int8？那就呵呵了…emm，此处并不是针对某 sw 哈哈）。另外，一些出现的新算法是不是能够用硬件高效实现也很关键，实现不了的话可能还是要选择老算法更好。 不断发展的 DL 算法在实验室里面可以任意瞎搞，效果好就好了，但是如果要应用到实际的生产环境中，那能不能实现/怎么高效实现就非常重要了。 再再另一方面，摩尔定律也逐渐受限，更低纳米制程的工艺难度越来越大，所有这些问题最后都会导向一个终极的解决方案，那就是 DL 专用的计算芯片/硬件了。 下面用 TPU 来举了个栗子：2015 年流片的 ASIC，92 TOPS 的峰值性能，相比 K80 有 30~80倍的性能功耗比。这些数据看着都吓人。 那为什么这么强呢？原因在于它直接硬件支持 8 位的整数 Inference（相比 16 位半精要节能 6~30 倍），大量的乘加运算部件（MACs）以及大量的片上存储。 TPU 主要的峰值运算性能都来自于右边的一大堆矩阵乘单元和累加器。光片上的 Unified Buffer 和 MMU 就占到了整块 TPU 超过 50% 的芯片面积。 这种设计也在于尽可能地提高数据的重用程度，提高计算密集度。 总结一下，像 TPU 这样的 DL 专用的加速器相对 CPU 和 GPU 主要有三个方面的特点： 通常需要显示管理片内的存储子系统，而 CPU 的 Cache 是隐式的，对程序员透明，GPU 则是可以有自己的 L1 Cache 同时也可以手动维护（Shared Memory）； 计算主要以 Tensor （矩阵或者向量）为单元，CPU 主要就是标量运算了，配合 SIMD 的话则跟 GPU 一样主要是向量计算； 如果为 Inference 设计则不需要太高的精度，低比特量化之后可能更适合。 下面是举了 3 种 Hardware/Software Co-design 的方法： Tensor 化（Tensorization）：把矩阵-向量运算变成矩阵矩阵运算，4x8 的矩阵乘上 8x1 的向量变成两个 4x4 和 4x1 的乘积（…提高计算密集度？） 存储结构针对计算内容做优化：面向卷积优化则需要一个较大的激活函数 Buffer（空间重用），较小的 FIFO 队列（存储参数）；面向 GEMM 优化则需要分配较多的空间用以累加器的块存储。 低比特量化：可以线性提高存储带宽。 。。。上面的这三个感觉理解的比较模糊。 再往下才是本节的重点内容——VTA。 TVM 构建的是软件栈，硬件加速器方面，他们也提出了一套开源的 FPGA 加速器设计方案，即 VTA（Versatile Tensor Accelerator）。 VTA 针对不同的带宽、存储和精度需求可以自定义 Tensor Core 的形状、数据类型、内存子系统分配、支持的运算操作等等；对不同类型的代码提供 CISC 或者 RISC 的指令集支持；并且还做了一些 Latency Hiding 的工作。 大致的框架设计如下： IF 单元从 DRAM 中获取到下一条指令后，会根据类型将其发送到目标部件对应的队列中。 Load 单元负责准备激活函数和计算核的存储资源、提供Micro-Op 的缓存，取出来的数据放在 Load Buffer 中。 计算单元负责根据前面的 Micro-Op 以及准备好的数据执行 ALU 运算或者 GEMM 运算，更新寄存器的内容。 Store 单元负责把前面 Store Buffer 中的寄存器值写回 DRAM。 整体的运行依靠多个任务队列来维护数据依赖关系，基本上是个数据流的设计。（。。。） VTA 的控制代码部分则依靠 TVM 来生成。 加上 VTA 之后，整个 TVM 的完整架构显得更复杂了： 顶层是各种成熟的深度学习框架，TVM 充当编译器的角色，底层的硬件执行部分则由 VTA 来实现。 Lecture 9: Memory Optimization还是在 System Components 这层，继续深入分析 DL 训练过程中存在的问题。 回到前面的自动求导部分，这里抛出来一个问题是为什么自动求导是采用往计算图中扩展反向计算的数据计算通路的方式，而不是直接在原来的图上进行反向计算（Backprop in Graph）？ 这个 Backprop in Graph 这里也没有更详细的说明，我大概理解成递归返回的那种样子，正向计算是不断递归向下，然后每层递归退出的时候执行反向，完美！ 其实说起来，本质上递归的这个顺序也就是数据依赖关系的拓扑序。 原因呢，则是在于内存上。 State-of-art 的很多模型都可能会有资源受限的问题，现在确实很多效果好的新模型都越来越大了，一方面计算量在增长，另一方面内存会成为一个很大的麻烦：CPU 的内存还好一点，如果是用在 GPU 上，目前单块卡的显存最多也只有几十 G 的量级。 先来看一下前向部分的内存使用情况，以下面这几个简单的运算组成的计算图为例： 朴素做法是为每个计算节点都开一块内存，则图越大、计算节点越多，需要的内存量越大。 然后我们发现这个过程中有很多内存是用过之后就不会再用了的，因此更高级一些的方案是配合内存池进行动态内存分配，例如上面 mul 算完之后所占有的内存就不再需要了，因此这块内存可以被内存池回收，然后用在 exp 的计算上。 再有另外一种方式则是静态内存规划，即拿到计算图之后，就按照尽可能重用内存的方案事先分配内存，基本上达到的效果应该要跟上一种内存池的动态分配方案差不多。这种做法有点类似编译器的寄存器重命名的过程。 基本的分配原则也非常简单，只是应用的时候另外要注意如果分配不好是有可能要影响计算的并发性的： Inplace：可行的情况下尽可能地原地存储，即把输出存到输入的内存里（前提是这份输入只被一个计算节点依赖） Reuse：不再用到的内存尽可能地重用。 下面的几个内存规划的例子也都比较简单：根据拓扑顺序依次分配和回收；或者先从起点到终点找一条最长路径，把路径上的内存全部设为 Inplace 重用，然后再找别的路径等等。 这里举的内存重用的例子都特别简单，实际上每个计算节点需要的数据尺寸和内存大小都不一定一样，不可能这么简单地就分配好了。 回到前面那个自动求导的两种方案的问题，可以很容易地体会出来往计算图中扩展反向通路的方案非常容易做内存优化： 只需要把 Inplace 和 Reuse 用好即可，在 MXNet 上测试出来的效果也非常好。 深度学习的 BP 算法中存在的最大问题在于反向运算时需要用到前向的一些结果，这事实上就大大地限制了 Reuse 策略的发挥，因为前向算出来的结果总需要找个地方暂存着。 针对一些内存需求特别大的场景，可以采用计算换空间的折中方案： 只保存前向结果的一部分，当反向运算中需要时，再重新从前面开始把缺失的部分重新算一遍，用 25% 的额外计算量可以把整体的内存使用降到原来的开方级别，在某些场景下还是有非常不错的收益的。 实验室的一位师姐之前在 RNN 的优化里面用到过这种方法，节省下内存之后可以跑更大的 BatchSize，最后得到的效果非常好。 再回到前面两种反向方案的讨论中，这一节的最后给了一个特别有趣的点：内存重用的优化方案从某种程度上来看有点像递归中的内存分布！！ 666666！ Lecture 10: Parallel SchedulingSystem Components 这层的另外一个方面是并行调度的问题。 用户写好一个计算图，如果框架没有能力把机器上所有的硬件资源全都调动起来那就太浪费了。 关于 DL 中模型并行数据并行这块就不再多提了，在常规的数据并行中，计算和通信之间存在着一个 Gap： 这样一张有着复杂的计算、通信需求的计算图要如何才能比较好地并行起来呢，答案就是我们需要一个自动的调度系统。 首先计算图本身可以很好地描述计算之间的数据依赖关系，那在这个基础上的 Scheduler 设计感觉其实也没啥好说的，基本上都是很常规能够想到的解决方案。TensorFlow、MXNet 等等的基础设计原则都是这样，只是实现上可能有所不同。 同样是队列的调度方案，这里的这种是以每个变量为单位有一个自己的队列，TensorFlow 中是线程池中的每个线程会有个自己的队列。 Lecture 11: Distributed Training and Communication Protocols这一节把目标放在上一节调度图中的 Synchronization 部分。 大量的篇幅是对 Allreduce 的讨论，也没啥好说的，跳过跳过… Parameter Server - Worker 架构的同步异步，也没啥好说的… Emm…这两节不是我偷懒，主要是内容比较基础，跟着 PPT 就好了，没什么特别值得注意的。 Lecture 12: Model Serving除去后面没有资料的几个 Guest Talk 以外，这节算是课程内容的最后一部分了，主要讲的是实验室的成果上线进行实用的过程中可能会有的问题，例如： 延迟限制：在云上提供服务的时候，Batch Size 没办法做到越大越好；如果是在终端设备上提供服务，则能够支持运行的模型本身也有轻量级的要求； 资源限制：设备有功耗、内存限制；云服务则有花费开销的限制； 精度限制：提供多级 QoS 等等。 在下面这个视频应用中： 终端设备的采集、处理、数据传输等各个不同有码率、功耗等限制，云端提供服务的部分也有带宽和花费开销的限制，事实上从 Workload 到 Budget 之间也存在一个巨大的 Gap。 下面的内容主要从模型压缩和服务系统两个方面来介绍。 Model Compression这部分是介绍如何对一个网络模型进行压缩。 首先是矩阵/向量的低秩分解，可以应用在全连接层和卷积层中，能够有效地降低整体的计算量和存储量。 这块还是自己的数学知识比较缺乏，暂时不往下细看了。 然后是网络剪枝：训好一个网络之后，通过一个 01 的 Mask 把参数中的某些部分置为 0，再重新训练达到之前相同的预测精度，不断重复以上过程并且逐渐增加 Mask 中 0 的比例，最后就可以得到一个想要的剪枝结果。 权值共享：对参数矩阵进行重新采样，把实际值存在一张表中，然后参数矩阵改成存储对应实际值在表中的索引。这也是尽可能地减少存储冗余。 低比特量化：这个也比较容易理解，就是降低数据类型的存储精度，32 位单精降到半精、int8 甚至是二进制的 01 值。预测结果可能会有一定的精度损失，但是在可以接受的精度损失范围内可以大大节省参数的存储量，并且配合上低精度的硬件预算部件也能大大加快运算速度。 知识蒸馏：用一个训练好的大模型来训练一个小模型。 Knowledge Distillation 这块感觉挺神奇，但是还没细看，不是很理解。 这里还给了一些参考的论文资料： Compression of deep convolutional neural networks for fast and low power mobile applications. ICLR 2016 Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. ICLR 2016 “XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016 EIE: Efficient Inference Engine on Compressed Deep Neural Network. ISCA 2016 MCDNN: An Approximation-Based Execution Framework for Deep Stream Processing Under Resource Constraints. MobiSys 2016 Serving System一个比较好的服务系统需要达到几个目标： 写应用的时候要有很高的灵活性 应用跑在 GPU 上要有很高的效率 满足各种不同的延迟 SLA 需求 然后举了个叫 Nexus 的系统为例，后面就不细看了。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几个多线程的练手 case]]></title>
    <url>%2F2018%2F09%2F10%2F2018-09-10-multithreadsproject%2F</url>
    <content type="text"><![CDATA[Emm……接前篇。 主要是整理几个多线程小例子，读写锁和定时器队列是面试的时候考到的，线程池是自己很早就想写着玩的（挖了好久的坑一直没填，内存池也是……）。 整理出来的代码会放在这里： jcf94/multithread_case ThreadPool参考了一下别人的实现方案： 【A Thread Pool with C++11】 总体上还是比较容易理解的，每个工作线程队里。 当任务队列为空时，每一条工作线程在一个条件变量上等待；当任务队列非空时，则空闲线程直接从任务队列中取出封装好的 std::function 来执行。 1234567891011121314151617181920212223void ThreadRunner::operator()()&#123; std::function&lt;void()&gt; task; while (true) &#123; &#123; std::unique_lock&lt;std::mutex&gt; lock(pool_.tasks_mutex_); while (!pool_.stop_ &amp;&amp; pool_.tasks_.empty()) &#123; pool_.cv_.wait(lock); &#125; if (pool_.stop_) return; task = pool_.tasks_.front(); pool_.tasks_.pop(); &#125; task(); &#125;&#125; TimerQueue定时器即让一个注册好的回调函数在某个设定时间到达之后执行。 话说现在应该有很多系统级的 API 都能够提供这种延迟执行或者阻塞当前线程一段时间的功能，但是当定时的任务比较多的时候，就有可能还是自己写一个定时器队列来维护性能更好点了（主要是不确定到时候所用的定时 API 是怎么实现的，例如如果所有的这些定时任务都用 epoll_wait 的超时触发做，那 epoll 的底层实际上是用红黑树来维护的，猜想效率应该不会太差）。 另外还有一个定时精度的问题，不同的 API 可能能够提供的精度支持也是不一样的。 当时面试的时候被问到这个是远程共享写代码，由于之前没接触过这种 case，一开始不是很明白面试官到底想考我什么，就觉得定时器这种东西不是现有的 API 一大堆嘛，然后一脸懵逼。 最后写了一个线程每隔一个最小的周期（Tick）处理一次队列中的任务，把到达时间的任务取出来执行，任务列表就用优先队列或者说小根堆来实现，保证每次都能够高效地把设定时间最靠前的任务取出来。 网上查了下，基本上比较不错的思路有两种，一种就是小根堆的实现，另外一种是 Linux 内核中采用的时间轮的定时器方法。 【Linux 下定时器的实现方式分析】 由于小根堆的堆顶元素是最近触发的一个任务，因此还可以动态改变下一次处理需要等待的 Tick 时间来节省 CPU 资源的消耗。但是感觉这种方式会不会影响定时精度？以及这样似乎没有办法处理“在堆顶元素之前又插入了一个更近的任务”这种情况，所以最后想想还是觉得固定 Tick 比较稳妥。 然后就是重点要来学习一下时间轮（Timing-Wheel）了。 这个思路其实很简单，想象有一个钟表盘，上面有一圈时间刻度（从 1 到 N），有一个指针每隔 Tick 时间转动一个刻度，而所有的任务也是按照等待时间除以 Tick 分布在每个刻度上面，当指针扫到某个刻度时，即遍历一遍这个刻度上的任务列表，把到时间的拿出来执行即可。 当 N 足够大时，显然这个定时器的各个操作的复杂度都是 O(1) 的。 但是如果 N 过大，则这个时间轮的内存消耗将会比较大，因此又有了多级时间轮的优化思路：以钟表上的时针、分针、秒针为例，时针分为 12 格，分针、秒针分别为 60 格，这样就有一共 132 个格子，每个格子都是一条任务链表。一分钟以内的任务被加到秒针的格子中，一小时以内的任务加到分针的格子中，更久的任务加到时针的格子中。秒针每个 Tick 移动一次，每次移动后直接把当前格子中的所有任务取出来执行；分针则是每 60 个 Tick 移动一次，每次移动把当前格子中的任务下放到秒针轮对应的格子中；时针也是类似。 更厉害的优化思路可见下面这篇： 【基于Hash和多级时间轮：实现定时器的高效数据结构】 ReadWriteLock最后这个读写锁是面试完之后让半小时内写完，然后邮件回面试官交作业…… 结果读写锁的逻辑很快写完之后 main 函数的测试 case 却调 bug 调了好久……狂汗 读写锁从功能上来看也可以叫做共享锁，相对普通的互斥锁来说，它会有三种状态：未锁定、读锁定和写锁定。 读写锁的读操作是可以共享的，即处于读锁定的时候另外一个线程还可以继续对其施加读锁定，而写锁定跟一般的锁一样是独占的。 因此基本的思路就是用一个读计数来记录读操作，当读写锁处于解锁或者读锁定状态时，获取读锁即增加读的锁定计数，而此时想要获取写锁就需要等到所有的读操作都释放之后才行。 写操作可以用一个布尔变量来记录，当读写锁处于写锁定状态时，继续获取读锁或者写锁都需要等前一次的写操作释放。 下面分别是简单地直接用一个锁的版本： 1234567891011121314151617181920212223242526272829303132void rwlock::getRead()&#123; while (is_writing == true) &#123; _lock.lock(); &#125; read_count++;&#125;void rwlock::getWrite()&#123; while (read_count!=0 || is_writing==true) &#123; _lock.lock(); &#125; is_writing = true;&#125;void rwlock::unlockRead()&#123; read_count--; if (read_count == 0) &#123; _lock.unlock(); &#125;&#125;void rwlock::unlockWrite()&#123; is_writing = false; _lock.unlock();&#125; 以及用条件变量实现的版本： 12345678910111213141516171819202122232425262728293031323334void ReadWriteLock::getRead()&#123; std::unique_lock&lt;std::mutex&gt; lock(cv_mutex_); cv_.wait(lock, [this]()&#123;return !this-&gt;is_writing_;&#125;); read_count_++;&#125;void ReadWriteLock::getWrite()&#123; std::unique_lock&lt;std::mutex&gt; lock(cv_mutex_); cv_.wait(lock, [this]()&#123;return this-&gt;read_count_==0 &amp;&amp; !this-&gt;is_writing_;&#125;); is_writing_ = true;&#125;void ReadWriteLock::unlockRead()&#123; std::lock_guard&lt;std::mutex&gt; lock(cv_mutex_); read_count_--; if (read_count_ == 0) &#123; cv_.notify_one(); &#125;&#125;void ReadWriteLock::unlockWrite()&#123; std::lock_guard&lt;std::mutex&gt; lock(cv_mutex_); is_writing_ = false; cv_.notify_one();&#125; Coroutine协程。 严格地说这个跟多线程关系不大，其实这个东西本身大概类似一种用户态的线程，然后关键在于这个线程的运行和调度都是要靠用户自己来管理的。 很多语言里面都自带协程的支持，例如，举一个别人的例子，一个 js 写的生成斐波那契数列的函数： 12345678910function fibonacii() &#123; let i = 1; let j = 1; for (let k = 0; k &lt; 10; k++) &#123; console.log(i); console.log(j); i += j; j += i; &#125;&#125; 用协程的写法则是： 12345678910111213function *fibonaciiUsingYield() &#123; let i = 1; let j = 1; while (true) &#123; yield i; yield j; i += j; j += i; &#125;&#125;------const gen = fibonaciiUsingYield();gen.next(); 相当于每一次这个函数都断点在 yield 对象上，当调用 next 函数时，函数向前执行一轮。 那么如何在本身不支持协程的 C/C++ 里面把这种特性用上呢？ 答案是手动实现一个状态机…… 详见轮子哥的考不上三本系列： 【考不上三本也能懂系列——前言】 下面是按照这种方式写的一个简单例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/* ***********************************************MYID : Chen FanLANG : G++PROG : Coroutine_test************************************************ */#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;algorithm&gt;using namespace std;void original_func()&#123; int i=0; while (i&lt;10) &#123; printf("%d\n", i); i++; &#125;&#125;class func_callable&#123;public: func_callable(int n) : total_(n) &#123;&#125; int result; bool operator()() &#123; int ans; while (true) &#123; switch(state_) &#123; case 0: i_ = 0; if (i_ &lt; total_) &#123;state_ = 1; continue;&#125; else &#123;state_ = 3; continue;&#125; break; case 1: result = i_; &#123;state_ = 2; return false;&#125; break; case 2: i_++; if (i_ &lt; total_) &#123;state_ = 1; continue;&#125; else &#123;state_ = 3; continue;&#125; break; case 3: &#123;state_ = -1; return true;&#125; break; default: throw system_error(); &#125; &#125; return ans; &#125;private: int i_; int total_; int state_ = 0;&#125;;func_callable coroutine_func(int n)&#123; return func_callable(n);&#125;int main()&#123; original_func(); auto a = coroutine_func(10); printf("Coroutine Test:\n"); a(); printf("%d\n", a.result); a();a(); printf("%d\n", a.result); a(); printf("%d\n", a.result); a();a(); printf("%d\n", a.result); return 0;&#125; Result： 1234567891011121314150123456789Coroutine Test:0235]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Multithreads</tag>
        <tag>ThreadPool</tag>
        <tag>TimerQueue</tag>
        <tag>ReadWriteLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程相关整理]]></title>
    <url>%2F2018%2F09%2F07%2F2018-09-07-multithreads%2F</url>
    <content type="text"><![CDATA[还是在整理秋招遭遇过的面试题的时候顺便补充一下多线程相关的东西。 一开始是分了 Basic 和 Project 两个大标题，主要还是想写三个面试遇到的多线程例子的实现，然后发现前面 Basic 部分的东西写的有点多了……篇幅有点长，然后想了想还是把 Project 部分另外开一篇吧。 这大概就是计划赶不上变化？……就是这么任性。 这里来理一下 C 和 C++ 里面与多线程相关的 API。 C Format在 C11 之前，C 标准库里面本身不带线程支持，通常需要用 UNIX/LINUX 系统库里面的 clone、fork 之类或者用 POSIX 标准的 pthread 库来实现。虽然这些东西都包含在 glibc 的库里面了，但是事实上不算 C 语言本身的标准，所以在 cppreference 里面是搜不到的（当时奇怪了好久）。 pthread 底层用来创建线程的实现应该也是调的 clone、fork 这些系统调用来完成的。 至于 mutex 锁、信号量这些原语，从 Linux 2.6.x 版本内核之后都是通过 FUTEX 系统调用来实现的，具体以后有空再看。 另外需要对线程和进程这两个概念作一下强调，Linux 中本身不分进程和线程统称为 task，后来用于区分这两个概念主要是看一个 task 所拥有的资源情况。进程有自己的内存空间，线程共享父进程的内存空间。 fork(), vfork(), clone()先来看一下三个系统接口。 系统调用 描述 fork() 创建父进程的完整副本，复制父进程的资源，包括所有内存的内容。写时复制。 vfork() 创建的子进程与父进程共享数据段，并且由 vfork 创建的子进程先运行，父进程在子进程返回前保持阻塞。 clone() 可以对创建的子进程做更多控制，启动的时候指定需要执行的函数内容。 需要注意的是这几个 API 在 Mingw 里面是用不了的。 Bash on Windows 大法好~ fork() 的使用方式特别简单： 12345678910111213141516171819202122232425262728293031323334/* ***********************************************MYID : Chen FanLANG : GCCPROG : ************************************************ */#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int main()&#123; int aaa = 1; pid_t fpid = fork(); int bbb = 1; if (fpid &lt; 0) &#123; perror("Fork Error!"); &#125; else if (fpid == 0) &#123; aaa++; bbb++; printf("This is son, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n", getpid(), aaa, &amp;aaa, bbb, &amp;bbb); &#125; else &#123; printf("This is Father, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n", getpid(), aaa, &amp;aaa, bbb, &amp;bbb); printf("Son's pid: %d\n", fpid); &#125; return 0;&#125; 结果： 12345678# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:45:20]$ gcc test.c# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:45:50]$ ./a.outThis is Father, with pid: 487, aaa: 1(0x7fffe2ce2d9c), bbb: 1(0x7fffe2ce2da0)This is son, with pid: 488, aaa: 2(0x7fffe2ce2d9c), bbb: 2(0x7fffe2ce2da0)Son's pid: 488 fork() 创建产生的子进程除了这个函数返回的 pid 值以外，与父进程完全一致，父子进程也都会从 fork() 函数返回之后继续向下执行相同的内容。另外，如果把本地的变量值的地址打出来，会发现它们的虚拟地址也都是一样的。 这里用了一个写时复制的技术，fork 出来的子进程一开始直接用的是父进程的内存空间，所有内容包括虚拟地址都是跟父进程完全一致的，直到发生了数据更改，才会在物理地址空间中作一个新的映射。这也就提高了创建子进程的效率，因为分配新的页表也会是一件比较耗时的工作。 然后是 vfork()，把上面这段代码中的 fork 直接改成 vfork 之后会得到这样的运行结果： 12345678910# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:46:37] C:134$ gcc test.c# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:46:42]$ ./a.outThis is son, with pid: 550, aaa: 2(0x7ffff4873a2c), bbb: 2(0x7ffff4873a30)This is Father, with pid: 549, aaa: 0(0x7ffff4873a2c), bbb: 1(0x7ffff4873a30)Son's pid: 550*** stack smashing detected ***: ./a.out terminated[1] 549 abort (core dumped) ./a.out vfork 调用之后，父进程会被阻塞，所以可以看到不同于之前的情况，这里永远都是 son 这句先输出，执行完毕之后才会轮到父进程执行。 那为什么会炸了呢…… 在 fpid 等于 0 的分支末尾加上 exit(0); 之后程序就能够正常执行了： 12345678# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:47:10] C:134$ gcc test.c# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [15:47:18]$ ./a.outThis is son, with pid: 584, aaa: 2(0x7fffc8d7641c), bbb: 2(0x7fffc8d76420)This is Father, with pid: 583, aaa: 2(0x7fffc8d7641c), bbb: 1(0x7fffc8d76420)Son's pid: 584 原因是 vfork 不同于 fork 的一点是，创建出来的子进程直接共享父进程的数据段，当子进程跑完之后，他会像一个正常的进程一样对自己的栈空间等等做回收，则之后当父进程开始执行的时候自身的内存数据就被子进程破坏掉了一部分，这也是为什么前面第一次父进程的 aaa 输出来的结果是不对的，而加上 exit(0); 之后，父进程可以正常输出 2。 话说网上说 vfork 出来的子进程如果用 return 来返回的话会出现很奇怪的 bug，不过我这里测试的时候没有见到，可能跟 gcc 和系统库的版本有关系。 那么为什么会有 vfork 这个看上去有点问题的实现呢？ 这就需要提到另外一个系统接口 exec 了。exec 的作用是拿另外一个程序的代码来替换到当前进程的正文、数据和堆栈，简单地说就是用来启动一个新程序。 exec 的接口实际上是一套，一共 6 个函数，具体的这里先不展开了。 vfork 自身设计的目标是为 exec 服务的，当需要创建一个新进程来执行一段完全不同的代码时，vfork 直接共享父进程地址空间的做法是开销最小的，即保证先有一个子进程，然后调用 exec 来载入一段新的代码并且创建自己的独立地址空间，在子进程开始新程序或者退出之前，内核保证父进程一直处于阻塞状态。 1234567891011121314151617181920212223242526272829303132333435/* ***********************************************MYID : Chen FanLANG : GCCPROG : ************************************************ */#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int main()&#123; int aaa = 1; pid_t fpid = vfork(); int bbb = 1; if (fpid &lt; 0) &#123; perror("Fork Error!"); &#125; else if (fpid == 0) &#123; aaa++; bbb++; printf("This is son, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n", getpid(), aaa, &amp;aaa, bbb, &amp;bbb); execv("hello.out", NULL); &#125; else &#123; printf("This is Father, with pid: %d, aaa: %d(%p), bbb: %d(%p)\n", getpid(), aaa, &amp;aaa, bbb, &amp;bbb); printf("Son's pid: %d\n", fpid); &#125; return 0;&#125; 结果大概是这样： 12345678910111213# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:07:41] C:1$ gcc test.ctest.c: In function ‘main’:test.c:27:9: warning: null argument where non-null required (argument 2) [-Wnonnull] execv("hello.out", NULL); ^# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:09:07]$ ./a.outThis is son, with pid: 654, aaa: 2(0x7fffec2f627c), bbb: 2(0x7fffec2f6280)This is Father, with pid: 653, aaa: 2(0x7fffec2f627c), bbb: 1(0x7fffec2f6280)Son's pid: 654Hello World 最后是 clone，先看下 man 里面的定义： 12345678910/* Prototype for the glibc wrapper function */#define _GNU_SOURCE#include &lt;sched.h&gt;int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ... /* pid_t *ptid, void *newtls, pid_t *ctid */ );/* For the prototype of the raw system call, see NOTES */ fn 是需要执行的函数指针，即 clone 出来的子进程需要执行的函数内容。 child_stack 就明显是给子进程分配的系统堆栈空间的位置。 flags 用于描述子进程需要从父进程中继承哪些部分的内容，因此通过这个值可以控制产生进程、线程、甚至非父子关系而是兄弟关系的进程等等，功能强大。 后面的就是传入新进程中的参数了 测试代码： 1234567891011121314151617181920212223242526272829303132/* ***********************************************MYID : Chen FanLANG : GCCPROG : ************************************************ */#define _GNU_SOURCE#include &lt;sched.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;int hello()&#123; printf("This is: %d, Hello World\n", getpid()); return 0;&#125;int main()&#123; void* stack = malloc(8192); int pid = clone(&amp;hello, (char*)stack+8192, CLONE_PARENT, 0); printf("Father pid: %d, new pid: %d\n", getpid(), pid); return 0;&#125; 上面这份代码中有两个地方需要额外注意一下： 在 &lt;sched.h&gt; 头文件引用前要加上 #define _GNU_SOURCE 的宏，表明下文代码不可移植，可能会用到一些非 GNU 标准的内容（例如 clone）。 clone() 中的第二个参数指定的是栈空间，然后因为栈是反向增长的！！，所以这里需要传入申请的空间的尾部。 结果： 1234567# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:46:24]$ gcc test.c# jcf @ J-CF-MSI in /mnt/c/Users/jcf/Desktop/multithread [16:46:54]$ ./a.outFather pid: 989, new pid: 990This is: 990, Hello World 这三个接口的最底层涉及到的都是 do_fork() 这个调用，只是传入的参数不同，clone 可以认为就是个 do_fork() 的 API 外衣。 pthreadspthreads 的全称应该是 POSIX Threads，是 POSIX 的线程标准，它定义了一套 C 语言标准的线程控制 API，由一个 &lt;pthread.h&gt; 的头文件和一个线程库来实现，主要包含了：线程管理、互斥锁、条件变量、线程同步等等这些线程操作的 API。 12345678910111213141516171819202122232425262728293031/* ***********************************************MYID : Chen FanLANG : GCCPROG : ************************************************ */#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;void* hello(void* arg)&#123; printf("This is: %d, Hello World\n", getpid());&#125;int main()&#123; pthread_t tt; if (pthread_create(&amp;tt, NULL, hello, NULL)) &#123; printf("Thread Create Error\n"); return 0; &#125; pthread_join(tt, NULL); return 0;&#125; 结果： 12[Running] cd "c:\Users\jcf\Desktop\multithread\" &amp;&amp; gcc test.c -o test &amp;&amp; "c:\Users\jcf\Desktop\multithread\"testThis is: 18308, Hello World 话说从使用方式上来看，pthread_create() 的接口跟 clone 就特别像，大概率底层实现就是用 clone 做的，不过传入的线程函数的格式不太一样。 pthread 也提供了互斥锁和条件变量这些结构： pthread_mutex_t 、pthread_cond_t。具体的使用方式跟后面的差别不大，下文再整理。 头文件 &lt;semaphore.h&gt; 中也提供了信号量的支持。 C11C11 之后，标准库里面提供了线程支持，包含在头文件 &lt;threads.h&gt; 中（这下可以在 cppreference 里面查到啦）。 基本也就是线程创建、等待、互斥、条件变量等等的支持，感觉看上去跟 pthread 基本一致。 而且不知道为什么，虽然在标准库里面查到了这个库，但是似乎用的人特别少。 那 C 的部分还是用 pthread 吧。 C++ Format 【学习c++多线程编程主要用pthread还是c++11中的thread类？】 从知乎讨论上面来看，大家对 C++11 的 thread 意见还是比较大的。 C++ 这部分……其实涉及到的东西非常多，需要一堆不同的库联合起来一起用： 线程支持库 &lt;thread&gt; 提供了线程创建、调度、等待等等一系列管理操作； 互斥库 &lt;mutex&gt; 提供了基本的互斥量 mutex，RAII 的锁控制方式 lock_guard 和 unique_lock 等等； 条件变量库 提供了条件变量的支持； 异步支持库 &lt;future&gt; 提供了像 promise、future、async 等等这种异步语义（在 Nodejs 里面用过，之前还真没听说 C++ 里面还带这种玩意）； 原子操作库 &lt;atomic&gt; 提供了一系列与原子操作相关的支持； 另外 C++ 中任何可以被调用的东西都是函数对象，前面用来创建线程用的目标函数也需要由函数对象库 &lt;functional&gt; 来管理，std::bind、std::invoke 等等这些管理参数调用，也可以用 lambda 表达式等等。 话说 &lt;thread&gt; 库是不是也还是 pthread 的封装？？？ functional有关 std::function 和 Lambda 表达式，很早之前稍微有记过一些： 【C++11 及之上的一些新东西】 mutex, lock_guard, unique_lockC++11 中的基础互斥锁结构是 std::mutex，用法应该基本跟 pthread 的一样。&lt;mutex&gt; 中额外还提供了两个符合 RAII 标准的锁控制封装，以更加异常安全的方式来管理互斥锁。 std::lock_guard 就是个简单的互斥封装容器，构造时锁定给定的锁，然后析构的时候自动释放。事实上它能操作的锁不一定只限于 std::mutex，任何有 lock() 和 un_lock() 两个成员函数的对象都可以。 std::unique_lock 功能更多一点。构造时可选地对传入的锁上锁（也可以选择不锁），析构时自动释放。并且同时它还提供了 lock()、try_lock()、unlock() 等等这些成员函数，使用起来就更灵活了，除了离开作用域自动析构释放这一点之外，在作用域中还可以手动控制加锁解锁。 condition_variable条件变量需要结合互斥锁一起使用，这里的 std::condition_variable 尤其在 wait 的时候必须配合 std::unique_lock 来用。 条件变量的核心操作是等待（wait）和唤醒（notify），通常情况下，需要在条件变量上等待的线程需要： 首先需要获得 std::unique_lock&lt;std::mutex&gt; 锁（重要！！）； 执行 wait()、wait_for() 或者 wait_until() ，这三个函数需要把前面的 unique_lock 作为参数传入，执行时将原子地释放传入的 unique_lock，然后挂起当前线程进入等待状态； 当条件变量被其他线程唤醒（notify）或者超时（对于 wait_for、wait_until）时，当前线程结束等待状态，unique_lock 自动获得锁，然后往下继续执行。 在条件变量上执行唤醒操作的线程需要： 首先同样要获得锁，这里不是一定要用 std::unique_lock&lt;std::mutex&gt;，其他方式管理也行； 可以对用于判断的其他数据进行操作； 对条件变量执行 notify_one() 或者 notify_all()，则其他处于等待状态的线程会被唤醒。 文档中对 notify_one 的描述是会唤醒一个等待的线程，但是并不一定是哪一个，跟进入 wait 状态的线程的先后顺序无关，notify_all 则是唤醒当前正处于等待状态中的所有线程。 由于 wait 操作本身自带对 unique_lock 的加锁解锁操作，因此 notify 这边也需要注意前面这个 mutex 锁的状况，必须保证 wait 在调用的时候 mutex 是锁上的，然后 wait 被唤醒的时候锁是开着的。如果 wait 唤醒时试图获取锁失败则会被阻塞在等互斥锁的状态，这个下面有测试。 C++11: why does std::condition_variable use std::unique_lock? 还有一个重要的注意点是多个线程对某个条件变量的 notify 和 wait 操作可以看成是对一个原子变量的顺序操作，这就意味着如果先调用 notify，再调用 wait，则 wait 是不会从唤醒中恢复的。 看上去这个注意点很正常啊，正常就应该是这样的啊。但是实际多线程操作中非常容易出现：自认为 notify 会发生在 wait 以后，实际执行却不是，然后导致死锁的 bug。 Talk is cheap, show me the code! 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/* ***********************************************MYID : Chen FanLANG : G++PROG : CV_TEST************************************************ */#include &lt;stdio.h&gt;#include &lt;condition_variable&gt;#include &lt;mutex&gt;#include &lt;thread&gt;#include &lt;chrono&gt;using namespace std;int main()&#123; mutex cv_m; condition_variable cv; // ----------- 1 ----------- // &#123; // unique_lock&lt;std::mutex&gt; lock(cv_m); // cv.notify_one(); // &#125; this_thread::sleep_for(chrono::seconds(2)); thread th1([&amp;cv, &amp;cv_m] &#123; unique_lock&lt;std::mutex&gt; lock(cv_m); printf("th1 Start Waiting\n"); cv.wait(lock); printf("th1 wake up\n"); &#125;); this_thread::sleep_for(chrono::seconds(2)); // ----------- 2 ----------- &#123; unique_lock&lt;std::mutex&gt; lock(cv_m); cv.notify_one(); &#125; // ----------- 3 ----------- // cv_m.lock(); // cv.notify_one(); // cv_m.unlock(); // ----------- 4 ----------- // cv_m.lock(); // cv.notify_one(); // this_thread::sleep_for(chrono::seconds(2)); // cv_m.unlock(); th1.join(); return 0;&#125; 做了个小测试，对于上面打上标记的 4 块代码： 中间加了个 sleep 2 秒来确保先调用 notify，然后 wait，妥妥的死锁！ 确保先调用 wait，然后 notify，正常工作！ 这么写也是可以正常工作的，但是如果把下面那段 cv_m.unlock() 删掉，则结果就会死锁！！！原因恰恰是在于 wait 被唤醒的时候要首先试图获得锁，由于 cv_m 这时候是锁着的，然后 wait 线程就被阻塞在获取互斥锁的状态了。 为了确认 3 里面的这一点，我又写了 4 这个测试。从这里可以明确的是，notify 操作之后，wait 线程虽然仍然阻塞，但是这个阻塞状态跟前面线程挂起的等待状态是不同的，而是卡在 cv_m 这个锁上。 所以看上去最好的方式是 notify 的时候根本就别管锁？如果不上锁，不就没这么多麻烦了吗……事实上，更好的写法应该是这样的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* ***********************************************MYID : Chen FanLANG : G++PROG : CV_TEST************************************************ */#include &lt;stdio.h&gt;#include &lt;condition_variable&gt;#include &lt;mutex&gt;#include &lt;thread&gt;#include &lt;chrono&gt;using namespace std;int main()&#123; mutex cv_m; condition_variable cv; bool ready = false; // ----------- 1 ----------- // &#123; // lock_guard&lt;mutex&gt; lock(cv_m); // ready = true; // cv.notify_one(); // &#125; this_thread::sleep_for(chrono::seconds(2)); thread th1([&amp;cv, &amp;cv_m, &amp;ready] &#123; unique_lock&lt;mutex&gt; lock(cv_m); printf("th1 Start Waiting\n"); //cv.wait(lock, [&amp;ready]&#123;return ready;&#125;); while (!ready) cv.wait(lock); printf("th1 wake up\n"); &#125;); this_thread::sleep_for(chrono::seconds(2)); // ----------- 2 ----------- &#123; lock_guard&lt;mutex&gt; lock(cv_m); ready = true; cv.notify_one(); &#125; th1.join(); return 0;&#125; 三种 wait 函数均有一种附加条件的多参数调用方式，等同于在一个 while 循环中调用单参数版的 wait 函数。另外用一个 ready 变量标识等待情况，在 notify 时，cv_m 这个锁实际上是用于保护这个 ready 变量用的。用这种方式写则无论 notify 代码块发生在 wait 线程前还是发生在之后，wait 线程均会正常返回了。 future这个头文件里面的内容很有意思，核心的类主要是 std::promise、std::packaged_task、std::future 这几个。 std::future 是一个对异步操作结果的封装类，一般需要配合 std::async、std::packaged_task 和 std::promise 的异步操作使用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* ***********************************************MYID : Chen FanLANG : G++PROG : future_test************************************************ */#include &lt;iostream&gt;#include &lt;future&gt;#include &lt;chrono&gt;using namespace std;int task(int i)&#123; this_thread::sleep_for(chrono::seconds(1)); return i;&#125;void promise_task(future&lt;int&gt;&amp; future_int)&#123; int x = future_int.get(); cout &lt;&lt; x &lt;&lt; endl;&#125;int main()&#123; // ----------- future + async ----------- future&lt;int&gt; a; a = async(task, 10); cout &lt;&lt; a.get() &lt;&lt; endl; // ----------- future + packaged_task ----------- packaged_task&lt;int(int)&gt; b_pack(task); future&lt;int&gt; b = b_pack.get_future(); b_pack(20); //thread thb(move(b_pack), 20); //thb.join(); cout &lt;&lt; b.get() &lt;&lt; endl; // ----------- future + promise ----------- promise&lt;int&gt; c_prom; future&lt;int&gt; c = c_prom.get_future(); thread thc(promise_task, ref(c)); this_thread::sleep_for(chrono::seconds(1)); c_prom.set_value(30); thc.join(); return 0;&#125; std::async 的作用是在另外一个线程中异步（默认操作，也可以设置在调用线程中同步执行）地执行给定的函数，函数执行的返回值则是由一个 std::future 对象来接收。 std::packaged_task 则是一个可调用目标（函数、Lambda表达式或者其他函数对象，即 std::function 的对象）的类模板封装，这个封装主要也就是把函数对象的执行和返回值给分开。package_task 对象可以直接加参数调用，或者放在另外一个线程中调用，结果会在函数体执行完毕之后存到对应的 future 结构中。 最后是 std::promise，这个对象感觉有点像 placeholder 占位符的作用。promise 通过 set_value() 来提供数据，在 future 绑定的 promise 准备完成之前，future 的 get() 会阻塞所在的线程。]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Multithreads</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（试图）深入理解 Cache]]></title>
    <url>%2F2018%2F09%2F04%2F2018-09-04-cache%2F</url>
    <content type="text"><![CDATA[可能是因为简历里面写了个大大的 “计算机系统结构方向”，然后面的几个厂的岗位也都是偏系统方向，秋招面试的时候被好几个面试官都按在地上狂问系统方面的问题。 其中大概尤其与 Cache 有关的内容比较有代表性，于是准备根据几次回忆出来的自己和小伙伴遇到的面试题好好理一理 Cache 这块的内容。 Basic上图是一张最基础也最常见的存储器层次结构图，表达的是计算机系统中各级存储器的速度、容量、价格等等的金字塔关系。 Cache 这个思想本身特别简单，利用的核心原理就是数据的局部性，即把最常用到的东西放在最容易拿到的地方，这种局部性即包含了数据的空间局部性也包含了使用数据的时间局部性，但实际用起来效果却是非常地好，并且广泛应用在各种不同的场合中，例如：CPU 中的 Cache 用来加速对主存数据的访问，TLB 可以看成是对虚拟地址-物理地址转换的页表的 Cache，分布式环境中有的时候也会做个本地数据缓存，也是 Cache 的思想。 其实《硬软件接口》里面已经有介绍过 Cache 了，前面也有记过： 【计算机组成与设计.硬件/软件接口 学习笔记（二）：CHAPTER 5. Large and Fast: Exploiting Memory Hierarchy】 【硬件/软件接口 Virtual Memory】 编程的时候 Cache 对程序员透明，写代码的人直接看到的是内存（……更准确地说看到的是虚拟的地址空间），但是 CPU 实际执行的时候访问的数据全部都是从 Cache 里面来的。每次访问一块新的内存数据时，首先检查 Cache 中是否存在，有就返回，没有就触发一次 Cache Miss，从主存把数据 load 到 Cache 中以后再返回。 然而 Cache 这个简单的想法在实际实现的时候可能会复杂的多，例如 Cache 分级，然后 Cache 还有多种数据和地址的映射关系，什么时候更新写回，地址冲突的时候怎么替换等等很多麻烦的问题都需要在实际实现的时候考虑。 这里还有两篇知乎上的小白科普文： 【L1，L2，L3 Cache究竟在哪里？】 【Cache是怎么组织和工作的？】 Interview Questions面试的时候我和我的小伙伴们在 Cache 上踩了很多坑，虽然平时工作中很少有遇到需要考虑的这么深的，也就一般很少想到 Cache 这一层的工作内容了，但是终究说起来还是自己的基础没打扎实。 虚拟地址 or 物理地址？ L1 Cache 标记数据块用的是虚拟地址还是物理地址？ 往后的 Cache 里面肯定用的是物理地址，这个没有任何悬念，关键在这个最靠近 CPU 的 L1 Cache 上。 这个问题一开始我脑子里冒出来的是当时看《硬软件接口》虚拟内存那章时画的那张图：CPU 先拿虚拟地址去查 TLB，然后再找 Cache，那妥妥的就是物理地址了。 后来一想这本书上是以一个假想的简单数据通路结构为例进行介绍，可能细节上后来有变动，然后去翻了下《量化》，里面没有详细讲，但是看图感觉像是用的虚拟地址。 再后来去网上搜这个问题的时候，说物理地址的也有，说虚拟地址的也有，还各有各的理由，于是就更迷了。 直到我在这个专栏文章中看到了这张图： MMU 是 CPU 中用于虚拟地址-物理地址转换的工作单元（主要是页表、TLB 甚至多级 TLB 等等组成），这张图中根据芯片实际实现时 MMU 和 Cache 的位置关系，则很明显 L1 Cache 可能用虚拟地址也可能用物理地址了。 那我们来考虑一下这两种实现的区别： Logical Cache/ Virtual Cache 感觉这种实现访问速度应该更快一点，因为不需要经过地址翻译就能够直接在 Cache 中搜索数据了。 问题是每个进程的虚拟地址空间都是独立的，如果有多个进程在同一个核上切换，则每次切换的时候都需要把 Cache 刷掉（话说我其实不太清楚这个代价会有多大，也不知道这个会不会成为制约性能的一个原因），当然，也有可能有多个虚拟地址对应到同一个物理地址上的这种情况，那这种情况要怎么处理就需要考虑更多东西了。 ……真麻烦啊。 Physical Cache 查 Cache 前不管怎么样先把地址转换做了，这样前面 Logical Cache 存在的问题也就不存在了，而且也不需要一切换进程就刷一遍整个 L1 Cache，如果有多个虚拟地址对应到同一个物理地址上也无所谓。 问题大概就在于 MMU 的地址转换上，如果 TLB Miss 了，查页表还是要访问到主存上，那再查 Cache 就需要花更多的时间了。 CPU_cache 的维基百科页对地址转换这块也有大段篇幅的介绍。Cache 受地址转换影响在 Latency、Aliasing、Granularity 等几个方面都需要考虑到，根据用虚拟地址还是物理地址进行查找和标记，常见的也有下面四种 Cache 实现方式： Physically Indexed, Physically Tagged(PIPT) 对应前面的 Physical Cache，没啥大毛病，就是慢 Virtually Indexed, Virtually Tagged(VIVT) 对应前面的 Logical Cache/ Virtual Cache，带来的麻烦问题很多 Virtually Indexed, Physically Tagged(VIPT) 可能是最好的一种实现方式了，目前市场上的一些现代处理器应该大多都是基于这种方式或者在这个基础上优化的。 由于查虚拟地址的索引和查 TLB 可以同时进行，这种方式的延迟会比 PIPT 低很多，但是实际数据还是要等到 MMU 把物理地址算出来之后对比 tag 才能确定。 另一方面由于用了物理地址作为 tag，VIVT 中可能会有的虚拟地址冲突的问题也解决了。 Physically Indexed, Virtually Tagged(PIVT) 这个…大概只会出现在文献中，实际实现的时候会集合 PIPT 和 VIVT 所有的缺点。 另外，对 Cache 数据的查找和标记还需要考虑到全相联、组相联等不同的映射关系的实现。 对 Cache 每一级的访存需要多少个 Cycle 有概念吗？ 对 Context Switch 和 Cache 访存需要大概花费多少个 Cycle 有概念吗？ 并没有……卒。 看一下网上找到的答案： Register：1 Cycle L1 Cache：3 Cycles L2 Cache：10+ Cycles L3 Cache：20~30+ Cycles Main Memory：~100 Cycles 同样都是用 SRAM 做的，为什么会有速度差异呢？原因大概有以下几个： 容量大小 显而易见的是，Cache 的容量会随着级数的增加而增大。由于 Cache 需要做到随机访存，即能够直接访问到存储器的任意一个位置，在制程和设计完全一致的情况下，容量越大就需要花费更多的时间来做到随机访存（延迟跟容量的开方大致成正比）。 芯片上与 CPU 的距离 在芯片面积有限的情况下，L1 Cache 会被放置在离 CPU 核心非常近的地方，而 L2 Cache 就只能放到边缘位置了。 L1 中的指令 Cache 会在 Fetch 单元附近，数据 Cache 会在 Load/ Store 单元附近，L2 Cache 就要在 CPU 流水线外面了，L3 更远要在核外了。 具体实现的差异 L1 和 L2 在设计时的侧重点会有所区别，L1 更注重速度，而 L2 要在 L1 Miss 之后才发挥作用，因此更注重节能和容量。 查询一个地址时，L1 Cache 会把多个 Cache Line 的 tag 和数据全部取出来，然后再比较 tag 看哪一个命中或者都没命中。 而 L2 Cache 虽然也是 N 路组相联，但是比较时会先取 tag，当找到命中的之后再去把对应的数据取出来。 L3 做在核外，通常是多个核共享，因此还需要额外考虑一致性等等更多的东西。 关于 Context Switch 这点，进程和线程的切换其实都要涉及到上下文的切换。 进程切换时由于虚拟地址空间不同，因此需要切换页面映射、刷新 TLB 等等，如果是线程切换则开销会小很多。 为什么需要分级？这个问题直接有别人解答了，感觉说的也还算清楚： 【Cache为什么有那么多级？为什么一级比一级大？是不是Cache越大越好？】 组成结构？当时被问到这个问题的时候有点懵逼，不确定面试官想表达的是什么意思，我回答 “Index、tag、cache line data” 的时候被否决了……然后后来就没有答上来了。 如果指的不是全相联、组相联这种实现的话，可能想问的是硬件实现？ 我对 Cache 硬件结构的了解就只是知道它是用 SRAM 做的，其他的就不懂了，卒。 这一块能够找到的资料也比较模糊，最后是从王齐的《浅谈 Cache Memory》中找到了比较靠谱的答案： 组相联方式组织的 Cache 会分成两个部分，Tag 部分和数据部分分开存放，例如一个 8 路组相联的 Cache 结构是这样的： 左边是地址 Tag 以及当前 Cache Line 的状态，右边是实际存放的数据。 这两类字段由于功能和特性不同，会使用两种不同类型的存储器来存放。Tag 阵列多使用 CAM（Content Addressable Memory）来存放，以利于并行查找，数据字段用的才是多端口多 Bank 的 SRAM。一般说的 Cache 大小也都指的是 SRAM 数据块的大小，CAM 这部分不包含在内。 CAM 对应的应该是前面结构图中一个 Set 中的多个 Way 的结构。首先根据需要访问的虚拟地址确定 Index 找到在哪个 Set 中，然后对该 Set 中的多条记录并行进行 Tag 的比对。 CAM 的基本结构如下： 上图的 CAM 有 3 个 Word，分别对应一条横向的 ML（Match Line），每一个 Word 由 4 个 Bits （CAM Cell）组成。在每一列中，Bits 分别与两个 SL（Search Line）对应。 使用 CAM 进行查找时，首先需要把需要搜索的目标（Tag）放入 Search Data Register/Drivers 中，分解成多个 Bits 之后，通过 SL 发送到所有的 CAM Cell 中，每个 Cell 的 Hit/Miss 信息会向右传递给各自的 ML，最终 ML 汇总得到自己的 Hit/Miss 情况，这样就能够确定下来在当前的多个 Way 里面有没有命中的数据了。 后面更细节的就跳过了。 如何测出一块未知 CPU 的 Cache 参数？ 假如你是一个 Intel 的工程师，有一天你的竞争对手 AMD 推出了一款新的 CPU，然后你想要知道关于其中的 Cache 的信息，要怎么做？没有其他任何的资料，只能通过实际测试的方式。 这个问题是紧接着上一个的，由于我被问到上一个问题的时候已经懵逼了，这题基本上完全没答上来，其实后来想想应该至少能把 Cache 的大小测出来，当时回答的时候真的是表现得太差了。 参考： 【cache测试及其矩阵优化】 【Gallery of Processor Cache Effects】 根据第一篇的程序稍微改了一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* ***********************************************MYID : Chen FanLANG : G++PROG : ************************************************ */#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;algorithm&gt;#include &lt;sys/time.h&gt;using namespace std;#define KB (1024/4)#define MB (1024 * KB)#define SIZE (128 * MB)double getTime()&#123; struct timeval tv; gettimeofday(&amp;tv, NULL); //获取秒 double sec = (double)tv.tv_sec; //获取微秒 double usec = (double)tv.tv_usec; //返回微秒数 return sec * 1000000 + usec;&#125;void cache_line_test(int *array)&#123; for (int stride=1;stride&lt;64*KB;stride*=2) &#123; double begin_time = getTime(); for (int i=0;i&lt;SIZE;i+=stride) &#123; array[i] *= 10; &#125; double end_time = getTime(); printf("Stride: %5d, Line Size: %5d Bytes, Average Cost: %10lf us\n", stride, stride*4, (end_time - begin_time) / (SIZE/stride)); &#125;&#125;void cache_block_test(int *array)&#123; for (int num=KB;num&lt;MB;num*=2) &#123; int len = num-1; int tot = SIZE-1; double begin_time = getTime(); for (int i=0;i&lt;tot;i++) &#123; array[(i*16) &amp; len] *= 10; &#125; double end_time = getTime(); printf("Num: %8d, Size: %8d KB, cost %10lf us\n", num, num*4/1024, end_time - begin_time); &#125; for (int num=MB;num&lt;16*MB;num+=MB) &#123; int len = num-1; int tot = SIZE-1; double begin_time = getTime(); for (int i=0;i&lt;tot;i++) &#123; array[(i*16) &amp; len] *= 10; &#125; double end_time = getTime(); printf("Num: %8d, Size: %8d MB, cost %10lf us\n", num, num*4/1024/1024, end_time - begin_time); &#125;&#125;int main()&#123; int *array = new int[SIZE]; for (int i=0;i&lt;SIZE;i++) array[i] = i; cache_line_test(array); cache_block_test(array); return 0;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839Stride: 1, Line Size: 4 Bytes, Average Cost: 0.002586 usStride: 2, Line Size: 8 Bytes, Average Cost: 0.002613 usStride: 4, Line Size: 16 Bytes, Average Cost: 0.002622 usStride: 8, Line Size: 32 Bytes, Average Cost: 0.003078 usStride: 16, Line Size: 64 Bytes, Average Cost: 0.004280 usStride: 32, Line Size: 128 Bytes, Average Cost: 0.011414 usStride: 64, Line Size: 256 Bytes, Average Cost: 0.011412 usStride: 128, Line Size: 512 Bytes, Average Cost: 0.007610 usStride: 256, Line Size: 1024 Bytes, Average Cost: 0.015221 usStride: 512, Line Size: 2048 Bytes, Average Cost: 0.015732 usStride: 1024, Line Size: 4096 Bytes, Average Cost: 0.000000 usStride: 2048, Line Size: 8192 Bytes, Average Cost: 0.000000 usStride: 4096, Line Size: 16384 Bytes, Average Cost: 0.000000 usStride: 8192, Line Size: 32768 Bytes, Average Cost: 0.000000 usNum: 256, Size: 1 KB, cost 86789.000000 usNum: 512, Size: 2 KB, cost 88739.000000 usNum: 1024, Size: 4 KB, cost 86736.000000 usNum: 2048, Size: 8 KB, cost 91756.000000 usNum: 4096, Size: 16 KB, cost 88762.000000 usNum: 8192, Size: 32 KB, cost 94745.000000 usNum: 16384, Size: 64 KB, cost 95744.000000 usNum: 32768, Size: 128 KB, cost 105752.000000 usNum: 65536, Size: 256 KB, cost 105684.000000 usNum: 131072, Size: 512 KB, cost 108742.000000 usNum: 262144, Size: 1 MB, cost 99733.000000 usNum: 524288, Size: 2 MB, cost 114694.000000 usNum: 786432, Size: 3 MB, cost 100699.000000 usNum: 1048576, Size: 4 MB, cost 130651.000000 usNum: 1310720, Size: 5 MB, cost 99732.000000 usNum: 1572864, Size: 6 MB, cost 120678.000000 usNum: 1835008, Size: 7 MB, cost 118715.000000 usNum: 2097152, Size: 8 MB, cost 151562.000000 usNum: 2359296, Size: 9 MB, cost 96772.000000 usNum: 2621440, Size: 10 MB, cost 104690.000000 usNum: 2883584, Size: 11 MB, cost 104754.000000 usNum: 3145728, Size: 12 MB, cost 152557.000000 usNum: 3407872, Size: 13 MB, cost 105716.000000 usNum: 3670016, Size: 14 MB, cost 156583.000000 usNum: 3932160, Size: 15 MB, cost 141678.000000 us 我笔记本是 i7 7700HQ，在网上可以查到详细的 Cache 信息 Cache: L1 data L1 instruction L2 L3 Size: 4 x 32 KB 4 x 32 KB 4 x 256 KB 6 MB Associativity: 8-way set associative 8-way set associative 4-way set associative 12-way set associative Line size: 64 bytes 64 bytes 64 bytes 64 bytes Comments: Direct-mapped Direct-mapped Non-inclusive Direct-mapped Inclusive Shared between all cores Cache Line 比较好测，上面的代码中可以明显看到从 64 Bytes 开始，平均访问时间出现跳变。 L1、L2、L3 各自的大小……说实话我觉得这样测效果并不好，变化倒是确实有，就是其中可能还有很多误差成分在。 CSAPP 上面讲 Cache 的那章用的是类似的方法，根据这样的测试结果可以画出一张“存储器山”的图。 多核情况下 Cache 会有什么问题？这个问的主要应该就是 Cache 的一致性、写回、写直达等等这些方面的内容。 另外还有一个叫做伪共享（False Sharing）的问题： 多核 CPU 通常都是 L1、L2 每个核独立，共享 L3。如上图这种情况，两个核实际操作的数据是独立的，但是它们恰好在一个 Cache Line 里面，则其中一个作了修改之后，另一个的 Cache Line 也会跟着失效，引起了本来不必要的效率问题。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Cache</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理一个 LCA 模版]]></title>
    <url>%2F2018%2F06%2F20%2F2018-06-20-lca%2F</url>
    <content type="text"><![CDATA[工作累了头昏脑涨，刷个题冷静一下。 顺手来理一下 LCA 的板子。 Tarjan 离线 LCA前面在 【Tarjan 大佬的算法们】 中提到过他的离线 LCA 算法，就从这里开始。 原始算法具体见前文吧，还引用了别人的一个链接，里面有个动画演示的挺清楚的。 离线 LCA 的关键在于 dfs 遍历整个树的过程中，对于被询问的点对 (v, w)，要求其中一个点要先被访问过，然后再遍历到另一个点则可以通过查询前一个点所属的并查集来找到它们的 LCA，例如 v 先被访问，接下来遍历到 w，则它们的 LCA 是 getfather(v)，反正若 w 先被访问，接下来遍历到 v，则它们的 LCA 是 getfather(w)。查询结束再将 v 和 w 合并到同一个集合中（向树上父节点的方向合并）。 Tarjan 论文中的原始算法的树节点应该是保证有序的，然后通过先处理每一对 (vi, wi)，使得 vi &lt; wi 来保证每次访问到 wi 时，它对应的 vi 都被访问过。 在通用的树中就只能在每一层 dfs 中询问跟当前层搜的父节点相关的边来查找这种对应关系了。 记两道模版题： HDU 2586【题意】 给出一棵树，询问树上的两个点，要求回答两个点之间的最近距离。 【分析】 树首先是任意给的，假定我们建出来的树根是 R，则对于一次询问的点对 (a, b)，可以在这棵树上找到它们的 LCA，记为 c。则 a 和 b 之间的距离就是： $$Dis(R, a) - Dis(R, c) + Dis(R, b) - Dis(R, c) = Dis(R, a) + Dis(R, b) - 2*Dis(R, c)$$ 在找 LCA 的 dfs 过程中顺手把每个节点到根节点的距离记出来即可。 HDU 2874【题意】 跟前面类似，加了个条件是可能有多棵树。 【分析】 每对点对是否在同一棵树上可以在读取数据的时候直接用并查集判断联通性记下来，之后对所有未访问过的点做 Tarjan LCA 即可。 【模版】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu2874************************************************ */#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;algorithm&gt;using namespace std;int father[10010];int getfather(int x)&#123; if (father[x] != x) father[x] = getfather(father[x]); return father[x];&#125;void link(int x, int y)&#123; father[getfather(x)] = getfather(y);&#125;typedef struct nod&#123; int a, b, c;&#125; node;bool op(node a, node b)&#123; if (a.a == b.a) return a.b &lt; b.b; else return a.a &lt; b.a;&#125;node edge[20010];int start[10010], num[10010], dist[10010];bool flag[10010];node q[2000010];int qstart[10010], qnum[10010];int res[1000010];void tarjan(int now)&#123; father[now] = now; flag[now] = true; for (int i=0;i&lt;num[now];i++) &#123; int next = edge[start[now]+i].b; if (!flag[next]) &#123; dist[next] = dist[now]+edge[start[now]+i].c; tarjan(next); link(next, now); &#125; &#125; for (int i=0;i&lt;qnum[now];i++) &#123; int next = q[qstart[now]+i].b; if (res[q[qstart[now]+i].c] != -1 &amp;&amp; flag[next]) &#123; int lca = getfather(next); res[q[qstart[now]+i].c] = dist[now]+dist[next]-2*dist[lca]; &#125; &#125;&#125;int main()&#123; freopen("in.txt", "r", stdin); int n, m, c; while (scanf("%d %d %d", &amp;n, &amp;m, &amp;c) != EOF) &#123; for (int i=1;i&lt;=n;i++) father[i] = i; for (int i=0;i&lt;m;i++) &#123; int index = i &lt;&lt; 1; scanf("%d %d %d", &amp;edge[index].a, &amp;edge[index].b, &amp;edge[index].c); edge[index+1].b = edge[index].a; edge[index+1].a = edge[index].b; edge[index+1].c = edge[index].c; link(edge[index].a, edge[index].b); &#125; memset(start, 0, sizeof(start)); memset(num, 0, sizeof(num)); m = m &lt;&lt; 1; sort(&amp;edge[0], &amp;edge[m], op); for (int i=0,o=-1;i&lt;m;i++) &#123; if (o!=edge[i].a) &#123; o = edge[i].a; start[o] = i; &#125; num[o]++; &#125; memset(res, 0, sizeof(res)); for (int i=0;i&lt;c;i++) &#123; int index = i &lt;&lt; 1; scanf("%d %d", &amp;q[index].a, &amp;q[index].b); q[index+1].a = q[index].b; q[index+1].b = q[index].a; q[index].c = i; q[index+1].c = i; if (getfather(q[index].a) != getfather(q[index].b)) res[i] = -1; &#125; memset(qstart, 0, sizeof(qstart)); memset(qnum, 0, sizeof(qnum)); c = c &lt;&lt; 1; sort(&amp;q[0], &amp;q[c], op); for (int i=0,o=-1;i&lt;c;i++) &#123; if (o != q[i].a) &#123; o = q[i].a; qstart[o] = i; &#125; qnum[o]++; &#125; memset(flag, 0, sizeof(flag)); memset(dist, 0, sizeof(dist)); for (int i=1;i&lt;=n;i++) if (!flag[i]) tarjan(i); c = c &gt;&gt; 1; for (int i=0;i&lt;c;i++) &#123; if (res[i] == -1) printf("Not connected\n"); else printf("%d\n", res[i]); &#125; &#125; return 0;&#125; dfs 过程中是先询问边还是先进行下一层的 dfs 不影响结果。 话说虽然前向星写习惯了，不过看人家写的数组邻接链表也挺好看的。 动态树（LCT）前面 【HDU 3966】 用过一次 LCT 和树链剖分来维护树上一条路径上的点值，翻回去看的时候……妈呀，以前题解怎么都没好好写，自己都看不懂了，顺便拿到这里重新理一下思路。 LCT 的核心思路是用 splay 森林来维护 Preferred Path，核心操作是调整 Preferred Path 的 Access 操作，对于 LCT 来说，找 a、b 两点的 LCA 只需要两步： 首先 Access(a) ，此时 a 所在的 splay 树即为从根节点到 a 点所构成的 Preferred Path； 之后 Access(b)，找到从根节点到 b 点所构成的 Preferred Path 之后，此时 Splay 的根即为 a 和 b 的 LCA。 当然也有可能两次 Access 操作之后的 Preferred Path 的根不是同一个，那就意味着这两个点是分属于两棵不同的树，不存在 LCA，特判一下就好了。 所以上面那题的 LCT 模版是： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU2874-LCT************************************************ */#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;typedef struct nod&#123; int a, b, c;&#125; node;bool op(node a, node b)&#123; if (a.a == b.a) return a.b &lt; b.b; else return a.a &lt; b.a;&#125;node edge[20010];int start[10010], num[10010], dist[10010];int sons[10010][2];int father[10010];bool root[10010];void bfs(int s)&#123; queue&lt;int&gt; q; q.push(s); root[s] = true; while (!q.empty()) &#123; int now = q.front(); for (int i=0;i&lt;num[now];i++) if (!root[edge[start[now]+i].b]) &#123; father[edge[start[now]+i].b] = now; dist[edge[start[now]+i].b] = dist[now] + edge[start[now]+i].c; root[edge[start[now]+i].b] = true; q.push(edge[start[now]+i].b); &#125; q.pop(); &#125;&#125;void rotate(int x,int w) //rotate(node,0/1)&#123; int y=father[x]; sons[y][!w]=sons[x][w]; if (sons[x][w]) father[sons[x][w]]=y; father[x]=father[y]; if (father[y]&amp;&amp;(!root[y])) sons[father[y]][y==sons[father[y]][1]]=x; sons[x][w]=y; father[y]=x; if (root[y]) &#123; root[x]=true; root[y]=false; &#125;&#125;void splay(int x) //splay(node)&#123; while(!root[x]) &#123; if (root[father[x]]) rotate(x,x==sons[father[x]][0]); else &#123; int t=father[x]; int w=(sons[father[t]][0]==t); if (sons[t][w]==x) &#123; rotate(x,!w); rotate(x,w); &#125; else &#123; rotate(t,w); rotate(x,w); &#125; &#125; &#125;&#125;void access(int v)&#123; int u=v; v=0; while(u) &#123; splay(u); root[sons[u][1]]=true; sons[u][1]=v; root[v]=false; v=u; u=father[u]; &#125;&#125;int check(int x, int y)&#123; access(x); int root_x = x; while (father[root_x]) root_x = father[root_x]; while (sons[root_x][0]) root_x = sons[root_x][0]; int u = y, v = 0; while(u) &#123; splay(u); root[sons[u][1]] = true; sons[u][1] = v; root[v] = false; v = u; u = father[u]; &#125; int root_y = v; while (sons[root_y][0]) root_y = sons[root_y][0]; if (root_x != root_y) return -1; else return dist[x] + dist[y] - 2*dist[v];&#125;int main()&#123; freopen("in.txt", "r", stdin); int n, m, c; while (scanf("%d %d %d", &amp;n, &amp;m, &amp;c) != EOF) &#123; for (int i=0;i&lt;m;i++) &#123; int index = i &lt;&lt; 1; scanf("%d %d %d", &amp;edge[index].a, &amp;edge[index].b, &amp;edge[index].c); edge[index+1].b = edge[index].a; edge[index+1].a = edge[index].b; edge[index+1].c = edge[index].c; &#125; memset(start, 0, sizeof(start)); memset(num, 0, sizeof(num)); m = m &lt;&lt; 1; sort(&amp;edge[0], &amp;edge[m], op); for (int i=0,o=-1;i&lt;m;i++) &#123; if (o!=edge[i].a) &#123; o = edge[i].a; start[o] = i; &#125; num[o]++; &#125; memset(root, 0, sizeof(root)); memset(dist, 0, sizeof(dist)); memset(father, 0, sizeof(father)); memset(sons, 0, sizeof(sons)); for (int i=1;i&lt;=n;i++) if (!root[i]) bfs(i); for (int i=0;i&lt;c;i++) &#123; int x, y; scanf("%d %d", &amp;x, &amp;y); int res = check(x, y); if (res == -1) printf("Not connected\n"); else printf("%d\n", res); &#125; &#125; return 0;&#125; 从最后的求解过程上来看，应该比离线的要更顺一点，毕竟在线算法不用多考虑记录输出顺序什么的。 树链剖分还是看前面 【HDU 3966】 的时候……发现树链剖分忘得差不多了，顺手再补一个树剖找 LCA 的模版吧。 树链剖分的原理是把整棵树按照一条一条树边组成的链划开，每条链相当于一个区间，那对树上的某条路径的操作就成了对树上的一条或者多条树链的操作了，具体维护区间的部分可以用树状数组啊、线段树啊什么的来做。 划分树链的基本思路是对整棵树进行轻重边划分，定义 size(x) 是以 x 为根的子树的节点个数，这里的重边就是 x 和它节点数更多的一个子节点（size 更大的一个子节点）组成的边。 这里会有两个性质： 若 (father, son) 是一条轻边，则 size(son) &lt;= size(father)/2 从树根到某一个点的路径上的轻边的个数不会超过 O(logn) 划分好轻重边之后，从根节点开始把所有连着的重边连起来，就成了一条重链，重链就是前面说的需要从树上剖出来的树链啦。 这个过程可以用一次 bfs 加两次队列遍历来完成： 首先 bfs 构图，把所有节点的父节点 father 和距离根节点的层数 level 标记好； 根据前面 bfs 过程中记录下来的队列顺序，反向遍历一遍，自底向上维护好每个节点的 size，标记重边也是在这里完成； 根据前面 bfs 过程中记录下来的队列顺序，正向遍历一遍，自顶向下对每条重边进行剖分，大概是一个切下来一条链放好，再切下一条链放好这样的过程。 那么 LCA 要怎么找呢？ 在树链结构上找 a、b 两点的 LCA 即走完各自所在的树链，沿着它的轻边向上找，直到找到各自路径上的两个点在同一条树链上，那么深度较浅的那个点就是 a、b 两点的 LCA 了。 由于从树根到某一个点的路径上的轻边的个数不会超过 O(logn) 这个性质，每次找 LCA 的复杂度是 O(logn)。 模版如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU2874-TreeCut************************************************ */#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;typedef struct nod&#123; int a, b, c;&#125; node;bool op(node a, node b)&#123; if (a.a == b.a) return a.b &lt; b.b; else return a.a &lt; b.a;&#125;node edge[20010];int start[10010], num[10010], dist[10010];int son[10010], father[10010], level[10010], size[10010], top[10010], pos[10010];int q[10010];void bfs(int n)&#123; int head = 0, tail = 0; memset(level, 0, sizeof(level)); memset(father, 0, sizeof(father)); memset(dist, 0, sizeof(dist)); memset(son, 0, sizeof(son)); for (int x=1;x&lt;=n;x++) if (!level[x]) &#123; q[head] = x; level[x] = 1; tail = head; while (head &lt;= tail) &#123; int now = q[head]; size[now] = 1; for (int i=0;i&lt;num[now];i++) &#123; int next = edge[start[now]+i].b; if (next != father[now]) &#123; father[next] = now; dist[next] = dist[now]+edge[start[now]+i].c; level[next] = level[now]+1; tail++; q[tail] = next; &#125; &#125; head ++; &#125; &#125; for (int i=n-1;i&gt;=0;i--) &#123; int now = q[i]; if (father[now]) &#123; size[father[now]]+=size[now]; if (son[father[now]]==0 || size[now]&gt;size[son[father[now]]]) son[father[now]] = now; &#125; &#125; int tot = 1; for (int i=0;i&lt;n;i++) &#123; int now = q[i]; if (son[father[now]] == now) top[now] = top[father[now]]; else &#123; top[now] = now; while (now) &#123; pos[now] = tot++; now = son[now]; &#125; &#125; &#125;&#125;int lca(int x, int y)&#123; while (top[x] != top[y]) &#123; if (level[top[x]] &lt; level[top[y]]) swap(x, y); x = father[top[x]]; &#125; if (x==0 &amp;&amp; y==0) return -1; if (level[x] &gt; level[y]) swap(x, y); return x;&#125;int main()&#123; freopen("in.txt", "r", stdin); int n, m, c; while (scanf("%d %d %d", &amp;n, &amp;m, &amp;c) != EOF) &#123; for (int i=0;i&lt;m;i++) &#123; int index = i &lt;&lt; 1; scanf("%d %d %d", &amp;edge[index].a, &amp;edge[index].b, &amp;edge[index].c); edge[index+1].b = edge[index].a; edge[index+1].a = edge[index].b; edge[index+1].c = edge[index].c; &#125; memset(start, 0, sizeof(start)); memset(num, 0, sizeof(num)); m = m &lt;&lt; 1; sort(&amp;edge[0], &amp;edge[m], op); for (int i=0,o=-1;i&lt;m;i++) &#123; if (o!=edge[i].a) &#123; o = edge[i].a; start[o] = i; &#125; num[o]++; &#125; bfs(n); for (int i=0;i&lt;c;i++) &#123; int x, y; scanf("%d %d", &amp;x, &amp;y); int res = lca(x, y); if (res == -1) printf("Not connected\n"); else printf("%d\n", dist[x] + dist[y] - 2*dist[res]); &#125; &#125; return 0;&#125; 话说从 HDU 提交的结果上来看，树链剖分是最快的，LCT 次之，Tarjan 离线最慢。]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>Tarjan</tag>
        <tag>LCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（八）：Dynamic Control Flow in Large-Scale Machine Learning]]></title>
    <url>%2F2018%2F06%2F11%2F2018-06-11-tfunpacking8%2F</url>
    <content type="text"><![CDATA[前篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device TensorFlow 拆包（五）：Distributed TensorFlow 拆包（六）：RDMA TensorFlow 拆包（七）：Profiling 踩坑 &amp; Benchmark 严格上来说本篇不应该算在拆包里面，因为记的是 TF 团队最近发的一篇论文里面的东西。 前面拆包的第二篇记过关于 TensorFlow 中的数据流模型实现，实际上这套数据流模型已经是非常完备的，只是目前大家用 Python 搭出来的简单网络形式还很难把它的真正潜力发挥出来。 正当我们往这个方向做的时候，得，Google 发论文了。 这篇 Dynamic Control Flow in Large-Scale Machine Learning 发表在 EuroSys 18 上，系统结构方向的 B 类会议。 其实文中所提到的几乎所有内容都是 TensorFlow 原有的，或者说 TensorFlow 当初设计架构的时候就已经考虑到了未来这种使用方式的需求，这篇文章只是整理了一下这部分的设计思路（内容大部分跟以前发的控制流白皮书是一致的，见 TF 拆包第二篇），然后做了一定的测试，从实践上证明给大家看这样做是有效的。 Introduction首先提了一下深度学习中对控制流的需求，主要是像 RNN、MoEs 这样的任务中会明确地需要一些控制流的支持。但从更宏观的角度来看，使用动态控制流对任何应用都是有用的，理论上可以比较好地地把计算和通信部分给 overlap 开，尤其对提高异构系统（CPU、GPU、TPU等等）的计算效率是有很大的好处的。 目前常见的一些机器学习框架基本上都是用数据流图的方式来组织计算。 关于如何实现数据流的控制部分，主要有两种方式： in-graph 方式：例如 TensorFlow 和 Theano，控制流部分可以作为一个 op 嵌入到计算图中； out-of-graph 方式：这也是大多数框架的常规方式，包括 MxNet、torch、Caffe 以及 TensorFlow 的常规用法，控制流部分由更上层的 host 语言来完成（主要指 Python） 除了 TensorFlow 以外，别的框架似乎都很少用数据流这个词来指代自己的设计，可能原因就在这里？其他框架虽然整体计算还是以数据流图的形式做的，但并不是真正用一套数据流的运行时去支撑的。 用 TensorFlow 来举例，方式 2 的写法通常是： 1234python for i in range(xxx): xxx sess.run(train_step) xxx 多轮控制是写在 Python 层的代码中，每一次循环只跑训练的一步。 恐怕我们见到的大部分 TF 代码都是这个样子的吧。 方式 1 则是控制部分已经是图的一部分了，那最后我们只要 sess.run(total_train_step) 一次，就能够达到跟前面一样的训练效果。 单一的计算图更便于进行全图的优化，且这种实现能保证整个计算过程都停留在运行时里面（而不是像原先那样，跑一轮进退一次运行时，再跑一轮再进退一次运行时），减少很多不必要的开销。 数据流运行时的特性是一旦某个 op 的依赖都满足了，它就马上可以被调度执行了，在 out-of-graph 方式中，这种数据流的调度粒度只限定在一次 step 中，而 in-graph 方式甚至能把并行性扩展到多次 step 间，这样就能够最大程度地挖掘数据流异步、并行的能力了。 最初的 TensorFlow 白皮书中也有介绍过关于数据流部分的实现，但是并没有给出详细的设计方案以及测试结果，这篇文章就是把这部分补上。 总的来说，本文的内容包括： In-graph 动态控制流的设计，包括自动求导部分 In-graph 动态控制流在 TensorFlow 中的具体实现，包括在多种异构设备上分发的能力 对动态控制流性能的测试，并且分析多种不同选择带来的影响 关于如何用好动态控制流的使用经验 话说前两部分都是 TensorFlow 原有的。。。。。。 Design and Implementation2、3、4、5 章的大部分内容与 TensorFlow 拆包（二）：TF 的数据流模型实现 中记录的类似，就不多重复了。 需要额外提一下的是，由于跨 step 的 op 有可能被并行执行，这也就意味着可能要用上更多的内存。TensorFlow 的控制流中也考虑了内存的问题，建立在 GPU 上的 frame 如果使用的显存超过某个上限则会自动做与 CPU 的内存切换的动作，把不用的部分数据换出去，把接下来要用的数据换进来。 例如 tf.while_loop() 的函数接口中就有个 swap_memory 的参数。 6666666666…. Evaluation说实话，这篇文章的测试结果部分我觉得写的有点乱。 前面都是搬以前原有的内容，然后在本文的重点部分又写的这么乱，Google 的大佬们你们是认真的吗？ 测试的系统配置是 Intel 服务器配上 K40 和以太网，每个节点一块卡，某些例子中用到了 8 卡的 DGX-1 V100。 一开始的两个测试用的是构造出来的模拟算例。 图 11 的结果感觉有点迷。 图 12 是模拟 RNN 的结构，把一个类似 8 层的 RNN 计算分布在 8 块卡上，把 tf.while_loop() 支持的并行 iteration 数从 1 调到 32，可以发现并行性发挥出来之后效果确实是挺好的，最高大约有 5 倍左右的性能提升。并行 iteration 数为 1 的时候其实就相当于跟 out-of-graph 一样。 后面模型并行的测试是把一个实际的 8 层 LSTM 分布在 8 块卡上，具体的并行方式与图 12 的测试类似。在 1~8 块卡上分别测试，加速比也还可以。 接下来对一个单层 LSTM 的测试是对比是否开启内存交换。不开内存交换时，序列长度加到 600 就出现超内存的现象了，而开启内存交换则可以在保证能跑的前提下还不会损失性能。 从追踪出来的 profiling 结果中也能看到，在这种计算模式下内存拷贝和 GPU 计算 overlap 得比较好，这也是性能不受影响的重要原因。 再下一个测试是固定 LSTM 的序列长度为 200，调整 Batch Size 的大小来对比动态 RNN 和手动循环展开的效果。动态 RNN 稍微损失了一点点性能，但是差距不大。 另一方面动态 RNN 比手动做循环展开在内存方面有更大的优势，类似上一个测试，动态 RNN 开了内存交换之后可以跑更大的 Batch Size。 最后是对 DQN 强化学习网络的测试，尽管 DQN 现在已经不用了被其他更好的方法替代了（？？？），还是希望能从它的测试中展现一下动态控制流的效果。 DQN 中包含了多个网络，根据不同的情况需要做出许多不同的操作。使用动态控制流的方法把所有的操作都包含在一个计算图中之后，最终能够比原始情况得到 21% 的性能提升。 Summary总结来看，这篇文章重新整理了有关 TensorFlow 中控制流部分的实现思路，证明 in-graph 方式的纯数据流实现是有意义的。但是我对它的测试部分并不太满意，用到的是模拟的 workload，说服力不够，并且感觉测试的内容还是偏少。]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Dataflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（七）：Profiling 踩坑 & Benchmark]]></title>
    <url>%2F2018%2F04%2F10%2F2018-04-10-tfunpacking7%2F</url>
    <content type="text"><![CDATA[接上篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device TensorFlow 拆包（五）：Distributed TensorFlow 拆包（六）：RDMA 开始分析性能瓶颈了，本篇记录一下研究 TF 中自带的 Profiling 工具时遇到的几个坑点。 profiler大概 17 年 5 月左右，/tensorflow/core/ 中新加了一个 profiler 的目录，里面是把原本在 contrib 中的 profiling 工具移过来了，大概正式 release 应该是在 1.6、1.7 里面。 TensorFlow Profiler and Advisor 关于生成 profiling 的 context 文件详见 tf.profiler 相关的内容，这里直接开始记录怎么用 tfprof 这个工具。 试了一下 pip 包里面应该是没有单独包含的，需要从源码手动编译： 1bazel build //tensorflow/core/profiler 然后使用也是要从 bazel-bin 的目录中打开： 1bazel-bin/tensorflow/core/profiler/profiler --profile_path=xxxxx profiler_uiprofiler 的 README 中，示例代码除了 profiler 以外还有个 profiler_ui，基本上是一个类似 tensorboard 的网页前端，方便调用后端的 profiler 进行可视化查看用的。 这里虽然写着暂未开源，但是在 TensorFlow 的 github 总目录里面可以找到一个叫 profiler-ui 的项目，就是那个未完善开源的 ui 版了。 看了下，安装需要用到 go 以及 Google 自家的 pprof 工具，可能是因为耦合的其他部件比较多，所以暂时还没有并入 TF 的主代码中去。不过这里的 Installation 已经足够我们自己装上了。 装 pprof 的时候会有个坑点，CentOS 库中可以找到 gperftools 这个工具，也是 Google 提供的，yum 装上之后可执行文件的名字也叫 pprof ！！但是跟这里用到的 pprof 不是一个玩意！！ 之后按照示例上的说明： 1python ui.py --profile_context_path=xxxx 即可启用。 在我尝试使用它的时候，距离这个库上一次 git 的更新已经过去 1 个月左右了，不知道是 python 版本还是什么原因，直接运行可能会遇到找不到 server 的路径等等的 bug，直接在 ui.py 里面稍微改一下就好。 Profiling运行 TF 时保存出来的 profiling 文件包含了大量信息，主要有几个方面： scope：应该是 python 层代码中用 tf.name_scope() 包起来的视图 graph：TensorFlow 计算图的视图 op：把 TensorFlow 计算图再细化一层 code：Python 代码视图 默认会按列表把所选的视图中的一些信息给输出出来，另外用-output 选项可以指定输出成另外的格式： 12345678910tfprof&gt;xxx xxx -output timeline:outfile=xxxxx# 把结果输出成 chrome 用的时间线 trace 文件，可以在 chrome 地址栏中输入 chrome://tracing 打开# 只支持 graph、scope、code 这 3 种视图xxx xxx -output pprof:outfile=xxxxx# 把结果输出成 pprof 用的可视化文件（所以前面装 pprof 就是为了这个）# 只支持 code 这种视图# --------------------------------------------------------------------------# pprof 可视化文件之后可以用 pprof 来变成图片（猜测大概是类似 GraphViz 的数据结构）pprof -svg --nodecount=10000 --sample_index=1 xxxxxx.prof &gt; xxxxxx.svg profiler_ui 打开时的第一个页面就是 graph 视图生成的 timeline： 其中包含了计算图中每个 node 在卡上的情况，运行时间、数据流动依赖关系等等。（话说显示的太复杂了，事实上我觉得还是很难看） 然后默认的 scope 视图以及 code 视图得到的 timeline 我也感觉并没有什么用。 code 视图输出成的 pprof 图片倒是还可以看一下，但是感觉用处也不大 所以最后感觉还是不知道该怎么用好这套 profiling 工具 Options在 tfprof 界面直接回车可以看到默认的选项，然后这里面的内容都是可以改的： 12345678910111213141516171819202122tfprof&gt; -max_depth 10 -min_bytes 0 -min_peak_bytes 0 -min_residual_bytes 0 -min_output_bytes 0 -min_micros 0 -min_accelerator_micros 0 -min_cpu_micros 0 -min_params 0 -min_float_ops 0 -min_occurrence 0 -step -1 -order_by name -account_type_regexes .* -start_name_regexes .* -trim_name_regexes -show_name_regexes .* -hide_name_regexes -account_displayed_op_only false -select micros -output stdout: 稍微挑几个写一下： -max_depth：指定显示前多少个 node（配合下面的 -order_by ？） -step：profiling 记录的文件可能包含了很多个 step，用这个选项来指定当前分析哪个 step 的信息，默认 -1 是对所有 step 做平均 -order_by：打出来列表的时候，按照什么来排序： name：node 的名称 depth：node 在节点树中的深度 bytes：占用的内存数 peak_bytes：占用的峰值内存数 residual_bytes：计算完成之后，还剩下不释放的内存数 output_bytes：输出的大小 micros：node 计算所花费的时间 accelerator_micros：node 计算所花费的加速卡时间（区别于 CPU 的其他设备） cpu_micros：node 计算所花费的 CPU 时间 params：node 中包含的参数量 float_ops：node 所需要的浮点运算次数 occurrence：node 在图中出现的次数 -account_type_regexes：筛选出类型里面带有某些前缀的 node 有多少个 -start_name_regexes：筛选出名字中带某些前缀的 node -trim_name_regexes：隐藏掉名字中带某些前缀的 node -show_name_regexes：筛选出名字中带某些字符的 node -hide_name_regexes：隐藏掉名字中带某些字符的 node -select：选择视图中的哪些内容（有点像从数据库里面找东西的感觉），输出 timeline 的时候配合这个应该能够得到不同的数据： bytes：占用的内存数 peak_bytes：占用的峰值内存数 residual_bytes：计算完成之后，还剩下不释放的内存数 output_bytes：输出的大小 micros：计算所花费的时间 accelerator_micros：计算所花费的加速卡时间 cpu_micros：计算所花费的 CPU 时间 params： 参数量 float_ops：浮点运算次数 occurrence：在计算图中出现的次数 tensor_value：tensor 数据的值（估计需要配合 checkpoint 用） device：op 放在哪个设备上 op_types：op 类型 input_shapes：输入的形状 Trace抛开上面那个目前还没有正式 Release 的 Profiling 接口不说，实际可以用来做分析的是一套生成 trace_file 的 API。 用法也很简单： 1234567891011options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)run_metadata = tf.RunMetadata()_ = sess.run(optimizer, options=options, run_metadata=run_metadata)fetched_timeline = timeline.Timeline(run_metadata.step_stats)chrome_trace = fetched_timeline.generate_chrome_trace_format()with open(FLAGS.trace_file, 'w') as f: f.write(chrome_trace)print('Chrome Trace File write in %s' % FLAGS.trace_file) 在 RunOptions 中设置好追踪的级别，然后作为参数一起参与 Session.run()，最后记录得到的每个 step 的追踪数据通过 run_metadata 的结构返回出来。通过对追踪结果的解析即可生成我们可以理解的图形数据了，这个用的是 chrome 支持的 json 格式，在 chrome 地址栏中输入 chrome://tracing/ 即可很方便地查看，timeline 最后出来的效果跟上面的是一致的。 应该说，前面这个 Profiling 的 API 应该底层封装的也是这套机制。 在 DirectSession 中可以非常容易地找到与 trace_level 相关的代码： 123456789101112131415161718 std::unique_ptr&lt;DeviceTracer&gt; tracer; if (run_options.trace_level() &gt;= RunOptions::HARDWARE_TRACE) &#123; tracer = CreateDeviceTracer(); // tracer may be NULL on platforms without accelerators. if (tracer) &#123; Status s = tracer-&gt;Start(); if (!s.ok()) &#123; run_state.executors_done.Notify(); delete barrier; return s; &#125; &#125; &#125;... if (tracer) &#123; TF_RETURN_IF_ERROR(tracer-&gt;Stop()); TF_RETURN_IF_ERROR(tracer-&gt;Collect(run_state.collector.get())); &#125; 其中 DeviceTracer 是一个预留给多种设备来方便进行性能分析的接口，可惜的是目前里面的实现只有 GPU 的，需要依靠 CUDA 提供的 CUPTI 库。所以大概追踪过程中得到的与 CPU 相关的信息应该也是 CUPTI 附带的，如果是纯 CPU 版本的 TensorFlow，CreateDeviceTracer() 直接返回的是一个空指针。 Distributed由于分布式环境下的 Session 的执行模式与单机情况下有所不同，因而分布式下运行 trace 的工作方式也会有所区别。 MasterSession 中首次执行 PartialRun 时会初始化 PerStepState： 12345678910111213141516171819202122232425262728293031323334// If this is the first partial run, initialize the PerStepState.if (!run_state-&gt;step_started) &#123; run_state-&gt;step_started = true; PerStepState pss; const auto count = run_state-&gt;count; pss.collect_timeline = req.options().trace_level() == RunOptions::FULL_TRACE; pss.collect_rpcs = req.options().trace_level() == RunOptions::FULL_TRACE; pss.report_tensor_allocations_upon_oom = req.options().report_tensor_allocations_upon_oom(); // Build the cost model every 'build_cost_model_every' steps after skipping // an // initial 'build_cost_model_after' steps. const int64 build_cost_model_after = session_opts_.config.graph_options().build_cost_model_after(); const int64 build_cost_model_every = session_opts_.config.graph_options().build_cost_model(); pss.collect_costs = build_cost_model_every &gt; 0 &amp;&amp; ((count + 1 - build_cost_model_after) % build_cost_model_every == 0); pss.collect_partition_graphs = req.options().output_partition_graphs(); std::unique_ptr&lt;ProfileHandler&gt; ph = run_state-&gt;rcg-&gt;GetProfileHandler( run_state-&gt;step_id, count, req.options()); if (ph) &#123; pss.collect_timeline = true; pss.collect_rpcs = ph-&gt;should_collect_rpcs(); &#125; run_state-&gt;pss = std::move(pss); run_state-&gt;ph = std::move(ph);&#125; 这里会根据 trace_level 的值来设置一些标记。 下一步，pss 中的内容又会被写到 exec_opts 结构中： 1234567891011121314151617181920// Collect execution cost stats on a smoothly decreasing frequency.ExecutorOpts exec_opts;if (pss-&gt;report_tensor_allocations_upon_oom) &#123; exec_opts.set_report_tensor_allocations_upon_oom(true);&#125;if (pss-&gt;collect_costs) &#123; exec_opts.set_record_costs(true);&#125;if (pss-&gt;collect_timeline) &#123; exec_opts.set_record_timeline(true);&#125;if (pss-&gt;collect_rpcs) &#123; SetRPCLogging(true);&#125;if (pss-&gt;collect_partition_graphs) &#123; exec_opts.set_record_partition_graphs(true);&#125;if (pss-&gt;collect_costs || pss-&gt;collect_timeline) &#123; pss-&gt;step_stats.resize(partitions_.size());&#125; 这个结构会被封装在 rpc 的 call 中发送给 WorkerService 来处理。 在 Worker 的运行结构中，可以看到这样的代码： 1234567StepStatsCollector* collector = nullptr;if (request-&gt;exec_opts().report_tensor_allocations_upon_oom() || request-&gt;exec_opts().record_timeline() || request-&gt;exec_opts().record_costs()) &#123; collector = new StepStatsCollector(response-&gt;mutable_step_stats()); // TODO(mrry,pbar): GPU tracing for distributed steps.&#125; 好吧，StepStatsCollector 都已经创建了，但是可惜后续具体的 GPU tracing 部分还没有往里面完善。 BenchmarkTensorFlow 官方的 Performance 页 和 Benchmarks 页 中给出了官方测性能用的 benchmark 脚本： Github: tensorflow/benchmarks 基本上把目前 TF 里最高效的 API 都用上了，并且包含了各种常见的多机多卡方案，很值得作为高效的样例脚本来参照。 有个问题是这个库目前没有定期 release，各种更新全都合并到 master 分支里面去了，然后随着 TF 版本的不断更新，它的 master 分支是跟着 TF 的 master 分支走的。 因此要想正常跑最新的 benchmarks，就需要装 tf-nightly-gpu 包或者源码编译一个比较新的 TensorFlow 分支。 简单分析一下这份脚本的结构。 从 tf_cnn_benchmarks.py 这个入口进去之后，核心的执行流程在 benchmark_cnn.py 中。 只测试前向走 BenchmarkCNN._eval_cnn()，测试训练全过程走 BenchmarkCNN._benchmark_cnn() 。 前面的 FLAG 解析什么的直接略过，从训练部分开始看。 _benchmark_cnn首先构建计算图：(image_producer_ops, enqueue_ops, fetches) = self._build_model() image_producer_ops 是处理输入数据的部分，enqueue_ops 涉及到计算图中的流水线队列，最后的 fetches 是等一下 sess.run() 中的目标 op。 设置 tf.summary 以及 tf.train.Saver 等等，Saver 中传入的是 variable_mgr.savable_variables()。 创建 tf.train.Supervisor 时同时完成变量初始化，初始化 op 组包含： tf.local_variables_initializer 初始化本地变量 tf.tables_initializer 初始化用到的各种表（哈希表等等） 本地变量初始化之后，执行variable_mgr.get_post_init_ops() 完成自定义的一些初始化执行动作，这个部分要根据不同的参数维护算法来定 如果有同步用的队列 barrier，也一起在这里完成初始化 之后 sv.managed_session 开始真正的执行循环： 123456789with sv.managed_session(...) as sess: 如果使用的是真实数据，则往 enqueue_ops 中插入提取数据到队列的 op 初始化 global_step 创建一个 global_step_watcher，在新线程中监控 global_step 的变动情况 while not done_fn(): 这个函数与 global_step_watcher 相关，用于控制训练循环什么时候结束 ... benchmark_one_step() 训练一个 step ... 后续再处理一些收尾内容 _build_model回到第一步看一下计算图的构建部分。 (image_producer_ops, image_producer_stages) = self._build_image_processing(shift_ratio=0) 创建输入数据。 1234对于当前设备上的每一块 GPU，variable_mgr.create_outer_variable_scope() 创建命名域： add_forward_pass_and_gradients() 添加网络的前向部分，并且计算得到梯度 根据当前的任务是训练还是预测，处理准备网络中需要返回的内容 从计算图中提取出 Batch Normalization 的更新部分，添加到 0 号卡的更新部分中，BatchNorm 只需要一块卡来计算 如果图中用了 staging_area 的数据组织方式，这里另外再添加一下，扩充 enqueue_ops。 fetches = self._build_fetches() 最终收集前面所有的信息，构建出等一下需要传入 sess.run() 中去的目标 完成前面的内容后，把 image_producer_ops，enqueue_ops，fetches 三部分内容返回给上一层的函数。 add_forward_pass_and_gradients创建随机数据作为输入，或者处理传入的数据产生器。 logits, aux_logits = self.model.build_network() 构建完整的前向网络。 添加输出结果以及计算 loss 误差。 variable_mgr.trainable_variables_on_device() 获取当前 GPU 上所有的可训练参数。 如果当前是最后一块 GPU 卡，那么再额外计算 L2_loss，添加到前面的 loss 中去，L2_loss 只需要计算一次。 grads = tf.gradients(scaled_loss, params, aggregation_method=aggmeth) 根据前面收集的当前 GPU 上的可训练参数信息构建反向的梯度计算图，返回得到的是图中所有的梯度。 接下来再获取一次 variable_mgr.trainable_variables_on_device() ，然后把得到的参数与前面的梯度打包在一起返回回去，准备接下来的参数更新。需要注意的是，第一次调用 trainable_variables_on_device 时传入了一个 writable=False 的参数，这里传入的是 writable=True，在某些特别的多卡参数管理算法中，用于梯度计算和最终梯度更新写回的目标是不一样的。 所有前面的这些都封装在一个 results 的 dict 中返回回去。 _build_fetches这里算是计算图构建的收尾部分了，传入的内容是所有 GPU 上计算图的合集。 variable_mgr.preprocess_device_grads() 预处理出需要在哪些设备上执行梯度更新操作。 123456对于梯度更新设备中的每一块 GPU： tf.reduce_mean() 计算前面所有卡上梯度的平均值 variable_mgr.get_gradients_to_apply() 获取有哪些梯度是要在当前设备上更新的 get_learning_rate() 计算学习率 get_optimizer() 获取梯度更新用的 Optimizer variable_mgr.append_apply_gradients_ops() 应用 Optimizer 进行梯度更新 把前面所有的东西打包在 fetches 这个 dict 中返回回去。 VariableMgr可以看到上面有很多核心的操作都是通过 variable_mgr 结构完成的，这套脚本定义了一个 VariableMgr 类，想要自己修改参数管理、更新的算法只需要重写这里面的一些函数即可。 前面出现过的比较有用的几个接口函数： def create_outer_variable_scope(self, device_num) 封装变量命名域，主要用于维护变量创建时要做的事情，一般情况下直接返回一个普通的 tf.variable_scope，需要对变量创建进行额外操作的话需要自己构造一个 custom_getter 作为参数传入tf.variable_scope。 def preprocess_device_grads(self, device_grads) 预处理出需要做梯度更新操作的设备，以及对应设备上的梯度和参数。 def get_gradients_to_apply(self, device_num, gradient_state) 与上一个函数对应使用，用于找出每个设备需要处理哪些参数更新任务。 def append_apply_gradients_ops(self, gradient_state, opt, grads, training_ops, loss_scale_params) 在设备上针对每一对需要更新的变量及其梯度，应用 apply_gradients 操作。 def get_post_init_ops(self) 用于额外附加一些在所有变量完成初始化之后，开始训练之前，需要执行的操作。 def get_devices(self) 返回当前节点中可用的 GPU 列表，在某些 PS-WORKER 的实现方式中，返回的是 tf.replica_device_setter 的封装。 def savable_variables(self) 返回哪些变量是需要被 tf.Saver 保存进检查点的。 def trainable_variables_on_device(self, rel_device_num, abs_device_num, writable=False) 返回当前设备上的可训练参数（即能计算梯度，可以进行反向更新的参数）。输入的两个 device_num 分别是 GPU 在当前节点中的编号以及在全局环境中的编号。 writable 用于标识需要被写回更新的参数，在有些情况下图中可能存在多份参数备份，writable 为 False 时返回的是图中用于求梯度以及构建反向数据通路用的参数，为 True 时返回的是等一下 apply_gradients 需要应用梯度更新操作的参数。 replica_device_setter &amp; variable_scope-custom_getter前面建图时用到的两个很重要的接口，用于额外处理 op 在设备上的分配操作。 benchmarks 脚本中的用法大概是这样： 123456for device_num in range(len(self.devices)): with self.variable_mgr.create_outer_variable_scope(device_num): # ... with tf.device(self.devices[rel_device_num]): # self.devices[] 里面是实现设好的 tf.replica_device_setter #...build_network 对节点中的每一块 GPU 卡，首先套上一个 variable_scope（里面可能会使用到 custom_getter），在构建 op 时再套一层 replica_device_setter。 tf.replica_device_setter 需要配合 tf.device 使用，作用范围是其 python 作用域以内的所有 op，这个函数简单地说就是对传入的 op 进行判断，如果是计算型的 op 就正常分配在运算设备上，如果是需要在 PS-WORKER 之间共享的参数型 op 则需要在参数服务器上。它的返回值是需要分配给的 device 的名字，所以直接用 tf.device 指定即可。 123def replica_device_setter(ps_tasks=0, ps_device="/job:ps", worker_device="/job:worker", merge_devices=True, cluster=None, ps_ops=None, ps_strategy=None) 具体的源码实现上，主要是对新创建 op 的类型进行判断，如果在 ps_ops 包含的范围内（为 None 时会用一个 STANDARD_PS_OPS 来作为检查范围）则用某种 ps 分配策略放到参数服务器上，否则放到默认的计算设备上。 默认的 ps_strategy 不指定的话就是用的 round-robin，简单地说就是按顺序依次分。 tf.variable_scope 中 custom_getter 的作用范围就只限于作用域以内所有的 tf.get_variable 调用了（注意，必须是 tf.get_variable，这个对 tf.variable 是无效的）。前面 replica_device_setter 只是指定了参数存放的位置，这里则可以对参数创建进行更多的改动。 例如 StagedVariableGetter 做的事情就是把变量封装上一层 StagingArea，计算图中需要读取变量的时候返回一个 StagingArea.get，对于 apply_gradient 这种需要修改变量本身的操作，则返回参数本体（也就是前面看到的 writable 这个参数起作用的方式）。 VariableMgr instances官方的 Benchmark 脚本中提供了 8 种内置的 VariableMgr 实例。 VariableMgrIndependent不同卡之间完全不作数据交互，单纯用来测单机多卡的理论计算速度用。 不需要封装 custom_getter 和 replica_device_setter。 VariableMgrLocalFetchFromPS多卡中的参数统一存储，不同卡在计算时直接从统一的 PS 中读取需要的数据。 不需要封装 custom_getter。 get_device 这里： 123456789101112if self.benchmark_cnn.local_parameter_device_flag == 'gpu': return [ variable_mgr_util.ParamServerDeviceSetter(d, raw_devices) for d in raw_devices ]else: return [ tf.train.replica_device_setter( worker_device=d, ps_device=self.benchmark_cnn.param_server_device, ps_tasks=1) for d in raw_devices ] 如果参数存放在 CPU 上，直接对每个 GPU 设备返回一个指定好 ps_device 的 replica_device_setter。 如果选择参数存放在 GPU 上，这里的做法是将所有参数均衡负载平分在各块卡上。 VariableMgrLocalFetchFromStagedPS多卡中的参数统一存储，相比之前的增加了 StagingArea 的流水线操作。 custom_getter 中为每个变量额外创建了一个 StagingArea，计算图中需要读取变量的时候返回对应的 StagingArea.get。 trainable_variables_on_device 中 writable 为 True 时，返回变量本体，否则返回对应的 StagingArea.get。 其他部分与上一种方式相同。 VariableMgrLocalReplicated每块卡上的计算图完全独立，各卡都是自己存储自己的参数，梯度更新的时候再采用某种 Allreduce 的算法对各卡上的参数做统一规约。 get_post_init_ops 在初始化完成后拷贝 GPU0 上的参数到其他卡上覆盖掉，保证所有卡的初始参数一致。 不需要封装 custom_getter 和 replica_device_setter。 preprocess_device_grads 中返回的梯度是调用某种规约算法去综合所有卡上的梯度值，之后再跟本地的参数一起交给 apply_gradient 去更新即可。 因此这里的计算流程是，初始所有卡上参数一致，训练完一步之后规约梯度，规约完成后所有卡上得到的梯度也都一致了，再 apply 更新到本地的卡上，这样下一步开始时所有卡上的参数仍然是一致的。 VariableMgrDistributedAllReduce用于分布式。 这是脚本中唯一一种需要用到 single_session 的模式，基本上跟 Replicated 的方式一致，每块卡上都独立存数据，更新时全局规约，特殊点在于这种方式只需要由一个 python 进程来启动，所有 worker 上的图构造等等都是由一个 controller 的角色完成，其他所有的 worker 都像平时的 ps 一样 join_server 即可。 大体实现上跟上一种一致 VariableMgrDistributedFetchFromPS用于分布式。 大体实现跟单节点的 FetchFromPS 一致。 custom_getter 使用了 OverrideCachingDevice，虽然由 replica_device_setter 指定好了所有参数都保存在 ps 上，但是在 worker 还可以做一次数据缓存。caching_device 这个参数与 tf.get_variable() 中的参数对应，即 worker 端的多块卡从远程 ps 获取数据只在第一块卡拉取数据时通过网络取一次，后续的几次直接从缓存中读取。缓存数据的分配方案跟单节点 ps 时 CPU/GPU 上存储参数的方案类似。 VariableMgrDistributedFetchFromStagedPS用于分布式。 在上一种的基础上加上了 StagingArea。 VariableMgrDistributedReplicated用于分布式。 计算流程其实跟 DistributedAllReduce 是一致的，大体上跟前面类似实现相一致。]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（六）：RDMA]]></title>
    <url>%2F2018%2F03%2F12%2F2018-03-12-tfunpacking6%2F</url>
    <content type="text"><![CDATA[接上篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device TensorFlow 拆包（五）：Distributed 本篇分析 TensorFlow 1.6.0 中的 RDMA 以及其他的传输优化的实现。 虽然我前面一篇论文做的工作也是这个，在当时 Yahoo 实现的 RDMA 版还没有收录进官方 Repo 的时候稍微做过一下对比，现在来看看收进官方库之后的具体实现是什么样的。 之前对 GPU Direct 的 API 不是很了解，后来才发现这里的 RDMA 实现其实也是直接支持 GPU Direct 的。 VerbsServerTensorFlow 分布式环境中的 Server 结构通过 ServerFactory 构建，这个工厂模式会根据传入的 protocol 选项创建不同的 Server。 当选择 protocol = grpc+verbs 时，tf.train.Server()创建 VerbsServer。 在代码里面搜 public ServerFactory 可以找到 4 个继承类，除了 verbs 之外，其他分别是： protocol = grpc 对应 GrpcServer，即原本默认的 gRPC 通信方式 protocol = grpc+mpi 对应 MPIServer，用 MPI 来完成通信 protocol = grpc+gdr 对应 GdrServer，即 GPU Direct 支持 从 protocol 的名字上也可以看出来，这几个新增的通信方式实现都还需要依赖 gRPC，例如 RDMA 一开始的 IB 卡配置、连接建立什么的都还需要先用 gRPC 来完成。 VerbsServer()在构造时直接初始化了一个GrpcServer()，之后的Init()、Start()也都是先启动 GrpcServer 中对应的方法。同时创建一个 VerbsService（重载的 ServerCompletionQueue 类）。 然后创建一个 RdmaMgr() 用于管理 RDMA 底层的连接，RdmaRendezvousMgr()用于管理数据存储。 RdmaMgrRdmaMgr 中维护了 RdmaAdapter 和 RdmaChannel 这两个结构，RdmaAdapter 负责维护 RDMA 通信需要的底层结构（rdma device context、protection domain、事件 channel、完成队列等等），RdmaChannel 则是代表每一个独立的 RDMA 连接。 RdmaMgr 初始化时对所有不在本地的 worker 创建一个 RdmaChannel（创建和设置 queue pair，初始化 buffer），插入到 channel_table_ 中，。 VerbsServer 启动时，首先启动 GrpcServer（GrpcServer::Start()），再通过 RdmaMgr 来建立 rdma 通道之间的链接（rdma_mgr_-&gt;SetupChannels()），之后在rdma_mgr_-&gt;ConnectivityCheck()中，分别测试各个 rdma 链接的连通性，然后用rdma_adapter_-&gt;StartPolling()启动 RdmaAdapter 中的守护进程，等待后续 RDMA 传输信息，后面 Grpc 的传输通道应该就不需要再用了。 RdmaRendezvousMgrRdmaRendezvousMgr 继承于 BaseRendezvousMgr，具体的实例类是从 BaseRemoteRendezvous 继承过来的 RdmaRemoteRendezvous，整个 RDMA 的传输过程由 RdmaRemoteRendezvous::RecvFromRemoteAsync() 开始。 基本上做 RDMA 优化的想法都是一致的，跟我们之前的实现很类似，其他的详见官方文档： How to compile, use and configure RDMA-enabled TensorFlow Memory Management说起来，事实上 RDMA 的移植过程中的最大问题还是内存，因为要用 RDMA 就需要事先把内存注册到 IB 卡上，这样 IB 卡才能够有权限直接读写内存并且保证这块被注册过的内存不会被换页换出去。 所以对于他们的 RDMA 实现来说，我最关心的还是这个部分。 TensorFlow 原本的实现就是在传输的时候动态给 send 操作分配内存。 通常的思路有三种： 数据传输的时候现场把需要被传输的 Tensor 内存注册到 IB 卡上，用过以后再释放掉。这样很明显会有很大的 overhead。 提前注册一块固定的内存，传输的时候把需要发送的数据 copy 进去，然后在那块预注册的内存上进行 RDMA 操作。Overhead 会小一点，但是中间需要的 memcpy 操作还是有点浪费。 内存池！！手动维护一个内存池，实现就把内存池注册到 IB 卡上，然后传输过程中，动态申请内存、用后动态释放内存都从内存池里面走。 想想都知道第三种方案是最好的，然而我们当时实现时没有成功把内存池写出来（后来，这事就成了挖好了但是一直没填上的大坑了【写着玩之 内存池】、【写着玩之 RDMA 轮子】。。。。。。很难受），所以其实采取的是 1、2 混合的方案。 当要发送的数据比较少的时候，memcpy 速度快，所以拷过去再发效果好点，发送的数据大的时候，就现场注册。然后我们还分别试了用了 RDMA_WRITE 和 RDMA_READ 这两种方式，想办法把注册内存的时间跟计算、通信这些 overlap 开（虽然效果很有限，但是能做一点是一点咯）。 官方库里目前的实现用上了内存池： 对于 DMAable Tensor（注册在支持 RDMA 的 CPU 上或者注册在支持 GPU Direct 的 GPU 上的 Tensor），都采用直接从源 Tensor 写到目标 Tensor 中的方案，完全避免了内存注册和内存拷贝。（66666666） 非 DMAable 的 Tensor，用 Protobuf 序列化之后通过预注册的内存传输（方案 2）。 不支持 GPU Direct 的 GPU 数据，虽然还是要拷回 CPU 端，但是 CPU 到 CPU 的传输用的还是 RDMA。 Memory Pool那么就来详细看一下这里的内存池实现，这里实际上是借用了 TensorFlow 本身自带的内存池（。。。嗯，我前面挖了很长时间的内存池坑到时候就扒这个来填吧）。TensorFlow 本身的内存分配就是通过自己维护的内存池来完成的。 关于 TensorFlow 的内存管理实现，可以参照这里的说明，主要采用的还是比较简单的 BFC 算法。 tensorflow/core/framework/allocator.[ch] 是内存分配器的主要入口，具体的分配器实现在 tensorflow/core/common_runtime/ 中的 xxx_allocator.[ch] 中。 RDMA 部分的内存管理由一个单例模式的 RdmaMemoryMgr 结构来完成。 在 VerbsServer::Start() 中有这么一步 rdma_mgr_-&gt;InitAllocators()： 首先获取到本地所有的 Allocator，CPU 分配器以及可能的 GPU 分配器。 把 RdmaMemory 中插入/删除 Memory Region 的操作（主要是处理内存到卡上注册的这一步）包装成 Visitor 的接口函数。这里的 Memory Region 跟 TensorFlow 自己内存池里面的不是一个概念（指的目标倒可能是一致的），而是指的一整块独立内存（一次性用 ibv_reg_mr 把一整块内存注册好，一块内存对应一个 mr）。 后面再检查当前环境是否支持 GPU Direct，是则同样把 GPU 的 Visitor 也加上。 之后需要用 RDMA 进行传输的时候，就可以从已有的 Memory Region 表中找出某块内存所对应的 mr、rkey 等等，而无需再注册了。 Transport protocolTensorFlow 中的传输设计是异步发送，阻塞接收，这个设计也很容易理解，没有收到数据之前，当前节点之后的肯定执行不了，而发送者将内容提交之后数据流图到这里为止就结束了，可以把计算资源用于其他地方。 这张图来源于代码中的文档部分。 RecvFromRemoteAsync一次完整的传输过程从 RdmaRemoteRendezvous::RecvFromRemoteAsync() 开始： 解析某条需要传输的 Tensor 的源地址和目标地址，确认目标地址是本机。 从 rdma_mgr_ 记录的 channel 中找出源地址对应的传输通道。 向传输通道中发送一条 Tensor 请求（构造一个新的 RdmaTensorRequest 并插入到 channel 的 request_table_ 中，启动 RdmaTensorRequest-&gt;Start()）。 接收端这边的所有操作都封装在 RdmaTensorRequest 结构中，相对的，发送端这边用来响应请求的所有操作都封装在 RdmaTensorResponse 结构中。 Start() 首先在 RdmaMemoryMgr 中检查本次需要接收的 Tensor 的Meta Data 是否存在（用 Rendezvous 的 key 作为关键字）。Meta Data 记录的是所传输的 Tensor 的详细信息： 1234567class TensorMetaData &#123; public: TensorShape tensor_shape_; DataType data_type_; size_t proto_size_; bool is_dead_;&#125; 最主要的内容是形状（TensorShape）和数据类型（DataType）。如果 Meta Data 记录存在，则可以直接新建一个 Tensor（构建 Tensor 需要传入的参数是 Allocator、数据类型以及形状）。从 RdmaMemoryMgr 的 Memory Region 表中直接可以查到所分配的内存的 mr 信息。如果本机不支持 GPU Direct，则把目标改成本地 CPU 端的内存接收；如果当前传输的 Tensor 的数据类型甚至都不支持 memcpy，则把它先序列化成 Protobuf 再传输。 话说这里序列化是现场 malloc 内存，现场 ibv_reg_mr，为什么不直接从 CPU 的 Allocator 里面分配呢？ 之后发送 RDMA_MESSAGE_TENSOR_REQUEST 消息。消息中包含的内容除了 key 之外，最重要的就是前面准备好的 Tensor 地址和 rkey 了，这个标识了远程的发送端等一下要把数据写到什么地方去。 Sender Side Recv Message守护进程 RdmaAdapter::Process_CQ 处理所有传入的 RDMA 消息，接收到 RDMA_MESSAGE_TENSOR_REQUEST 时，解析出收到的数据，构造一个新的 RdmaTensorResponse 并插入到 channel 的 responses_table_ 中，启动 RdmaTensorResponse-&gt;Start()）。 这一步调用 RecvLocalAsync() 从本地的 Rendezvous 中异步获取接收端所请求的 Tensor 数据，成功完成本地数据提取之后用 RdmaTensorResponse::RecvHandler() 开始准备数据的回传： 对比本地提取的 Tensor 数据与接收端需要的 Meta Data 的各项是否一致（话说为什么会有不一致的情况呢？） GPU： 数据一致且支持 GPU Direct：同步一次 GPU 流，完成后直接用 RdmaTensorResponse::SendContent() 发出去； 数据一致但不支持 GPU Direct：把 Tensor 从 GPU 上拷到 CPU 上，发出去； 数据不一致：需要重新请求同步 Meta Data，但是这条 Tensor 在数据流图中可能还会被其他节点的运算改变，因此需要先把 Tensor 从 GPU 上拷出来，然后发送更新后的 Meta Data 信息给接收端； 数据不支持 memcpy：把 GPU 数据序列化之后发送出去。 CPU： 数据一致：直接发送； 数据不一致：发送更新后的 Meta Data； 数据不支持 memcpy：序列化之后发送。 这个 Meta Data 重新请求的部分可以改成接收端直接用 RDMA READ 来抓取。 发送完成之后回收用过的资源，然后把当前 RdmaTensorResponse 从 responses_table_ 中删掉。 RecvTensorContent接收端收到传入的 Tensor 数据之后，根据是否需要从 CPU 拷回 GPU 或者是否需要反序列化等等，做出对应的操作，调用 RdmaTensorRequest 创建时上层传入的 done() 回调函数，最后回收资源，把当前 RdmaTensorRequest 从 request_table_ 中删掉。 看到这里思路可以说是很清晰了，感觉上这里的 RDMA 实现也已经相当完善了（比我们当时做的好多了），细节上可能还能再抠一抠，不过再往上应该不会再能有什么大的性能提升了。]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（五）：Distributed]]></title>
    <url>%2F2018%2F03%2F09%2F2018-03-09-tfunpacking5%2F</url>
    <content type="text"><![CDATA[接上篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device 单节点的运行流程基本上已经有个大体印象了，接着就要来拆我所关注的重点所在——分布式运行时了。 Architecture进入代码部分之前，首先看一下官方文档中，对整个 TensorFlow 结构的介绍。以下是一个典型的分布式 TensorFlow 的架构图： 这里的 Master 和 Worker 服务只在分布式运行时中有，可以认为单节点的 Session 是包含了这两个服务的全部内容。 Client 指的是面向用户的前端编程接口，通常能用的就是 Python 和 C++ 了，client 完成运算图的构建，然后把图的定义通过 session 对象用tf.GraphDef这个 Protobuf 结构传给后面的 Master 服务来跑（即 Python 层定义好计算图，然后通过 Protobuf 的接口进入 C 部分的运行时）。 所以源码中会有/tensorflow/python/client这个目录，其中的内容做的就是架构图中 client 这个概念的任务。 分布式环境中的 Master 有以下几个任务： 精简并且优化计算图，根据当前次 client 提交运行的输入输出目标，提取出一个子图来 把子图划分到硬件上（graph -&gt; Partition -&gt; Device） 缓存前面那一步的结果，以便以后的 steps 能够重用而不用再把上面两步再执行一遍 这些事情看上去好眼熟啊…没错！这就是DirectSession 中 Executor 的任务啊！ 在 tensorflow::Partition()中，划分子图到硬件上，对于不在同一个设备上的边需要补充一对 send/recv 的 op 接口，例如上面那张目标图： Master 接下来再把任务分给具体的 Worker 服务来完成。每一个 Worker 服务都有自己对应的 tasks，ps 负责存储数据，worker 负责具体的计算。 注意 Worker Service 和 Worker task 的区别，Worker Service 可以有 ps 和 worker 这两种 tasks。 截两张实际运行中 Client、Master、Worker 的关系图。 上面一种做法是在 ps 和 worker1 中调用 Server.join()，建图等等的事情全部由 worker0 这边的一份代码完成，大概可以理解成某些代码中的 “Single Session” 的模式。 更常见的写法是下面这种，在 ps 上开 Server.join()，然后每个 worker 分别跑一遍完整的 Python 脚本，自己构建自己本地的计算图。 那我们看到 Client（Python 脚本）运行时是通过本地的 Master Service 来管理整个计算过程，Master 相当于单节点环境的 Session 的角色，由它去分配任务给本地的 worker 或者远程的 worker。 Worker 服务有以下几个任务： 处理 Master 交过来的请求 拿到自己的子图之后，调度其中的 op 完成具体的计算 处理与其他 task（即其他的 Worker 服务）之间的数据通信问题 第 2 步的详细处理也是前面分析过的，即ExecutorState 的 RunAsync() 和 ScheduleReady()部分处理的事情了。 关于 send/recv： CPU 和 GPU 之间通过 cudaMemcpyAsync() 来 overlap 计算和数据传输 两个本地 GPU 之间通过 DMA 直接传输 在 task 之间（不同的 Worker 服务、不同的计算节点之间）通过 gRPC 或者后来增加的 RDMA 来传输 Master 和 Worker 简单地想可以认为是把 DirectSession 中的 Executor 相关的结构功能给拆了出来。 接下来看看具体的代码实现。 tf.train.Supervisor()代码从单节点改造成分布式只需要替换掉几个固定的 API 即可，先从 Python 层 API 的 Supervisor 说起。 Supervisor 是一个对 Coordinator、Saver、SessionManager 等结构的封装类，用于管理运行的分布式 Session，在运行中建立检查点，并处理异常情况的恢复等等。 1.6.0 版用这个 API 的时候会有警告说将在未来移除，建议换成 tf.train.MonitoredTrainingSession，但是改用这个新 API 实测性能会下降一截，可能是配置方式需要做一下改变，暂时先放下不作研究。 Supervisor 的构造函数有一堆输入参数，挑几个比较重要的记一下： graph：运算图，不指定则使用默认图（这个跟单机版一致） is_chief：分布式环境中可能存在多个 worker 节点，但是其中需要有一个作为 chief worker 节点。chief 需要负责初始化整个运行图，其他 worker 节点将从 chief 节点获取计算图的信息 init_op：图中用于初始化所有变量的 op summary_op：用于收集整个运算过程中的有关信息的 op saver：chief 将把有关的信息写到 log 中去 global_step：在分布式环境中全局共享的一个变量，标识当前跑到了第几次迭代 session_manager：用于管理执行具体运行的 session，也负责异常恢复等等，如果不指定则会创建一个 创建结束时，Supervisor 所关联的计算图将会被锁定，不再允许修改，因为这个图可能会被多个线程共享。 Session分布式环境下的 C 运行时中存在 3 种 Session 结构，分别是 WorkerSession、MasterSession 以及 GrpcSession，基本上跟前面的 Architecture 是能对应起来的。下面从它们在代码中的调用顺序开始分析： WorkerSessionWorkerSession 在创建 tf.train.Server()时就被构造出来。 C 层面的 Server 是一个用于管理当前进程中的 Master 和 Worker 服务的结构，通过Start()、Stop()、Join()构成了下图的状态机： 1234567// Join() Join()// ___ ___// Start() \ / Stop() \ /// NEW ---------&gt; STARTED --------&gt; STOPPED// \ /// \________________________/// Stop(), Join() GrpcServer 在被初始化时： 检查当前可用的所有计算设备，构建 device 列表（与 DirectSession 中做的 AddDevices()一致） 创建了 RpcRendezvousMgr 检查传入的 cluster 信息中，其他 tasks 的端口等等的信息 注册一个 Grpc 的通信 server 创建 Master 以及 GrpcMasterService 创建 GrpcWorker 以及 GrpcWorkerService 启动 Grpc 的通信server 创建 WorkerCache 创建一个 SessionMgr，并随后在这个 SessionMgr 中创建 WorkerSession 这里没有马上创建 MasterSession，而是保存好创建 MasterSession 所需要的信息（大概是因为 ps 中不需要 Master？而 Worker 是所有节点都要有的） 创建 LocalMaster Work 类用于管理 WorkerSession、处理子图、运行子图、接收 Tensor 数据。GrpcWorker 继承了 Worker 类之后重载了其中的数据传输部分，添加的是一个额外的传输方法，用于在传输大数据时不经过 Protobuf 序列化而直接传（调用 send/recv op 的接口的话，应该是默认要序列化之后再传吧）。 GrpcWorkerService 重载的是 AsyncServiceInterface 这个类，AsyncServiceInterface 抽象的是一个异步等待服务，即创建一个新的线程，用 polling 循环来等待传入的 RPC 请求。 GrpcWorkerService 底层关联的是 WorkerService 这个通过 Protobuf 定义用于 RPC 的结构。 WorkerSession 相对而言反而是个比较简单的结构： 1234567891011121314151617181920212223242526272829// WorkerSession encapsulates all of the state relating to a given session.struct WorkerSession &#123; // The name of the session. const string session_name; // The name of the worker. E.g., /job:mnist/replica:0/task:1. const string worker_name; // Object from which WorkerInterface instances can be obtained. const std::unique_ptr&lt;WorkerCacheInterface&gt; worker_cache; // Collection of local devices. These devices are typically RenamedDevices // in all except the SessionMgr.legacy_session_. legacy_session_.device_mgr // == worker_env_.device_mgr, which holds the true devices. const std::unique_ptr&lt;DeviceMgr&gt; device_mgr; // graph_mgr keeps track of the registered graphs of this session. // // Note: graph_mgr must be deleted before rendezvous_mgr! // Note: graph_mgr must be deleted before device_mgr! const std::unique_ptr&lt;GraphMgr&gt; graph_mgr; std::unique_ptr&lt;ClusterFunctionLibraryRuntime&gt; cluster_flr; WorkerSession(const string&amp; session_name, const string&amp; worker_name, std::unique_ptr&lt;WorkerCacheInterface&gt; worker_cache, std::unique_ptr&lt;DeviceMgr&gt; device_mgr, std::unique_ptr&lt;GraphMgr&gt; graph_mgr);&#125;; 保存了名字啊、worker_cache啊、device_mgr啊、graph_mgr啊这样的内容。 GrpcSession下一个断点首先是 GrpcSession 再被触发。 分布式环境下对应的 Session 结构为 Supervisor 中创建的managed_session()，对于 chief 节点，调用自己 SessionManager 中的 _restore_checkpoint() 来在 C 层面创建出 GrpcSession 结构，并且负责完成图的构建等等，之后检查本次运行是否有对应的检查点，有则把检查点的信息恢复出来。而非 chief 节点调用的是wait_for_session() ，创建 GrpcSession 之后等待 chief 节点完成图的构建。 GrpcSession 是从 Session 类继承出来的，其负责的任务跟单机版中的 DirectSession 很像，跟它是同一个层级的东西。 或者说 Session 类在整个 TensorFlow 架构中更确切的应该叫它 Client Session，它们与 Python 层的 sess = tf.Session() 这种结构是直接对应的，是用户编程界面与 TF 运行时的入口。 但 DirectSession 发挥功能的函数都是在本身中直接定义出来的，而这里的 GrpcSession 却可以说基本上是围绕 MasterService 的封装。通过 MasterInterface 来调用 MasterService 的功能来完成任务，可以说 GrpcSession 只是最上图中架构中 client 与 Master 服务之间的接口。 这里的 Master 接口有两种，LocalMaster 用于进程间的直接通信，GrpcMaster 用于 Grpc 通信，GrpcSession 在创建时会根据选项选择所需的 MasterInterface。通常情况下，由于 GrpcSession 都是是直接跟本地的 Master 进行交互，所以默认添加的是 LocalMaster。 MasterSession上面managed_session()在创建完 C 层面的 GrpcSession 返回之后，会很快执行一次 sess.run()，有检查点的情况是恢复检查点时的变量数据，没有检查点时是执行 init_op 来完成变量初始化。 这里执行的 sess.run()与单节点版本的行为相同，需要首先执行_extend_graph()，不同的是这里执行的是tensorflow::GrpcSession::Extend()，最终到tensorflow::LocalMaster::CreateSession()、tensorflow::Master::CreateSession()。 话说 TensorFlow 中跟 Master 这个概念相关的结构有一堆，一层套一层，而且功能上跟 Worker 又有很多区别的地方。类比起来，大概 MasterSession 也就是跟 Executor 比较像，每一次 Client Session 要 Run 一个子图时（sess.run(...)），启动一个 MasterSession。 MasterSession 追溯到最后是由 GrpcSession.Extend()、GrpcSession.Create()在构建运行图或者修改运行图的时候创建。调用栈大概是这个样子，层次看起来还是比较乱： 12345tensorflow::GrpcSession::Create() -&gt;tensorflow::GrpcSession::CreateImpl(): master_-&gt;CreateSession() -&gt;tensorflow::LocalMaster::CreateSession(): master_impl_-&gt;CreateSession() -&gt;tensorflow::Master::CreateSession() -&gt; （在一个闭包中运行）tensorflow::MasterSession::Create() 注释中对 MasterSession 的介绍是： 负责分配 node 到 device 添加额外的边（例如 send/recv） 发射 commands 给 worker 来运行 具体来看，还是从sess.run()入手： 12345678tensorflow::GrpcSession::Run() -&gt;tensorflow::GrpcSession::RunHelper() （开始准备 req 和 resp，用于异步请求和响应的结构）-&gt;tensorflow::GrpcSession::RunProto(): master_-&gt;RunStep() -&gt;tensorflow::LocalMaster::RunStep(): master_impl_-&gt;RunStep() -&gt;tensorflow::Master::RunStep() -&gt; （在一个闭包中运行）tensorflow::MasterSession::Run() -&gt;tensorflow::MasterSession::DoRunWithLocalExecution() -&gt;tensorflow::MasterSession::ReffedClientGraph::RunPartitions() 最后的 ReffedClientGraph 是与计算图和 Worker 相关的内容了，具体的实现相当复杂，封装层次也是特别多，大致看了下RunPartitions()这里的注释： 匹配 fed tensors 和它们在 req 中的 index 给每个 partition 准备一个将发给 worker 的 call 通过tensorflow::MasterSession::ReffedClientGraph::Part::worker（这是一个 WorkerInterface）的RunGraphAsync()方法，把运行的 call 提交给 worker 跑 等待 RunGraph 的 calls 返回结果 最后处理收到的运行结果 画张图稍微理一下上面这些结构的关系： 然后还有来自这里的一张图： 『深度长文』Tensorflow代码解析（五） WorkerInterface这两个类是作为 TensorFlow 运行时调用 gRPC 的接口基类。 从源码中可以看到，WorkerInterface 类定义了一堆诸如GetStatusAsync()、CreateWorkerSessionAsync()、DeleteWorkerSessionAsync()等等这样的虚函数接口，可以认为是跟 GrpcWorkerService 支持的 GrpcWorkerMethod 是一一对应的： 1234567891011121314// Names of worker methods.enum class GrpcWorkerMethod &#123; kGetStatus, kCreateWorkerSession, kDeleteWorkerSession, kRegisterGraph, kDeregisterGraph, kRunGraph, kCleanupGraph, kCleanupAll, kRecvTensor, kLogging, kTracing,&#125;; 当然这个同时也是要跟 Protobuf 的配置要一一对应。 具体的实现在它的两个继承类 Worker 和 GrpcRemoteWorker 里面。 从代码上来看，GrpcRemoteWorker 类中的每一个函数都是调用 IssueRequest() 发起一个异步的 gRPC 调用，远程的 GrpcWorkerService 作为守护进程处理传入的 gRPC 请求。 Worker 类中的对应实现则都是直接在本地做。 Work Flow最后回到前面的运行部分。 在tensorflow::MasterSession::ReffedClientGraph::RunPartitions()中，MasterSession 运行每一个已经划分好的 partitions 用的是 part.worker-&gt;RunGraphAsync() 调用。 part.worker 是每个 partitions 对应的 WorkerInterface 对象，很容易猜想到如果分配在远程对应的应该是 GrpcRemoteWorker 实例，否则对应的应该是 Worker 实例。 那再看数据收发部分的send/recv，之前已经知道了数据传输由recv部分发起，最终调的是RpcRemoteRendezvous::RecvFromRemoteAsync()： 继续往下看，检查各项参数，准备 RpcRecvTensorCall，之后启动 call-&gt;Start()，Start()里面调的是StartRTCall()： 12345678910111213141516void StartRTCall(std::function&lt;void()&gt; recv_done) &#123; resp_.InitAlloc(dst_device_, alloc_attrs_); using namespace std::placeholders; StatusCallback cb = std::bind( [this](std::function&lt;void()&gt; recv_done, // Begin unbound arguments. const Status&amp; s) &#123; if (!s.ok()) &#123; mutex_lock l(mu_); status_.Update(s); &#125; recv_done(); &#125;, std::move(recv_done), _1); wi_-&gt;RecvTensorAsync(&amp;opts_, &amp;req_, &amp;resp_, std::move(cb));&#125; wi_ 同样是一个 WorkerInterface 的结构。 这样就很清晰了，无论是 Master、Worker 相互之间的控制还是send/recv的数据传输都是通过 WorkerInterface 的派生类作为接口完成的，接口的另一头是底层的 gRPC 通信库。 那么再看到响应 gRPC 调用的那一边，在 GrpcWorkerService 创建时，守护进程HandleRPCsLoop()就启动了： 123456789101112131415161718192021222324252627282930313233343536void HandleRPCsLoop() &#123; // TODO(ncteisen): This may require performance engineering. We can // change the number of threads, the number of handlers per thread, // or even decide to specialize certain threads to certain methods. ENQUEUE_REQUEST(GetStatus, false); ENQUEUE_REQUEST(CreateWorkerSession, false); ENQUEUE_REQUEST(DeleteWorkerSession, false); ENQUEUE_REQUEST(CleanupAll, false); ENQUEUE_REQUEST(RegisterGraph, false); ENQUEUE_REQUEST(DeregisterGraph, false); // TODO(ncteisen): Determine a better policy for enqueuing the // appropriate number of each request type. for (int i = 0; i &lt; 1000; ++i) &#123; EnqueueRecvTensorRequestRaw(); &#125; for (int i = 0; i &lt; 100; ++i) &#123; ENQUEUE_REQUEST(RunGraph, true); &#125; for (int i = 0; i &lt; 100; ++i) &#123; ENQUEUE_REQUEST(CleanupGraph, false); &#125; ENQUEUE_REQUEST(Logging, false); ENQUEUE_REQUEST(Tracing, false); void* tag; bool ok; while (cq_-&gt;Next(&amp;tag, &amp;ok)) &#123; UntypedCall&lt;GrpcWorkerServiceThread&gt;::Tag* callback_tag = static_cast&lt;UntypedCall&lt;GrpcWorkerServiceThread&gt;::Tag*&gt;(tag); CHECK(callback_tag); callback_tag-&gt;OnCompleted(this, ok); &#125;&#125; 首先准备好一系列 gRPC 调用的等待队列，11 种调用请求与前面的 GrpcWorkerMethod 一一对应，插入完成之后就是 gRPC 部分的任务了。每个方法对应的处理过程的代码也都列在后面，随便挑一个举例： 12345678void GetStatusHandler( WorkerCall&lt;GetStatusRequest, GetStatusResponse&gt;* call) &#123; Schedule([this, call]() &#123; Status s = worker_-&gt;GetStatus(&amp;call-&gt;request, &amp;call-&gt;response); call-&gt;SendResponse(ToGrpcStatus(s)); &#125;); ENQUEUE_REQUEST(GetStatus, false);&#125; 响应 gRPC 请求时这里把要做的任务都封装到线程池里面去执行，然后向队列中重新补充一个相同的等待调用。具体执行的是 worker_（其实是一个 GrpcWorker），完成后向调用方返回一个 gRPC 的 Response。 最后的一个 while 循环是读取 gRPC 完成队列中的内容，处理 gRPC 调用完成之后的收尾工作，RequestReceived、ResponseSent、Cancelled这三种状态。 话说这种完成队列的方式跟 RDMA 的还是挺像的。 MasterInterfaceMasterInterface 的结构跟 WorkerInterface 基本类似，不过话说从代码上能看出来不像是一拨人做的啊（命名风格等等），很奇怪。 支持的一些调用： 123456789static const char* grpcMasterService_method_names[] = &#123; "/tensorflow.MasterService/CreateSession", "/tensorflow.MasterService/ExtendSession", "/tensorflow.MasterService/PartialRunSetup", "/tensorflow.MasterService/RunStep", "/tensorflow.MasterService/CloseSession", "/tensorflow.MasterService/ListDevices", "/tensorflow.MasterService/Reset",&#125;; 它所派生出来的两个类 GrpcRemoteMaster 和 LocalMaster 从名字上就能够看出来是分别针对远程和本地的调用接口。 乍一看 GrpcRemoteWorker 和 GrpcRemoteMaster 实现远程调用的写法居然完全不一样，很尴尬。仔细往下分析会发现 GrpcRemoteWorker 的 IssueRequest 里面封装的 RPCState 里面的内容跟 GrpcRemoteMaster 的 Call 中的内容很类似。所以为什么不用统一的写法呢。。。 然后 LocalMaster 这个类竟然只是个壳你敢信？。。。里面真正实现本地功能的是 Master 类。 话说前面 Worker 这个类实现的是本地功能，但是 Worker 类是直接继承的 WorkerInterface，到了这里 Master 类跟 MasterInterface 类没有关系，继承 MasterInterface 的是 LocalMaster 类，但是你又发现这个 LocalMaster 类居然是 Master 类的壳。。。相当于跟 Worker 差不多的结构，但是中间多包了一层。 再来看到 GrpcMasterService 的守护进程： 12345678910111213141516171819202122232425void HandleRPCsLoop() override &#123; ENQUEUE_REQUEST(CreateSession, true); ENQUEUE_REQUEST(ExtendSession, false); for (int i = 0; i &lt; 100; ++i) &#123; ENQUEUE_REQUEST(PartialRunSetup, false); ENQUEUE_REQUEST(RunStep, true); &#125; ENQUEUE_REQUEST(CloseSession, false); ENQUEUE_REQUEST(ListDevices, false); ENQUEUE_REQUEST(Reset, false); void* tag; bool ok; while (cq_-&gt;Next(&amp;tag, &amp;ok)) &#123; UntypedCall&lt;GrpcMasterService&gt;::Tag* callback_tag = static_cast&lt;UntypedCall&lt;GrpcMasterService&gt;::Tag*&gt;(tag); if (callback_tag) &#123; callback_tag-&gt;OnCompleted(this, ok); &#125; else &#123; // NOTE(mrry): A null `callback_tag` indicates that this is // the shutdown alarm. cq_-&gt;Shutdown(); &#125; &#125;&#125; 基本的结构跟前面 Worker 是一致的。 Worker 的远程调用实际发生在： 本地 Master 处理好计算图的 partition 情况 根据 partition 是在本地还是远端，分别请求本地 Worker 或者 GrpcRemoteWorker 来执行 远程的 GrpcWorkerService 守护进程收到请求之后，调用自己本地的 Worker 进行处理，完成后将结果返回 话说 GrpcRemoteMaster 我还没找到到底是在什么情况下用到的。 后续： TensorFlow 拆包（六）：RDMA]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（四）：Device]]></title>
    <url>%2F2018%2F03%2F07%2F2018-03-07-tfunpacking4%2F</url>
    <content type="text"><![CDATA[接上篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node 这篇要分析的是 TensorFlow 中跟计算设备相关的内容。 DeviceFactory::AddDevices()TensorFlow 在创建 Session 时会首先扫描当前设备中存在哪些可用的计算资源（CPU、GPU）。 Python 层的 Session 构建时通过 tf_session.TF_NewDeprecatedSession() 这个接口调用 C 层的运行时来在 C 层新建 Session。 单节点下创建 DirectSession 由 DirectSessionFactory::NewSession()完成，其中又通过 DeviceFactory::AddDevices()获取当前可用的设备信息。 不同的计算设备分别注册各自的 DeviceFactory，AddDevices()调用不同设备的CreateDevices()，最终将所有的 device 信息传递回上层。 这里device_factories()中是一个静态的 std::unordered_map&lt;string, FactoryItem&gt;，比较奇怪的是我找了半天没找到这个东西是在哪赋值的，gdb 调试进去，明明第一次调用的时候应该是创建一个空的新对象，结果里面居然就有内容了 ： 1234std::unordered_map with 2 elements = &#123; ["CPU"] = &#123;factory = std::unique_ptr&lt;tensorflow::DeviceFactory&gt; containing 0x15fcf00, priority = 70&#125;, ["GPU"] = &#123;factory = std::unique_ptr&lt;tensorflow::DeviceFactory&gt; containing 0x15f2de0, priority = 210&#125;&#125; GPU 版编译的 TensorFlow 中，CPU 端对应的是tensorflow::GPUCompatibleCPUDeviceFactory::CreateDevices，GPU 端对应的是 tensorflow::GPUDeviceFactory::CreateDevices。 这两个 DeviceFactory 的定义在 /tensorflow/core/common_runtime/gpu/gpu_device_factory.cc 中，然后在这里面还比较惊喜地发现了两条与前面几篇中注册 Op 时很像的宏： 12REGISTER_LOCAL_DEVICE_FACTORY("GPU", GPUDeviceFactory, 210);REGISTER_LOCAL_DEVICE_FACTORY("CPU", GPUCompatibleCPUDeviceFactory, 70); 这应该就是用于注册 Device 到前面的std::unordered_map&lt;string, FactoryItem&gt;结构中的内容了，但是关于注册这个过程是在什么时候运行的我还不是很明确，因为 gdb 在这里加了断点却没有进。 在整个 TensorFlow 的源码中，除了上面的两条以外，还可以找到： 12REGISTER_LOCAL_DEVICE_FACTORY("CPU", ThreadPoolDeviceFactory, 60);REGISTER_LOCAL_DEVICE_FACTORY("SYCL", SYCLDeviceFactory, 200); 两种设备，ThreadPoolDeviceFactory应该是用于纯 CPU 版的 TensorFlow。事实上GPUCompatibleCPUDeviceFactory就是继承的ThreadPoolDeviceFactory，额外加了一些与 GPU 结合的选项。 目前ThreadPoolDeviceFactory和GPUDevice最终都是用到的LocalDevice，配合 Eigen 使用。 AddDevices()得到的 device 列表存在 DeviceMgr 中传入 DirectSession。 Add a new type of device to TF官方在这块内容基本上没有给什么太详细的说明，不过从前面的分析也可以很容易看出来，要创建一个新类型的设备首先要继承一个新的 tensorflow::Device，以及其生成用的工厂模式tensorflow::DeviceFactory，中间涉及到一些必要的函数需要重载，然后用REGISTER_LOCAL_DEVICE_FACTORY(...)注册即可。 这里有一个用 CPU 改个名字来虚拟新硬件的测试例子。 Allocate nodes with DevicesPlacer::Run()tensorflow::GraphExecutionState::Extend()创建完整的运行图时，用了一个tensorflow::Placer结构来处理图中的运行节点与设备的关联问题。 这里的分配策略非常简单，用一个并查集（！！666！！）来维护所有节点的连通性，然后把连通在一起的节点分到同样的设备上。其中有手动指定的话就按手动指定的来，没有手动指定的则按 device 优先级来，默认 GPU 最高，然后是 OpenCL 的 SYCL，最后才是分配到 CPU 上。 这里有几个额外规则： source node 和 sink node 必须分到 CPU 上 对于没有输入，有一个输出的 GeneratorNode，分配到它的目标节点所在的设备上； 对于直接在原数据上进行操作（例如说 reshape）这样的 MetadataNode，分配到它的源节点所在的设备上。 tensorflow::Partition()上篇中，在DirectSession::GetOrCreateExecutors()的过程中，用DirectSession::CreateGraphs()根据当前的输入、输出等信息从完整图中创建出了一个当前运行所用的子图。 之后需要用到tensorflow::Partition()来完成需要运行的子图与设备的分配关联： 为图中的每一个节点和边创建内存和设备类型信息 检查每一个节点的输入输出、以及其目标节点的输入信息等 给每一个节点上添加控制边等，如果数据传输不在同一个设备上就添加一对 send/recv 的 node Devices &amp; Compute事实上，感觉 DirectSession 中创建的很多结构都像是有一一对应关系的，比如前面Partition()得到的子图、子图所关联的设备、以及子图所构建的 Executor。 在tensorflow/core/common_runtime/device.h中，可以找到 Device 类的计算函数的定义： 12345678910111213// Performs the actual compute function.//// Subclasses may override this function if they wish to perform// some initialization before each compute.virtual void Compute(OpKernel* op_kernel, OpKernelContext* context) &#123; op_kernel-&gt;Compute(context);&#125;// Asynchronous kernel's compute.virtual void ComputeAsync(AsyncOpKernel* op_kernel, OpKernelContext* context, AsyncOpKernel::DoneCallback done) &#123; op_kernel-&gt;ComputeAsync(context, std::move(done));&#125; 其实就是运行传入的 Op 所注册的 OpKernel 函数。 ThreadPoolDevice 类重载了这两个函数，加上一些额外需要记录的信息，核心部分还是运行 OpKernel。 Summary这里把前面几篇中的涉及到的内容稍微做一下总结： 后续： TensorFlow 拆包（五）：Distributed]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（三）：Graph 和 Node]]></title>
    <url>%2F2018%2F02%2F28%2F2018-02-28-tfunpacking3%2F</url>
    <content type="text"><![CDATA[接上篇： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 先来拆一下第一篇里面 DirectSession::Run 里面跑的那个 graph 里面到底都是些什么内容。 DirectSession::GetOrCreateExecutors前面分析到 Executor 的时候，中间看到 DirectSession::GetOrCreateExecutors 这个函数生成了一堆 Executor，其中 CreateGraphs() 做的就是根据输入的 op 名建图的过程。 函数调用在： 12345678910111213141516171819direct_session.cc: 1131 // Nothing found, so create the executors and store in the cache. BuildGraphOptions options; options.feed_endpoints = inputs_sorted; options.fetch_endpoints = outputs_sorted; options.target_nodes = tn_sorted; options.use_function_convention = !run_state_args-&gt;is_partial_run; if (!run_state_args-&gt;debug_options.debug_tensor_watch_opts().empty()) &#123; options.debug_options = run_state_args-&gt;debug_options; &#125; std::shared_ptr&lt;ExecutorsAndKeys&gt; ek(new ExecutorsAndKeys); // The executor_lock_ is intentionally released while executor is // being created. std::unordered_map&lt;string, std::unique_ptr&lt;Graph&gt;&gt; graphs; TF_RETURN_IF_ERROR(CreateGraphs(options, &amp;graphs, &amp;ek-&gt;flib_def, run_state_args, &amp;ek-&gt;input_types, &amp;ek-&gt;output_types)); 这个调用很有意思，ek 和 graphs 这两个东西都是现场创建的，传地址进去其实用来作为函数的输出结果，所以实际的输入只有 options 和 run_state_args。 run_state_args 里面保存的是一些额外的运行信息，用于调试等等。options 的 feed_endpoints 和 fetch_endpoint 分别表示的就是当前运行中的输入点和输出点。 然后看一下 CreateGraphs() 的具体实现： 123456Status DirectSession::CreateGraphs( const BuildGraphOptions&amp; subgraph_options, std::unordered_map&lt;string, std::unique_ptr&lt;Graph&gt;&gt;* outputs, std::unique_ptr&lt;FunctionLibraryDefinition&gt;* flib_def, RunStateArgs* run_state_args, DataTypeVector* input_types, DataTypeVector* output_types) 创建一个 GraphExecutionState* execution_state 用于保存当前次运行真正要用到的运行图。 DirectSession 对象中的 execution_state_ 成员保存的是环境中的完整的图信息。若当前次运行需要用精简的图，则从 execution_state_ 中提取出需要用到的一部分内容放进前面创建的 execution_state 中，如果不需要精简，则直接复制 executor_state_ 到 execution_state 中。 完成的图会输出到 client_graph 这个结构中。 检查输入输出的数量跟准备好的 client_graph 的输入输出是否对应 保存 Stateful placements（？？不知道是干嘛用的） 用tensorflow::Partition()把运行的图切分到当前可用的 device 上，返回的是一个 std::unordered_map&lt;string, GraphDef&gt;的结构，放在 partitions 这个变量中 对 partitions 中的每一组 GraphDef，用 ConvertGraphDefToGraph() 转化成 Graph，存入前面的 std::unordered_map&lt;string, std::unique_ptr&lt;Graph&gt;&gt; 结构，也就是 outputs 这个指针中 对图进行一定的优化，然后通过 outputs 指针返回到上一层去 Graph &amp; GraphDef 其实 Graph 本身实现的思路还是很容易接受的，但是加上 Protobuf 定义之后就变得… 贼 TM 复杂！！！ 有的地方用 Graph，有的地方又是转成 GraphDef 然后重新提取信息用。 GraphDef 是 TensorFlow 中对图的 Protobuf 定义结构，主要方便保存啊、传输啊等等，真正运行的时候要转成 Graph 这个结构用。 我原本还奇怪为什么 TF 里面的很多东西都要用字符串来唯一标识，本来我觉得对象解析这种事情应该在比较高的层次上比如 Python 那层就做完，结果这里是到底层还要用字符串。 大概很大的原因就是为了方便 Protobuf 的序列化？ 下面这个链接中给出了 GraphDef 和 Graph 这两个结构的简单关系： 图解tensorflow源码] Graph 图模块 （UML视图） 引用一下： Graph in C有关 Graph 的定义，基本上都在 tensorflow/core/graph/graph.h这个头文件里面，几个类都分的比较清晰： Graph：表示计算图的一个大类，里面有整个图的完整结构，这里的图的定义是唯一起点和唯一终点，以及可用的计算设备表 Node：计算图中的节点，定义里面包含了当前节点的详细信息，以及输入输出的信息（输入节点、输出节点、输入边、输出边） 节点类型里面，switch、merge、enter、exit、next_iteration 这五个在上一篇里面讲了是 TF 的控制流部分，其他的也基本上是 TF 中的一些特殊用途的类型。 有关计算内容的定义似乎是要配合 Graph 中注册好的 Ops 表来完成的，这里还不是很明白这个过程具体是什么样的，猜测计算用的节点应该是属于 NC_OTHER 这种类型，具体的计算内容的定义写在 props_ 这个 NodeProperties 结构中。 Edge：计算图中的边 其他还有几个 iter，重载了运算符用来方便对 Graph 中的 Edge 和 Node 进行标识、对比什么的 用 Graph 中定义的一些函数例如 AddNode、RemoveNode、AddEdge 等等就可以轻松地把整个表示出来了。 这里的实现上很多地方是 Protobuf 的 Def 结构和非 Def 结构混用的，比如 AddNode 这个函数的输入参数是个 NodeDef，感觉很难受啊。 剩下的实现倒是没什么特别的。 Graph &amp; Op in PythonPython 层的 Graph 定义在 /tensorflow/python/framework/ops.py 中，这个类的结构本身算是比较简单，主要就是一堆 Op 和 Tensor 的集合（_nodes_by_id和_nodes_by_name 两个dict() ， _unfeedable_tensors和_unfetchable_ops两个set()，还有几个关系标识）。往 Graph 中添加 Op 的函数_add_op即把 Op 或者 Tensor 加到dict()中。 TF 中的 Python Op 有两种定义方式，在 Python 层中直接定义的 Op 函数的核心部分是： 1with ops.name_scope(name, default_name, value) as name: 这个类封装。由它来找到 Op 的输入所在的 Graph，处理依赖关系以及把当前 Op 加入到 Graph 相应的列表中去。 ……在代码里面搜with ops.name_scope这组关键词可以找到很多的 Op 定义。 另外一种 Op 建立方式是通过load_library.load_op_library()来载入编译好的 C 层的 Op 函数，然后包装成 Python 层的 Op。 How to organize the Op to GraphTF 官方有个创建自定义 Op 的教程： Adding a New Op 先通过这个来了解一下 Op 的完整运行过程。 Adding a New Op教程中的示例是要创建一个输入一串 int32 的数组，把除了第一个数字以外的其他数字变成 0 后输出的 op。这里的创建从 C 层面开始，创建一个 zero_out.cc 文件： 123456789101112#include "tensorflow/core/framework/op.h"#include "tensorflow/core/framework/shape_inference.h"using namespace tensorflow;REGISTER_OP("ZeroOut") .Input("to_zero: int32") .Output("zeroed: int32") .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) &#123; c-&gt;set_output(0, c-&gt;input(0)); return Status::OK(); &#125;); REGISTER_OP 是一个宏，这套注册的过程是所有 op 首先要做的，打开 tensorflow/core/ops/目录下的每一个自带的 op 文件中也都是这些内容。 这个宏注册的内容是给上层的 Python 层构建 Op 封装的时候用的。 .SetShapeFn()定义了输出的形状。 然后要写的是上面这个 Op 的 OpKernel，即 C 层实际运算的部分，从 OpKernel 继承出一个新的类，重写它的 Compute 函数，Compute 就是到时候扔到 TF 运行时里面跑的内容。从 OpKernelContext 里面可以获取到这个 OpKernel 在执行时的上下文信息： 1234567891011121314151617181920212223242526272829#include "tensorflow/core/framework/op_kernel.h"using namespace tensorflow;class ZeroOutOp : public OpKernel &#123; public: explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) &#123;&#125; void Compute(OpKernelContext* context) override &#123; // Grab the input tensor const Tensor&amp; input_tensor = context-&gt;input(0); auto input = input_tensor.flat&lt;int32&gt;(); // Create an output tensor Tensor* output_tensor = NULL; OP_REQUIRES_OK(context, context-&gt;allocate_output(0, input_tensor.shape(), &amp;output_tensor)); auto output_flat = output_tensor-&gt;flat&lt;int32&gt;(); // Set all but the first element of the output tensor to 0. const int N = input.size(); for (int i = 1; i &lt; N; i++) &#123; output_flat(i) = 0; &#125; // Preserve the first input value if possible. if (N &gt; 0) output_flat(0) = input(0); &#125;&#125;; 之后再用一个宏，把这个注册好的 Op 和 OpKernel 关联在一起，C 部分的实现就完成了： 1REGISTER_KERNEL_BUILDER(Name("ZeroOut").Device(DEVICE_CPU), ZeroOutOp); 这个宏中 Name()里面是前面注册的 Op 名，Device()定义了当前这个 Kernel 函数的运算设备，最后是需要注册的 Kernel 函数名。 tensorflow/core/user_ops/fact.cc中也是一个自定义 op 的示例。 把 C 实现编译成动态链接库之后，在 Python 中调用tf.load_op_library()方法，把前面注册好的 C 层面的 Op 以及它的 OpKernel 封装成一个 Python 层的 Op 对象。 之后这个 Op 就可以像 TensorFlow 中其他自带的 Op 一样使用了。 如果需要让这个 Op 支持自动求导，只需要在 Python 中注册好它的梯度函数即可： 123@ops.RegisterGradient("ZeroOut")def _zero_out_grad(op, grad): xxxxxxxxx C 层还有另外两个名字很像的注册梯度函数的宏（……谁起的这名字！！！）： 12REGISTER_OP_GRADIENT("OpName", OpGradientDef);REGISTER_GRADIENT_OP("OpName", OpGradientKernel); 到这里为止，我们对 TensorFlow 中 Python 层与 C 层的 Op 结合过程有了一个大体的印象。 那么 C 层的 Graph 构建是什么时候发生的呢？回到前面创建 Executor 时的CreateGraphs()函数，可以看到此时DirectSession 对象中的 execution_state_ 成员已经保存了当前 Session 环境中的完整的图信息了，那么 execution_state_ 中的图是哪里来的？ Back to TF_Run()之前在TensorFlow 拆包（一）：Session.Run()篇中已经对 TF_Run() 关于执行图的计算的部分进行了分析，现在需要把关注点放回到这里，看一下 Python 层中的 Graph 与 C 层中的 Graph 是如何联系在一起的。 以下是 Python 层的调用栈： 123456789101112131415161718192021222324252627282930#8 File "dbg_mnist.py", line 62, in simple_dnn train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1]&#125;)#7 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 2213, in run _run_using_default_session(self, feed_dict, self.graph, session)#6 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 4790, in _run_using_default_session session.run(operation, feed_dict)#5 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 895, in run result = self._run(None, fetches, feed_dict, options_ptr, run_metadata_ptr)#4 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1128, in _run results = self._do_run(handle, final_targets, final_fetches, feed_dict_tensor, options, run_metadata)#3 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run return self._do_call(_run_fn, self._session, feeds, fetches, targets, options, run_metadata)#2 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1350, in _do_call return fn(*args)#1 File "/home/jcf/tf-run-1.5.0-rc0-cuda-dbg/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1329, in _run_fn return tf_session.TF_Run(session, options, feed_dict, fetch_list, target_list, status, run_metadata)#0 &lt;built-in method TF_Run of module object at remote 0x7f7e938c6bd8&gt; 从栈底开始逐步往内部看： #8、#7、#6 Operation.run()：通常我们的用法可能都是 sess.run(Operation) ，在设置好默认的 Session 之后，Operation 类中的 run() 方法就是调用默认 Session 的 run() 方法 #5 BaseSession.run()：fetches 是需要得到的输出目标，feed_dict 是喂进去的输入数据 #4 BaseSession._run()：检查 session，设置 feed_dict，创建一个 _FetchHandler，这个结构会根据 fetches 和 feed_dict 生成一个需要得到的 Tensor 列表和需要运行的 Op 列表（大概是遍历图？），final_fetches 中存放为了运行当前 Op 所需要得到的 Tensor，final_targets 中存放为了运行当前 Op 所需要运行的前置 Op #3 BaseSession._do_run()：……贼多层 API 封装，_run_fn() 和_prun_fn() 是两种运行方式，跟参数一起传入下一层的函数 #2 BaseSession._do_call()：这层封装是用来处理异常的，其实要执行的是前面传进来的两个运行函数之一 #1 BaseSession._run_fn()： 准备进入 C 层的运行库，Python 层到这里结束。 在执行 TF_Run() 之前，这里还有一个_extend_graph() 的过程，初次执行时，C 部分的运行时会为 DirectSession 初始化一个 GraphExecutionState 结构，即前面所提的保存了环境中初始的图信息的 executor_state_ 。！！关键在这里！！ #0 tf_session.TF_Run()：这就是tensorflow/c/c_api.cc 中 C 层运行时的入口函数了。 整理一下上面的部分，Python 层的 API 要做的只是根据输入数据和输出目标找到整个图中的所有依赖项（包括 Tensor 和 Op），然后把这些内容传入 C 层。 那么最后再把前面的整个运行过程整理一遍： 用 Python 层的接口构建出计算图 如果不定义新的 Graph 结构，则所有的 Op 都会放在默认图中 调用 Session.run(...) ，Python 层遍历计算图，整理出为了执行目标所需要提供的前置数据（Tensor）以及得到这些数据所需要执行的所有 Op 列表 首次运行 _extend_graph() 时，为 C 层的 DirectSession 对象初始化 GraphExecutionState 结构，这里面保存了 C 层的完整计算图定义 Python 层整理完的 feed_dice、fetch_list、target_list 通过 TF_Run() 接口传入 C 层 接下里是 DirectSession::Run() 中的内容，详细可见TensorFlow 拆包（一）：Session.Run()篇，为当前需要执行的部分创建 Executor、线程池等等，完成整个计算图的执行 Output the C level Graph!!!在环境变量中加上TF_CPP_MIN_VLOG_LEVEL等于 2 以上的级别时，TensorFlow 运行时会输出比较详细的运行 log 来。其中就包含了 C 层面的建图相关的信息，于是用了几个 awk 脚本把这部分内容抓出来了： get_graph.sh 123456789101112#!/bin/shLOGFILE=$1.logROUGH_LOGFILE=$1_rough.logFILTERED_LOGFILE=$1_filterd.logROUGH_DOTFILE=$1_rough.dotif [ -f $LOGFILE ]; then awk 'match($0, /.*\|\|\s+(.*)/, out) &#123;print out[1]&#125;' $LOGFILE &gt; $ROUGH_LOGFILE awk -f get_graph_filter.awk $ROUGH_LOGFILE &gt; $FILTERED_LOGFILE awk -f get_graph.awk $FILTERED_LOGFILE &gt; $ROUGH_DOTFILEfi get_graph_filter.awk 第一步从 log 中抓出图部分的信息之后，用这个删掉其中的重复信息。 1234567891011121314151617181920#!/bin/awkBEGIN &#123; RS = ""; FS = "\n"; count = 0; list[0] = "";&#125;&#123; list[count] = $0; count ++;&#125;END &#123; asort(list) print list[0]; for (i=1;i&lt;count;i++) if (list[i] != list[i-1]) print list[i];&#125; get_graph.awk 最后用这个脚本生成 GraphViz 的图。 123456789101112131415161718192021222324252627282930313233343536#!/bin/awkBEGIN &#123; count = 0; print "digraph newgraph &#123;\n";&#125;&#123; if (match($0, /(\w+)\s=\s(\w+)\[(.*)\]\((.*)\)/, out)) &#123; name = out[1]; newname = sprintf("c%dn", count); gsub("n", newname, name); content = out[3]; gsub("\"", "\\\"", content) printf(" %s[label=\"%s\", tooltip=\"%s\"];\n", name, out[2], content); if (out[4]) &#123; inpt = out[4]; gsub("n", newname, inpt); printf(" %s -&gt; %s;\n", inpt, name); &#125; &#125; else &#123; print "#", $0; if (match($0, /\(.*\&#123;/)) &#123; printf(" subgraph cluster_%d &#123;\n label=\"c%d\";\n", count, count); count ++; &#125; else if (match($0, /\&#125;/)) print " &#125;"; &#125;&#125;END &#123; print "&#125;";&#125; 稍微修正一下最终的输出图，我们就可以得到： Simple DNN Simple DNN Distributed Simple CNN Distributed 为了比较好的视觉效果，上面输出来的图中或多或少被我删掉一点不重要的内容，有的在相同变量上也还没做整合。ApplyGradientDescent、ApplyAdam、Assign 这些有多出来的虚线我加的也不一定对，暂时先批判地看待上面这几张图吧。 C 层面的图结构比 Tensorboard 里面的 Python 层要稍微多点东西（比如跨设备的 send/recv 等），然后有的地方信息又不太全（比如上图中最右侧的部分，对照 Tensorboard 才知道是 adam 中两个值的平方，从 C 层面这些 node 本身的信息上体现不出来），不过大致上还是一致的。 Assign、Identity关于图中的 Assign 和 Identity 这两个 op，可以见这里的一些介绍。 简单来说，Variable 持有一个内存中的 Tensor 实例，Assign 是对这块内存中的数据进行修改的操作。 Stack Overflow 上对 Identity 有个讨论，然而我感觉高票答案的 tf.control_dependencies() 的例子其实引的不好，根本说明不清楚问题。 从官方文档里面只能看出来是做了一个别名引用，下面做一个简单的测试： 123456789101112131415161718192021222324252627import tensorflow as tfa = tf.Variable(0)a_i = tf.identity(a)b = tf.assign_add(a, 1)b_i = tf.identity(b)sess = tf.Session()sess.run(tf.global_variables_initializer())print('a:', sess.run(a))print('a_i:', sess.run(a_i))print('b:', sess.run(b))print('a:', sess.run(a))print('a_i:', sess.run(a_i))print('b_i:', sess.run(b_i))print('a:', sess.run(a))print('a_i:', sess.run(a_i))print('b:', sess.run(tf.assign_add(b, 1)))print('a:', sess.run(a))print('a_i:', sess.run(a_i)) 得到的输出结果是： 1234567891011a: 0a_i: 0b: 1a: 1a_i: 1b_i: 2a: 2a_i: 2b: 4a: 4a_i: 4 这里 a_i 和 b_i 分别是对 a 和 b 的 tf.identity() 操作。 首次输出的 a 和 a_i 都是 a 的初始值 0，a_i 在这里就是对 a 的直接引用。 接下来，输出 b 之后，再次输出 a 和 a_i，得到的结果与前面相同，都是 a 执行加一之后的 1，可见tf.assign_add() 是直接对 a 所代表的 Tensor 数据本身进行的操作。 然后再测试 b_i，结果与前面运行 b 一致。 后续： TensorFlow 拆包（四）：Device]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intel 处理器架构演进]]></title>
    <url>%2F2018%2F02%2F13%2F2018-02-13-intel%2F</url>
    <content type="text"><![CDATA[刚刚把《硬/软件接口》重新过完了一遍，觉得对微处理器中间的结构有点意犹未尽，真的是很有趣啊，然鹅翻开《量化分析方法》的目录看了看，又吓得我把书扔回去了……内容略多，留着慢慢看吧。 其实 Intel 历年处理器架构演变这事老早我就很好奇了，尤其在 SC17 上今年我们摸过的 Xeon Platinum 8176 那一代 CPU 性能比上代 E5-269x 暴涨了一大截，更是让人好奇这里面有些什么变化。 所以准备来理一理 Intel 处理器架构的演进史。 Intel Micro-architecture下面这个表是从维基百科里面扒的： Archi­tectural change Fabri­cation process Micro-architecture Code names Release date Processors 8P/4P Server 4P/2P Server/ Workstation Enthusiast/ Workstation Desktop Mobile Tick (new fabrication process) 65 nm P6, NetBurst Presle, Cedar Mill, Yonah 2006-01-05 Presler Cedar Mill Yonah Tock (new micro-architecture) Core Merom 2006-07-27 Tigerton Woodcrest Clovertown Kentsfield Conroe Merom Tick 45 nm Penryn 2007-11-11 Dunnington Harpertown Yorkfield Wolfdale Penryn Tock Nehalem Nehalem 2008-11-17 Beckton Gainestown Bloomfield Lynnfield Clarksfield Tick 32 nm Westmere 2010-01-04 Westmere-EX Westmere-EP Gulftown Clarkdale Arrandale Tock Sandy Bridge Sandy Bridge 2011-01-09 (Skipped) Sandy Bridge-EP Sandy Bridge-E Sandy Bridge Sandy Bridge-M Tick 22 nm Ivy Bridge 2012-04-29 Ivy Bridge-EX Ivy Bridge-EP Ivy Bridge-E Ivy Bridge Ivy Bridge-M Tock Haswell Haswell 2013-06-02 Haswell-EX Haswell-EP Haswell-E Haswell-DT Haswell-MB (notebooks) Haswell-LP (ultrabooks) Refresh Haswell Refresh, Devil's Canyon 2014-05-11, 2014-06-02 Tick 14 nm Broadwell 2014-09-05 Broadwell-EX Broadwell-EP Broadwell-E Tock Skylake Skylake 2015-08-05 Skylake-SP Skylake-SP Skylake-X Skylake Optimizations (refreshes) Kaby Lake 2017-01-03 Kabylake-X Kabylake Kaby Lake R 2017-08-21 Coffee Lake 2017-10-05 Coffee Lake Process 10 nm Cannon Lake 2018 Architecture Ice Lake Ice Lake 2018/ 2019? Optimization Tiger Lake 2019? Process 7 nm Architecture Optimization Process 5 nm Architecture Optimization P6P6 是 Intel 的第六代微架构，最早用于 1995 年的 Pentium Pro 处理器，后面 2000 的 NetBurst 感觉应该也算是包含在 P6 这个大系列里面，一直到 2006 年的 Core 为止。 这个横跨了将近 10 年的架构系列最早是 600nm 的工艺，一直到最后达到了 65nm，算是不断摸索完善出来的，也是 Intel 走上比较规则的架构发展之路的一个起点。 P6 相对于之前的架构加入了很多新的技术： 预测执行（Speculation）和乱序执行！！！ 14级流水线，第一代奔腾的流水线只有 5 级，P6 的 14 级在当时是最深的 片内的 L2 cache！！！ 物理地址扩展，达到最大36位，理论上这个位宽最大可以支持到 64G 的内存（虽然制程的地址空间还是只能用到 4G） 寄存器重命名！！！ MMX 和 SSE 指令集扩展，开始 SIMD 的思路了 以上这些都是现代处理器中非常重要的设计。 更重要的是从这里开始，奠定了 Intel 沿着摩尔定律发展的 Tick-Tock 架构演进道路： Tick 改进制程工艺，微架构基本不做大改，重点在把晶体管的工艺水平往上提升 Tock 改进微架构设计，保持工艺水平不变，重点在用更复杂、更高级的架构设计 然后就是一代 Tick 再一代 Tock，交替演进。 P6 的末尾阶段，首次出现了双核，当时的双核还是基本上像是把两个单核用胶水粘在一起的感觉。 Core 最早的名字里面带 Core 这个牌子的处理器是 Core Duo，它的架构代号是 Yonah，其实还是算是个 NetBurst 的改版，只是跟后期的 NetBurst 走向了不同的发展道路，虽然名字上有 Core 但不是 Core 架构。主要的设计目标是面向移动平台，因此很多设计都是偏向低功耗，高能效的。 再后来的 Core 2 Duo 才是采用 Core 架构的新一代处理器，全线 65nm，然后微架构在 Yonah 之上做了比较大的改动。 Core 架构把 NetBurst 做深了的流水线级数又砍下来了，主频虽然降下来了（而且即使后来工艺提升到 45nm 之后也没有超过 NetBurst 的水平），但是却提高了整个流水线中的资源利用率，所以性能还是提升了；把奔腾4上曾经用过的超线程也砍掉了；对各个部分进行了强化，双核共享 L2 cache 等等。 从 Core 架构开始是真的走向多核了，就不再是以前“胶水粘的”伪双核了，这时候已经有最高 4 核的处理器设计了。 Nehalem Core 从 65nm 改到 45nm 之后，基于 45nm 又推出了新一代架构叫 Nehalem，这一代的提升引入了相当多的新技术，算是个非常重要的里程碑。 Core 这个名字变成了桌面 PC 以及笔记本处理器的系列名，后面架构继续更新，然而 Core（酷睿） 这个名字就留下来了，然后系列开始细分，这个架构推出了第一代的 i7。 相对上一代的主要改进： 引入了片内 4-12 MB 的 L3 cache！！！ 重新加入超线程（奔腾4时代有，后来砍掉了，这一代开始重新引入） Intel Turbo Boost 1.0！！！ 分支预测器分级！！！ 二级的 TLB 每个核上有 3 个整数 ALU, 2 个向量 ALU and 2 个 AGU 采用 Intel QPI 来代替原来的前端总线！！！ PCIE 和 DMI 控制器直接做到片内了，不再需要北桥 IMC（集成内存控制器），内存控制也从北桥移到了片内 第二代的 Intel 虚拟化技术 流水线加到 20 到 24 级 其他指令扩展升级等等 相对上一代的性能： 同等功耗下，10-25% 的单线程性能提升，20-100% 的多线程性能提升！！！ 同等性能下功耗降低 30% 15-20% 的 clock-to-clock（不知道这个词应该怎么翻译） 性能提升 工艺提升到 32nm 的 Westmere 后，推出了第一代的 i5 和 i3。 Xeon 系列也从 Westmere 开始推出了第一代 E 命名的 E7-x8xx 系列。 Sandy Bridge 32nm 的下一代 Tock 是 Sandy Bridge，二代 Core i 系列以及第一代 Xeon E3、E5 系列也基于这个架构： Intel Turbo Boost 2.0 增大了 L1 和 L2 cache 共享的 L3 cache 也同时支持片上的核芯显卡 IMC 强化成了 GMCH（integrated graphics and memory controller），片上显卡共用主存作为它的显存 每个核上的运算部件增强 分支预测增强 微操作译码部分新增了一个 cache（uop cache） 14 到 19 级指令流水线！！！（长度区别基于上面那个 uop cache 是否命中） 多个核间、核芯显卡、cache 间用了环状总线（ring bus） Intel Quick Sync Video，支持视频的硬解码 其他指令扩展升级等等 Ring Bus: 真是令人惊叹的操作啊。 这个故事教育我们，cache 这个思路很多地方都能用到！！！这个简单的想法能起到的效果可不简单~~ 相对上一代的性能： 11.3% 的 clock-to-clock 性能提升 2 倍的显示性能提升（…这个不用想都知道会很多…） Tick 到 22nm 的下一代架构叫 Ivy Bridge，三代 Core i 系列和二代 Xeon E 系列： 16 位浮点指令 片内硬件随机数生成器 PCIE 3.0 其他各个部分都做了很多提升 Haswell 22nm 的 Tock 到了 Haswell，四代 Core i 系列和三代 Xeon E 系列： 每个核内的部分进一步升级，更多的 ALU、各种带宽增加等等 支持 DDR4 内存 提供部分雷电接口（Thunderbolt）支持 完整集成电压调节器（FIVR），把主板上的一部分电源控制做到了片内 更高级的功耗控制系统，增加了 L6 和 L7 两级 CPU 睡眠状态 其他指令扩展升级等等 相对上一代的性能： 8% 的向量运算能力提升 5% 的单线程性能和 6% 的多线程性能 好像提的不是很多，Intel 开始挤牙膏了 14nm 的 Tick 到了 Broadwell，五代 Core i 系列和四代 Xeon E 系列。各种指令集升级、支持了很多新功能特性。 Skylake14nm 的 Tock 到了 Skylake，进入 XXlake 时代，六代 Core i 系列。 一系列指令集升级、新功能特性等等。上一代加入的 FIVR 这里又拿掉了，其他包括雷电 3.0 等等好多升级什么的。 从比较粗粒度的架构图来看，Skylake 的架构基本上跟前面那张 Haswell 的没什么差别，大概就是寄存器什么的数字上往上涨了一些，所以图这里就不贴了。（当然细节上肯定还是有挺多升级的）挤牙膏啊挤牙膏，疯狂挤牙膏 这个阶段的微架构除了升级指令、加上更多扩展功能以外，不像 Nehalem 和 Sandy Bridge 那时候能有更多革新的设计了，而且由于制程已经达到了很小的程度，再往下可能很快就要碰到工艺极限了，所以摩尔定律开始放缓，性能很难有特别大的提升了。 所以 Intel 开始从 Tick-Tock 两步升级战略转变到 Process-Architecture-Optimization 的三步升级战略，分别是提升工艺制程，升级微架构，然后再在微架构基础上进行优化。 其实这个三步战略从上面 Haswell 时代就已经开始了，Broadwell 前面还有个 refresh 的 Haswell 升级版，i3/i5/i7 4x20 系列。 Skylake 优化版的下一代是 Kaby Lake，即七代 Core i 系列。相比 Skylake 提升了主频，频率切换更快，提升了显示核心等等。 Kaby Lake 继续优化到了 Coffee Lake，八代 Core i 系列。这个系列的 i3 提到了 4 核，i5、i7 都从 6 核开始起步，然后继续提升主频，各种优化等等。 What’s new!话说 Kaby Lake 和 Coffee Lake 这个时代，Intel 又推出了新的 Core i 系列，命名为 Core i9，第一代的桌面版 Core i9 是 Skylake 架构（Skylake-X），第一代笔记本版 i9 是 Coffee Lake 架构。 那么本该在这个时候推出的第五代 Xeon，也就是 E3/E5/E7 的 xxxx v5 版呢？ Skylake 的第五代 Xeon 摆脱了原本的系列名，而是重新改成了 Bronze、Silver、Gold、Platinum 4 个系列（一股浓浓的网络游戏装备风，说不定再下次改名就可能改名叫稀有、史诗、传说什么的，→_→）。青铜和白银系列支持双路（原本的 E5-24xx、E7-28xx 系列），黄金系列支持四路（原本的 E5-46xx、E7-48xx 系列），白金系列支持八路（原本的 E7-88xx 系列）。 这里还有个重要变动，Intel 沿用了很多年的 Ring Bus 片内总线从 Skylake-X 开始改掉了！前面说 Sandy Bridge 开始，微架构设计上已经全面采用了 Ring Bus，其实最早到 Nehalem 时代的 Xeon 系列就已经开始用上 Ring Bus了，主要用于解决多核（非常非常多的核）之间的共享、扩展等等的问题。 然而随着 CPU 的发展，核越来越多，所以一个 CPU 片内还可能有多个 Ring Bus，就像下面这样。这会有什么问题呢？ 以前我们考虑多路服务器里面的 CPU 亲和性的时候，只考虑过 socket 之间的 NUMA 关系，两片 CPU 核之间会有亲和性问题。。。。。。谁想过在下面这种结构的单片 CPU 里面其实也已经是个 NUMA 结构了！！！ 但是当核的数量持续增长，Ring Bus 的延迟也会越来越高，终究不是个办法，Intel 在 KNL 上已经试过 2D Mesh 的总线结构了，大概是效果比较好，于是从 Skylake-X 开始，之后的系列开始全面改用 Mesh 结构。 破茧化蝶，从Ring Bus到Mesh网络，CPU片内总线的进化之路 后记emmm……所以 Platinum 8176 疯狂吊打 E5-269x v4 是妥妥的事情。毕竟微架构差了一代，而且白金版原本对应的是八路的 E7 系列，再加上 Mesh 相比 Ring Bus 解决了很多问题。 然后我就有个疑问了：话说……为啥以前我们不用 E7 呢 更新一下两个资料参考网站： WikiChip MOEPC]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Intel</tag>
        <tag>Microarchitecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成与设计.硬件/软件接口 学习笔记（三）]]></title>
    <url>%2F2018%2F02%2F12%2F2018-02-12-cod3%2F</url>
    <content type="text"><![CDATA[接上篇： 计算机组成与设计.硬件/软件接口 学习笔记（一） 计算机组成与设计.硬件/软件接口 学习笔记（二） 本篇为第六章的内容。 CHAPTER 6. Parallel Processors from Client to Cloud从客户端到云端的并行计算，这一章主要关注于处理器之间、集群之间的网络连接，并行性等等。 6.1. Introduction关于并行的好处，前面的章节也已经有提到过了。 这里再重新明确一下串行（Serial）、并行（Parallel）、顺序（Sequential）、并发（Concurrent）的概念： 顺序 并发 串行 在Intel Pentium 4上运行一个MATLAB的矩阵乘法程序 在Intel Pentium 4上运行Windows Vista操作系统 并行 在Intel Core i7 上运行一个MATLAB的矩阵乘法程序 在Intel Core i7 上运行Windows Vista操作系统 6.2. The Difficulty of Creating Parallel Processing Programs事实上，现在的处理器技术中比如超标量和乱序执行的设计就已经是进行了指令级并行了，对于程序员来说，不需要做任何事情，编译器和计算机自己就会完成中间的步骤。 而要把一个串行的程序写成并行的，则会另外引入许多新的问题，包括：调度、将任务划分成多块、平衡多部分之间的负载、同步、多部分通信等等。而且这种情况是核心数量越多，越麻烦。 根据Amdahl定律，可以得出如下结论： 为了充分利用好多核，程序中任何一个很小的部分都需要并行化。为了在100个核心上达到90倍的加速比，程序中顺序执行的部分只能够占到0.1%。 核心数的增加只有在数据规模也同样提升时才能有比较明显的效果。 负载均衡很重要。 书上有详细的举例和计算。 也就是说，理论计算上的并行加速比其实是很难达到的，中间有太多的限制条件在，实际编程时也是非常困难。 6.3. SISD, MIMD, SIMD, SPMD, and Vector基于指令流和数据流的数量，并行硬件有如下一些分类： SISD（Single Instruction stream Single Data stream），就是常见的单核处理器结构。 MIMD（Multiple Instruction streams Multiple Data streams），常见的多核处理器结构。 SIMD（Single Instruction stream Multiple Data streams）。 举例： Single Data stream Multiple Data streams Single Instruction stream Intel Pentium 4 x86上的SSE指令集 Multiple Instruction streams 目前还没有 Intel Core i7 通常在MIMD的结构上写程序的方法是只写一个单独的程序跑在多个核心上，根据判断语句来让多个核心分别执行不同的部分，这样的开发方法叫做SPMD（Single Program Multiple Data）。 MISD非常少见，最接近的情况可能是“流处理器”：对于一个输入流，用一系列的流水线指令去处理数据。 反之SIMD则很常见，例如出现在向量计算机上，对于同一条指令，同时有多个ALU分别参与计算（也可以叫做数据级并行）。它的优点是所有并行执行单元都是同步的，从程序员的角度看，这个其实非常接近于SISD（尽管每个单元都执行相同的指令，但是每个单元都有独立的地址寄存器和独立的数据地址），可能一个顺序应用程序在串行硬件上编译时按SISD组织，到并行硬件上编译就会按照SIMD组织。 基于SIMD的想法，x86处理器中的MMX（Multimedia Extension）、SSE（Streaming SIMD Extension）和后来的AVX（Advanced Vector Extension）得到了发展。 SIMD的一个更加古老和优雅的称呼是向量机体系结构（Vector Architecture）。 这种结构的特点在于拥有一些特殊的向量寄存器。 用Linpack基准测试中的DAXPY（Double Precision a*X Plus Y）循环来举例： 其中X和Y是64位双精度浮点数的向量，a是一个双精度标量。 常规的MIPS代码为： 123456789101112start: l.d $f0,a($sp) :输入标量a addiu $t0,$s0,#512 :设定上界loop: l.d $f2,0($s0) :输入x(i) mul.d $f2,$f2,$f0 :a*x(i) l.d $f4,0($s1) :输入y(i) add.d $f4,$f4,$f2 :a*x(i)+y(i) s.d $f4,0($s1) :写回y(i) addiu $s0,$s0,#8 :递增x的索引 addiu $s1,$s1,#8 :递增y的索引 subu $t1,$t0,$s0 :计算边界 bne $t1,$zero,loop :检查是否完成 向量版的MIPS代码为： 1234567start: l.d $f0,a($sp) :输入标量a lv $v1,0($s0) :输入向量X mulvs.d $v2,$v1,$f0 :a*X lv $v3,0($s1) :输入向量Y addv.d $v4,$v2,$v3 :a*X+Y sv $v4,0($s1) :写回 向量处理器大大降低了动态指令带宽，仅用6条指令就完成了接近600条普通MIPS指令的工作：原因一是整个向量操作是在多个数据元上同时进行的，二是节省了原本在普通MIPS中接近一半开销的循环指令。 另外普通的MIPS指令在流水线中是存在阻塞的，每次add.d必须等待mul.d，每次s.d也必须等待add.d，而在向量处理器中，每条向量指令只会在每个向量的起始数据元阻塞，随后的数据元会流畅地通过流水线。 效率被大大提高了。 向量结构对比标量结构、多媒体扩展来说都有很大的优势。 6.4. Hardware Multithreading硬件多线程是跟 MIMD 相关的一个重要概念。 MIMD需要用多处理器去解决多项任务，为了保证效率的提升，需要硬件多线程技术来尽量填满每个处理器，即当一个线程发生阻塞时马上切换到另一个线程上运行，保证每个处理器都有较高的利用率。 线程与进程： 线程：包括PC、寄存器状态、栈的状态。可以是轻量的进程，线程通常共享一个单独的地址空间。 进程：进程包含一个或多个线程、一个地址空间、操作系统状态。 进程切换通常要涉及到操作系统层面，要花上几百到几千个时钟周期，而线程切换则只是处理器上的切换，处理器设计时就要保证能够快速切换。 硬件多线程的几种形式： 细粒度多线程（Fine-grained Multithreading） 每一个指令结束后都能进行线程切换，多线程循环交错地执行。 这种方式下，无论线程的阻塞时间长短，总体吞吐量不会受影响，因为通常会采用 round-robin 算法，一条线程出现阻塞时总能切换到另外的线程上去。缺点是降低了单条线程的执行速度，如果这条线程上不存在阻塞，则本来可以更快地执行完毕，现在中间还有穿插执行其他线程。 粗粒度多线程（Coarse-grained Multithreading） 这种方式是在一条线程出现较大的阻塞之后再切换至另一条，比如末级Cache缺失等。 对于单一线程来说，执行速度受的影响就很小了。 但这种方式的问题出现在线程流水线的启动上。正常运行单一线程时，单一线程填充在流水线中，当阻塞发生时，则需要清空流水线，并把另外一条线程的状态填入流水线。阻塞开始后执行的新线程必须要在指令完成之前填充流水线（否则还不如继续执行原来的呢）。 流水线启动开销在阻塞时间短的情况下影响特别明显，一般只有在阻塞时间比较长时，才能忽略掉这个启动开销。 同时多线程（SMT，Simultaneous Multithreading） 主要还是依靠多发射动态调度的流水线的能力来挖掘线程级并行，同时保持指令级并行。这种方式的核心思想是多发射处理器有多个功能单元，而单个线程可能根本用不上这些多余的硬件资源，依靠动态调度、寄存器重命名等等技术，来自不同线程的多条指令之间完全不会有数据依赖，能够被同时发射出来处理。 这种方式应该是最完美的，但具体实现起来还是有很多困难点的。需要用到超标量、寄存器重命名、动态调度等等技术，解决来自不同线程的多条指令的相关性等等问题。 最新的Intel Core i7处理器也只能够支持2个线程的实时多线程处理。 上面这张图的各种实现已经是基于超标量了。 可以很明显地看出，SMT下超标量+实时多线程的效果之强。 6.5. Multicore and Other Shared Memory Multiprocessors这里讲的是一些常规的各种并行问题。 书上在这里也提到了OpenMP。 6.6. Introduction to Graphics Processing Units这一节是GPU 的介绍。 随着游戏、图像等需求的发展，一类主要用于浮点运算的芯片开始发挥重要作用，用GPU把它们与CPU分类开来。用很便宜的价格就能够买到一个具有上百个流处理器核心的GPU。 GPU是辅助CPU的加速卡，因而它们只需要致力于提高浮点运算性能即可，其他的功能和控制能力可以很弱，甚至并不需要，交给CPU来完成即可。 GPU和CPU最大的区别是，GPU不需要依赖多级Cache来解决内存延迟，而是依靠硬件多线程，对于一个运算请求，通常GPU可以启动成百上千个独立的线程。它的设计完全是用来面向数据级并行问题的，就是说要大量用到 SIMD 特性的问题才适合在 GPU 上跑。 GPU 是一堆多线程 SIMD 处理器组合在一起的 MIMD 处理器结构。 GPU的主存是面向带宽而不是面向延迟的，GPU的主存通常能够提供更大的带宽，而容量通常要小于CPU的主存。 上面是个简单的GPU流处理器结构。 刚看到这里的时候，感觉书上讲的 Fermi 架构跟网上查到的还不太一样。 所以先查了一下官方资料。 维基百科上的 Fermi 一个Fermi流处理器（SM，Streaming Multiprocessor）里有32个单精 CUDA 核，16个 load/store 单元以及4个特殊功能单元（Special Functional Unit）。 CUDA 核内部是一个 32 位的整数 ALU（支持 64 位扩展运算），以及一个 FPU（支持单双精浮点运算以及 FMA）。64位的双精运算需要一排的两个 CUDA 核共同完成。所以一个 SM 在一个时钟周期内最多可以完成 16 个双精 FMA 操作。 接下来要说到流处理中的线程调度策略，Fermi 架构采用两级硬件调度器： Block 级的线程调度器，这里叫 GigaThread global scheduler，在 SM 的外面，负责把线程 block 调度给空闲的 SM 来处理。 SM 本身能处理的指令都是 SIMD 的，这里的一条 SIMD 指令可以同时用上上图绿色部分的任意一排，即 16 个 CUDA 核、16 个 load/store 单元或者 4 个 SFU。 Wrap 级的线程调度器，就是上图中的 Warp Scheduler，在 SM 里面，且一个 SM 中有两个独立的 Warp 调度器和两个指令分发单元（dispatch unit），负责把 Wrap 中的线程扔给具体的执行单元去跑。 Wrap 本身表示的是 CUDA 里面的一组线程，即 32 个相似的线程打包在一起算是一个 Wrap。一个 SM 一次可以发射两个 Wrap，上图 4 排硬件中的任意两排可以同时执行。 当然如果是两个用 SFU 的 Wrap或者两个用 load/store 单元的 Wrap 肯定就不能双发射了。 然后回到书上那张图，这样就能明白是什么意思了。 书上的 SIMD 通道其实就是把 CUDA 核和 load/store 单元给放在了一起，相当于半个 SM 的硬件数据通路的结构。抽象成这个样子之后，这个抽象结构跟之前 6.4 节的传统多线程处理器基本是一致的，只是这里的每条指令都是 SIMD 的。 接下来是 GPU 上的存储结构： 一个 SM 中的寄存器大小是 32k * 32 位，被所有这些 CUDA 核共享，均摊下来相当于每个 CUDA 核可以用到 1k 个 32 位的寄存器，这个也就是前面那张书上的图上画着的数据。实际运行中单个线程最多可以用到 63 个向量寄存器，每个向量寄存器由 32 个 32 位的寄存器元素组成（不超过 64 * 32 = 2k 个 32 位寄存器）。 片上有一块 64KB 大小的 shared memory 作为每个 block 的 local memory，这块片上内存可以配置成 L1 cache 或者手动来管理 shared memory。材质是 SRAM。 再往下一级，被所有的 grid 共享的就是 GPU 上的主存，材质是 GDDR 内存。 6.7. Clusters, Warehouse Scale Computers, and Other Message-Passing Multiprocessors简要地介绍了一下集群中的环境，多节点之间的通信方式这时候就要用到消息传递了，因为已经没有了统一的共享内存。 6.8. Introduction to Multiprocessor Network Topologies多处理器网络拓扑结构介绍。 资料： 破茧化蝶，从Ring Bus到Mesh网络，CPU片内总线的进化之路 6.9. Communicating to the Outside World: Cluster Networking与外界的通信：集群网络 6.10. Multiprocessor Benchmarks and Performance Models多处理器基准测试和性能模型 6.11. Real Stuff: Benchmarking Intel Core i7 versus NVIDIA Tesla GPU 538实例：Intel Core i7与NVIDIA Tesla GPU 538的基准性能对比 6.12. Going Faster: Multiple Processors and Matrix Multiply这一章肯定就是把原本单核的改成多核啦，16 线程的 OpenMP 大约又提升了 14 倍不到的加速比，那么再算上前面所有的这些加速方案，到这里为止，相比第三章中一开始的最初版程序一共提高了 212 倍的性能！ 6.13. Fallacies and Pitfalls6.14. Concluding Remarks6.15. Historical Perspective and Further ReadingPDF材料 6.16. Exercises答案 APPENDICES A. Assemblers, Linkers, and the SPiM SimulatorA.1. IntroductionA.2. AssemblersA.3. LinkersA.4. LoadingA.5. Memory UsageA.6. Procedure Call ConventionA.7. Exceptions and InterruptsA.8. Input and OutputA.9. SPIMA.10. MIPS R2000 Assembly LanguageA.11. Concluding RemarksA.12. Exercises APPENDICES B. TH-2 High Performance Computing SystemB.1. IntroductionB.2. Compute NodeB.3. The Frontend ProcessorsB.4. The InterconnectB.5. The Software StackB.6. LINPACK Benchmark Run (HPL)B.7. Concluding Remarks APPENDICES F. Networks-on-Chip]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>组成与设计.硬软件接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成与设计.硬件/软件接口 学习笔记（二）]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-06-cod2%2F</url>
    <content type="text"><![CDATA[重新理了一下，把内容分开，其实大部分内容都是以前记的，接上篇： 计算机组成与设计.硬件/软件接口 学习笔记（一） 本篇为第四、五章开始。 CHAPTER 4. The Processor从这一章开始就是讲处理器里面的设计了，还是以 MIPS 为例展开。 4.1. Instruction指令实现 overview： 首先所有指令都存在内存里面，向内存发送 PC（program counter）来获取（fetch）下一条指令 根据指令具体要做的事情，读取一个或者两个寄存器的数据。 前面两步是所有指令都要做的，再之后就是根据指令做具体的操作了。 4.2. Logic Design Conventions这里把 MIPS 实现的数据通路上的逻辑元素分成两类：结合性元素（combinational element）和状态元素（state element）。 结合性元素指的是执行操作的元素，输出只与输入有关，任何时候，只要给一样的输入，就会有一样的输出结果，例如 ALU。 另外一些元素不是结合性的，而是可能包含运行状态，即带有内部存储结构，例如指令存储器、寄存器等。 触发电平用于控制各个元素的执行。这里假定都用边沿触发。 4.3. Building a Datapath数据通路的具体实现，首先是处理指令需要的几个基本元素： 指令存储器用于存储所有的程序指令，并且给它们编上地址 PC一个地址寄存器，用于存放指令地址，即指向指令存储器中当前正在执行的指令 加法器用于改变PC的值，指向下一条指令，以使程序可以继续向后执行 对于寄存器的操作： 读寄存器输入1个寄存器号，输出该寄存器的内容。可随时读取。 写寄存器输入1个寄存器号，以及要写入写寄存器的值。需要给寄存器文件一个写信号才能写入。 以R-format指令为例：一般需要读取2个寄存器的内容，经过ALU运算，写入另一个寄存器。结合上面的寄存器操作，则寄存器访问需要4个输入（2个要读的寄存器号，1个要写的寄存器号，1个要写入的寄存器值），2个输出（分别是输出2个要读的寄存器的内容），以及一个写信号脉冲。寄存器号的3个口需要5位的数据宽度（第二章中rs、rt、rd的长度都是5位），写入与读出的3个口则分别需要32位的数据宽度。 再看I-format指令：lw $t1,offset($t2)存储地址是将$t2中的内容（32位基地址）加上offset（16位偏移地址）作为目标地址，然后将内容写入寄存器$t1。 这里需要的是一个地址加法器（加16位和32位的数）以及数据存储器。 beq $t1,$t2,offset首先读取$t1和$t2的内容到ALU进行对比，若通过，则将offset（16位偏移地址）加到PC上作为下一次的目标地址。 需要注意的是：lw中目标地址是直接把offset和$2中的内容相加，而beq中offset指的是按4字节寻址的地址，PC中存的是完整的地址，offset要作2位偏移之后再加到PC中去 上图中进行R-format指令操作时：多路选择器ALUSrc选0，输入两个寄存器的内容到ALU中；多路选择器MemtoReg选0，将运算结果输出至寄存器文件的写入数据端，写入寄存器。 进行I-format指令操作时：多路选择器ALUSrc选1，输入一个寄存器的内容以及将offset从16位符号扩展至32位之后的值到ALU中；多路选择器MemtoReg选1，将数据存储器中读到的内容输出至寄存器文件的写入数据端，写入寄存器。 在上图的基础上把PC以及指令寄存器的内容加上去就是一个比较完整的数据通路了。 PC后的两个加法器用于改变PC的地址。将offset从16位符号扩展至32位之后的值作2位左移与地址累加是完成beq这一类的跳转，用PCSrc进行数据选择。 4.4. A Simple Implementation Scheme然后来模拟实现一个简单的的指令集，包含： load word (lw) store word (sw) branch equal (beq) 算数逻辑运算：add, sub, AND, OR set on less than 要实现上面的这些功能，操作数只需要2位： 00表示lw或者sw，这里对于ALU来说只需要作地址的相加即可。 01表示beq，需要用ALU对两个寄存器相减。 10表示R-format操作，需要配合6位的Funct再进行各种情况的选择。 根据上述的2位ALUOp和6位Funct，生成出真正的4位ALU操作码 结合第二章表格里面每一种类型的指令的格式，再加上控制单元部分，可以把数据通路图进一步画成： 多路选择器RegDst用于区分R-format和I-format中目标寄存器在操作数中的位置（rt在20:16，rd在15:11） 单独有一个ALUOp用来根据最后的Funct（5:0）来产生真正的ALU控制码，这里其实只是一个真值表的处理。 指令的31:26位用一个单独的控制器处理，来完成各种控制信号的产生。 跳转的指令是6位操作数和26位地址，这个地址也是按4字节寻址的。 则加上跳转就是将原来PC+4的高4位，以及26位地址偏移2位组成新的PC地址，完成跳转。 至此，单指令周期的数据通路构造完成。 而由于指令数的庞大，单周期指令实际运行时非常慢，必须等到一条指令运行完毕之后才能跑下一条，而每一条指令其实只占用了数据通路的一部分（这里只是实现了几条简单的指令集，完整的可以想象是更加庞大的），剩下的部分都是空闲没用的。 因此要用到下一节之后的流水线技术来处理这个矛盾。 4.5. An Overview of Pipelining流水线思想是为了提高吞吐量，其实最简单的理解就是把每个不同部分错开同时执行，其本身并不能提升单个指令的执行速度，但是当指令数量达到一定量级，且每级流水线的工作时间设置的比较合适时，使用流水线比不使用流水线要快大约流水线级数倍。 处理1条 MIPS 指令包含5个步骤： 从指令存储器中读取指令（Instruction Fetch） 指令译码，并且同时读取寄存器的内容（Instruction Decode） 执行操作或者计算地址（Execute） 从数据存储器中读取操作数（Memory Access） 将结果写回寄存器（Write Back） 本章讨论的流水线也是分成5个部分。 当然，前面说的能加速流水线级数倍的前提是所有指令都能够用流水的方式执行，事实上在实际情况下这是有问题的，下一条指令可能需要依赖于上一条执行完毕之后才能执行，这里就有个流水线冒险的问题： 结构冒险：由于硬件结构导致的冒险，两条指令没办法同时使用一块相同的硬件，比如两条指令在不同的阶段需要同时访问相同的寄存器。 这个需要从硬件结构上来解决。 数据冒险：下一条指令需要的数据，上一条指令还在计算中。 例如这两条指令按顺序进入流水线时： 12add $s0, $t0, $t1sub $t2, $s0, $t3 当sub指令需要读取$s0的数据时，add只执行到第三个阶段，还要等到第五个阶段才能把结果写回到$s0中去。 数据冒险是特别常见的情况，编译器可以通过改变指令顺序处理掉一部分的数据冒险，另外的就需要通过增加额外的硬件结构来完成转发（forward）和旁路（bypass）。 转发指的是下一条指令读取寄存器数据时直接把上一条 ALU 计算的结果拿过来。 对于某些特别的指令，比如读取数据，就需要给流水线暂停一个时钟周期再转发，相当于插入一条气泡指令。 可以通过指令重排，往气泡的位置插入一条跟这两条无依赖关系的指令来避免浪费。 控制冒险/分支冒险：遇到例如beq这类指令时，指令正在译码，但是流水线就要紧接着读入下一条指令，而此时下一条指令的PC还没有确定，因此产生矛盾。比较简单的解决方案是随便预测一个方向先执行，如果不对再忽略前面执行的指令转去执行正确的指令，这样如果预测是正确的，则流水线任然是全速运行，如果是错误的那么与原来不采用预测的方式耗时一样。 使用额外的硬件预测器可以把准确率提高到90%以上。另外在MIPS中还有一种方案是将一条不影响分支的指令放到分支之后。 4.6. Pipelined Datapath and Control流水线中的数据通路和控制 IF：Instruction fetch指令预取 ID：Instruction decode and register file read指令译码并读取寄存器 EX：Execution or address calculation执行指令或者计算地址 MEM：Data memory access访问数据存储器 WB：Write back寄存器回写 数据流是从数据通路的左端开始，一直向右进行。图中仅有的两条反向流动通路是PC和寄存器的回写，这两个反向流也就是后面需要解决冒险的地方。 为了使得一个数据通路里面的五个部分之间相互不影响，解决方法就是在两个相邻的部分之间使用额外的寄存器来传递数据，保证上一条指令的结果能够保存下来传给下一个部分来执行，同时能继续执行下一条指令的该部分。 考虑一条lw $dest, offset($addr)指令： IF：根据 PC 的值，从指令存储器中获取到这条指令，存到 IF/ID 寄存器中。PC 自增 4 并写回，等待下一个时钟周期来读下一条指令，同时这个时候的 PC 值也存到 IF/ID 寄存器中，以备以后使用。 ID：分拆指令的各个部分，读寄存器，把读到的寄存器值（这里只有一个）以及 32 位扩展后的偏移值存入 ID/EX，自增过的 PC 也存入 ID/EX。 EX：把 32 位扩展值以及寄存器 1 的内容用 ALU 加起来，就是需要访问的最终目标地址，算出目标地址之后存入 EX/MEM。 MEM：用目标地址去访问内存，把读到的数据存入 MEM/WB。 WB：把读到的数据写回到寄存器中。 分析完 lw 和 sw 中数据流动的特点之后会发现，这里可能会存在一个 bug：即写寄存器这一步发生在整个数据流的最后一步，要写入寄存器的内容和要写的寄存器号其实都是在最后一步 WB 中给出的。因此在第二步 ID 中得到的要写的寄存器号在这一步中没有用到，而是需要随着整个数据通路一直向后传递，直到最后一步，然后传回来。上图中蓝色的线就是对这个问题的修改。 这里先不考虑发生冒险的情况，以下面这段程序为例： 12345lw $10,20($1)sub $11,$2,$3add $12,$3,$4lw $13,24($1)add $14,$5,$6 运行过程中流水线中的内容： 把上图中第五个时钟周期中每个部分执行的内容连起来看： 流水线每一级能够独立工作就是完全靠了每一级之间的这些流水线寄存器。 控制部分与前面的单周期数据通路类似，这里主要强调控制指令也是逐级往后传递，在本级没有用到的控制字直接传递给下一级的状态寄存器，给数据通路中的每一个组件加上控制信号等等即可。 4.7. Data Hazards: Forwarding versus Stalling然后来看看数据冒险的解决。 以下面这段代码为例： 12345sub $2, $1,$3and $12,$2,$5or $13,$6,$2and $14,$2,$2sw $15,100($2) 将上述代码画在图中即： 后面四条指令均用到了第一条指令的结果：其中第二条$2作为rs，第三条作为rt，第一条的结果还没来得及写入寄存器即马上要被调用，发生数据冒险虽然第三条的读取寄存器与第一条的写入寄存器发生在同一个时钟周期内，但是可以认为写入发生在前半个时钟周期内，读取发生在后半个时钟周期内，无冲突第四条读取寄存器时，数据已完成写入寄存器，无冲突 这里的解决方案是作数据转发，上图是从寄存器角度来看这个过程，如果从状态寄存器上看： 第二条指令的ALU需要输入时实际上答案已经算出来了，存在第一条的EX/MEM状态寄存器中，只要作一下转发即可避免冲突的产生。 转发这个操作从图上很好理解，但是发生在一个流水线通路中，那么从硬件实现上面就是增加多路选择器以及额外的控制模块（Forwarding Unit）来实现： 但是，另外仍然有一个无法用转发避免的数据冒险，发生在 lw 这样读取数据存储器的指令后： 这里正确的结果出现在MEM/WB状态寄存器中，解决方案只有将后面的指令作一个时钟周期的延时，以完成转发： “气泡指令”即输入一条空指令，将所有控制状态字都置为0，则整个数据通路中状态寄存器不发生改变，相当于保持原状延时1个时钟周期。 4.8. Control Hazards接下来来看一下控制冒险的问题。 碰到条件跳转等等情况，我们根本确定不了下一条指令是什么，那要怎么往流水线里面放指令呢？答案就是先默认选一条（比如默认先不跳转，直接把下一条指令放进来），然后把本来要在 EXE 级完成的比较计算提前。 通过增加额外的硬件，可以把 beq 判断的结果和转移地址的计算都控制在 ID 级完成，并默认先顺序读取下一条指令。当结果判断完毕时，转移的地址也同时被计算出来了。若分支未发生，则不需要改变，流水线任然可以看做是全速运行的；若分支发生，则因为此时是在 ID 级结束，只需要在给 IF/ID 一个 IF.Flush 信号删掉之前读入的指令，并读入正确的指令即可，相当于一次“气泡”延时。 另外考虑到 beq 指令之前，还可能出现数据冒险的情况：若 beq 前为 add、sub 这类的计算执行指令，结果将在 EX 级之后被计算得到，则需要将 beq 指令延迟一个时钟周期，等待数据转发；若 beq 前为 lw 这类的数据存储指令，结果将在 MEM 级之后得到，需要将 beq 指令延迟两个时钟周期，等待数据转发。 比默认选择一条路径更优的是利用“历史记录”等等进行动态对分支方向进行选择。 分支预测和指令预取这块深入下去还有很多人研究，也是一个很大的研究点，貌似这几年已经能达到80~90%的准确率了。 CPU 的分支預測器是怎樣工作的？ 4.9. Exceptions最影响流水线性能的是异常和中断，因为这类情况的发生是不可预知的，而发生时则需要暂停当前正在运行的程序，跳转执行至异常处理程序中。 在MIPS异常发生时，需要保存下当前的指令地址 EPC 以及错误发生的原因 Cause。 到这里为止，基本上完整的流水线数据通路已经完成了： 4.10. Parallelism via Instructions流水线技术其实表现的是计算机执行指令时的并行潜力，这里有个专门的词就叫指令级并行（ILP，Instruction Level Parallelism）。 本书中关于指令级并行的内容只有13页，基本上只是个概览，关于详细的要去看《量化分析方法》。 大汗……把这里的 13 页扩展到 200 多页。 进一步增强流水线的潜力一般有两种思路： 增加流水线的深度，这样流水线中就能够同时容纳更多的指令。当然，要修改流水线级数还需要考虑到重新把每一级的执行时间调整到差不多相同的程度（作为一个时钟周期）； 复制流水线中每一级的部件，让流水线在相同的周期内能同时运行多条指令（也是一种解决结构冒险的思路），这个通常叫做指令多发射，也即单周期多指令并行。 第一章中提到过 CPI 这个概念，如果单个时钟周期内可以处理多条指令，那么 CPI 的值就能做到小于 1，或者要改用 IPC（Instruction per clock cycle）来做性能指标了。工艺上提升主频，再从设计上提高 IPC，那么一个 4 GHz 的 4 路多发射处理器就能每秒处理 16 G 条指令。 然鹅理想很美好，要做到指令的多发射虽然能够显著地提升速度，但是随之而来的也有很多的问题： 怎么同时跑多条指令？这里有个发射槽的概念，那么就是说如何合理地将指令分配到不同的发射槽中，使得指令能够做到多发射？有多少条指令，哪些指令可以在一个指令周期内一起发射？ 指令多发射之后，数据冒险与控制冒险的情况就更为严重了。 第一个问题的解决方案基本上还是要靠前面的分支预测技术，只不过要考虑的方面要更复杂了，单独有个单词来特指这个部分的工作：推测（Speculation）。 多发射通常采用两种思路： 静态多发射：简单地说就是将前几节的数据通路作进一步拓展，每一级流水线中都有多套设备，或者干脆就是把一条数据通路复制几份，这样就能一次读取多条指令，一次同时执行多条指令了。主要依靠编译器来帮助把指令打包在一起以及处理各种冒险，编译器需要针对特定结构的CPU作优化，如果要移植到不同的设备上，需要作重新编译。这种思路最开始的实现叫超长指令字（VLIW），即这种实现结构中指令特别长，单条指令中有多个操作数，定义了一组相互独立的操作。 为了保证高效，就需要尽可能地让每条流水线都跑满，这个想想就是一个特别复杂的问题。 动态多发射，又叫超标量（Superscalar）：扩展了基本的多发射思路，在编译器排好指令顺序之后，处理器按顺序读取指令，然后由处理器来决定能不能同时发射多条指令进去跑，核心是要依靠动态流水线调度技术，流水线结构相对前面几节的数据通路来说应该已经完全改变了。程序经编译器优化后得到的代码应该始终是正确的，编好的代码放到不同结构的超标量处理器里面都应该能够得到相同的结果。 超标量流水线的结构已经不同于前面的简单数据通路了，而是分成了 3 个部分，一个指令发射单元，多个功能单元，一个提交单元，顺序发射指令，乱序执行指令， 最后顺序提交指令： 每个功能单元里面都有个保留站（Reservation Station）用于存储当前指令的操作数和操作码，只要功能单元硬件就绪，并且所有操作数就绪，它就可以执行了，当计算已经执行完毕，结果就会被马上转发到其他正在等待的保留站，同时存入提交单元的重排缓存（Reorder Buffer）中，经过提交后写入寄存器或者存储进内存等等。 发射单元按顺序取指令并译码，将操作码、操作数发送给合适的功能单元去执行，存入对应功能单元的保留站，任何在寄存器或者重排缓存中就绪的数据也会直接被拷贝进来。一旦指令被发射出去，数据完成了到保留站的拷贝，则原寄存器的数据锁定也可以解除了，若此时有写寄存器的操作发生，可以直接覆盖，因为正确的内容已经被功能单元保存下来了； 若保留站中的数据没有准备好，则该单元需要等待至数据就绪（数据完成转发）之后再进行计算。 这个地方的处理思路，就是数据流啊！！！ 为了使整个过程看上去像是与顺序流水线一样，取指与提交都要是顺序的：取指和译码单元顺序发射指令，并同时记录程序中的依赖关系。而提交单元也需要按照顺序将结果写回寄存器和存储器。当异常发生时，处理器可以找到最后执行的那一条指令。 4.11. Real Stuff: The ARM Cortex-A8 and Intel Core i7 Pipelines这里简单介绍了 ARM Cortex-A8 和 Intel Core i7 920 这两块现代处理器的流水线结构。 Processor ARM A8 Intel Core i7 920 TDP( Thermal design power) 2W 130W Clock rate 1GHz 2.66GHz Cores/Chip 1 4 Floating Point? No Yes Multiple Issue? Dynamic Dynamic Peak instructions/clock cycle( IPC) 2 4 Pipeline Stages 14 14 Pipeline schedule Static In-order Dynamic Out-of-orderwith Speculation Branch prediction 2-level 2-level 1st level caches/core 32KB I, 32 KB D 32 KB I, 32 KB D 2nd level caches/core 128 - 1024 KB 256 KB 3rd level cache(shared) - 2 - 8 MB ARM A8 的流水线是动态多发射，但是静态顺序运行，其 14 级流水线结构如下： 前三级能同时取出 2 条指令，并维护一个 12 条指令的预取缓存。两级分支预测，但是没有在这里做额外的预测判断，所以如果预测错了就要等 13 个时钟周期之后才能知道。 接下来是把一对（2条指令）进行五级指令译码，这里也负责判断数据依赖等等。 之后是六级指令运行流水线。 SPEC2000 测试集跑下来的结果是，CPI 最好的情况 1.4，最差 5.2，跟理想的 0.5 还是有一定差距的。平均情况 80% 的停顿是源于流水线冒险（分支预测错了、结构冒险、数据依赖）。这种流水线结构基本上是要完全依靠编译器的指令重排来解决结构冒险和数据依赖的问题。 x86 处理器的流水线结构就要复杂多了： x86 是一种复杂指令集，一条指令在译码的时候会先转换成一种类 MIPS 结构的指令（微操作，micro-operations）。配合动态多发射，动态调度，乱序执行，分支预测等等，最高可以达到单指令周期 6 个微操作的速度。 指令读取：多级分支预测、涉及 cache 等等一堆复杂的部分，最终从指令 cache 中得到 16 个字节的指令数据。预测错误需要浪费大约 15 个指令周期。 指令预译码：16 个字节的指令数据预译码后变成微操作码（micro-op code），放入指令队列。 微操作译码：由于 x86 指令集的复杂性，这里还分不同的微操作译码器，译码完成后按照原本指令的顺序放入微操作 buffer。 循环流检测：如果找到了特定条件的一系列循环操作，就能直接发射这些微操作了，而不必再经过指令读取和指令译码（？？？）。 开始进行基本的指令发射：在寄存器表中查找寄存器位置、寄存器重命名、分配重排缓存入口、从寄存器或者重排缓存中提取结果等等。 保留站发送操作给功能单元进行执行，6个功能单元最多同时支持6个操作的执行。 功能单元执行操作，把计算完的结果转发给其他要用的保留站，同时发给寄存器回收单元来更新寄存器。重排缓存中的对应内容标记成完成。 当重排缓存中的一条或多条指令标记为已完成，则可以执行寄存器更新等等操作了。 4.12. Going Faster: Instruction-Level Parallelism and Matrix Multiply循环展开加上AVX指令的SIMD，直接把性能提高了 8.8 倍。 6666666666666 4.13. Advanced Topic: An Introduction to Digital Design Using a Hardware Design Language to Describe and Model a Pipeline and More Pipelining Illustrations提升主题：介绍如何使用一种硬件设计语言来描述和模拟流水线 PDF材料 4.14. Fallacies and Pitfalls谬误：流水线很简单 呵呵。。。动态多发射乱序执行的这个想想就可怕，看了 i7 的流水线结构就知道这玩意有多复杂了。 谬误：流水线思想可以与工艺无关 陷阱：没有考虑到指令集的设计会反过来影响流水线 4.15. CondudingRemarks4.16. Historical Perspective and Further ReadingPDF材料 4.17. Exercises答案 CHAPTER 5. Large and Fast: Exploiting Memory Hierarchy这章的内容主要是存储器结构层次，大部分篇幅在 cache 上。 5.1. Introduction 越靠近CPU，速度越快，容量越小，价格越高。 越远离CPU，速度越慢，容量越大，价格越低。 5.2. Memory Technologies一般常见的几种存储器技术： |存储器技术|典型访问时间|价格/GB|特点||-|-||SRAM半导体存储器|0.52.5 ns|$5001000|数据用晶体管存储，可直接读取数据||DRAM半导体存储器|5070 ns|$1020|数据用电容存储，会不断漏电，需要定期刷新（充电）||FLASH半导体存储器|5,00050,000 ns|$0.751.00|EEPROM，电擦除可编程存储器，有读写次数上限||磁性硬盘|5,000,00020,000,000 ns|$0.050.10|用磁盘、磁头等进行磁性存储和读写| 5.3. The Basics of CachesCache是CPU直接用来进行读取的存储器，在第四章的数据通路部分没有指明的是，指令和数据都必须要到了Cache中才能够被CPU访问。 整个Cache区域分成多个条目，每个条目的存储结构由3部分组成：本条是否有效、条目标签以及数据块内容 低位地址作为Cache索引条目的索引地址，高位地址作为一条Cache内容的标签号，用Valid标识该条目是否有效，最后将连续的多个字节内容存入Cache数据块中 查找内容时，首先根据地位地址找到索引条目，对比该条的标签与高位地址一致并且条目为有效则访问命中，可以直接读取数据块内容给CPU；若没有命中，则要将数据请求下放一级，并从下一级获取数据到该级，然后返回数据给CPU，在取数据到Cache的过程中，CPU需要等待数据到位 若缓存Cache块中同时存放了多个字的数据，则可以进一步加一个多路选择器来解决： 正如上面所介绍的，读取数据时CPU首先从Cache中找数据，若未命中，则要多花时间先从下一级提取内容至Cache中，这里就造成了Cache延时；而写入数据时，也会发生冲突：需要考虑用写直达法，同时更新Cache与下一级存储器中的内容，或者写回法，先写到Cache，当该块Cache需要被替换时再写回存储器。 5.4. Measuring and Improving Cache Performance 首先是评估Cache的性能 第一章中的CPU时间可以作进一步的划分： $$CPU时间=(CPU执行时钟周期数+存储器阻塞时钟周期数)*时钟周期时间$$ 阻塞时间： $$存储器阻塞时钟周期数=读取阻塞周期数+写入阻塞周期数$$ $$读取阻塞周期数=\frac{读取次数}{程序数}*读取缺失率*读取缺失延时$$ $$写入阻塞周期数=\frac{写入次数}{程序数}*写入缺失率*写入缺失延时+写缓冲区延时$$ 将写入与读取统一为存储器操作，则： $$\begin{align}存储器阻塞时钟周期数&amp;=\frac{存储器操作数}{程序数}*缺失率*缺失延时\&amp;=\frac{指令数}{程序数}*\frac{缺失数}{指令}*缺失延时\end{align}$$ Cache的缺失和阻塞是不可能避免的，而经过简单的计算分析，可以发现Cache的阻塞延时对整体运行时间的影响甚至超过了CPI对运行时间的影响！即如果只是单纯地提高CPU的速度，而存储器的速度没有跟上的话，整体性能的流失会更加严重。 另外一个比较重要的因素与Cache的大小有关：一味地加大Cache的大小也是没有用的，Cache越大则访问Cache本身的时间将会延长。 资料： Cache为什么有那么多级？为什么一级比一级大？是不是Cache越大越好？ 将访问Cache的时间和Cache命中情况综合考虑，得出一个平均存储器访问时间（Average Memory Access Time）： $$AMAT=命中时间（访问Cache的时间）+缺失率*缺失延时$$ 通过更灵活地调整Cache的结构，改变关联度来减少缺失率 5.3节中介绍的Cache结构是直接匹配的方式，虽然存放和查找非常方便，但是存在的问题是出现多个具有相同Cache索引的地址的反复调用时，就需要反复覆盖同一条索引记录，而每一次的访问都会是Cache缺失的 全相联方式：任意的地址都能任意存在Cache的任意一条索引上…这个，每次查找的时候就需要把整个Cache遍历一遍…想想都慢，虽然多加几个判断硬件可以做成并行的，但是成本也加大了。适用于Cache很小的时候。 折中的办法是组相联方式：首先还是按照直接匹配的方式计算索引号，然后就是一条索引号对应一个集合中的几个数据块，这样可以同时存放索引相同的几个不同地址的记录。 然而整个Cache的物理大小是固定的，增加组数，就是减少了索引数。需要在缺失率和命中时间之间平衡考虑。 在组联合结构中，为了使一条索引中的每块数据同时判断（并行判断），增加一些比较器即可： 多级Cache 由于物理条件的局限性以及高时钟频率的要求，一级Cache的容量是有限的。为了进一步减少缺失率，采用多级Cache结构。 二级Cache通常是一级Cache的10倍以上。当一级Cache缺失时，首先会到二级Cache中找，二级Cache的访问速度肯定是要比直接访问到更下层的主存储器更快的。 与上面的关联度相同，二级Cache太大则访问的时间也会变长，多级Cache也需要根据实际情况权衡考虑。 软件优化 在代码上，如果同一块操作（比如多个嵌套循环）需要用到的数据能够用Cache一次性存下来，那么就能保证在操作过程中的Cache访问都能够命中。 反之，如果需要重复访问的数据太多，那么访问到后面的数据时就会覆盖掉前面的，然后再嵌套回到前面的操作又覆盖掉了后面的数据，增加了很多不必要的Cache缺失 5.5. Dependable Memory Hierarchy先来定义一些概念： 故障：机器从正常的运行状态中被中断，跳到对应的故障解决服务中。若不能从故障中恢复就是永久性故障，故障还可能是间歇性的。 可靠性：估量机器从某一点开始能够持续正常服务的时长。 平均故障时间（Mean Time To Failure）：平均持续正常工作不发生故障的时长。 年故障率（Annual Failure Rate）：给定MTTF之后，一年中发生故障的时间比率。 平均修复时间（Mean Time To Repair）：一旦发生故障之后，从故障中恢复平均需要花费的时间。 平均故障间时间（Mean Time Between Failure）：MTTF和MTTR的总和 有效性：用于估量机器在整个故障及恢复过程中正常工作市场的比率。 $$有效性=\frac{MTTF}{MTBF}=\frac{MTTF}{MTTF+MTTR}$$ 可见要想提高有效性，就需要减小MTTR或者增加MTTF。 为了提高MTTF的三种方案： 从设计上避免故障发生； 依靠冗余备份，使得即使故障发生了也能够正常运行； 提前预测可能发生故障的情况，并提前修正错误。 5.6. Virtual Machines虚拟机的概念在近期重新兴起，主要是源于： 现代系统中的隔离和安全性变得越来越重要 标准操作系统中存在安全性和可靠性方面的故障 计算机的共享，尤其是云计算近几年的发展 处理器的原始速度在近期得到了非常迅猛的提升，因此虚拟机的开销变得更加可接受了 这一节虽然短，但是留下来的疑问比较多，回头要重新花时间看下。 比如I/O-intensive、I/O-bound，似乎后者表示I/O密集，但是前一个单词的intensive本身就是密集的意思，可是这两个词还应该是不一样的。。。 5.7. Virtual Memory另开了一篇单独记录： 学习笔记：硬件/软件接口 Virtual Memory 5.8. A Common Framework for Memory Hierarchy这一部分算是对前面内容的一次总结。 |机制名称|组数|每组的数据块数|定位方法|需要比较的次数||-|-||直接映射|Cache中的块数|1（关联度最小）|索引|1||组相联|Cache中的块数/关联度|关联度（一般为2~16）|按组索引，组中分别查找|关联度||全相联|1|Cache中的块数（关联度最大）|遍历|Cache中的块数||全相联|1|Cache中的块数（关联度最大）|独立的查找表|0| 随着关联度的增加，缺失率下降，但是访问时间与设备的代价增加。 全相联的情况下，若采用遍历的方式查找条目，则耗时很久；若采用单独的查找表，则设备上花费很大。 可以采用3C模型来评价Cache的整体表现。 5.9. Using a Finite-State Machine to Control a Simple Cache使用有限状态机来控制一个简单的Cache 附录D：PDF材料 5.10. Parallelism and Memory Hierarchies: Cache Coherence多处理器的Cache中还可能存在一些问题，也是并行一定会出现的问题，即：相同数据的冲突问题。 这里提到的与一般的并行冲突处理思路没什么不同。 5.11. Parallelism and Memory Hierarchy: Redundant Arrays of Inexpensive DisksPDF材料 5.12. Advanced Material: Implementing Cache ControllersPDF材料 5.13. Real Stuff: The ARM Cortex-A8 and Intel Core i7 Memory Hierarchies这里是简单介绍了 A8 和 i7 里面的 cache 结构。 具体内容也没什么特别的，就是 i7 的 TLB 都有 2 级……666……以前还真没想过。 5.14. Going Faster: Cache Blocking and Matrix Multiply5.15. Fallacies and Pitfalls陷阱：写程序或者在一个编译器中生成代码时，忽略了存储器系统的行为。 陷阱：在模拟Cache时，忘记说明字节编址或者Cache块的大小。 陷阱：对于共享Cache，组相联度少于核心的数量或者共享该Cache的线程数。 陷阱：用存储器平均访问时间来评估乱序处理器的存储器层次结构。 陷阱：在未分配地址空间的顶部增加段来扩充地址空间。 谬误：域内磁盘错误率与他们的额定值相匹配。 谬误：操作系统是规划磁盘访问的最好的地方。 陷阱：在不为虚拟化设计的指令集系统结构上实现虚拟机监视器。 5.16. Goncluding Remarks5.17. Historical Perspective and Further ReadingPDF材料 5.18. Exercises答案 下一部分： 计算机组成与设计.硬件/软件接口 学习笔记（三）]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>组成与设计.硬软件接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（二）：TF 的数据流模型实现以及自动求导]]></title>
    <url>%2F2018%2F01%2F23%2F2018-01-23-tfunpacking2%2F</url>
    <content type="text"><![CDATA[接上篇： TensorFlow 拆包（一）：Session.Run() 写着写着越写越多了，所以想想还是分成多篇来了，要不一页内容有点多。 Control Flow in TF从 Executor 的运行实现里面往下继续的时候遇到了点问题，代码里面有个叫 Frame 的概念，但是注释里面很多东西都写的不清不楚的，不知道在干吗，于是在网上找了点关于 TF 的整个控制流方面的资料： Request for documentation: Loop implementation - GitHub Implementation of Control Flow in TensorFlow 核心的问题是从 TensorFlow 的循环控制里面引出来的，这块内容具体涉及到的也其实就是 TensorFlow 中的数据流模型的实现了，这里的数据流模型原型是基于 Jack Dennis 和 Arvind 等人所提出的数据流机。 TensorFlow 中，每一个 Op 都在一个 Execution Frame（前面看代码见到的结构）中运行，由以下五种控制流原语来负责 Execution Frame 的创建和维护。例如每个 while 循环在跑的时候都会先创建一个 Execution Frame，然后循环里面的所有 Op 都会在这个 Frame 里面完成运行。Frame 也可以嵌套，比如说嵌套的循环就会有嵌套的 Frame。不同 Frame 中的 Op 只要相互之间没有依赖的都可以并行运行。 Switch：根据控制 p，把输入 d 传到某个输出里。只要 d 和 p 都可用时就可以执行 Merge：把某个输入传到输出里。只要有任意一个输入就能执行，如果有多个输入都可用，这里并不会指明最终会输出哪一个输入 Enter(“name”)：把输入传到某个 Frame 里。这个 Op 用于把一个 Tensor 传到某个子 Frame 里去。一个 Frame 也可以有多个 Enter Op，Frame 在它的第一个 Enter Op 执行的时候完成实例化 Exit：从某个子 Frame 中把数据传出来（传给它的父 Frame）。一个 Frame 也可以有多个 Exit Op NextIteration：把输入传到当前 Frame 的下一次迭代中。例如 while 循环的每一次迭代执行的 Op 是一样的，TF 运行时会记录下 Frame 的迭代次数。一个 Frame 中也可能会有多个 NextIteration Op。当第 N 次迭代中的第一个 NextIteration Op 执行的时候就开始了第 N+1 次迭代，第 N 次迭代每提交一个 NextIteration Op，第 N+1 次迭代就会有一个 Tensor 输入可用，然后激活后续执行的 Op 用上述的这几个原语就可以完成一些复杂操作了，例如： 1tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y)) 可以这样实现： 然后对于一个 while 循环： 1tf.while_loop(lambda i: i &lt; 10, lambda i: tf.add(i, 1), [0]) 实现出来是这样的： 上面这个例子里面的循环非常简单，当循环更复杂一点，例如变量有多个时，就会有多个 Enter、多个 NextIteration 和 多个 Exit Op。这样每一层迭代中的不同部分就能够充分发挥并行性。 Executor 中运行产生的每个 Tensor 都可以用一个三元组来表示(value, is_dead, tag)，value 显然是实际的数据，is_dead 用于标识当前 Tensor 是不是在一个选择语句的 untaken branch 上（感觉怎么翻译都难受，还是原文好），tag 这个标记是唯一的，其中包含了 Executor 上下文信息、Tensor 的生产者等等，在 send/recv 的过程中 tag 也是通信 key 的一部分。 分布式环境下有可能出现条件选择语句在 Device A 上，某个分支的具体执行在 Device B 上的情况。运行时中遇到 source node 都是无条件执行的，例如 recv Op 也是一种 source node，所以实际实现的时候会做 is_dead 标记的广播。 Distributed While Loop再来看一下分布式环境下的循环是怎么实现的。 一种 naively 的实现是这样的： 这里比较尴尬的的是，循环的控制体在 Device A 上，循环体却是在 Device B 上（话说实际会有这种划分方式吗？想一下 GPU 的运行是不是就是这样的呢？控制 batch 和 epoch 的部分在 CPU 上，然后中间实际的图的运算在 GPU 上？）。 按照前面原本的实现方式，Device B 并不知道自己所跑的子图是一个循环的一部分，因此按照正常情况来处理，它所负责的每个 Op 都只执行一次就结束了，那如果要让这个循环一直跑下去，可能就要在每次 Device A 调用 send 之后，Device B 再创建一个新的 Executor Frame，果然是 naive 啊。 TensorFlow 的解决方案是： 在 Device B 上加上一个控制循环的状态机，把 Device A 中的 switch 和 next 也加到 B 上去。使用这种方式实现的循环在更多设备参与的更复杂循环中能够发挥出更好的并行性。例如 Device B 只要接收到了 P 就能开始下一层迭代或者退出了，而不用等到 Device A 跑完循环的其他部分，多个不同的设备也可以在同一时刻跑着一个循环的不同迭代层的内容。接收一个 P 的开销可以被高度的并行性隐藏掉。 嵌套循环的情况类似，也是继续构造一个状态机，把外层循环的 switch 输入到内层循环状态机的 Enter 里去即可。 Automatic differentiation首先需要理解 BP 算法里面的梯度计算规则，参考资料在这里： Calculus on Computational Graphs: Backpropagation 基本原理就是链式求导法则，这里有个前向求导和反向求导的区别，例如对于一个这样的计算过程： 前向求导是从第一层开始，逐层计算梯度 $\frac{\partial}{\partial X}$ 到最后一层： 反向求导是从最后一层开始，逐层计算梯度 $\frac{\partial Z}{\partial}$ 到第一层： 前向求导关注的是输入是怎么影响到每一层的，反向求导则是关注于每一层是怎么影响到最终的输出结果的。 对于一个多输入单输出的计算网络，如果想要得到每个输入和输出之间的关系，用前向求导需要对每一个输入都做一次，而用反向求导则只需要做一次就可以得到输出和每个输入之间的关系。 对于单输入多输出的计算网络，则应该用前向求导才能一次获得一个输入对多个输出的关系。 神经网络中每一层的参数都算是一种输入，可以说是输入数量远大于输出数量的情况，因此用反向求导更加合适。 举个栗子： 假定有一个网络，输入数据是 $X_{in}$（形状是 N x D），结果一共有 C 种分类，则对应的 Label 一共有 N 个，处理之后每一个都是一个 1 x C 的向量，每个向量中都只有唯一一个位置是 1，代表了这个样本属于哪个类，其他 C-1 个位置都是 0，设这个对应的结果是 $Y_{label}​$ （形状是 N x C）。 网络有多个层，每层有参数（$W_i$ 和 $b_i$）： 这个过程写成公式大概是： $$\begin{align}Y_0 &amp;= f_0(W_0, b_0, X_{in})\Y_1 &amp;= f_1(W_1, b_1, Y_0)\Y_2 &amp;= f_2(W_2, b_2, Y_1)\&amp;…\Y_n &amp;= f_n(W_n, b_n, Y_{n-1})\Y_{out} &amp;= Softmax(Y_n)\\end{align}$$ BP 算法的训练过程则是根据网络计算得到的 $Y_{out}$ 和实际的真实结果 $Y_{label}$ 来计算误差，并且沿着网络反向传播来调整前面公式中提到的所有 $W_i$ 和 $b_i$，使误差达到最小。 强调一下，深度学习里面 BP 的本质目标是让误差达到最小，所以要用误差对中间出现过的所有影响因素求偏导。 最后一层的误差和梯度：$$\begin{align}Error &amp;= Loss(Y_{out}, Y_{label})\\Delta{W_n} &amp;= \frac{\partial Error}{\partial W_n} = \frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial W_n}\\Delta{b_n} &amp;= \frac{\partial Error}{\partial b_n} = \frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial b_n}\end{align}$$ 看一下上面两个等式右边的部分，$Y_n$ 对 $W_n$ 和 $b_n$ 的导数只与第 n 层的计算有关（全连接、卷积或者其他什么，每一种层都直接有自己固定的梯度计算公式）。 $Error$ 对 $Y_n$ 的导数则是要看最后的损失函数（最常见的如 Softmax 和交叉熵）怎么定义，也有直接固定的梯度计算公式 倒数第二层以后：$$\begin{align}\Delta{W_{n-1}} &amp;= \frac{\partial Error}{\partial W_{n-1}} = \frac{\partial Error}{\partial Y_{n-1}}\color{red}{\frac{\partial Y_{n-1}}{\partial W_{n-1}}}=\frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial W_{n-1}}\\Delta{b_{n-1}} &amp;= \frac{\partial Error}{\partial b_{n-1}} = \frac{\partial Error}{\partial Y_{n-1}}\color{red}{\frac{\partial Y_{n-1}}{\partial b_{n-1}}}=\frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial b_{n-1}}\\Delta{W_{n-2}} &amp;= \frac{\partial Error}{\partial W_{n-2}} = \frac{\partial Error}{\partial Y_{n-2}}\color{red}{\frac{\partial Y_{n-2}}{\partial W_{n-2}}}=\frac{\partial Error}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial Y_{n-2}}\frac{\partial Y_{n-2}}{\partial W_{n-2}}=\frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial Y_{n-2}}\frac{\partial Y_{n-2}}{\partial W_{n-2}}\\Delta{b_{n-2}} &amp;= \frac{\partial Error}{\partial b_{n-2}} = \frac{\partial Error}{\partial Y_{n-2}}\color{red}{\frac{\partial Y_{n-2}}{\partial b_{n-2}}}=\frac{\partial Error}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial Y_{n-2}}\frac{\partial Y_{n-2}}{\partial b_{n-2}}=\frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial Y_{n-2}}\frac{\partial Y_{n-2}}{\partial b_{n-2}}\&amp;…\\Delta{W_0} &amp;= \frac{\partial Error}{\partial W_0} = \frac{\partial Error}{\partial Y_0}\color{red}{\frac{\partial Y_0}{\partial W_0}}\\Delta{b_0} &amp;= \frac{\partial Error}{\partial b_0} = \frac{\partial Error}{\partial Y_0}\color{red}{\frac{\partial Y_0}{\partial b_0}}\end{align}$$ 公式里面标红的部分都只与该层的计算内容相关，直接有梯度公式。 那么另外的那部分呢……$Error$ 对 $Y_i$ 的求导其实可以一路链式求下去： $$\begin{align}\frac{\partial Error}{\partial Y_{n-1}} &amp;= \frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial Y_{n-1}}\\frac{\partial Error}{\partial Y_{n-2}} &amp;= \frac{\partial Error}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial Y_{n-2}}=\frac{\partial Error}{\partial Y_n}\frac{\partial Y_n}{\partial Y_{n-1}}\frac{\partial Y_{n-1}}{\partial Y_{n-2}}\&amp;…\\frac{\partial Error}{\partial Y_i}&amp;=\frac{\partial Error}{\partial Y_{i+1}}\frac{\partial Y_{i+1}}{\partial Y_i}=\frac{\partial Error}{\partial Y_{i+2}}\frac{\partial Y_{i+2}}{\partial Y_{i+1}}\frac{\partial Y_{i+1}}{\partial Y_i}=…\end{align}$$ 这中间 $Y_{i+1}$ 对 $Y_i$ 的求导即每一层的输出对输入求导，也是只与每层的计算内容有关。 这就是整个神经网络的完整训练过程了。 话说，链式求导法则本身很简单，关键这里面的每个变量都坑爹的是个矩阵啊。。。矩阵对矩阵求导的具体计算要涉及到矩阵微分了，这个恕我数学不好，没学过。。。溜了。。。 这里有一个关于怎么求矩阵和矩阵的梯度的介绍： 【道理我都懂，但是神经网络反向传播时的梯度到底怎么求？】 TensorFlow 的做法是每一个 Op 在建图的时候就同时包含了它的梯度计算公式（即上面公式中的 $\frac{\partial Y_i}{\partial Y_{i-1}}$、$\frac{\partial Y_i}{\partial W_i}$、$\frac{\partial Y_i}{\partial b_i}$），构成前向计算图的时候会自动建立反向部分的计算图，前向计算出来的输入输出会保留下来，留到后向计算的时候用完了才删除。 对前面描述的5个控制流原语来说，反向的梯度部分很大程度就是把整个图翻一下，Enter 和 Exit 互换、输出和输入互换、Switch 和 Merge 互换等等，当然很多细节上还要再修正一下。 看到这个地方我感觉有点明白了为什么要设 Frame 这个概念了。 有 Enter 和 Exit 这两个控制流原语在，再加上数据流图单向依赖的这个特性，图的结构是具有结合律的，一个大 Frame 就能很方便地拆成多个子 Frame。 反向求梯度的这个过程是一样的，以 Frame 为单位反向串起来就好。 PropagateOutputs()接上篇，后面回来看看一个 node 处理完了之后是怎么做后续的新 node 的，这里就出现了前文提到的 Frame 的概念。 tensorflow::(anonymous namespace)::ExecutorState::PropagateOutputs()根据当前处理的 node 的类型分成了 4 种情况： 普通计算 node： 调用tensorflow::(anonymous namespace)::ExecutorState::FrameState::ActivateNodes()来激活当前 node 的后继 node，主要内容是检查后继 node 的依赖是否满足，是则加入 ready 队列； 调用tensorflow::(anonymous namespace)::ExecutorState::FrameState::DecrementOutstandingOpsLocked()来递减当前 frame 的当前次迭代的 outstanding_ops，减到 0 的时候就表示当前次迭代已经完成了。 Enter node： tensorflow::(anonymous namespace)::ExecutorState::void ExecutorState::FindOrCreateChildFrame()，进入一个子 Frame，如果是第一次调用 Enter，则创建一个新的 Frame； 判断是否作为循环变量，否则作为一般 node 处理，调用 ActivateNodes() 激活后继 node； DecrementOutstandingOps()。 Exit node： 退出当前 Frame，用 ActivateNodes() 激活父 Frame 中的后继节点； DecrementOutstandingOps()。 Next_iter node： 判断目标迭代次数是否达到，如果是就保存状态退出； 继续迭代则调用 tensorflow::(anonymous namespace)::ExecutorState::FrameState::IncrementIteration进入下一个迭代，如果是第一次调用，会对下一次迭代做初始化； DecrementOutstandingOpsLocked()。 到这里为止，当前 node 的计算过程才算完全结束，之后再对 node 所属的 Frame 判断一下是否结束即可。 Optimizer in TF引用一下这个知乎问题： tensorflow的自动求导具体是在哪部分代码里实现的？ 回到 TensorFlow 的 Python 代码层面，自动求导的部分是靠各种各样的 Optimizer 串起来的，构图的时候只需要写完前向的数据流图部分，然后在最后加上一个 Optimizer（例如 GradientDescentOptimizer、AdamOptimizer 什么的），然后调用它的 minimize() 方法就会自动完成反向部分的数据流图构建。 tensorflow/python/training/optimizer.py 中定义了所有 Optimizer 的基类，从介绍中可以看到，整个反向通路的构建过程其实是分成三个部分的： 计算每一个部分的梯度，compute_gradients() 根据需要对梯度进行处理 把梯度更新到参数上，apply_gradients() minimize()的完整定义： 1234def minimize(self, loss, global_step=None, var_list=None, gate_gradients=GATE_OP, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None) 它就是一次性把上面这些都做了，函数体中只有很短的几行，串起计算流图，逐层计算梯度，往最小化 loss 的方向更新 var_list 中的每一个参数。 compute_gradients那么再来看一下compute_gradients()做了什么事情： 如果没有给定 var_list，则这里会去自动找到计算图中所有的 trainable_variables 放到 var_list 里面去，这些就是整个网络中的参数 往图中插入一个 gradients 的 Op，所以很明显反向求导的这个串图的过程就是在这里完成的了！！ 这个方法的返回结果是一个(gradient, variable)的 list 接下来就看看 gradients，先来做一个简单的测试： 12345678910In [2]: a = tf.Variable(1)In [3]: b = a * 2In [4]: g = tf.gradients(a + b, [a, b])In [5]: gOut[5]: [&lt;tf.Tensor 'gradients/AddN:0' shape=() dtype=int32&gt;, &lt;tf.Tensor 'gradients/Fill:0' shape=() dtype=int32&gt;] g 中的两个部分分别是对 a 和 b 进行求导的结果。后面创建一个 tf.Session() 跑一下得到的 g 是 [3, 1]。 gradients 的实际定义在 tensorflow/python/ops/gradients_impl.py中。把整个求导过程抽象成一个 $ys = f(xs)$ 的函数，xs 就是 var_list 里面输入的变量（在这个过程中其实这里存的是每个变量对应过来在计算图中的 op）。 根据原本计算图中所有的 op 创建一个顺序的 list，这个顺序在图上来说其实也是拓扑序，反向遍历这个 list，对每个需要求导并且能够求导的 op（即已经定义好了对应的梯度函数的 op）调用其梯度函数，然后沿着原本图的方向反向串起另一部分的计算图即可（输入输出互换，原本的数据 Tensor 换成梯度 Tensor）。 这里有个叫 gate_gradients 的输入参数，用于控制梯度计算过程的并行性。 GATE_GRAPH 很好理解，即整个图中间的梯度计算（后向过程）和梯度更新是单独分开的，计算过程严格按照前向、后向、更新的步骤来，等到所有的参数都完成梯度计算之后，再统一发起更新。 GATE_NONE 和 GATE_OP 的差别在于梯度更新会不会影响到后续的其他计算。例如某个 op 有 n 个输入 $x_0, x_1, …, x_{n-1}$ ，梯度的计算和更新需要对所有这 n 个输入求导，在 GATE_NONE 模式下，$x_0$ 的梯度计算完了之后，对 $x_0$ 的更新就马上开始了，那么在算其他输入（例如 $x_{n-1}$）的梯度时，有可能此时 $x_0$ 的值已经变了（数学上来说这就 bug 了吧？），因此按注释中说的可能会出现“不可复现”的结果，因为这个过程不可预料，是先更新完还是梯度先计算完，可能跑两次的结果都不一定一样。 GATE_OP 即产生一些控制依赖，确定某个变量不再会被用到之后才进行更新，保证正确性的同时最大化并行性。 事实上 TensorFlow 中所有的 op 的并行性是由其数据流计算模型来保证的，实现上来看，GATE_GRAPH 是在所有梯度计算 op 后面加上一个 control_flow_ops，GATE_OP 类似。 apply_gradients接下来的这个部分是根据前面求好的梯度，增加一个更新前面涉及到的所有参数的操作，Optimizer 基类的这个方法预留了_create_slots()，_prepare()，_apply_dense()， _apply_sparse()四个接口出来。 后面新构建的 Optimizer 只需要重写或者扩展 Optimizer 类的某几个函数即可，所以在 TF 给出的几个默认的 Optimizer 定义都不是很长。 apply_gradients()核心的部分就是对每个 variable 本身应用 assign，如果有设了 global_step 的话，顺便再加个 1。 在看这部分的时候我还有个问题，global_step 是怎么保证在多个 worker 同时写的情况下不发生写冲突，在代码中并没有发现任何处理多线程上锁或者队列等等的操作。 后续： TensorFlow 拆包（三）：Graph 和 Node]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写了一个画框图的轮子]]></title>
    <url>%2F2018%2F01%2F18%2F2018-01-18-vizgraph%2F</url>
    <content type="text"><![CDATA[这事的起因是想给前面那篇 TF 拆包画个函数调用图，然后试了若干画图工具都不是很满意，最后准备试试看能不能造个轮子出来。 Electron 上手还是很容易的，即使对我这样前端基础甚少的第一次写这种项目的人，找了个别人做的网页实现照着做，也只花了一个下午就写出来一个初版了。 后来慢慢修修补补，往上加功能，例如图片导出、菜单、编辑器设置、Dot文件的打开和保存等等。 这个项目目前起了个名字叫 VizGraph： VizGraph 组成部件主要是： Node.js 的桌面应用框架 Electron 左边的超强网页编辑器 Ace Editor 核心是封装了 GraphViz 成 npm 包 的 Vis.js 借助 CircleCI 和 electron-builder，还能对每次 git push 做一次项目 build，创建 release 包。 真是让人想感叹： 开源的世界真的是方便啊。 版本号在写完我一开始想要的结果之前应该永远不会改到 1.0。 我原本的目标是想再写个图形化的编辑界面，拖 node 到画布上，然后可视化连线，右键可以添加文字、样式等等各种属性，根据这些东西生成 .dot 的脚本文件，然后再用 GraphViz 转换成比较好看的矢量图。 至于有了图形化编辑界面之后为什么不直接保存图片，而要曲线救国先转成 .dot 再用 GraphViz 转成图呢？ 。。。 。。。 还不是因为根本就不知道该怎么做吗。 直接存图感觉处理难度比较大，因为我想要的是框图出来最后要能是矢量图的格式。 如果不需要记录位置信息，只是把每一次添加 node，连边，右键设置参数等等的这样的操作变成文本保存，感觉上实现难度应该会更低一些。再有就是 GraphViz 本身真的是一个很强大的后端工具，生成出来的图片效果也是很让人满意的。 事实上直接拖图，然后保存结果成矢量图的这种需求我之前用的两个在线工具 Lucidchart 和 Gliffy 就可以用。再有就是 Visio 这种大杀器也是很强大的。 无论如何，最后还是做了这个东西出来。 那这个项目之后有空应该还会花时间去改，现在还有一些小想法还没完善，例如说保存关闭前的状态，下次打开程序的时候恢复，然后还有编辑器的设置页面重新调整一下，整体的界面样式也比较难看（可是我不会美工啊）等等。 当然最主要的问题是现在也没想好图形化编辑界面到底要怎么做，所以只能先搁着，回去干正事了。]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Node.js</tag>
        <tag>GraphViz</tag>
        <tag>VizGraph</tag>
        <tag>Electron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 拆包（一）：Session.Run()]]></title>
    <url>%2F2018%2F01%2F13%2F2018-01-13-tfunpacking%2F</url>
    <content type="text"><![CDATA[18年的第一篇，开一个估计又是会持续超长时间的坑。 要来拆包 TensorFlow 啦。 嗯，话说这件事情前年、去年就一直在做，做完 RDMA 写完论文就扔一边了，也没再整理过。没想到之后的工作还是回到了这里，所以重新过一遍，也好好整理一下。 GDB 调试大法扔一个当时写的简单指南： TensorFlow-SourceCode-Reading.pdf 最近的新版 TF 在 dbg 模式下编 GPU 版会有奇怪的 bug，Eigen 在做内存分配的时候有个地方会报错，Github 上也能找到相关的 issues，但是至少目前的 1.4、1.5 版本都还没解决。 于是尝试了一下曲线救国的方案，在 opt 模式下面手动在编译选项里加上“-g”，虽然还不清楚 dbg 模式具体加了什么，但是至少后面 gdb 调试的时候 label 都是有的，还算能用，于是就这么用了。 1$ bazel build -c opt --config=cuda --copt="-g" --cxxopt="-g" //tensorflow/tools/pip_package:build_pip_package 在要调的 python 代码前面加上这么一段： 12345import osPID = os.getpid()print('Program pid:', PID)print('Pause here to enter DBG')os.system("read") 然后 gdb -p PID进去就好了。 另外补充一个 python 和 gdb 的交互脚本。在 python 的源码目录下面有个 Tools/gdb目录，里面是一个 python 脚本 libpython.py。 启动 gdb 之后： 123456(gdb) python&gt;import sys&gt;sys.path.append('/path/to/libpython.py')&gt;import libpython&gt;end(gdb) ... 或者把libpython.py拷出来放到 gdb 启动的地方也行，比如为了让 gdb 能直接找到代码，通常我会在 TF 的目录下面开 gdb，那就把这个脚本放过去。导进来之后，后面 gdb 里面就会多出来一堆 py-开头的命令，除了 gdb 原有支持的查看 c/c++ 层面的信息以外，可以用这些新命令查看 python 层面的东西 TF’s Dirs源码中主要的部分在 /tensorflow目录下： 目录 说明 /c C++ API，也是一些 Python API 与 C 层的接口部分 /cc /compiler 即时编译的工具内容 /contrib 一些额外的库，大部分由第三方添加，其中一些正式确定的内容会移出去 /core TensorFlow 的核心运行时代码 /core/distributed_runtime 分布式运行时代码 /core/framework 运行时中相对最底层的架构部分，涉及到很多基础结构的定义、与 Protobuf 的结合部分等等 /core/graph 运行时中对计算图的定义和处理 /core/kernels 计算图中 Op 的核心计算部分（即 Op 的 Kernel 函数） /core/lib 运行时中调用的其他库的接口？ /core/ops C 部分的 Op 分成两个部分，核心计算函数在前面的 /kernels 目录中，这里存的是 Op 面向上层 Python 运行时的注册部分内容。详见 TensorFlow 拆包（三）：Graph 和 Node /core/platform 针对不同平台的额外内容 /core/profiler 运行时的调优工具？ /core/protobuf Protobuf 的定义 /core/util 其他的一些工具 /python TensorFlow Python 部分的运行时和 API Session.run()python 代码里面的 run 函数是Session 类的父类BaseSession里面来的。 BaseSession中对 run 这个方法有详细的说明，调用一次run是执行一遍数据流图， 在 TensorFlow 的训练代码中通常是在一个循环中多次调用sess.run()，一次 run 即为训练 过程中的一步。 fetches 是 run 方法的一个输入参数，这个参数可以是很多种形式的数据，run 最后的 返回值也会和 fetches 有相同的结构。 TF_Run()中间追了一大圈，最后 c++ 的入口函数是TF_Run()。 123456789101112131415161718192021#0 tensorflow::DirectSession::Run ( this=0x7fb03026c390, run_options=..., inputs=std::vector of length 0, capacity 0, output_names=std::vector of length 0, capacity 0, target_nodes=std::vector of length 1, capacity 1 = &#123;...&#125;, outputs=0x7fff23b07e50, run_metadata=0x7fff23b07ea0) at tensorflow/core/common_runtime/direct_session.cc:437#1 0x00007fb1130a469a in TF_Run_Helper ( session=0x7fb03026c390, handle=handle@entry=0x0, run_options=run_options@entry=0x0, input_pairs=std::vector of length 0, capacity 0, output_tensor_names=std::vector of length 0, capacity 0, c_outputs=c_outputs@entry=0x7fff23b08220, target_oper_names=std::vector of length 1, capacity 1 = &#123;...&#125;, run_metadata=run_metadata@entry=0x0, status=status@entry=0x7fb030307f60) at tensorflow/c/c_api.cc:698#2 0x00007fb1130a49d4 in TF_Run ( s=0x7fb03026bdd0, run_options=0x0, c_input_names=&lt;optimized out&gt;, c_inputs=0x7fff23b081d0, ninputs=&lt;optimized out&gt;, c_output_names=&lt;optimized out&gt;, c_outputs=0x7fff23b08220, noutputs=0, c_target_oper_names=0x7fff23b084d0, ntargets=1, run_metadata=0x0, status=0x7fb030307f60) at tensorflow/c/c_api.cc:753 然后再往下走到底是tensorflow::DirectSession::Run()，其中具体做的事情是： 累加 Session 计数器 处理输入，准备线程池，根据输入输出 tensor、目标节点，从当前 Session 中已有的 executor 中找是否存在 一个相同任务的 executor，找到则将其返回，否则创建一个新的 executor 设置一个 FunctionCallFrame，似乎是用来处理 executor 输入输出的一个接口结构。从前面解析过的输入中提取出具体的 Tensor，封装到 FunctionCallFrame 里面去 创建一个 RunState，用于标记运行的状态；一个 IntraProcessRendezvous 用于本地 Tensor 数据的管理；一个 CancellationManager，用于让 Session 响应 Session::Close() 中间还有用于预估网络情况相关的 cost_model、性能追踪工具等等的设置 接下来的一段就是让 executor 配合线程池去执行运行了，具体的代码是item.executor-&gt;RunAsync(args, barrier-&gt;Get()) 主线程这个时候会停下来等待 executor 跑完 结束返回之后，检查是否有输出，然后处理输出部分，返回到上层的 python 部分 Executor看一下 executor 的情况： 123456789101112131415161718192021222324struct PerPartitionExecutorsAndLib &#123; Graph* graph = nullptr; // not owned. Device* device = nullptr; // not owned. FunctionLibraryRuntime* flib = nullptr; // not owned. std::unique_ptr&lt;Executor&gt; executor;&#125;;struct ExecutorsAndKeys &#123; ExecutorsAndKeys() : step_count(0) &#123;&#125; std::atomic_int_fast64_t step_count; std::unique_ptr&lt;Graph&gt; graph; NameNodeMap name_to_node; std::unique_ptr&lt;FunctionLibraryDefinition&gt; flib_def; std::unique_ptr&lt;ProcessFunctionLibraryRuntime&gt; proc_flr; std::vector&lt;PerPartitionExecutorsAndLib&gt; items; std::unordered_map&lt;string, size_t&gt; input_name_to_index; std::unordered_map&lt;string, string&gt; input_name_to_rendezvous_key; std::unordered_map&lt;string, size_t&gt; output_name_to_index; std::unordered_map&lt;string, string&gt; output_name_to_rendezvous_key; DataTypeVector input_types; DataTypeVector output_types;&#125;; ExecutorsAndKeys-&gt;items里面的每一个元素都是一个 Executor 的衍生类。注释里面写了 Executor 的用法： 12345678910111213// Executor runs a graph computation.// Example:// Graph* graph = ...;// ... construct graph ...// Executor* executor;// TF_CHECK_OK(NewSimpleExecutor(my_device, graph, &amp;executor));// Rendezvous* rendezvous = NewNaiveRendezvous();// TF_CHECK_OK(rendezvous-&gt;Send("input", some_input_tensor));// TF_CHECK_OK(executor-&gt;Run(&#123;ExecutorOpts, rendezvous, nullptr&#125;));// TF_CHECK_OK(rendezvous-&gt;Recv("output", &amp;output_tensor));// ... ...//// Multiple threads can call Executor::Run concurrently. Executor 这个类本身只有一些基础接口，比较核心的是 RunAsync 这个虚函数接口，Run 函数是对 RunAsync 的同步封装。 为了搞清楚 items 里面的 Executor 到底是什么，回到上面看一些这个东西是怎么创建的，看一下DirectSession::GetOrCreateExecutors()这个函数： 首先根据输入、输出、以及参数等等情况，在 DirectSession 的记录（一个 unordered_map）里面找是不是前面有创建过一样的 Executor 如果没有找到那么就准备创建一个新的。 用CreateGraphs()创建出当前运行需要的图，这个地方产生的 graphs 具体是个什么东西先放着待研究，ek-&gt;items.reserve(graphs.size())这句表明等下创建的 Executor 的个数应该是跟 graphs 里面元素的个数相同的 处理 GraphExecutionState、FunctionLibraryRuntime 然后对 graphs 里面的每一个元素，分配运算设备、Runtime、用 GraphOptimizer 优化一遍，再用 NewLocalExecutor()创建对应的 Executor 放到 ExecutorsAndKeys 这个结构的 items 里面去 NewLocalExecutor()创建的是 ExecutorImpl，那问题就清楚了，ExecutorsAndKeys-&gt;items里面的每一个东西其实是个 ExecutorImpl 回到上面，看一下 ExecutorImpl 的 RunAsync 函数，又引出来一个新的结构： 123void ExecutorImpl::RunAsync(const Args&amp; args, DoneCallback done) &#123; (new ExecutorState(args, this))-&gt;RunAsync(std::move(done));&#125; RunAsync() &amp; ScheduleReady()每一次的ExecutorImpl::Run()都会产生一个 ExecutorState 的封装结构，它负责追踪 node 的前驱节点的状态，当依赖满足、可以执行的时候把它调度到线程池里面执行。例如ExecutorState::AsyncState这个结构就是相当于打包了整个运行环境的上下文信息，用于多线程执行 node，把 node 当前运行需要的资源信息全部包在这里面扔进线程池，这样等新线程跑完 node 的内容还能够继续后续工作。 具体对图的执行是这样的： 首先展开 context map，这个地方已经是对对应的 device 进行调用了，在目标设备上准备运行时的上下文 初始化 ready 队列，这里面放的 TaggedNode 就是图里面入度为 0 的 note，也就是 root_node 接下来调用ScheduleReady()，交给线程池开始跑。这个函数本身的描述是： 1234// Schedule all the expensive nodes in 'ready', and put all the inexpensive// nodes in 'ready' into 'inline_ready'.void ScheduleReady(const TaggedNodeSeq&amp; ready, TaggedNodeReadyQueue* inline_ready); 关键是这个 expensive node 到底是怎么定义的呢？在代码里面看的不是很清楚。ExecutorImpl::RunAsync()调用的时候是把 root_node 放在 ready 队列里面传进来，inline_ready 给的是空指针，那么看 ScheduleReady()的实现，这部分在这里执行之后就返回了： 123456789# executor.cc:2085if (inline_ready == nullptr) &#123; // Schedule to run all the ready ops in thread pool. for (auto&amp; tagged_node : ready) &#123; runner_([=]() &#123; Process(tagged_node, scheduled_usec); &#125;); &#125; return;&#125; 后面的部分先跳过，回头再看。 runner_()接下来考虑一下这个runner_()是什么东西： 12345678910111213141516class Executor &#123; ... struct Args &#123; ... typedef std::function&lt;void()&gt; Closure; typedef std::function&lt;void(Closure)&gt; Runner; ... &#125; ...&#125;class ExecutorState &#123; ... Executor::Args::Runner runner_; ...&#125; 这玩意就是个套了很多层的std::function&lt;void()&gt;！！！在 gdb 里面追踪起来像函数指针一样难受，只能继续翻源码看它是从哪来的。它的初始化在 ExecutorState 的构造函数里面直接完成，赋值的是上面传进来的 args.runner，向上需要追溯到tensorflow::DirectSession::Run()里面。 123456789101112131415161718192021# direct_session.cc:585Executor::Args::Runner default_runner = [this, pool](Executor::Args::Closure c) &#123; SchedClosure(pool, std::move(c));&#125;;for (const auto&amp; item : executors_and_keys-&gt;items) &#123; // TODO(zhengxq): support partial run. // TODO(zhengxq): if the device picks its own threadpool, we need to assign // less threads to the main compute pool by default. thread::ThreadPool* device_thread_pool = item.device-&gt;tensorflow_device_thread_pool(); if (!device_thread_pool) &#123; args.runner = default_runner; &#125; else &#123; args.runner = [this, device_thread_pool](Executor::Args::Closure c) &#123; SchedClosure(device_thread_pool, std::move(c)); &#125;; &#125; item.executor-&gt;RunAsync(args, barrier-&gt;Get());&#125; 好了，所以是runner_()就是 SchedClosure()，删掉这部分代码里面跟安卓有关的部分是这样的： 1234void DirectSession::SchedClosure(thread::ThreadPool* pool, std::function&lt;void()&gt; c) &#123; pool-&gt;Schedule(std::move(c));&#125; runner_([=]() { Process(tagged_node, scheduled_usec); })即把 Lambda 里面的 Process() 传给对应线程池的Schedule() 函数。这个函数然后又套了一层： 1234void ThreadPool::Schedule(std::function&lt;void()&gt; fn) &#123; CHECK(fn != nullptr); impl_-&gt;Schedule(std::move(fn));&#125; impl_ 是 ThreadPool 这个类的实际对象，继承于 Eigen::ThreadPoolTempl&lt;EigenEnvironment&gt;这个模版，继承的目标这部分已经不在 TensorFlow 的源码包里面了，而是来源于 Eigen 的源码，在workspace.bzl里面可以找到 ： 12345678910tf_http_archive( name = "eigen_archive", urls = [ "https://mirror.bazel.build/bitbucket.org/eigen/eigen/get/c2947c341c68.tar.gz", "https://bitbucket.org/eigen/eigen/get/c2947c341c68.tar.gz", ], sha256 = "f21f8ab8a8dbcb91cd0deeade19a043f47708d0da7a4000164cdf203b4a71e34", strip_prefix = "eigen-eigen-c2947c341c68", build_file = str(Label("//third_party:eigen.BUILD")),) 这条线就先不展开了，总之是扔进线程池里面跑就是了。 Process()接下来来看tensorflow::(anonymous namespace)::ExecutorState::Process()，也就是线程池里面实际跑的东西的内容： 处理好上下文以及一些运行中需要用到的参数 inline_ready 是需要运行的 node 队列，这里的核心过程就是一个 while 循环，不断从队列里面把 node 给 pop 出来跑 准备当前 node 的输入等等一系列东西 如果 kernel 是异步的，那么调用 device-&gt;ComputeAsync()，否则调用device-&gt;Compute() 处理输出结果ProcessOutputs()，然后PropagateOutputs()，这个函数很重要，除了把算完的结果沿着当前 node 的出边传给下一个 node 以外，还把接下来满足依赖关系的 node 放到 ready 队列里面 最后调用tensorflow::(anonymous namespace)::ExecutorState::NodeDone()表明当前 node 的计算完成了 NodeDone() &amp; ScheduleReady()NodeDone() 的最后一步执行的也是ScheduleReady()。有意思的是，异步 kernel 跑完之后，往NodeDone()里面传进去的 inline_ready 还是空指针，那ScheduleReady()的执行方式就跟前面一样，往线程池里面扔进去新的Process()任务；同步的 kernel 往NodeDone()里面传进去的就是当前线程的 inline_ready，那么回到上面继续看这个函数的后半部分： 123456789101112131415161718192021222324252627282930# executor.cc:2092const GraphView&amp; gview = impl_-&gt;gview_;const TaggedNode* curr_expensive_node = nullptr;for (auto&amp; tagged_node : ready) &#123; const NodeItem&amp; item = *gview.node(tagged_node.node-&gt;id()); if (tagged_node.is_dead || !item.kernel_is_expensive) &#123; // Inline this inexpensive node. inline_ready-&gt;push_back(tagged_node); &#125; else &#123; if (curr_expensive_node) &#123; // Dispatch to another thread since there is plenty of work to // do for this thread. runner_(std::bind(&amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec)); &#125; curr_expensive_node = &amp;tagged_node; &#125;&#125;if (curr_expensive_node) &#123; if (inline_ready-&gt;empty()) &#123; // Tail recursion optimization inline_ready-&gt;push_back(*curr_expensive_node); &#125; else &#123; // There are inline nodes to run already. We dispatch this expensive // node to other thread. runner_(std::bind(&amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec)); &#125;&#125; 下面这个循环一开始我是没看懂的，看明白了发现这里的逻辑还是挺有意思的。检查 ready 队列中的每一个元素，如果下一个 node 不是耗时的计算任务（expensive node），那就直接加到 inline_ready 队列里面去，否则当前线程至多只做一个耗时任务，其他的任务都扔到线程池里面去交给别的线程做。 也就是说，当前线程要么把全部的不耗时任务做了，要么只做一个耗时任务。 到这里为止，运行部分的逻辑闭环就结束了。这部分的整个流程图大概是这个样子： 后面继续深入 Executor 的运行时实现往下看看： TensorFlow 拆包（二）：TF 的数据流模型实现]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式机器学习 / 深度学习论文整理]]></title>
    <url>%2F2017%2F12%2F20%2F2017-12-20-distributeddl%2F</url>
    <content type="text"><![CDATA[给毕业论文方向找资料ing，虽说具体要做的东西目前还在思考比较多，从之前的 【整理一下看过的论文】 里面把相关的论文理出来了。 大致分成三个方面： Distributed Machine Learning System Distributed Deep Learning System Large Scale Neural Network Training 虽说重点主要集中在后面两块上，不过其他方面的机器学习毕竟发展的时间比深度学习更早，分布式系统方面还是有参考价值的。 把第二三两部分分开整理主要考虑一个是偏框架和算法设计，一个是偏向针对某个具体的应用问题做的大规模实现。 Distributed Machine Learning System2013 NIPS - More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server 话说我第一眼看这个系列的文章还以为讲的就是深度学习了，后来才发现这些全是主要针对机器学习的。 分布式方法还是主要基于传统的 Parameter Server 对 Worker 的形式，但是提出了一种 SSP(Stale Synchronous Parallel) 模型来解决普通的同步或者异步模式下训练会有的问题。SSP 模型大致是说会在本地维护一个参数的 cache，每个工作的 node 直接从本地的 cache 中拿数据，跟 PS 之间的同步问题应该是另外处理，这样就把每个工作 node 等待网络的时间给降下来了。 Introduction 中首先分析了机器学习训练中可能有的问题：massive data volume 和 massive model size，数据量太大和模型太大这两个问题即使可以通过尽可能地缩减、压缩，也终有个尽头，未来总有解决不了的时候，因此不得不需要用到分布式的训练环境。 分布式机器学习系统需要解决的最终目标是： 最大化利用计算资源（把更多的时间花在运算上） 训练完之后要能支持 inference 保证正确性（保证分布式之后网络仍然是能够收敛的） 文中对 PS 的定义是：一个共享的键值对存储模型，同时要具备读取和更新参数的同步机制。共享的键值对存储方式能简化编程复杂度，数据同步是为了保证整个训练过程的正确性。 SSP 这种模型的重点在于像前面说的本地保留尽可能新的旧参数，这里给了个微软一篇技术报告的引用： Replicated Data Consistency Explained Through Baseball 如果说同步可以达到更好的 quality，异步可以达到更好的 quantity 的话，SSP 是取了这两种方案的折中，最终比这两种都要有效。 规定每个 worker 运行时有个 clock 计数器（时间戳），记的是当前到了第几轮迭代。给定一个阈值 s，SSP 模型遵循以下几个规则： 最慢的和最快的 worker 之间跑的参数之间的 clock 差不能超过 s，否则最快的 worker 就要强制等待后面较慢的 worker 算完了赶上来 当一个当前 clock 值是 c 的 worker 提交一个参数更新时，那个更新的时间戳为 c 当一个当前 clock 值是 c 的 worker 读取参数时，根据上述规则，它读取到的更新至少会包含 c-s-1 时间戳之前的所有更新内容 一个 worker 读取到的更新总会比它自己产生的所有结果更新 事实上作者用 c 和 s 这两个定义就把 BSP 和异步给总结到一起了。如果 s 为 0，相当于所有的 worker 都是完全同步的，当 s 是无穷大的时候，就是完全异步的。 这个 s 就是 SSP 模型中需要 overlap 的部分，s 太小则可能经常有些 worker 需要等待，并行性提不上去，s 太大影响问题整体的收敛率。 具体实现上，作者用了一个类似 cache 页表的 SSP table，思想大致也是类似的。在多个 worker 跑在单个节点中的情况下，节点内还可以再多做一层线程间的数据 cache。 2014 ATC - Exploiting bounded staleness to speedup Big Data analytics这篇 paper 的内容是上面那篇的后续研究（话说作者都差不多），大体上是对上篇内容作了更多的补充和提升。作者在实验中发现应用了 bounded staleness 思想的 BSP 模型到最后效果跟 SSP 是差不多的，因此对这其中的详细情况和原因也作了深入分析。 引入 bounded staleness 思想的 BSP 模型这里成为 A-BSP，BSP 是每次迭代之后一个大同步，A-BSP 就是把这个限制放开点，每 n 次迭代之后一个大同步。用上面一篇提过的归纳方式，BSP、A-BSP、SSP 都可以归到一起。 SSP 说起来确实有点像是在 A-BSP 的基础上做个流水线的感觉。当整个分布式系统中，各个线程做的任务不太平衡，有明显较慢的 worker 存在时，SSP 能更好地提高并行度（负载均衡？）。但 SSP 相对 A-BSP 的通信次数是更多的，当各个 worker 的运行速度差距很小时，可能用 A-BSP 会有更好的效果。 这里用来维护数据 cache 的数据结构叫 LazyTable，其实就是前面 SSP Table 的升级版，实现大致都是类似的。 另外这里还做了预取和容错。 2014 SoCC - Exploiting Iterative-ness for Parallel ML ComputationsTo be read. 2014 OSDI - Scaling distributed machine learning with the parameter serverTo be read. 2015 - Distributed Machine Learning via Sufficient Factor BroadcastingSFB。 To be read. 2015 - Efficient Machine Learning for Big Data: A Review机器学习与大数据结合方面的一篇综述。 To be read. 2015 - Petuum: A new Platform for Distributed Machine Learning on Big Data一种分布式机器学习的平台设计。 To be read. 2015 EuroSys - Malt: distributed data-parallelism for existing ml applications一个叫 Malt 的库，用于把常用的机器学习应用从单机改造成分布式的。 To be read. 2015 SoCC - Managed Communication and Consistency for Fast Data-Parallel Iterative Analytics一套叫 Bosen 的分布式机器学习系统。 To be read. 2016 EuroSys - STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning.To be read. 2016 UAI - Lighter-Communication Distributed Machine Learning via Sufficient Factor BroadcastingSFB。 To be read. 2017 - Machine learning on big data Opportunities and challengesTo be read. Distributed Deep Learning System其他资料 【知乎专栏 - ML@Scale】 2012 NIPS - Large Scale Distributed Deep NetworksGoogle 的第一代深度学习系统 Distbelief，由 Jeffrey Dean 大佬带头，其实是 TensorFlow 的前身。 现在说的深度神经网络最初的来源就是传统机器学习里面的受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）以及多层感知器（Multilayer Perceptron，MLP）等等想法，这俩演变出来了一个叫深度信念网络（Deep Belief Network，DBN）的东西，基本跟今天的 DNN 已经很像了。Distbelief 里面的 Belief 指的应该就是 DBN 吧。 出发点基本跟后来的文章大同小异，文章的重点在于： Downpour SGD：前面这个单词是描述倾盆大雨的，不知道整个词组应该怎么翻译比较好； 基于 Sandblaster 框架实现了一个 L-BFGS，用于处理整个训练过程的数据并行和模型并行。 相比普通的 SGD + 普通的 L-BFGS 实现要快上不少。 异步 SGD 之前在机器学习领域很少被用在非凸的优化问题上，但是经过试验验证之后发现异步 SGD 用在神经网络上效果很好，尤其是配合 Adagrad 这种学习率修正算法的时候。然后在资源足够的情况下，L-BFGS 的性能不会弱于 SGD。 论文中做到的最大规模是把一个模型拆到 32 个节点上进行模型并行。 2013 ICML - Deep Learning with COTS HPC首次把分布式机器学习里面的数据并行和模型并行引入深度学习。 主要实现在 InfiniBand 网络上，然后偏重在模型并行上。 To be read. 2014 ICASSP - On parallelizability of stochastic gradient descent for speech DNNS这篇文章是从理论上对比了模型并行和数据并行中分布式 SGD 训练的效率，指出增大 minibatch 的大小可以提高数据并行训练的效率。 To be read. 2014 OSDI - Project Adam: Building an Efficient and Scalable Deep Learning Training System微软在分布式 DL 训练方面做的工作。 To be read. 2014 Proceedings of the VLDB Endowment - Mariana： Tencent Deep Learning Platform and its Applications腾讯做的一个叫 Mariana 的深度学习平台。 To be read. 2015 ACM MM - SINGA: Putting Deep Learning in the Hands of Multimedia Users一套叫 SINGA 的分布式深度学习框架。 To be read. 2016 - Asynchrony begets momentum, with an application to deep learning分析了异步与动量法调整的学习率之间的影响关系。 动量法是一种梯度下降里面对学习率自动调节的方法。 To be read. 2016 - How to scale distributed deep learning用 ImageNet 对比了同步和异步 SGD 的实测结果，指出可能在更大规模下其实同步 SGD 效果更好。 To be read. 2016 SoCC - Ako: Decentralised deep learning with partial gradient exchange去中心化（不再采用 PS-Worker 方式）的分布式深度学习思路。 To be read. 2016 Eurosys - GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-Specialized Parameter Server这篇文章很厉害了，主要内容是说做了一套叫 GeePS 的 Parameter Server 机制，主要针对 GPU 做了特别的优化和改进，克服了数据并行和模型并行，BSP 和异步这些老方法中存在的问题，最终结果性能爆炸。GeePS 还支持在卡上跑超过显存容量的网络，直接解决了对模型并行的需求。 背景介绍部分分析了一下 Parameter Server 模式存在的问题，这里也有提到前面 13 年那篇 SSP 模型方面的工作（…其实这篇论文还是同一拨人做的…）。要应用 PS 模式到 GPU 上，采用多个 worker 配合多个 ps，每个物理节点内都有 ps 和 worker 这种形式会比较合适。 但是直接简单地把 ps 搬到 GPU 上效果非常不好，后文接下来就是讲他们如何解决这个问题，即他们提出来的 GeePS 是怎么做的。 第一个优化策略是在 GPU 上做一个参数 cache，嗯这个思想大概跟往异步 PS 里面引入本地 cache（应用 SSP 模型）是一个道理。因为前面的性能瓶颈主要在每一次推送参数更新上了，引入了 CPU 跟 GPU 之间的数据传输之后总会把整个计算过程卡下来。 这种方法提升性能的关键在于能够成功地把计算和 CPU/GPU 的数据拷贝给 overlap 开，这样就能最大化 GPU 的使用率啦，GPU 只要拿到了新的 input 数据就能一直跑。 第二个策略是数据的输入和参数的更新都是以 batch 为单位的，利用了 GPU 的 SIMD 特性，增加了数据的吞吐量，这一块详细的要见他们前面的一篇文章（Exploiting Iterative-ness for Parallel ML Computations）。 第三个就厉害了，目测应该工作量挺大的。为了解决模型太大，GPU 上放不下的问题，他们手动维护了一个 GPU 和 CPU 之间的内存池，每次把不用的数据换到主存上，把下一波计算需要用的数据换到 GPU 上。 用手动维护的内存池完全接管整个 GPU 的内存分配、释放操作，CPU 跟 GPU 之间的数据传输用另外一个线程在后台完成，把计算时间和数据拷贝延迟完全 overlap 开。由于神经网络层与层之间的顺序性是显示存在的，因此数据在 GPU 显存上的换入换出就是完全可以做到的了。 第四点是对 PS 模式下异步方式的思考，虽说把 BSP 改成异步的可以增加计算资源的利用率，但是收敛速度会放慢是肯定的，之前的不少研究也是在这两个方面作了取舍，才能让最终训练到相同效果的总体时间更短。这篇文章在同步延迟能够保证的情况下，测试结果偏向于用 BSP 收敛效果会更好。 中间的详细实现先放着，留待之后回来看。 2017 ATC - Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clustersemm…这篇文章跟上一篇还是同一拨人做的。Motivation、目标什么的基本上差不多，工作方向上从不同的角度出发来做。 文章首先指出了限制分布式深度学习可扩展性的两个瓶颈： 每次更新的梯度都可能是大矩阵，很容易就把网络带宽给占满； 由于神经网络运算的迭代特性，在完成一轮迭代之后才更新参数。因此其通信表现是短时间内有一个通信量的暴增，而其他时间没有通信。 本文解决问题的思路也从这两点开始出发： 把要更新的梯度矩阵做一定的划分，通过重新调度，在时间上把整个通信平摊掉； 想办法减少每次更新的梯度矩阵的大小，从整体通信量上做文章。 最后要达到的效果呢，也是分两方面：首先整个系统的吞吐量增加了，同时迭代的收敛速度并不受影响，不需要增加迭代次数就能达到一样的效果。 减少梯度矩阵大小用的是一个叫 SFB（Sufficient Factors Broadcasting）的技术，具体的文章在这里。 作者认为核心的瓶颈还是主要在通信上。 这里给出了一个对 Alexnet 的粗略分析（不是特别准确，但是基本上是同一个量级），假定计算跟通信能够完全 overlap 开，用 Taitan X 节点来分布式训练大约也要至少 26 Gbps 的网络带宽，这个压力对一般的以太网来说基本上是很难应对的。当然 overlap 计算和通信这事本身就很难做到了。 作者首先分析了整个训练过程的思路： 定义训练第 l 层网络的前后向为 $f_t^l$ 和 $b_t^l$，那么整个训练的计算部分是这样的： $$C_t = [f_t^1, f_t^2, … , f_t^L, b_t^L, b_t^{L-1}, … , b_t^1]$$ 加上通信同步部分，$O_t^l$ 和 $I_t^l$ 分别表示第 l 层网络参数的输出和输入（更新）： $$S_t = [O_t, I_t] = [O_t^L, O_t^{L-1}, … , O_t^1, I_t^L, I_t^{L-1}, … , I_t^1]$$ 这样组成的一次 $C_t, S_t$ 就是一次迭代的完整过程了。 那么就提出了这里的第一个思路： 分层把通信和计算给 overlap 开，这里称为无等待的反向传播算法（WFBP） 通信的数据依赖关系只在同一层内，即： $b_t^l$ 结束之后就可以马上做 $O_t^l$ 和 $I_t^l$ 了，即第 l 层的参数同步可以跟第 l-1 以及以后层的后向计算同时进行。示意图如下： 这种思路尤其适用于参数集中在后几层（例如后几层是全连接），然后计算集中在前几层（例如前几层是卷积）这样的网络（例如 VGG 和 AdamNet），这样就能把顶层的通信时间掩藏在底层的计算时间中。 大概是这种样子： $$\begin{align}[C_t, S_t] = [f_t^1, f_t^2, … , f_t^L, b_t^L, &amp;b_t^{L-1}, b_t^{L-2} … , b_t^1] \&amp;[O_t^L, O_t^{L-1}, … , O_t^1] \&amp;\qquad [I_t^L, I_t^{L-1}, … , I_t^1]\end{align}$$ 但是这样对于带宽受限的网络来说仍然不够，所以有了接下来的第二个思路： 采用 PS 和 SFB 混合的通信机制 正如上面所分析的，事实上不只是神经网络不同层的计算和通信可以无关，不同层之间的通信也是完全可以独立的，因此考虑对不同层数据的特点采用不同的方式进行组织通信。PS 模式或者 SFB 模式： 并且神经网络的结构是只要训练开始了，自始至终都不会再改变，因此可以事先算出参数的总量来估计整个网络会产生的通信开销，在训练开始前就能够选择合适的通信组织方式。 这里又举了个例子： VGG19，假定 batch size K = 32，8 个 PS 和 8 个 Worker，参数均分在 8 台 PS 上，全连接层 M 和 N 都是 4096。2 个全连接层每一步迭代产生 2 * 4096 * 4096 = 34M 个参数。 PS 模式下：每个 Worker 每次要发送 34M 个参数，每个 PS 管 34M/8 个参数，但是要从 8 个 Worker 那里接收，所以总的通信数据量还是 34M。如果实际物理机就 8 台，每台上面跑一个 PS 和一个 Worker，那么每台机上本地更新 34M/8 的数据，每次要送出去 7/8 * 34M 的数据，再收回来 7 个 34M/8 的数据，一共 2 * 7/8 * 34M = 58.7M 的数据量。 SFB 模式下：（这种通信模式我还没细看，所以不知道为什么这么算）没有 PS，直接是 8 个 Worker 进行数据处理，每个节点需要负责的通信量只有 2 * K *(M + N)(P1 -1) = 2 * 32 * 8192 * 7 = 3.7M 的数据量。虽然收到数据之后要恢复成梯度矩阵需要额外的运算，但是这跟节省下来的通信时间相比是可以忽略不计的。 卷积层的更新梯度由于是不可分解的，并且是稀疏的，所以还是采用 PS 模式更好。 所以根据不同层的参数特性采用不同的通信机制的方法是有很大潜力的。 整个 Poseidon 系统的设计分成三个部分： Coordinator：用于维护网络模型和集群设备之间的配置关系 集群中 worker 和 ps 的数量，对应的 ip 地址，网络模型的结构等等。负责建立各节点之间的通信端口，分析网络模型，决定哪些参数用 PS 哪些参数用 SFB。 当然这些数据是在初始化的时候就完成了。 KV store：一个共享内存的键值对存储部件，其实就是 Parameter Server 为什么这里的 PS 要突出”键值对“这个概念呢？ Poseidon 在这里做了一个操作，参数不是按照层来划分的（！！！），而是把所有的参数按照 2MB 划分之后再均分到各个 PS 上去，这样就能保证每个 PS 上面存储的数据量尽可能地一致，让各个节点需要的网络带宽尽可能地平均。 另外还有 checkpoint 的设计，保存多个阶段的参数用于容错恢复等等。 Client Library：用于适配到不同的深度学习框架中 对每个层都单独创建一个 syncer 负责处理其参数一致性。 在 CPU 上维护一个线程池（用于后台处理网络通信），在 GPU 上维护一个 stream 池（用于后台处理 CPU 和 GPU 之间的数据拷贝）。 之前作者的文章里面也提到了 BSP 的收敛性相对异步来说始终还是更稳定的，有了上面那个分层同步、通信几乎完全掩藏在计算背后的设计之后，BSP 跟异步之间的延迟差距可能已经能够减少到足以忽略的程度了。 因此在参数一致性方面的维护上，Poseidon 直接采用了 BSP 同步。 worker 这边，每个 client library 维护一个长为 syncer 数的 01 向量，每次迭代开始前初始化为 0，当一层的数据同步完了之后设为 1，当整个向量全 1 时就可以进入下一个迭代了。 PS 这边，KV stroe 在每次迭代开始前维护一个值为 0 的计数器，每当有一个 KV 更新计数加一，当计数器达到 worker 的数量时，发起一次参数广播。 整个训练过程的伪代码： 123456789101112131415161718192021function TRAIN(net) for iter = 1→T do sync count = 0 net.Forward() for l = L→1 do net.BackwardThrough(l) thread pool.Schedule(sync(l)) endfor wait until(sync count == net.num layers) endforendfunctionfunction SYNC(l) stream = stream pool.Allocate() syncers[l].Move(stream, GPU2CPU) syncers[l].method = coordinator.BestScheme(l) syncers[l].Send() syncers[l].Receive() syncers[l].Move(stream, CPU2GPU) sync count++endfunction 后面是 evaluation，测试结果就是很强…各方面都很强。 2015 - Poseidon: A system architecture for efficient GPU-based deep learning on multiple machines这篇 paper 是在上面那篇发出来之前放在 arxiv.org 上的，应该算是正篇前的一些基础工作，看完 2017 年的正篇再看下这个。 2017 - Can Decentralized Algorithms Outperform Centralized Algorithms A Case Study for Decentralized Parallel Stochastic Gradient Descent又一篇去中心化思路的文章。 To be read. 2017 ICML - Device Placement Optimization with Reinforcement LearningGoogle 做的关于模型并行中 Op 和 Device 对应关系的研究。 在多块卡上做模型并行时需要考虑把整个网络的哪些部分分在哪种设备上，这篇文章的思路是先生成一套 Op 和 Device 的对应关系，然后扔进 TensorFlow 里面跑，跑完结果再反馈到一个强化学习的网络里面去，让网络去自动重新调整 Op 和 Device 的对应关系，直到得到一套最高效的分配方案。 666666！ 2017 - ChainerMN: Scalable Distributed DeepLearning Framework下面列的那个 15 分钟跑完 ImageNet 的日本机构用的是他们自己写的框架，名字就叫 ChainerMN。 To be read. 2018 NIPS - Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training刚刚看到的一篇 NIPS 的工作，嗯，从题目就基本上可以知道写的是啥了。 Introduction 中首先讨论了一下网络层、数据量的增长带来的计算量的需求，尽管各种神经网络加速器（GPU、ASIC 等等）和并行计算的发展缓解了很大的压力，从另一方面却更加剧了多机通信的 Overhead 造成的影响，毕竟前后向的计算时间是减少了，但是通信时间不变啊，在整体一次训练迭代中占的比重就增大了。 这篇文章首先分析了神经网络训练过程中的通信时间和计算时间的关系，然后提出了一套模型来评估各方面因素对它们的影响（例如延迟、集群规模、网络带宽、神经网络模型等等），在这套评估模型的基础上接下来推出了 Pipe-SGD 框架来实现他们想要的训练机制，最终在评估测试中得到了很好的结果。 Background 部分是对神经网络以及分布式训练的介绍，值得一提的是最后提到好多人为了减少通信时间会采用压缩的方案，例如低比特量化、传输时直接扔掉某些层的梯度等等。 既然 Background 里面提到了压缩，说明后面他们的框架中也有压缩方面的工作。 下图分别是中心化的 PS-Worker 异步模式、去中心化的同步模式以及本文的流水线模式的示意图。 马后炮一下，看到 c 图的时候我惊了一下……大概去年年底的时候也想到了类似这种思路，无奈限于一直做的框架都是 TensorFlow，没想到应该怎么在 TF 里面实现，于是一直没有把想法变成实现。 另外一点是，由于 TensorFlow 本身的数据流设计，一轮迭代中的计算和通信本就能够很大程度上自动 overlap 起来，因此即使在 TF 中实现了也可能意义不大。 这里把每一次的训练迭代分成三个部分：模型更新、梯度计算、梯度传输。 在一个常规的同步训练过程中，每一轮迭代中的三个部分是相互依赖的，如果用 T 表示总的训练迭代次数，则整个训练时间可以表示成这样： $$ l_{total \_ sync} = T * (l_{up}+l_{comp}+l_{comm})$$ Pipe-SGD 的思想呢则是把这种通信和计算的依赖关系解开，如果一个 Worker 上是多条流水线交替进行，则通信和计算的时间就有机会完全 overlap 开了，本轮迭代的数据只与前 K 轮的迭代数据相关，而相邻的两次迭代之间则完全没有依赖了。在这种方式下，总的时间可以表示为： $$l_{total \_pipe} = T / K * (l_{up} + l_{comp} + l_{comm})$$ 更进一步的分析是，由于网络中的计算和通信时间往往很难完全对等起来，事实上整个计算过程是资源受限（Resource Bound）的，则最后实际受限的结果是要看计算和通信哪部分更花时间： $$l_{total \_pipe} = T * max(l_{up}+l_{comp}, l_{comm})$$ 下面更进一步地给出了一个更详细的时间模型，把网络延迟、模型的参数量、传输带宽、worker 数量等等都包含在内了，最终分析完了给出了这样一个结论： Pipe-SGD 选择 K=2 为最佳，整个计算过程是计算受限的，并且应该采用顺序的梯度传输原则。 这里的顺序梯度更新原则指的是把梯度计算和梯度传输这两个过程完全分开，即计算完整个网络的所有梯度之后再一次性传输完，在 TensorFlow 中默认并不存在这种分离关系，可以认为是自动的分层梯度传输，一层梯度算完之后马上就可以开始传输过程，并且这个传输并不影响后续其他层的梯度计算。 …… 话说我觉得这里的结论直接想也很容易得出来啊，可能严谨的数学模型推论就是人家为什么能中 NIPS 的原因吧。 另外还有为什么这里要特意提到压缩呢？ 下图的 a 是无压缩情况下的通信和 Reduce 计算关系，可以看到即使采用了流水线的方式，Reduce 的实现序列上还是会有大量的空闲。 而如 b 图所示，梯度压缩操作减少了传输时间，但却会略微增加一些计算量，而这种增加的计算量到了流水线中反而可以很好地被隐藏掉。 。。。 最后的实验验证部分说实话我是比较失望的，4 台单 GPU 节点组成的集群实在不是个太大的规模啊……而且这里也没有给出与其他框架的对比性能来。 2018 - Beyond data and model parallelism for deep neural networks看的时候这篇文章还在 SysML19 审稿中。 主要工作是建立了一套名为 SOAP（Sample，Operation，Attribute，Parameter）的并行方案搜索空间，设计了一个专门的模拟器来评估要训练的神经网络的性能情况，然后在这个搜索空间里面找到最佳的并行方案。听上去有点玄，不过摘要中说可以达到比 State-of-the-Art 还要大 3.8 倍的吞吐量，还是很厉害的。 Introduction 部分提了下 DNN 并行的大背景，目前市面上常见的框架中都只是提供了一些简单的并行方案，要达到理想的性能通常需要研究者自己去根据网络的特性手工调整。举例来说 Google 14 年有个把前半部分的卷积用数据并行，最后的全连接改用模型并行的方案，以及 Google 对自己机器翻译模型做的一些并行方案设计。另外还有依靠强化学习等方法去找到最佳的并行方案（例如 Google 的模型并行设备分配工作，以及这里引用了另外一个他们自己的工作）。 Google 对模型并行做的强化学习每次是试跑一遍得到实际执行时间来作为一种方案的结果（常规操作没毛病），因此当搜索空间很大的时候就不可避免地需要花费很长时间。那这篇文章的关键就在于他们模拟器的设计，用模拟器直接评估网络的执行性能自然是是要比实跑快很多的（这里说快了好几个数量级），问题是这样评估出来的结果是否可靠？ 文章主要的依据有两个： 大多数神经网络的模型都是由常见的结构组成，只会有很少量的特殊网络层； 神经网络每一层的运行时间基本上只与硬件本身相关，输入规模固定之后基本不会受其他因素影响。 因此文章中的模拟器的做法是预先得到不同输入规模下各个网络层的执行时间，之后再用这些数据来评估网络模型的执行时间。相比 Google 的做法，这样可以更快，而且速度快了以后整体的方案搜索过程对硬件资源的需求也就少的多了（Google 用了 160 个 4 GPU 节点，本文只需要单个节点）。后面对优化方案的搜索采用的是马尔科夫链蒙特卡洛的方法，然后各种测试的效果都很不错。 相关工作的对比里面还有提到用网络流来优化任务调度的（666666）。 Large Scale Neural Network Training这个分类下面主要是实际应用层面的工作。 2013 ICASSP - Building High-level Features Using Large Scale Unsupervised Learning谷歌做的无监督学习图像分类的工作。 在一个超过 1000 个节点的集群上用了模型并行和异步 SGD。 文中的网络叫稀疏深度自编码器（Sparse Deep Autoencoder），基本上跟现在的 DNN 差不多，应该是当时深度学习这个概念还没有完全定型。 实现方面没有具体写的很清楚，主要参看前面12年NIPS的那篇工作吧。 这里的几篇各家拼 ImageNet 训练速度的笔记整理到新帖里了： 【Faster and Faster – ImageNet】]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>Paper</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 3436 Queue-jumpers Splay 离散化]]></title>
    <url>%2F2017%2F10%2F28%2F2017-10-28-hdu3436%2F</url>
    <content type="text"><![CDATA[Queue-jumpersTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionPonyo and Garfield are waiting outside the box-office for their favorite movie. Because queuing is so boring, that they want to play a game to kill the time. The game is called “Queue-jumpers”. Suppose that there are N people numbered from 1 to N stand in a line initially. Each time you should simulate one of the following operations: Top x :Take person x to the front of the queue Query x: calculate the current position of person x Rank x: calculate the current person at position x Where x is in [1, N]. Ponyo is so clever that she plays the game very well while Garfield has no idea. Garfield is now turning to you for help. InputIn the first line there is an integer T, indicates the number of test cases.(T&lt;=50) In each case, the first line contains two integers N(1&lt;=N&lt;=10^8), Q(1&lt;=Q&lt;=10^5). Then there are Q lines, each line contain an operation as said above. OutputFor each test case, output “Case d:“ at first line where d is the case number counted from one, then for each “Query x” operation ,output the current position of person x at a line, for each “Rank x” operation, output the current person at position x at a line. Sample Input 3 9 5Top 1Rank 3Top 7Rank 6Rank 86 2Top 4Top 57 4Top 5Top 2Query 1Rank 6 Sample Output Case 1:3 58 Case 2:Case 3:3 6 题意一个队列，初始时每个人从 1 到 n 按顺序标记，3 种操作： Top x：把标号为 x 的人提到队列的第一位； Query x：查询标号为 x 的人的当前位置； Rank x：查询当前从左数第 x 个位置的人的标号是多少。 分析队列的结构是一直在变的，所以要想低复杂度地解决，也必须要用动态的数据结构来存，Splay 这时候就体现出优势了。 对 1 到 n 按顺序构造出 Splay，然后开始处理请求： Top x：提到队首相当于先把 x 这个节点从树中删除，然后再插入到队首，在 Splay 中可以用一种更加简单的操作来实现。把 x 旋转到根，如果 x 没有左子树，说明已经在队首了，否则把右子树的最左节点旋转到右子树的根，这样旋转完之后，右子树的根节点是没有左子树的，此时再把根（x）的左子树切下来连到右节点的左空位上即可。 Query x：把 x 旋转到根，左子树的 size 加 1 就是根节点当前的序号。 Rank x：查找第 x 个位置的节点号，也是二叉搜索树的基本操作。 关键在于这道题还有个坑点，N 的范围最大到了 10^8，略大，同时可以发现询问数只有 10^5，因此可以考虑用离散化把范围分拆一下。 离散化之后，每个树节点可以代表的 size 不再限于 1 了，这块得注意处理一下。Top 和 Query 都是对于原本的序号进行操作，因此这两个操作中的 x 需要作为离散化的标记，离散化之后这两个操作的处理也基本上是一样的。 Rank 操作的 x 不需要离散化，队列动态变动之后早就不是原本的顺序了。对于这个操作，需要对原本的进行一定改动，每个树节点能够代表的可能是多个队列点，首先用 x-size[sons[spt][0]] 算出要询问的队列点是当前树节点代表的队列点中的第几个，再根据当前树节点代表的队列点区间范围找出原本的序列号。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 3436************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 100010int op[MAXN], x[MAXN];struct node&#123; int x, index;&#125;;bool sort_op(node a, node b)&#123; if (a.x == b.x) return a.index &lt; b.index; else return a.x &lt; b.x;&#125;node nodelist[MAXN*2];int sons[MAXN][2];int father[MAXN], size[MAXN], data[MAXN], ori[MAXN];bool flag[MAXN];int spttail, spt;void rotate(int x, int w) //rotate(node,0/1)&#123; int y = father[x]; sons[y][!w] = sons[x][w]; if (sons[x][w]) father[sons[x][w]] = y; father[x] = father[y]; if (father[y]) sons[father[y]][y==sons[father[y]][1]] = x; sons[x][w] = y; father[y] = x; size[x] = size[y]; size[y] = size[sons[y][0]]+size[sons[y][1]]+data[y];&#125;void splay(int x, int y) //splay(node,position)&#123; while(father[x] != y) &#123; if (father[father[x]] == y) rotate(x, x==sons[father[x]][0]); else &#123; int t = father[x]; int w = (sons[father[t]][0]==t); if (sons[t][w] == x) rotate(x, !w); else rotate(t, w); rotate(x, w); &#125; &#125; if (!y) spt=x;&#125;void select(int x, int v, int p) //select(root, k, position)&#123; while(v&lt;size[sons[x][0]]+1 || v&gt;size[sons[x][0]]+data[x]) &#123; if (v &lt;= size[sons[x][0]]) x = sons[x][0]; else &#123; v -= size[sons[x][0]]+data[x]; x = sons[x][1]; &#125; &#125; splay(x, p);&#125;void insert(int x)&#123; spttail ++; data[spttail] = x; sons[spttail][1] = 0; father[spttail] = 0; if (spttail &gt; 1) &#123; size[spttail] = size[spttail-1]+x; sons[spttail][0] = spttail-1; father[spttail-1] = spttail; &#125; else &#123; size[spttail] = x; sons[spttail][0] = 0; &#125;&#125;void out(int x)&#123; if (sons[x][0]) out(sons[x][0]); printf("%d ", x); if (sons[x][1]) out(sons[x][1]);&#125;int main()&#123; freopen("in.txt", "r", stdin); int T; scanf("%d", &amp;T); for (int tt=1;tt&lt;=T;tt++) &#123; printf("Case %d:\n", tt); int n, q; scanf("%d %d", &amp;n, &amp;q); // .... getchar(); for (int i=0;i&lt;q;i++) &#123; char s[10]; scanf("%s %d", &amp;s, &amp;x[i]); switch(s[0]) &#123; case 'T': op[i] = 0; break; case 'Q': op[i] = 1; break; case 'R': op[i] = 2; &#125; nodelist[i].x = x[i]; nodelist[i].index = i; &#125; sort(&amp;nodelist[0], &amp;nodelist[q], sort_op); int last = 0; spttail = 0; for (int i=0;i&lt;q;i++) if (op[nodelist[i].index] != 2) &#123; if (last &lt; nodelist[i].x) &#123; if (nodelist[i].x-last&gt;1) &#123; insert(nodelist[i].x-last-1); ori[spttail] = last+1; &#125; insert(1); ori[spttail] = nodelist[i].x; last = nodelist[i].x; &#125; x[nodelist[i].index] = spttail; &#125; if (last &lt; n) &#123; insert(n-last); ori[spttail] = last+1; &#125; spt = spttail; for (int i=0;i&lt;q;i++) &#123; switch(op[i]) &#123; case 0: splay(x[i], 0); if (sons[spt][0]) if (sons[spt][1]) &#123; int temp = sons[spt][1]; while (sons[temp][0]) temp = sons[temp][0]; splay(temp, spt); sons[temp][0] = sons[spt][0]; sons[spt][0] = 0; father[sons[temp][0]] = temp; size[temp] = size[sons[temp][0]]+size[sons[temp][1]]+data[temp]; size[spt] = size[temp]+data[spt]; &#125; else &#123; sons[spt][1] = sons[spt][0]; sons[spt][0] = 0; &#125; break; case 1: splay(x[i], 0); if (sons[spt][0]) printf("%d\n", size[sons[spt][0]]+1); else printf("1\n"); break; case 2: select(spt, x[i], 0); printf("%d\n", x[i]-size[sons[spt][0]]+ori[spt]-1); break; &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>Splay</tag>
        <tag>离散化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tarjan 大佬的算法们]]></title>
    <url>%2F2017%2F09%2F27%2F2017-09-27-tarjan%2F</url>
    <content type="text"><![CDATA[逛知乎的时候发现的这个问题： Tarjan（自己/合作）创造了哪些算法和数据结构？ 一开始只是觉得这个名字熟悉（脑子里面冒出来的是 Tarjan 离线 LCA），点进去一看越看越心惊。 强连通分量？哦，那个经典算法好像确实叫 Tarjan。LCT 动态树…卧槽他写的？斐波那契堆…卧槽他写的？Splay 伸展树…卧槽他写的？ 从 1971 年至今，Tarjan 大佬大概发了 300 篇左右的论文，还在不断产出，他创造了一堆图论、树结构上的经典算法，还获得过 1986 年的图灵奖。 后面准备把他的一些经典算法都重新过一遍，看看他的原著论文什么的。 我知道的几种算法/数据结构： Tarjan 双连通分量 &amp; 强连通分量【1972 - Depth-first search and linear graph algorithms】 Google 学术引用数 6049。 这篇论文似乎可以算是 Tarjan 的开山之作，其中提到了图上 DFS 序的一些重要特性。 论文里面直接提出了经典的无向图点双连通分量算法和有向图强连通分量算法。Tarjan 的这两个算法都是基于 DFS 栈的。 首先是割点的概念：一个连通图中，如果删掉一个顶点，整个图变成了两个连通图，则该顶点叫割点。 点双连通分量就是一张图中不含有割点的子图（块）。 论文中给出的 Tarjan 点双连通的过程大致是这样： dfn 是 DFS 过程中访问节点的顺序（或者说时间戳？），low 是 DFS 过程中，某个点能够连接到的栈中的最底层的点。DFS 栈中存边。 12345678910111213141516171819202122232425262728void tarjan(int x,int fa)&#123; dfn[x]=low[x]=++idx; repe(i,fst[x]) // 对于 x 邻接的每一条边 i &#123; if (i-&gt;v!=fa) // 不走回头路 &#123; if (!dfn[i-&gt;v]) // 如果该边对应的顶点没有被访问过 &#123; st[++top]=i; // 边入栈 tarjan(i-&gt;v,x); // 继续 DFS low[x]=min(low[x],low[i-&gt;v]); if (low[i-&gt;v]&gt;=dfn[x]) // 找到一个割点 &#123; ++fct[x]; // 标记 x 是割点 // 记录双连通分量 bl[++tot].clear(); bl[tot].push_back(x); do &#123; bl[tot].push_back(st[top]-&gt;v); &#125; while (st[top--]!=i); &#125; &#125; else low[x]=min(low[x],dfn[i-&gt;v]); &#125; &#125;&#125; 有向图的强连通分量，我以前用的一直是 Kosaraju 算法，配合时间戳正反两次 DFS。Tarjan 的强连通分量算法跟双连通非常像，大致是这样的： dfn 和 low 的意义相同，DFS 栈中存点。 1234567891011121314151617181920212223242526272829void tarjan(int i)&#123; DFN[i]=LOW[i]=++idx; instack[i]=true; Stap[++Stop]=i; // 点入栈 for (edge *e=V[i];e;e=e-&gt;next) // 对于 i 邻接的每一条边 e &#123; int j=e-&gt;t; if (!DFN[j]) // 该边对应的顶点没有访问过 &#123; tarjan(j); if (LOW[j]&lt;LOW[i]) LOW[i]=LOW[j]; &#125; else if (instack[j] &amp;&amp; DFN[j]&lt;LOW[i]) LOW[i]=DFN[j]; &#125; if (DFN[i]==LOW[i]) // 当 dfn 和 low 相等时，栈顶的点构成了强连通分量 &#123; Bcnt++; do &#123; j=Stap[Stop--]; instack[j]=false; Belong[j]=Bcnt; &#125; while (j!=i); &#125;&#125; 嗯，看这篇论文的时候有点在看《算法导论》的感觉，证明、推理过程都非常严谨（虽然我从不看证明）。 【1973 - Algorithm 447: efficient algorithms for graph manipulation】 Google 学术引用数 795。 文中用流程图描述了找连通分量、找双连通分量、把图划分成多个简单路径的算法，大致的想法跟上面那篇 DFS 基本是一致的。划分简单路径要在双连通分量的基础上做。 Tarjan 离线 LCA【1979 - Applications of path compression on balanced trees】 Google 学术引用数 356。 额…在 Google 学术上下到的 1979 年版是个迷之影印版，另外搜到个写着 1975 年的版本，感觉过去太多年以后，论文的时间确实都有点乱了。 看到这篇论文的标题“路径压缩”首先想到的是并查集，论文里面开头确实也提到了前人写的有关“等价问题”的算法，估计跟并查集有点关系。 文章一开始是一堆图和树的定义，然后定义了一个树上求值和构造的操作。求值是定义一个函数 eval(v) 计算从当前节点到根节点的某种数值关系，构造 link(v, w, x) 连接两个节点，树边长度是 x。由于这里求值函数的定义是从当前节点算到根节点，因此是有个结合律在里面的，所以可以用到路径压缩。 论文这里讲的路径压缩的思路大概跟并查集（关系传递）和线段树（lazy标记）里面用到的方法类似。嗯，跟线段树对比可能还不是很一致，线段树算是自顶向下求，这里应该是从当前节点一直向上求。 果然，文章的第 4 部分讲到了 2 个这套方法的算法应用：并查集和 LCA。 并查集没啥好说的，这里重点要说的 Tarjan 的离线 LCA。 为什么要特别指出是“离线”的 LCA 呢？看完算法的整个过程就知道了： Tarjan 的 LCA 用 DFS 来遍历整棵树，并且依靠并查集来完成线性时间的查询。 首先对所有的 LCA 询问进行排序，交换 {vi, wi} 使得 vi &lt;= wi 然后在树上把所有的 wi 标记出来 之后在图中开始 DFS： 找到一个带标记的节点 wi，则对应的 vi 肯定已经事先访问过了（排序的作用），直接返回 getfather(vi) 就是它们的 LCA 了 DFS 回溯时，将当前节点并进父节点的集合中 这个算法要求 LCA 询问事先得到，并且之后树的结构肯定是不能再改变的，所以是个离线算法。 参考这里的一个 Flash 动画演示可能会更清楚一点。 后文暂时没再往下看了，基本讲的是 eval 和 link 这两个操作的扩展玩法，甚至还有验证最小生成树的用途（？），应用面看起来还是非常不错的。 Link/Cut Tree 动态树【1981 - A Data Structure for Dynamic Trees】 Google 学术引用数 1069。 我之前知道的动态树算法是跟 Splay 配合用的，更进阶的还有树链剖分跟这个也有点关系，前面在这里记过两道题。不过论文顺序上居然是 LCT 先写的。 摘要部分介绍了这是一种用于维护一个由若干棵不相交树组成的森林的数据结构，主要有link和cut操作来连接两棵树或者把一棵树拆成两棵。当然复杂度是非常优的，保证在O(logn)（复杂度太高的话就没有意义了）。 应用方面，摘要提到了 LCT 可以求： LCA 最近公共祖先 多种网络流问题，包括最大流等等（！） 某些特殊情况下的最小生成树（！） 某种最小费用流的简化算法（？） 如果单纯地只是要实现这样的数据结构，难度不是很大，直接模拟就好，最简单粗暴的实现也可以达到link和cut这样的操作在O(1)时间内完成，但是树上的其他查询操作就可能要花上O(n)（那就没有意义了）。 所以还是得想个办法解决具体实现的问题。 Tarjan 的高效实现不直接存树的原始结构，而是把整棵树按照路径集合的方式来维护。论文第三章是对这种转化的复杂度证明，保证 m 个对动态树的操作可以转化成O(mlogn)个对路径集合的操作。然后第四章接下来讲如何用二叉树来维护这个路径集合，话说这部分才是这个算法的核心实现吧。 嗯，我现在所知道的 LCT 的实际基本都是基于 Splay 的，话说论文里面后面这部分写到的二叉树实现中的左右旋等等的操作看上去就已经是 Splay 的雏形了，不过 Tarjan 的 Splay 论文是发在这一篇之后的，可能后面还有经过更多的改进或者验证吧。 这段暂时跳过，直接看看现代经过后人补充修正过的 LCT 是怎么实现的好了： 【SPOJ375 QTREE 解法的一些研究 杨哲】 （OneDrive 上共享的，可能链接会被墙，不过在网上用这个标题应该也能搜到对应的文章。） QTREE 一文中的Preferred Path跟 Tarjan 论文中的Solid Path应该是一致的。这里的核心操作是Access(v)，访问 v 这个节点所经过的路径就是这次访问所产生的Preferred Path，LCT 算法就是把原本的树结构转化为维护这样的一些Preferred Path集合，树本身是可以分解成一堆Preferred Path的，只需要再知道这些路径之间的连接关系，就可以表示出这棵树了。Preferred Path保存在 Splay 上，Splay 就是 LCT 这个数据结构的辅助树（Auxiliary Tree）。 所以 Preferred Path 和 Auxiliary Tree 这俩名字是哪来的呢？。。。Tarjan 的论文里面好像没出现这两个词。 QTREE 文中对 LCT 的定义： Link-Cut Trees 就是将要维护的森林中的每棵树 T 表示为若干个 Auxiliary Tree, 并通过 Path Parent 将这些 Auxiliary Tree 连接起来的数据结构. 比较尴尬的是这里的实现确实都是要基于 Splay 来理解了，所以先跳一步，重新来复习一下下面的 Splay 先。 有了 Splay 之后，接下来 LCT 的操作就很简单了。前面说过，LCT 的核心操作是 Access()。 以 QTREE 一文中的例子来解释（找不到什么比较好的画图工具，只好手画了）： LCT 存储的是树结构上的 Preferred Path，例如对左边的树进行 Access(N) 操作之后，从根节点 A 到目标节点 N 就形成了一条 Preferred Path，跟这条路径上的点相连的原有的 Preferred Path 都恢复成普通边（例如 A-B 和 N-O 都从 Preferred Path 恢复成了普通边）。 我们把 Preferred Path 存储在 Splay 森林上，然后每次 Access 操作之后对 Preferred Path 进行的修改则相应地需要依靠 Splay 来进行维护。 左下角的这棵树就是 Access(N) 操作前的 Splay 森林的结构，红边是 Splay 的树边，黑边是两棵 Splay 树的连接关系。这里其实一共有 7 棵 Splay 树：ABE，D，F，CGHJ，IK，LNO，M。可以看到每一棵 Splay 实际上表示的是一条 Preferred Path 或者单个节点，Preferred Path 在原树中的深度就是对应的 Splay 中节点的索引值（！！这个对理解很重要）。 然后访问 Access(N) 会发生什么呢？ 把 N 旋转到它的 Splay 的根，然后断开 N 和右子树的连接 检查 N 的父节点 I，把 I 旋转到它的 Splay 的根，然后断开 I 和右子树的连接，把 N 接到右子树上 检查 I 的父节点 H，把 H 旋转到它的 Splay 的根，然后断开 H 和右子树的连接，把 I 接到右子树上 检查 H 的父节点 A，把 A 旋转到它的 Splay 的根，然后断开 A 和右子树的连接，把 H 接到右子树上 检查 A 已经没有父节点了，结束 然后我们就得到了右上角这棵 Splay 树，因为经过了 Splay 操作的维护调整，可能最终具体的形态不是这样的，但这棵树上的所有节点就是这 7 个。其他节点也还在，不过这里没画出来了。 有了 Access 之后，接下来我们有了 3 个进一步的操作： FindRoot(N)：找到 N 所在的树的根节点 首先 Access(N)，根节点就是 N 所在的 Preferred Path 的最顶端的那个点了。把 N 旋转到它所在的 Splay 的根，然后向左走到底就是整个树的根了。 Cut(N)：把 N 连同它的子树从它所属的原有的树中切出来 首先 Access(N)，把 N 旋转到它所在的 Splay 的根，然后断开 N 和它左子树的连接。 Link(N, M)：把 N 这棵树连接到 M 这个节点下（N 是树根，且 N 和 M 不在同一棵树中） 首先 Access(N)，把 N 所在的 Splay 的根节点的父节点设成 M，然后再 Access(N)。 然后我们来思考一下 LCT 能干嘛。其实 Splay 本身已经是一种具有切分、组合能力的动态树结构了，但是从上面这个例子的描述中，我们也能很直观地发现，Splay 只能最多处理到二叉树的动态变化，而 LCT 可以处理多叉树的动态结构。 QTREE 文中最初给出的目标题目是： 给一棵共有 n 个结点的树, 每条边都有一个权值, 要求维护一个数据结构, 支持如下操作: 修改某条边的权值； 询问某两个节点之间的唯一通路上的最大边权。 第二个问题就是很直观的一个 LCA，关键在这个最大边权上。 首先来看用 LCT 怎么求两个点 N 和 M 的 LCA：首先还是 Access(N)，然后下一次做 Access(M) 时，由于 Access 一直在反复找父节点旋转到根，当最后一步找到的最顶层的 Splay 就是 N 所在的 Preferred Path，所以此时 Splay 的根就是 N 和 M 的 LCA 了。 边权的处理：每个节点记录到它的父节点的边的边权，同时记录以它为根的 Splay 树中所有节点的最大边权值。最大边权值是满足结合律的，在 Splay 旋转的过程中注意维护即可。 然后是 LCA 的过程中，假设 N 和 M 的 LCA 是 L。Access(M) 的最后一步需要把 M 所在的 Splay 树接到 L 的右子树上，L 原来的右子树的最大边权值就是 N 到 L 的最大边权值，L 新接的右子树的最大边权值就是 M 到 L 的最大边权值，取这两个中的最大值即可。 HDU 2475 BOX 是一道裸的 LCT 题，把树的从属关系对应到盒子的包含关系上。HDU 3966 Aragorn’s Story 一次性修改一条路径上所有点的权值，也是与 LCA 有关。 所以总结下来，LCT 可以处理多叉树的动态变化，以及与树上路径相关的一些操作（涉及到在线 LCA 的可能都能用 LCT 做吧？）。 Tarjan 论文中把 LCT 应用在网络流上也是一种创新的做法了，不过奇怪的是复杂度低为什么 OI 里面几乎从来没听说过有人这么解的呢？有待以后继续挖掘思考。 Splay 伸展树【1985 - Self-Adjusting Binary Search Trees】 Google 学术引用数 1375。 论文首先介绍了 Tarjan 提出这种平衡树的出发点：虽然其他平衡树的实现，例如height-balanced trees、weight-balanced trees等等等最差情况下的复杂度都能有O(logn)，但实际使用中出现的访问不会这么均匀，而且这些实现都需要额外的信息来维持整棵树的平衡性，所以其实并不是一直这么高效（求验证）。 Tarjan 认为应该关注于一堆操作序列的均摊时间，而不是关注于某个最坏情况的时间。所以 Splay 的设计有几个特点： 均摊情况下不要太考虑平衡树的常数问题（话说 Splay 的常数还真的挺大的） 节省内存，不要记录一些没用的平衡信息，这样可以节省一些操作的时间 访问和更新的操作要简单，易于实现 带来的后果就是，根据这个思想实现出来的 Splay： 需要更多的局部调整，通常一般的平衡树只是在修改的时候需要调整结构，而 Splay 在访问的时候就要对树的结构做改动了 单个操作可能会很费时（话说，最坏情况下 Splay 是会退化成O(n)的链的。。。） Splay 的核心是splaying操作，即把最近访问过的顶点放到树根，以此来达到访问均摊时间相对较优的效果。论文里面还提到了用 Splay 来实现字典搜索树和多维搜索树以及 LCT 的应用。 Splay 的思想其实特别简单，实现方面我在以前写 Splay 题的时候基本上也是先参考的其他人的模版改的。杨哲的 QTREE 论文中提到了他对 Splay 实现方面的思考和优化，准备按照他的介绍试着改改看我以前的代码。 。。。然后很尴尬的是好像失败了。光从代码结构上来看 QTREE 论文中的 Splay 代码确实要少好多行，结果按照他的伪代码改起来有点小问题，跟我原本的代码对不上，有待后续再看看，暂时情况下，我原本的 Splay 模板已经够用了。 把被访问的顶点放到树根这件事情实现起来有很多种方式，而 Splay 中比较特别的是限定了几种特定的旋转规则（花式旋转）： p(x) 指 x 的父节点。 zigp(x) 是根节点。则直接旋转 x 和 p(x)。（一次左旋或者一次右旋） zig-zigp(x) 不是根节点，同时 p(x) 是其父节点的左节点且 x 也是 p(x) 的左节点，或者 p(x) 是其父节点的右节点且 x 也是 p(x) 的右节点。则先旋转 p(x) 和 p(x) 的父节点，再旋转 x 和 p(x)。（两次左旋或者两次右旋） zig-zagp(x) 不是根节点，同时 p(x) 是其父节点的左节点且 x 是 p(x) 的右节点，或者 p(x) 是其父节点的右节点且 x 是 p(x) 的左节点。则先旋转 x 和 p(x)，再继续旋转 x 和 p(p(x))。（一次右旋一次左旋或者一次左旋一次右旋） 具体的图可以见论文。 这样做的好处是，经过 Splay 操作之后，不仅仅是成功把 x 节点移到了树根，同时还把树根到 x 节点的路径降低了一半的深度（这个很关键！）。这部分的算法证明我就没看了，默默地跳过。 得益于 Splay 相比常见的普通平衡树的优点，就是它的灵活性（也可能是缺点，这也导致了事实上 Splay 并不是那么…额，平衡），Splay 可以做出两个一般平衡树很难实现的操作：join(t1, t2) 和 split(i, t)。 join(t1, t2) 是把两棵 Splay 树合成一棵，当然这里需要假定 t1 中的所有元素都比 t2 中的要小。实现是访问 t1 中最大的元素，这样这个元素就到了树根并且右子树是空的，把 t2 接到右子树的位置上即可。或者访问 t2 中最小的元素，把 t1 接到左子树的位置上。 split(i, t) 把 Splay 中小于 i 的所有元素切出来。实现也很直观，找到元素 i 旋转到树根，然后把左子树切出来就好了。 NOI2004 郁闷的出纳员 就是典型的平衡树性质加上 split 操作的应用。 然后 Splay 的单个元素插入和删除也可以有两种实现：类似一般平衡树的做法，或者基于上面的 join 和 split 操作。 嗯，我以前的写法都是按普通平衡树那样写的，据论文上的说法，特殊实现能有更好的均摊时间边界： insert(i, t)，做一次 split(i, t)，然后把切出来的两棵 Splay 分别接到新节点 i 的左右子树上去。 delete(i, t)，在树 t 中访问节点 i，把它旋转到根，然后把根的左右两个子树 join 在一起。或者先把 i 的左右子树 join 在一起，再把 i 的父节点旋转到根。 HDU 1890 Robotic Sort 是删根操作加上线段树 lazy 标记方法的应用。HDU 3487 Play with Chain 是 spilt 和 join 操作的综合应用，这里还用到了不仅仅是要切出小于某个值的部分，而是要切出一个区间范围内的值，同时也涉及到了 lazy 标记的应用。算是相当综合了。 看完 Splay 之后，再回到上面继续 LCT。 持久化数据结构【1986 - Making Data Structures Persistent】 Google 学术引用数 806。 持久化数据结构的定义是：经过多次修改之后，还要能保证在任何时候都能得到到该结构的任何版本（不管是老的还是新的）。听上去很玄。。。 大致的做法是在原有的数据结构上增加一些额外的信息域，然后修改内容时保留原有的不变，增加新节点等等。例如持久化的二叉树，增加节点时，保留原有的不变，把原本的节点复制一份增加新节点等等，做好标记。 具体的操作、实现等等，先留待以后看了。 关于持久化数据结构，CLJ 和 FHQ 两位大佬也整理过一些东西： 【范浩强-wc2012谈谈各种数据结构】 【可持久化数据结构研究】 斐波那契堆【1987 - Fibonacci heaps and their uses in improved network optimization algorithms】 Google 学术引用数 2943。 这个也先留着吧，可以参见《算法导论》上的章节。]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>Splay</tag>
        <tag>伸展树</tag>
        <tag>动态树</tag>
        <tag>LCT</tag>
        <tag>Paper</tag>
        <tag>Tarjan</tag>
        <tag>Algorithm</tag>
        <tag>LCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写着玩之 RDMA 轮子]]></title>
    <url>%2F2017%2F09%2F18%2F2017-09-18-rdmalib%2F</url>
    <content type="text"><![CDATA[嗯，原本打算写着玩系列写个内存池、线程池啥的，前段时间做医疗影像 DL 项目的时候跟实验室的小伙伴一起用 C++ 和 Cuda 写过一个 Inference 的框架。 那个项目后面还不知道要怎么发展，似乎暂时没办法把框架公开出来。然后内存池只写了个开头，准备等有空再出来补补吧。线程池…嗯，还不知道在哪呢。 最近整理以前的资料的时候又给自己找了个新坑。嗯，虽说之前在 gRPC 和 TensorFlow 上都改过 RDMA，然而这两个都是当时直接改在它们原本的源码里面的，要有人让我再改一个别的东西那我就傻了，又得从头重新走一遍。要我给个示例的话，手上只有当时最早学 RDMA 时从 rc_pingpong 里扒出来的 client 和 server 了。 于是准备开个新坑，整理个自己的的 RDMA 轮子库出来。 To be continued.]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>RDMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几道 线段树&树状数组 的题]]></title>
    <url>%2F2017%2F08%2F23%2F2017-08-23-segbit%2F</url>
    <content type="text"><![CDATA[发现以前会的好多算法现在都忘得差不多了。。。翻了翻以前写过的一些比较复杂的题目，发现现在真是两眼一抹黑。 准备有空重新开始刷一波题。 先开个线段树的专题吧。 整理了一下模版，快速过了 4 个典型例题。 经典例题HDU1166【线段树/树状数组】 单点更新，区间求和。 嗯，树状数组天生就是做这个的，没啥说的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 1166************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int c[50000];int n;int lowbit(int s)&#123; return s&amp;-s;&#125;void update(int s,int x)&#123; while (x&lt;=n) &#123; c[x]+=s; x+=lowbit(x); &#125;&#125;int sum(int x)&#123; int t=0; while (x&gt;0) &#123; t+=c[x]; x-=lowbit(x); &#125; return t;&#125;int main()&#123; freopen("in.txt", "r", stdin); int T; scanf("%d", &amp;T); for (int tt=1;tt&lt;=T;tt++) &#123; printf("Case %d:\n", tt); memset(c, 0, sizeof(c)); scanf("%d", &amp;n); for (int i=1;i&lt;=n;i++) &#123; int s; scanf("%d", &amp;s); update(s, i); &#125; char ss[10]; while(scanf("%s", ss)) &#123; int i, j; if (ss[0]!='E') &#123; scanf("%d %d", &amp;i, &amp;j); switch(ss[0]) &#123; case 'Q': printf("%d\n", sum(j) - sum(i-1)); break; case 'A': update(j, i); break; case 'S': update(-j, i); break; &#125; &#125; else break; &#125; &#125; return 0;&#125; 线段树也是基础用法，不过说起来以前我写的线段树基本上都是用的结构体数组，压缩一下子节点的标记号增长方式可以把空间限制在 2N~3N 左右。 这次换了个模版，标号增长直接用的是二叉树倍增的方式，空间要浪费一点，需要开到 4N，不过拿掉了其他多余的东西（结构体等等），好像效果也还好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 1166************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int a[50000],ans[4*50000];void pushup(int rt)&#123; ans[rt]=ans[rt&lt;&lt;1]+ans[rt&lt;&lt;1|1];&#125;void build(int l,int r,int rt)&#123; if (l==r) &#123; ans[rt]=a[l]; return; &#125; int mid=(l+r)&gt;&gt;1; build(l,mid,rt&lt;&lt;1); build(mid+1,r,rt&lt;&lt;1|1); pushup(rt);&#125;void add(int L,int C,int l,int r,int rt)&#123; if (l==r) &#123; ans[rt]+=C; return; &#125; int mid=(l+r)&gt;&gt;1; if (L&lt;=mid) add(L,C,l,mid,rt&lt;&lt;1); else add(L,C,mid+1,r,rt&lt;&lt;1|1); pushup(rt);&#125;int query(int L,int R,int l,int r,int rt)&#123; if (L&lt;=l&amp;&amp;r&lt;=R) return ans[rt]; int mid=(l+r)&gt;&gt;1; int ANS=0; if (L&lt;=mid) ANS+=query(L,R,l,mid,rt&lt;&lt;1); if (R&gt;mid) ANS+=query(L,R,mid+1,r,rt&lt;&lt;1|1); return ANS;&#125;int main()&#123; freopen("in.txt", "r", stdin); int T; scanf("%d", &amp;T); for (int tt=1;tt&lt;=T;tt++) &#123; printf("Case %d:\n", tt); int n; scanf("%d", &amp;n); for (int i=1;i&lt;=n;i++) scanf("%d", &amp;a[i]); memset(ans, 0, sizeof(ans)); build(1, n, 1); char ss[10]; while(scanf("%s", ss)) &#123; int i, j; if (ss[0]!='E') &#123; scanf("%d %d", &amp;i, &amp;j); switch(ss[0]) &#123; case 'Q': printf("%d\n", query(i, j, 1, n, 1)); break; case 'A': add(i, j, 1, n, 1); break; case 'S': add(i, -j, 1, n, 1); break; &#125; &#125; else break; &#125; &#125; return 0;&#125; HDU1754【线段树】 单点更新，区间求最值。 线段树基础用法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 1754************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int a[200010], amax[4*200010];int max(int x, int y)&#123; return x&gt;y ? x:y;&#125;void build(int l, int r, int rt)&#123; if (l == r) &#123; amax[rt] = a[l]; return; &#125; int mid = (l+r)&gt;&gt;1; build(l, mid, rt&lt;&lt;1); build(mid+1, r, rt&lt;&lt;1|1); amax[rt] = max(amax[rt&lt;&lt;1], amax[rt&lt;&lt;1|1]);&#125;void update(int L, int C, int l, int r, int rt)&#123; if (l == r) &#123; amax[rt] = C; return; &#125; int mid = (l+r)&gt;&gt;1; if (L &lt;= mid) update(L, C, l, mid, rt&lt;&lt;1); else update(L, C, mid+1, r, rt&lt;&lt;1|1); amax[rt] = max(amax[rt&lt;&lt;1], amax[rt&lt;&lt;1|1]);&#125;int query(int L, int R, int l, int r, int rt)&#123; if (L&lt;=l &amp;&amp; r&lt;=R) return amax[rt]; int mid = (l+r)&gt;&gt;1; int temp = 0; if (L &lt;= mid) temp = max(temp, query(L, R, l, mid, rt&lt;&lt;1)); if (R &gt; mid) temp = max(temp, query(L, R, mid+1, r, rt&lt;&lt;1|1)); return temp;&#125;int main()&#123; freopen("in.txt", "r", stdin); int n, m; while(scanf("%d %d", &amp;n, &amp;m)!=EOF) &#123; for (int i=1;i&lt;=n;i++) scanf("%d", &amp;a[i]); memset(amax, 0, sizeof(amax)); build(1, n, 1); for (int i=0;i&lt;m;i++) &#123; char c; int x, y; scanf("\n%c %d %d", &amp;c, &amp;x, &amp;y); if (c=='Q') printf("%d\n", query(x, y, 1, n, 1)); else update(x, y, 1, n, 1); &#125; &#125; return 0;&#125; HDU1698【线段树】 区间更新，区间求和。 修改区间的时候就要多加一个 lazy 标记，基础用法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 1698************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int res[4*100010], lazy[4*100010];void build(int l, int r, int rt)&#123; if (l == r) &#123; res[rt] = 1; return; &#125; int mid = (l+r)&gt;&gt;1; build(l, mid, rt&lt;&lt;1); build(mid+1, r, rt&lt;&lt;1|1); res[rt] = res[rt&lt;&lt;1] + res[rt&lt;&lt;1|1];&#125;void pushdown(int rt, int ln, int rn)&#123; if (lazy[rt]) &#123; lazy[rt&lt;&lt;1] = lazy[rt]; lazy[rt&lt;&lt;1|1] = lazy[rt]; res[rt&lt;&lt;1] = lazy[rt]*ln; res[rt&lt;&lt;1|1] = lazy[rt]*rn; lazy[rt] = 0; &#125;&#125;void update(int L, int R, int C, int l, int r, int rt)&#123; if (L&lt;=l &amp;&amp; r&lt;=R) &#123; res[rt] = C*(r-l+1); lazy[rt] = C; return; &#125; int mid = (l+r)&gt;&gt;1; pushdown(rt, mid-l+1, r-mid); if (L &lt;= mid) update(L, R, C, l, mid, rt&lt;&lt;1); if (R &gt; mid) update(L, R, C, mid+1, r, rt&lt;&lt;1|1); res[rt] = res[rt&lt;&lt;1] + res[rt&lt;&lt;1|1];&#125;int query(int L, int R, int l, int r, int rt)&#123; if (L&lt;=l &amp;&amp; r&lt;=R) return res[rt]; int mid = (l+r)&gt;&gt;1; pushdown(rt, mid-l+1, r-mid); int temp = 0; if (L &lt;= mid) temp += query(L, R, l, mid, rt&lt;&lt;1); if (R &gt; mid) temp += query(L, R, mid+1, r, rt&lt;&lt;1|1); return temp;&#125;int main()&#123; freopen("in.txt", "r", stdin); int T; scanf("%d", &amp;T); for (int tt=1;tt&lt;=T;tt++) &#123; int n; scanf("%d", &amp;n); memset(res, 0, sizeof(res)); memset(lazy, 0, sizeof(lazy)); build(1, n, 1); int m; scanf("%d", &amp;m); for (int i=0;i&lt;m;i++) &#123; int x, y, z; scanf("%d %d %d", &amp;x, &amp;y, &amp;z); update(x, y, z, 1, n, 1); &#125; printf("Case %d: The total value of the hook is %d.\n", tt, query(1, n, 1, n, 1)); &#125; return 0;&#125; HDU1556【线段树/树状数组】 区间更新，单点求值。 这是最经典的线段覆盖问题。 单点求值的时候有点不同的是只要用到 lazy 标记就够了，沿着路径一路走到叶子结点，把一路上的 lazy 标记全部加起来就好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 1556************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int lazy[4*100010];void pushdown(int rt, int ln, int rn)&#123; if (lazy[rt]) &#123; lazy[rt&lt;&lt;1] += lazy[rt]; lazy[rt&lt;&lt;1|1] += lazy[rt]; lazy[rt] = 0; &#125;&#125;void update(int L, int R, int l, int r, int rt)&#123; if (L&lt;=l &amp;&amp; r&lt;=R) &#123; lazy[rt] += 1; return; &#125; int mid = (l+r)&gt;&gt;1; pushdown(rt, mid-l+1, r-mid); if (L &lt;= mid) update(L, R, l, mid, rt&lt;&lt;1); if (R &gt; mid) update(L, R, mid+1, r, rt&lt;&lt;1|1);&#125;int query(int X, int l, int r, int rt)&#123; if (X&lt;=l &amp;&amp; r&lt;=X) return lazy[rt]; int mid = (l+r)&gt;&gt;1; int temp = lazy[rt]; if (X &lt;= mid) temp += query(X, l, mid, rt&lt;&lt;1); else temp += query(X, mid+1, r, rt&lt;&lt;1|1); return temp;&#125;int main()&#123; freopen("in.txt", "r", stdin); int n; scanf("%d", &amp;n); while (n) &#123; memset(lazy, 0, sizeof(lazy)); for (int i=0;i&lt;n;i++) &#123; int a, b; scanf("%d %d", &amp;a, &amp;b); update(a, b, 1, n, 1); &#125; printf("%d", query(1, 1, n, 1)); for (int i=2;i&lt;=n;i++) printf(" %d", query(i, 1, n, 1)); printf("\n"); scanf("%d", &amp;n); &#125; return 0;&#125; 树状数组解法是把单点求值转化为求前缀和，把区间更新转化为两次单点更新即可。 每次要插一条区间的时候，在区间开头加一，区间结尾减一，这样下次求前缀和的时候就相当于原本的单点求值了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 1556************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int c[100010];int n;int lowbit(int s)&#123; return s&amp;-s;&#125;void update(int s,int x) &#123; while (x&lt;=n) &#123; c[x]+=s; x+=lowbit(x); &#125;&#125;int sum(int x)&#123; int t=0; while (x&gt;0) &#123; t+=c[x]; x-=lowbit(x); &#125; return t;&#125;int main()&#123; freopen("in.txt", "r", stdin); scanf("%d", &amp;n); while (n) &#123; memset(c, 0, sizeof(c)); for (int i=0;i&lt;n;i++) &#123; int a, b; scanf("%d %d", &amp;a, &amp;b); update(1, a); update(-1, b+1); &#125; printf("%d", sum(1)); for (int i=2;i&lt;=n;i++) printf(" %d", sum(i)); printf("\n"); scanf("%d", &amp;n); &#125; return 0;&#125; 再深入一点翻了下，博客里面以前记过一题，顺便也整理到这里来。然后还有几次以前比赛里面没补的题，看看能不能补一波。 HDU5172不更新，但是区间判断的东西比较有意思。每次询问的是给个区间，假设区间长度是 n，问这个区间中的数字是不是刚好能从 1 排到 n。 HDU 5172 GTY’s gay friends 线段树 HDU502314 年广州网赛的 B 题，当时没怎么写。 The 2014 ACMICPC Asia Regional Guangzhou Online 30 种颜色，每次用一种颜色涂一些线段，询问一段区间内有什么颜色，把颜色全部列出来。 内心OS：哇。。。这么基础的线段树染色题我当年在干什么。。。捂脸 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115/* ***********************************************MYID : Chen FanLANG : G++PROG : hdu 5023************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int res[4*1000010], lazy[4*1000010];void build(int l, int r, int rt)&#123; if (l == r) &#123; res[rt] = 2; return; &#125; int mid = (l+r)&gt;&gt;1; build(l, mid, rt&lt;&lt;1); build(mid+1, r, rt&lt;&lt;1|1); res[rt] = res[rt&lt;&lt;1] | res[rt&lt;&lt;1|1];&#125;void pushdown(int rt)&#123; if (lazy[rt]) &#123; lazy[rt&lt;&lt;1] = lazy[rt]; lazy[rt&lt;&lt;1|1] = lazy[rt]; res[rt&lt;&lt;1] = lazy[rt]; res[rt&lt;&lt;1|1] = lazy[rt]; lazy[rt] = 0; &#125;&#125;void update(int L, int R, int C, int l, int r, int rt)&#123; if (L&lt;=l &amp;&amp; r&lt;=R) &#123; lazy[rt] = 1 &lt;&lt; (C-1); res[rt] = 1 &lt;&lt; (C-1); return; &#125; int mid = (l+r)&gt;&gt;1; pushdown(rt); if (L &lt;= mid) update(L, R, C, l, mid, rt&lt;&lt;1); if (R &gt; mid) update(L, R, C, mid+1, r, rt&lt;&lt;1|1); res[rt] = res[rt&lt;&lt;1] | res[rt&lt;&lt;1|1];&#125;int query(int L, int R, int l, int r, int rt)&#123; if (L&lt;=l &amp;&amp; r&lt;=R) return res[rt]; int mid = (l+r)&gt;&gt;1; pushdown(rt); int temp = 0; if (L &lt;= mid) temp |= query(L, R, l, mid, rt&lt;&lt;1); if (R &gt; mid) temp |= query(L, R, mid+1, r, rt&lt;&lt;1|1); return temp;&#125;int main()&#123; freopen("in.txt", "r", stdin); int n, m; scanf("%d %d", &amp;n, &amp;m); while(!(n==0 &amp;&amp; m==0)) &#123; memset(res, 0, sizeof(res)); memset(lazy, 0, sizeof(lazy)); build(1, n, 1); for (int i=0;i&lt;m;i++) &#123; char com; int a,b; scanf("\n%c %d %d", &amp;com, &amp;a, &amp;b); if (com == 'P') &#123; int c; scanf("%d", &amp;c); update(a, b, c, 1, n, 1); &#125; else &#123; int ans = query(a, b, 1, n, 1); int count = 1; bool blank = false; while (ans) &#123; if (ans&amp;1) &#123; if (blank) printf(" "); printf("%d", count); blank = true; &#125; ans = ans &gt;&gt; 1; count ++; &#125; printf("\n"); &#125; &#125; scanf("%d %d", &amp;n, &amp;m); &#125; return 0;&#125; 30 种颜色，用位运算可以很方便地表示出来。 嗯。。。下面的 H 题等下次什么时候想把树链剖分翻出来的时候再研究吧。]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>线段树</tag>
        <tag>树状数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整理一下看过的论文]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-paper%2F</url>
    <content type="text"><![CDATA[还是得多看别人的论文，看多了等自己有东西能写的时候才能写得出来。 迷迷糊糊地就发出去第一篇 Paper 了。。。回想起来也算是幸运吧。 把以前看过的论文都理一理，共享在 OneDrive 上了（似乎被墙了）： 【My Paper Reading List - OneDrive】 Top Conference嗯。。。当年曾经想看看顶会上的文章的，但是那时候水平有限，只是简单地翻了一下，以后有机会再多看看吧。 ISCA 2014 会议论文整理 RDMA这几篇文章主要都是我改 RDMA 版 TensorFlow 的时候看的，借鉴其中的通信协议的设计思路，这里面的很多想法其实都是大同小异。 用 RDMA 有个比较严重的问题是：为了做到 kernel bypass，通信用到的内存都必须提前注册成 MR，这样 IB 卡就能直接读写内存而不用通过 CPU。现场注册内存是一件比较耗时的事情，如果处理不好会在这里浪费很多时间。 大致的想法很简单，这下面的几篇文章讲的也都差不多是类似的内容：内存管理/通信/计算 overlap。 主要是实现起来还是有点麻烦。 2001 - An introduction to the infiniband architectureIB 架构的介绍，没怎么看，这条就纯用来引用。 2004 - Host-Assisted Zero-Copy Remote Memory Access Communication on InfiniBandTo be read. 2006 - RDMA Read Based Rendezvous Protocol for MPI over InﬁniBand Design Alternatives and Beneﬁts在 MPI 库层面做的工作，改的是 MVAPICH，可能到今天已经整合进 MPI 的官方 RDMA 实现里面了吧。 主要内容是用 RDMA_READ 换掉原本 MPI 实现中的 RDMA_WRITE 协议，配合中断机制，达到计算和通信的最大程度 overlap。 To be read. 2007 - Memory Management Strategies for Data Serving with RDMATo be read. 2008 - An efficient design for fast memory registration in RDMA跟前面的 RDMA_READ 不同，这篇文章主要讲了一种 Conditional RDMA Write 的机制，目标也是在用 RDMA_WRITE 的时候 overlap 通信和计算。 关于 CRW 这种机制，RDMA 的官方文档里面我没找到，不知道这种方案是不是他们自己实现的。 To be read. 2009 - Minimizing the Hidden Cost of RDMA主要还是针对 RDMA 通信过程中的内存管理进行优化，方案基本上还是那几个： 根据发送的数据大小决定是重用 buffer 还是创建一个新的。例如小数据直接 memcpy 到注册好的内存里，大数据创建新的 MR； 把通信和内存管理 overlap 开； 注册内存的时候考虑内存在物理上的分页； 并行注册 MR。 嗯，有一种道理我们都懂，关键看你怎么实现的感觉。 To be read. 2011 - Scalable Memory Registration for High Performance Networks Using Helper Threads创建一个新线程来专门维护 RDMA 通信用到的内存，MR 的注册什么的都通过新线程来做。 其他的大同小异。 To be read. 2016 - Revisiting RDMA Buffer Registration in the Context of Lightweight Multi-kernelsTo be read. Deep Learning2012 Alexnet - Imagenet classification with deep convolutional neural networks大名鼎鼎的 Alexnet。 2012 年 ImageNet 的冠军算法，这一历史性的事件似乎可以看成是深度学习热潮的起点。 最早我开始接触深度学习方面的知识的时候分析过 Alexnet 的结构，事实上我对 CNN、卷积、全连接等等有个比较明确的认识也就是从这个网络开始的，之后在我的文章里面也是拿 Alexnet 作为我的 Benchmark。 Alexnet 比较突出的贡献还有就是提出了一些防止过拟合的方法： 对整个 256x256 的图片做一遍水平翻转，同时按照 224x224 做滑窗，相当于把原本的 1 张图变成了 2048 张 Dropout 层！！ 嗯，跑完整个 ImageNet 的数据集，作者用 2 块 GTX 580 跑了五六天。 2014 - Adam: A Method for Stochastic Optimization一种效果比梯度下降更好的优化修正方法。 To be read. 2014 Baidu - Deep Speech Scaling up end-to-end speech recognition百度做的端到端语音识别框架。 Mozilla 用 TensorFlow 把他们的这个工作实现了一遍，在这里。 To be read. 2017 HKBU - Benchmarking State-of-the-Art Deep Learning Software Tools香港浸会大学对几个主流 DL 框架做的一些对比评测，实验室师兄据说看出问题来了。详细的还没仔细看。 也就是 DLBench 项目。 To be read. Distributed Machine Learning System / Distributed Deep Learning System / Large Scale Neural Network Training这块内容有点多，单独整理了一篇出来： 【分布式机器学习 / 深度学习论文整理】 TensorFlow花了比较多的时间在 TensorFlow 这个框架上，源码也折腾过很久，但是最后还是觉得这玩意改起来过于复杂。 首先是 TF 的 Google 4 部曲： 1 - TensorFlow A System for Large-Scale Machine LearningTensorFlow 开山之作，TF 本体也是跟着一起发布，基本把整个框架的情况都讲了一遍。 2 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed SystemsTensorFlow 分布式版的介绍，v0.8 版 TF。 话说其实在第一篇里面就已经介绍过分布式了，这篇是更详细地介绍其运行机制。 3 - Deep Learning with Dynamic Compution GraphsTF 的动态图功能。 暂时还没研究过。 4 - In-Datacenter Performance Analysis of a Tensor Processing UnitTPU 的介绍。 发布这篇文章的时候其实 TPU 在 Google 内部已经测试了快一年了，基本上可以说是跟 TF 同期开始做的东西。 可怕。 2016 - DeepSpark A Spark-Based Distributed Deep Learning Framework for Commodity Clusters用 Spark 来做上层框架，底下跑单节点版的 TF 来完成多节点分布式。 2016 - Distributed TensorFlow with MPI跟上面那篇 Spark 的类似，这是用 MPI 做上层框架（大概是 Python-MPI），底下跑单节点版的 TF 来完成多节点分布式。 Algorithm &amp; Math1991 - Wait-free parallel algorithms for the union-find problem多线程下的并行并查集实现，说起来这个也是该花时间好好研究一下。 下面有个别人的实现 Lock-free parallel disjoint set data structure。 1971 ~ Today - Tarjan’s Algorithm &amp; Data StructureRobert E Tarjan，1986 年图灵奖得主，发明了一堆经典算法（主要是图和树的）。 Tarjan 大佬的算法们 Other帮本科生看 SCC17 赛题的时候读的论文： 2014 - A practical guide to identifying members of the Bemisia tabaci species complex and other morphologically identical species一篇有关一种白蚁物种分类的综述。 某个白蚁种群中有大约 34 种不同的种类，区分这些种类并且对一个未知个体来说，如果能快速识别出属于哪一个种类对于生物学研究来说至关重要。但从形态学和种系演变方面很难做到。 为了生物研究能够继续做下去，科学家开始从基因和概率分析方面想办法。 在已有一个成熟的线粒体 DNA 库的基础上，目前可行的方案是对个体进行 DNA 采样和 PCR 倍增，然后用 HPC 的技术来分析 DNA 的结构，找出进化树，最后判别属于哪个种群。 2001 - An Introduction to Bayesian Inference of Phylogeny介绍了如何用贝叶斯推论的方法来分析物种的种系进化，就是上面那篇综述中的可行方案。 贝叶斯推论这种方法大概是：根据看到的现象来反推出初始情况（？我的理解可能有误） 文中一开始是拿骰子举例，假设盒子里面有 100 个骰子，90 个是正常的，10 个是有偏重的。从中随机取出一个骰子投两次，根据这两次投出的结果，我们可以算出正常骰子出现这种结果的概率和偏重骰子出现这种结果的概率，再反推出这个骰子是正常的还是偏重的的概率。 大概是这么个问题，实际情况下会更难一点，假定我们不知道 90/10 这个数量信息，也不知道偏重骰子投出每个点数的概率，然后还要根据观察到的现象推断出骰子的情况（。。。666）。 种系生成学中，物种的进化关系可以用一棵生成树来表示，每种基因都有一定概率突变成其他的基因，生成树上的分支就是由于基因突变产生的。跟贝叶斯推论的方法结合在一起，就是事先根据一定的模型定下生成树的结构，然后枚举所有进化情况的概率（？大概是这样），从里面找出似然度最大的一种，就是最终的进化推断了。 嗯，上面这个方法看上去就很有问题。。。首先，只要用于分析的 DNA 链不短，那么进化树的可能情况应该是指数上涨的，并且贝叶斯推论需要的后验概率也没有办法确定下来。 所以就需要马尔科夫链蒙特卡洛方法。构造一条马尔科夫链，用它的平稳分布作为问题所需要的后验分布，基于马尔科夫链达到平稳分布时的有效样本进行蒙特卡洛积分（？其实并不是很懂）。 马尔科夫链有个初始状态，然后计算转化到下一个状态的概率，如果超过某个值就是可以接受的，马尔科夫链转化成下一个状态然后继续进行下一轮迭代。在这个迭代过程中，对马尔科夫链进行采样（？），采样出的分布作为进化树的后验分布然后计算贝叶斯推论（？）。 例如 MrBayes 中的几个参数：马尔科夫链迭代多少代，每多少代采样一次 后面又举了个实际的例子：用 5 条 DNA 数据进行分析，分别来自鱼、青蛙、鸟、鼠类和人。这 5 个物种的进化树有 15 种可能（根据某种模型计算出来的）。马尔科夫链迭代 100000 代，跑完 10000 代之后开始每 100 代采样一次，这样就得到了 900 个采样结果（900 棵进化树？）用这 900 个采样的数据作为后验分布进行贝叶斯推论分析就得到了最后的结果。（某一条进化枝的后验概率，就是所有树中包含这条进化枝的概率和？）（最后结果大概就是这 15 种可能的进化树分别的概率，概率最高的一种就是推论得出的进化树答案了） Mine2017 - Improving the performance of distributed TensorFlow with RDMA Jia, C., Liu, J., Jin, X. et al. Improving the Performance of Distributed TensorFlow with RDMA. Int J Parallel Prog (2017). https://doi.org/10.1007/s10766-017-0520-3 16 年参加 RDMA 竞赛的时候，赛题就是分布式版 TensorFlow 的 RDMA 实现。当时采用的方法是把 TF 中的通信模块 gRPC 改成 RDMA 的，这样 TF 本体就不用动了（事实上当时试过直接改 TF，但是不知道该从什么地方下手）。 当时比赛结束之后没有放弃这个题目，继续尝试从 TF 本体上下手，把整个 gRPC 给干掉。 最后终于成功做出来了，想想也是悲惨。。。如果重新经历一遍我不敢说能不能再做到了。 后来发现了 Yahoo 的 RDMA 版 TF 项目，跟我们做的工作基本上是一样的，心里一凉，不过东西都做了大半年了，想想还是硬着头皮把文章写完了。投文章的时候看着他们的项目从 pull request 开始，一点点被 TF 官方的 Repo 吸收，最后终于在 1.2.0-rc0 收录了。特别煎熬。 最后中了 NPC 有很大的运气成分吧。 To be continued.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>整理</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UEFI -- 又被坑了一次]]></title>
    <url>%2F2017%2F08%2F06%2F2017-08-06-efi%2F</url>
    <content type="text"><![CDATA[事情的起源是这样的： 想把笔记本上的 1060 用起来，但是 Windows 下 Cuda 的 nvcc 只跟 VS 的编译器绑定，没法换成 Mingw，这就很不开心了。虽然实在为了在 Windows 下跑程序方便，还是把 VS 2015 装上了（吃了我整整几十个 G 的空间，心疼），但是用起来仍然是非常不爽。虽说以前也经常折腾 Win + Linux 的双系统，这次换完笔记本之后觉得破坏原本的分区引导什么的贼不清真 ╮(╯_╰)╭ 然后并不想装。…那最后还是要用啊，怎么办，好烦。…后来终于有了个（自己觉得）特别 6 的想法。我搞个 U 盘装上就好了哇！用 Linux 的时候插上，不用拔了，特别清真！ 然后。。。晚上下了个单，第二天 U 盘就到啦。。。（还特意买个新 U 盘。。。主要是老 U 盘容量都太小）。 然后。。。就没有然后了，中间一直坑。 感觉主要原因还是自己对 EFI 的了解没有原来想象中的那么清楚，一知半解下瞎搞，出问题自然怪不了别人。 前面写过一篇测试： BIOS内核载入的方式（引导实验） 主要是针对 BIOS 的引导和 MBR 分区格式的简单分析。 BIOS+MBR vs UEFI+GPT老一代的系统用的是 MBR 分区表加主板上的 BIOS 引导方式。 MBR 看上去就像个单链表的形式，简单地说就是在每个磁盘分区的开头会固定有一块区域（512B）用于标记整个分区的信息，包括分区大小、是否有引导等等，然后一个分区的标记结束之后再根据偏移就能找到下一个分区。整块硬盘的第一个扇区还标记了整个盘上的引导情况。 BIOS 要做的就是直接读硬盘开头的 512B 的内容就好了，特别简单粗暴。当然相对而言，当时如果系统引导坏了，修起来也比较麻烦，要用专门的工具软件。 后来由于 MBR 分区的缺陷，比如长度有限，所以主分区最多只能有 4 个，硬盘总大小不能超过 2T 等等，当年还是妥妥的够的，不过随着科技的不断发展，SSD 都有好多 T 的了，老的标准已经不再适用，后来就出现了 GPT 分区格式。 GPT 其实我仍然没有详细了解，直观上感觉就是 MBR 的升级版，相比 MBR 更长，因而能表达更多的信息，支持更多的分区和容量，还有备份，所以不容易坏，等等等等，还有很多很厉害的特性。 UEFI（Unified Extensible Firmware Interface） 是一种用来替代 BIOS 的标准，话说它一开始叫 EFI，后来统一之后改叫 UEFI，不明真相的我之前一直以为这个 U 是 “Un” 的前缀，就是非 EFI 的意思…太天真。 相对 BIOS 来说，UEFI 的固件也是固化在主板上的，大致的功能都是类似的，也是由它来读取硬盘上的引导信息，然后加载。 UEFI 一般来说都是跟 GPT 搭配的，当然据说 BIOS+GPT 和 UEFI+MBR 也有，不过会出各种问题，这就在本篇讨论的范围之外了。 既然 GPT 的分区信息不再限制在扇区开头的 512B 范围内了，相对应的 UEFI 检索引导信息的时候也会更加灵活。它会扫描整块硬盘上所有的分区，找到 EFI 分区读取里面的 .efi 文件就可以直接引导了。 所谓 EFI 分区就是个 FAT32 的分区，只是额外带有一个 EFI 标记。所以 EFI 分区不再限制必须要放在开头的第一个分区里面，它可以在整块硬盘的任意一个分区位置。当然，读还是从前向后读的，所以默认情况下是最先找到的 EFI 分区中的 .efi 文件会被加载。 好了，上面是我以前的了解。当时并没有了解清楚里面的细节，然后就坑了。 What’s Wrong?装 U 盘我是直接在 VMware 里面完成的，把 U 盘做成 GPT 分区，UEFI 装机模式。 一开始我往 U 盘里面装的是 Manjaro，这是个基于 Arch 的发行版，然后发现在虚拟机里面能引导起来，关机重启之后无效。不明觉厉，当时觉得可能是这玩意太新了，蜜汁 Bug。 然后退而换 Fedora。 Fedora 的装机过程很顺利，装好之后 U 盘都不用拔，直接重启笔记本就很顺利地引导进去了。然后开始装 Nvidia 的显卡驱动等等等等。 过程也挺顺利的，关键是最后发现我用 DP 口输出出来的第二块屏幕无效，这强迫症就很不爽了。原因大概是我笔记本上自带的屏幕直连的是集显，但是 DP 口是连在独显上的，Intel 的驱动和 Nvidia 的驱动在这块上需要额外配置一下才能正常使用。 然后就一顿操作，然后就失败了。网上也搜不到很好的解决方案。 最后看到有人说目前已知的笔记本双显卡的 Linux 解决方案里面最简单的就是直接用 Ubuntu 了，这里面官方自己配过，装显卡驱动也只要在设置里面点一下就好。 自己手动配置实在太麻烦，我妥协了。 然后又格了 U 盘开始装 Ubuntu。 重点来了。 装 Ubuntu 的时候发现这玩意做 EFI 分区的行为跟 Fedora 不太一样。 我原本虚拟机里面是有个 UEFI 的系统的（Win10）。Fedora 的做法是我指定了装在 U 盘这块驱动器上，它就不会动其他的部分，EFI 分区什么的都只会在 U 盘上进行操作。 装 Ubuntu 时，虽然我已经指定了目标位置是 U 盘，装完之后 U 盘上会建立一个 EFI 分区，但是里面是空的，它在安装时扫描到虚拟机硬盘里面的 EFI 分区之后，直接把它给改了！！！ 这就是为什么装双系统的时候，你会发现原本的 Windows boot loader 会被干掉。而如果你从 Ubuntu 做的 Grub 里面选择 Windows 来启动，下次 Windows 又会把 Grub 干掉。网上能找到大量的这种求助帖。 原因就是它们用的是同一个 EFI 分区。 于是我把虚拟机里面那块硬盘给删了，又重新插 U 盘装了一遍。这次 EFI 分区里面有内容了。 遂重启。 然而失败了。笔记本上的 EFI 固件死活读不到 Ubuntu 这个启动项。我以为是 EFI 里面的内容在装机时出错了，重新在虚拟机里测试的时候却是正常的。 又重新装了几次 Ubuntu 和 Fedora，我才发现 Ubuntu 生成的 EFI 分区跟 Fedora 生成的有区别。Fedora 装完生成的分区目录是这样的： 1234567/EFI--/BOOT --/BOOTx64.efi--/fedora --/xxx.efi --/xxx.efi -- ... 而 Ubuntu 生成的 EFI 分区中少了 /BOOT 这个目录！！ 12345/EFI--/ubuntu --/xxx.efi --/xxx.efi -- ... 。。。那么看来问题就出在这里了。 BOOTx64.efi在网上找到一份这样的资料： 关于Windows Boot Manager、Bootmgfw.efi、Bootx64.efi、bcdboot.exe 的详解 里面大致把 UEFI 的引导问题都讲的比较清楚了。 所以这下真相大白了。虽然 UEFI 固件会自动识别出来 EFI 分区并且去读里面的 .efi 文件，但是一般的 UEFI 固件读取的只会是 EFI/BOOT/BOOTx64.efi 这个文件。一些设置项更多的 UEFI 固件可能能支持让你手动选择 .efi 文件，例如 VMware 里面那个固件就是。 这就是为什么我用 U 盘做好的 Ubuntu 能在虚拟机里正常引导，但是没办法在笔记本上启动。 那么问题就解决了。 打开 EFI 分区，新建一个名字叫 BOOT 的文件夹，把 ubuntu/shimx64.efi 和 ubuntu/grubx64.efi 拷过去。然后把 shimx64.efi 改名为 BOOTx64.efi。 重启，Enjoy。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>efi</tag>
        <tag>bios</tag>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向 Node.js 的底层再走一步]]></title>
    <url>%2F2017%2F07%2F10%2F2017-07-10-libuv%2F</url>
    <content type="text"><![CDATA[关于 Node.js 的异步编程之前已经稍微了解过了，再来继续深入研究一下用来支持它异步能力的底层事件库 —— Libuv。 这个标志还是很霸气的，也象征了这个库简单粗暴又高效的特点。 先扔两段代码： 单线程异步： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/* ***********************************************MYID : Chen FanLANG : G++PROG : libuv thread test************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include "uv.h"using namespace std;uv_loop_t *loop;uv_async_t async[5];int wait[5] = &#123;0, 1, 1, 2, 2&#125;;void update_wait(int index)&#123; wait[index]--; if (wait[index] == 0) uv_async_send(&amp;async[index]);&#125;void f0(uv_async_t *handle)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f0 run in " &lt;&lt; id &lt;&lt; endl; update_wait(1); uv_close((uv_handle_t*)&amp;async[0], NULL);&#125;void f1(uv_async_t *handle)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f1 run in " &lt;&lt; id &lt;&lt; endl; update_wait(2); update_wait(3); update_wait(4); uv_close((uv_handle_t*)&amp;async[1], NULL);&#125;void f2(uv_async_t *handle)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f2 run in " &lt;&lt; id &lt;&lt; endl; update_wait(3); uv_close((uv_handle_t*)&amp;async[2], NULL);&#125;void f3(uv_async_t *handle)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f3 run in " &lt;&lt; id &lt;&lt; endl; update_wait(4); uv_close((uv_handle_t*)&amp;async[3], NULL);&#125;void f4(uv_async_t *handle)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f4 run in " &lt;&lt; id &lt;&lt; endl; uv_close((uv_handle_t*)&amp;async[4], NULL);&#125;int main()&#123; cout &lt;&lt; "Test Start" &lt;&lt; endl; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; endl; loop = uv_default_loop(); uv_async_init(loop, &amp;async[0], f0); uv_async_init(loop, &amp;async[1], f1); uv_async_init(loop, &amp;async[2], f2); uv_async_init(loop, &amp;async[3], f3); uv_async_init(loop, &amp;async[4], f4); uv_async_send(&amp;async[0]); uv_run(loop, UV_RUN_DEFAULT); cout &lt;&lt; "Loop End" &lt;&lt; endl; return 0;&#125; 多线程（queue_work 自带线程池）异步： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110/* ***********************************************MYID : Chen FanLANG : G++PROG : libuv thread test************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include "uv.h"using namespace std;uv_loop_t *loop;uv_async_t async[5];int wait[5] = &#123;0, 1, 1, 2, 2&#125;;void update_wait(int index)&#123; wait[index]--; if (wait[index] == 0) uv_async_send(&amp;async[index]);&#125;void f0(uv_work_t *req)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f0 run in " &lt;&lt; id &lt;&lt; endl; update_wait(1); uv_close((uv_handle_t*)&amp;async[0], NULL);&#125;void f1(uv_work_t *req)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f1 run in " &lt;&lt; id &lt;&lt; endl; update_wait(2); update_wait(3); update_wait(4); uv_close((uv_handle_t*)&amp;async[1], NULL);&#125;void f2(uv_work_t *req)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f2 run in " &lt;&lt; id &lt;&lt; endl; update_wait(3); uv_close((uv_handle_t*)&amp;async[2], NULL);&#125;void f3(uv_work_t *req)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f3 run in " &lt;&lt; id &lt;&lt; endl; update_wait(4); uv_close((uv_handle_t*)&amp;async[3], NULL);&#125;void f4(uv_work_t *req)&#123; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "f4 run in " &lt;&lt; id &lt;&lt; endl; uv_close((uv_handle_t*)&amp;async[4], NULL);&#125;void after(uv_work_t *req, int status) &#123;&#125;void new_run(uv_async_t *handle)&#123; cout &lt;&lt; "New Run" &lt;&lt; endl; uv_work_t *req = new uv_work_t(); uv_queue_work(loop, req, (uv_work_cb)(uv_work_cb*)handle-&gt;data, after);&#125;int main()&#123; cout &lt;&lt; "Test Start" &lt;&lt; endl; uv_thread_t id = uv_thread_self(); cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; endl; loop = uv_default_loop(); async[0].data = (void*)&amp;f0; async[1].data = (void*)&amp;f1; async[2].data = (void*)&amp;f2; async[3].data = (void*)&amp;f3; async[4].data = (void*)&amp;f4; uv_async_init(loop, &amp;async[0], new_run); uv_async_init(loop, &amp;async[1], new_run); uv_async_init(loop, &amp;async[2], new_run); uv_async_init(loop, &amp;async[3], new_run); uv_async_init(loop, &amp;async[4], new_run); uv_async_send(&amp;async[0]); uv_run(loop, UV_RUN_DEFAULT); cout &lt;&lt; "Loop End" &lt;&lt; endl; int aaaaaa; cin &gt;&gt; aaaaaa; return 0;&#125; To be continued …]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Node.js</tag>
        <tag>libuv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[医学影像 & AI]]></title>
    <url>%2F2017%2F06%2F21%2F2017-06-21-3dct%2F</url>
    <content type="text"><![CDATA[嗯，在杭州待了半个月，想了想还是把一个月前的这篇从草稿堆里面恢复出来了。 被老板外派出去参与健培公司的合作项目，正常的话，大概今年8月就能在央视的节目上看到我们的工作啦 ^0^/ 这篇日志里面记录的都是网上的开源资料，主要来源于 Kaggle。 参考资料： 3D ConvNet For Kaggle Data Science Bowl 2017 Lung Cancer Detection Guido Zuidhof’s Full Preprocessing Tutorial Introduction深度学习的这把火已经烧遍了几乎所有领域，尤其在图像识别方面。 医疗放射图像方面的疾病监测自然也早就有人盯上了。 LUNA16是一个肺部结节检测的数据集…0.0…不知道是关于肺结节的深度学习检测火了才有了这个数据集还是先有了这个数据集然后肺结节检测才火了。 反正在我们这些领域外行来看，好像特别多的机构和公司都在钻这块的研究。其他的，好像有例如乳腺癌检测等等也是个做的比较多的课题。 除了 LUNA16 数据集本身的排名以外，阿里的天池，还有Kaggle都搞了类似的比赛。 不过相较国外的比赛，阿里的最后要求队伍还要上交所有代码，我感觉就有点砸钱套算法的意味了。嗯，你有钱你牛。 Kaggle 上学术分享的氛围很浓，赛方并没有什么要求，而比赛的前几名往往都会主动把代码开源出来供整个社区一起学习。 Pre-Processing首先是数据的预处理部分。 Reading CT DataCT 数据扫描出来会保存成 .dcm 格式的数据，每个文件表示一层 CT 扫描的结果。一个病例则是将一组好多层的结果拼在一起的三维影像。 .dcm 格式中包含了像素大小，图片像素与实际物理尺度的关系等等，相当复杂，python 这边可以用 dicom 这个库直接读取 .dcm 文件，处理起来还比较方便。纯 C++ 操作的话，有个叫 simpleitk 的库可以用。 1234567891011121314import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import dicomimport osimport scipy.ndimageimport matplotlib.pyplot as pltfrom skimage import measure, morphologyfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection# Some constantsINPUT_FOLDER = '../input/sample_images/'patients = os.listdir(INPUT_FOLDER)patients.sort() 上面这段是从 Kaggle 提供的病例数据中把每个病例的目录列在 patients 中。 之后需要做的就是使用 dicom 库把某个病例目录下所有的 .dcm 文件都读进来，然后处理成人眼能够看的灰度图像。 CT 数据用的是一个叫 Hounsfield 的单位，表示身体的某个部分对于 X 光的不透光性（这个好神奇…）。 根据维基百科上的数据： |Substance|HU||-||Air|−1000||Lung|-700 to −600||Fat|−120 to −90||Chyle|−30||Water|0||Urine|-5 to +15||Bile|-5 to +15||CSF|+15||Kidney|+20 to +45||Liver|60 ± 6||Lymph nodes|+10 to +20||Blood|+30 to +45||Muscle|+35 to +55||White matter|+20 to +30||Grey matter|+37 to +45||Soft Tissue, Contrast|+100 to +300||Bone|+200 (craniofacial bone), +700 (cancellous bone) to +3000 (cortical bone)| 人体中的每个不同组织的透光性不同，因此可以通过这个来直接过滤出需要的部位数据来。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Load the scans in given folder pathdef load_scan(path): slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)] slices.sort(key = lambda x: float(x.ImagePositionPatient[2])) try: slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2]) except: slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation) for s in slices: s.SliceThickness = slice_thickness return slicesdef get_pixels_hu(slices): image = np.stack([s.pixel_array for s in slices]) # Convert to int16 (from sometimes int16), # should be possible as values should always be low enough (&lt;32k) image = image.astype(np.int16) # Set outside-of-scan pixels to 0 # The intercept is usually -1024, so air is approximately 0 image[image == -2000] = 0 # Convert to Hounsfield units (HU) for slice_number in range(len(slices)): intercept = slices[slice_number].RescaleIntercept slope = slices[slice_number].RescaleSlope if slope != 1: image[slice_number] = slope * image[slice_number].astype(np.float64) image[slice_number] = image[slice_number].astype(np.int16) image[slice_number] += np.int16(intercept) return np.array(image, dtype=np.int16)# ----------------------------------------------first_patient = load_scan(INPUT_FOLDER + patients[0])first_patient_pixels = get_pixels_hu(first_patient)plt.hist(first_patient_pixels.flatten(), bins=80, color='c')plt.xlabel("Hounsfield Units (HU)")plt.ylabel("Frequency")plt.show()# Show some slice in the middleplt.imshow(first_patient_pixels[80], cmap=plt.cm.gray)plt.show() get_pixels_hu 这个函数返回的直接是一个 numpy 的三维矩阵了，其中每个点的数据就是 CT 拍出来的 HU 值。 CT 扫描出来的各个方向的物理尺度不一定一致，之后需要对原数据进行重新采样。 1234567891011121314151617def resample(image, scan, new_spacing=[1,1,1]): # Determine current pixel spacing spacing = np.array([scan[0].SliceThickness] + scan[0].PixelSpacing, dtype=np.float32) resize_factor = spacing / new_spacing new_real_shape = image.shape * resize_factor new_shape = np.round(new_real_shape) real_resize_factor = new_shape / image.shape new_spacing = spacing / real_resize_factor image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode='nearest') return image, new_spacingpix_resampled, spacing = resample(first_patient_pixels, first_patient, [1,1,1])print("Shape before resampling\t", first_patient_pixels.shape)print("Shape after resampling\t", pix_resampled.shape) 现在我们得到了一个三个维度缩放比例一致的图像，可以输出来看一下。 123456789101112131415161718192021222324def plot_3d(image, threshold=-300): # Position the scan upright, # so the head of the patient would be at the top facing the camera p = image.transpose(2,1,0) verts, faces = measure.marching_cubes(p, threshold) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection='3d') # Fancy indexing: `verts[faces]` to generate a collection of triangles mesh = Poly3DCollection(verts[faces], alpha=0.70) face_color = [0.45, 0.45, 0.75] mesh.set_facecolor(face_color) ax.add_collection3d(mesh) ax.set_xlim(0, p.shape[0]) ax.set_ylim(0, p.shape[1]) ax.set_zlim(0, p.shape[2]) plt.show()plot_3d(pix_resampled, 400) measure.marching_cubes 这个函数是在三维的图中对二维表面进行分割……或者说就是在找三维凸包？ 400 可以从上面的表里面找到是骨骼的 Hounsfield 值，所以这里打印出来的会是一张肋骨和胸骨的图像。 Lung Segmentation有了数据，后面需要做的是把肺这个部分从整块 CT 图像中分割出来，毕竟后面的处理和操作都是针对肺的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def largest_label_volume(im, bg=-1): vals, counts = np.unique(im, return_counts=True) counts = counts[vals != bg] vals = vals[vals != bg] if len(counts) &gt; 0: return vals[np.argmax(counts)] else: return Nonedef segment_lung_mask(image, fill_lung_structures=True): # not actually binary, but 1 and 2. # 0 is treated as background, which we do not want binary_image = np.array(image &gt; -320, dtype=np.int8)+1 labels = measure.label(binary_image) # Pick the pixel in the very corner to determine which label is air. # Improvement: Pick multiple background labels from around the patient # More resistant to "trays" on which the patient lays cutting the air # around the person in half background_label = labels[0,0,0] #Fill the air around the person binary_image[background_label == labels] = 2 # Method of filling the lung structures (that is superior to something like # morphological closing) if fill_lung_structures: # For every slice we determine the largest solid structure for i, axial_slice in enumerate(binary_image): axial_slice = axial_slice - 1 labeling = measure.label(axial_slice) l_max = largest_label_volume(labeling, bg=0) if l_max is not None: #This slice contains some lung binary_image[i][labeling != l_max] = 1 binary_image -= 1 #Make the image actual binary binary_image = 1-binary_image # Invert it, lungs are now 1 # Remove other air pockets insided body labels = measure.label(binary_image, background=0) l_max = largest_label_volume(labels, bg=0) if l_max is not None: # There are air pockets binary_image[labels != l_max] = 0 return binary_imagesegmented_lungs = segment_lung_mask(pix_resampled, False)segmented_lungs_fill = segment_lung_mask(pix_resampled, True)plot_3d(segmented_lungs, 0) 这里肺分割采用的方法是连通域分析。 先把 -320 阈值以上的所有点都筛掉，剩下在这个值以下的就是肺和空气了。 用 measure.label 这个函数做一次连通域分析，包裹在肺外部的是空气，与 [0][0][0] 相连通的部分全部标记成空。 由于 measure.label 函数默认把 0 作为背景值，因此这里用于连通域分析的二值图像的值实际是 1 和 2。 完成第一次标记之后，将数据恢复成 0 和 1 的二值，再做第二次连通域分析。 保留整个图中最大的连通域就是肺了，剩下的部分都是需要被筛掉的空气。 上面这段代码的输出就是题图的结果。 在输出肺的时候用的是 CPU 进行直接渲染，非常消耗内存，显示出来也很慢。 之后可以对这一块初步分割出来的肺部做进一步的处理，让结果更加精准。 例如 Improved Lung Segmentation using Watershed 等等 Kaggle 上的 Kernels 还提供了更多的参考。 To be continued.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 踩坑]]></title>
    <url>%2F2017%2F06%2F15%2F2017-06-15-nodejs%2F</url>
    <content type="text"><![CDATA[Node.js 现在已经发展成一个很强大的框架了，像 Hexo 这样的博客系统、Visual Studio Code 这样的文本编辑器、甚至Steam客户端（？）等等都是用这个写的。还有一堆，哪哪都是 Node。 很早就想学一下这玩意，结果各种事情一直拖着。 考完试终于有时间可以瞎折腾了。 记一下学 Node 的时候遇到的一些坑点。 Learning 我的学习路线大概是这样，找个 Node.js 写的爬虫的示例照着写，写完就学会了怎么写爬虫，顺便就把 JavaScript 和 TypeScript 的基本用法学会了。（理想情况） 实际情况大概是其中会遇到一些问题，那么再针对性地解决好了。（例如 Node.js 里面的异步逻辑） 然后，被异步这块绕的不行，所以干脆找了个对 Node.js 底层实现的分析。了解整个运行原理之后应该就好了。 参考资料： TypeScript in 5 minutes 用TypeScript开发爬虫程序 深入理解Node.js：核心思想与源码分析 下面记的好多东西都来源于上面的 深入理解Node.js。 Node.js 的整体架构： 底层是 C/C++ 写的，所以 Node.js 的代码跑起来会比 Python 这样的脚本语言要快很多。 异步 Node.js 是一个单线程的事件驱动的异步系统。 Node.js 本身的 API 里面有同步函数也有异步函数，简单地说整个调度过程是这样的： JavaScript 线程启动，宿主环境创建堆和栈。堆用于存储 JavaScript 对象，栈用于存储执行上下文，当然每个异步函数都有一个对应的执行上下文，可以理解成一个函数要执行就要进栈。 栈内执行任务的顺序是同步的，跟 C/C++ 等等其他非异步的完全一样，自顶向下按顺序来，执行完退栈。当异步任务要执行时，异步任务不入栈，而是把相关消息通知线程（大概是把自己的信息注册到线程上去），然后进入等待状态。 当（前面注册过的异步任务对应的）事件被触发或者异步响应返回时，线程向消息队列插入一条事件消息。 栈内的同步任务先全部执行完，然后线程从消息队列取出一个事件消息，事件消息对应的异步任务入栈，如果消息上面绑了回调函数那就执行它。 当执行栈空了，就再从消息队列取出下一个事件消息，然后继续执行。这个就是事件循环。 关于单线程，从 深入理解Node.js 里面对事件部分的源码分析也可以很清楚地看到，事件循环的核心部分代码就是个简单的 do-while 循环。 异步则是因为 Node.js 底层用了 libuv，下面的事件消息通知是异步驱动的。（所以相当于整个调度是单线程的，而每个异步IO实际上是多线程进行的？）Linux 下用了 epoll。 举个栗子：（发现各种教程里面举异步的例子都喜欢用 setTimeout …） 12345678910console.log("start")for (var i=0;i&lt;=5;i++)&#123; setTimeout(function() &#123; console.log(i); &#125;, 0);&#125;console.log("end"); 这段代码除了 setTimeout 这个函数，其他的 API 包括这个 for 循环都是同步的，所以同步的部分先执行了。 6 个 setTimeout 任务进入等待状态。 在执行同步任务的过程中，setTimeout 的计数器到时间了（因为本身就设的是 0），消息队列里面会插进去 6 条事件，分别是触发对应的那条 setTimeout。 然后同步部分执行完毕，start 和 end 都输出了。开始从消息队列中取消息，每取一条 setTimeout，就执行它的回调函数 console.log(i)。 ！！然后要注意这里的 i 是个全局变量，i 在 for 循环执行完毕的时候变成了 6，所以后面每一次输出的 i 都是 6。 实际执行的结果是： 12345678startend666666 这里如果把 for 循环中的 var 换成 let，最后得到会是这样的结果： 1234567startend01235 再改一下： 12345678910111213141516171819function test() &#123; console.log("123");&#125;console.log("start")test();for (let i=0;i&lt;=5;i++)&#123; setTimeout(function() &#123; setTimeout(function() &#123; console.log(i); &#125;, 0); console.log(i); &#125;, 0);&#125;console.log("end"); 输出结果是： 123456789101112131415start123end012345012345 从上面可以看出来的是： 普通的函数是同步的！…… 之前一直以为这里面的函数全是异步的，看来只要不涉及到异步的 API 就都是同步的 因为消息队列是个队列，所以 i=0 那条触发的 setTimeout 中增加的 setTimeout 事件只会被插到队列的再后面去。 爬虫 爬虫的基本思路就是，把页面的 html 拿下来，然后用解析的工具从里面匹配出需要的信息来。 最简单的两个包是 superagent 和 cheerio，superagent 用于获取网页的 html 源码，cheerio 用于解析 html 信息。 这段代码是简单地抓了一下博客首页： 12345678910111213141516171819202122232425262728293031let cheerio = require('cheerio');let superagent = require('superagent');export const doit = async function () &#123; superagent.get('http://jcf94.com') .end(function (err, sres) &#123; if (err) &#123; return err; &#125; let $ = cheerio.load(sres.text); let items = []; $('#posts .post-title-link').each(function (idx, element) &#123; let $element = $(element); //console.log($element); items.push(&#123; title: $element.text(), href: $element.attr('href') &#125;); &#125;); console.log(items); let pics = []; $('#posts img').each(function (idx, element) &#123; let $element = $(element); pics.push(&#123; src: $element.attr('src') &#125;); &#125;); console.log(pics); &#125;);&#125; 然后是 request 包，比 superagent 更常用一些，用于请求网页等等各种资源。 简单的爬虫学会了，然后我就想试试能不能把网易云上歌曲的评论数抓下来，，这里遇到个坑点。 网易云音乐的页面用了 iframe 框架来动态加载内容，打开页面之后直接获取 html 的源码会发现抓下来的只有框架，没有内容。 这就抓瞎了。。。 然后就上了 selenium-webdriver 这个包。 selenium 原本是用于页面的自动化测试的，具体使用起来就是 Node.js 代码会控制一个浏览器完成点击、输入等等操作（按键精灵有木有？）。 简单的抓页面的代码是这样，这里关键在于这个 driver.switchTo().frame(&#39;contentFrame&#39;)，用于切换到 iframe 加载出来之后的页面。 切过去之后才能正常抓到页面内容。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859let webdriver = require('selenium-webdriver'), By = webdriver.By, until = webdriver.until;let driver = new webdriver.Builder() .forBrowser('chrome') .build();let fs = require('fs');const getComments = async (url: string) =&gt; &#123; const promise = new Promise&lt;number&gt;((resolve, reject) =&gt; &#123; driver.get(url); driver.switchTo().frame('contentFrame'); driver.findElement(By.xpath('//*/div[1]/span/span')).getAttribute('innerHTML').then(result =&gt; &#123; if (result) resolve(result); else reject(0); &#125;); &#125;); return promise;&#125;export const doit = async () =&gt; &#123; driver.get('http://music.163.com/#/discover/toplist?id=3778678'); driver.switchTo().frame('contentFrame'); //driver.getPageSource().then(res=&gt;console.log(res)); let urls = await driver.findElements(By.xpath('//*/td[2]/div/div/div/span/a')); let titles = await driver.findElements(By.xpath('//*/td[2]/div/div/div/span/a/b')); let total = urls.length; console.log("Total " + total + " Songs find."); let list = []; for (let i=0; i&lt;total; i++) &#123; let ur:string = await urls[i].getAttribute('href'); let ti:string = await titles[i].getAttribute('title'); list.push(&#123; no: i, title: ti, url: ur, comments: 0 &#125;) &#125; for (let i=0;i&lt;total;i++) &#123; let co:number = await getComments(list[i].url); console.log("Get song " + i + " ."); list[i].comments = co; &#125; driver.quit(); //------------------ fs.writeFileSync('netease.json', JSON.stringify(list)); console.log("Finish");&#125;]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Node.js</tag>
        <tag>JavaScript</tag>
        <tag>Typescript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存池]]></title>
    <url>%2F2017%2F04%2F19%2F2017-04-19-memorypool%2F</url>
    <content type="text"><![CDATA[话说最近很长一段时间都是在看别人的代码和改别人的代码比较多，自己从头开始写东西好像已经是很久前才有过的事情了。改 TensorFlow 的过程中也学到不少东西，准备平时想到啥了就随便写着玩。 内存池简单地讲就是我自己首先用 malloc 向系统申请一大块内存，然后接下来再用到内存的时候手动从这大块内存中划，不用了还回去。这个之后申请、释放内存都是手动实现的，自己写个内存池的 malloc 和 free 来完成就好了。 具体用内存池的好处，网上好多人都有说，其实一般情况下自己手写的东西都还是比不上人家造好的轮子的，但在某些特殊的应用场景下手写会有很多好处。例如 Nginx 就有个自己的内存池，用来应对频繁的小块内存申请等等。 我之前遇到的场景主要是 RDMA 在发送数据前需要往 IB 卡上绑定内存，这个过程相当耗时，所以有个想法是开头申请一大块内存出来，之后收发数据时用的 buffer 都从内存池里面划，就能省去很多的时间了。 如何设计内存池？ 目前做的就是上面那个回答里面的第一条~ 代码在这里 To be continued.]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>内存池</tag>
        <tag>Memory Pool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AlexNet]]></title>
    <url>%2F2017%2F04%2F04%2F2017-04-04-alexnet%2F</url>
    <content type="text"><![CDATA[半路上车，Deep Learning 这块还在断断续续地慢慢学，但是由于现在测 TensorFlow 性能的需要，所以来学一下 AlexNet（额，听说 CNN 的计算量不少）。 AlexNet 是 2012 年 ImageNet 竞赛冠军获得者 Alex Krizhevsky 设计的，所以叫这个名字，这个也一直是入门 CNN 的经典模型。 参考的材料有这里，以及从这里盗了张图： 组成部分分析卷积层（Convolutional）CNN 既然叫卷积神经网络，最重要的就是卷积了。 卷积这个操作是用于提取图像的某些特征，图像上的二维卷积其实与以前在信号系统中学过的一维卷积非常类似，公式大概是这个样子： $$f[x,y]*g[x,y]=\sum_{n_1=-\infty}^\infty \sum_{n_2=-\infty}^\infty f[n_1, n_2] \cdot g[x-n_1, y-n_2]$$ 用卷积核 g 对图像 f 作卷积就是将 g 沿着反对角线翻转，然后跟 f 上的对应位相乘，对相乘之后的所有位求和，所得的值作为卷积结果图的一个像素。 CNN 中由于卷积核全是随机生成的，反对角线翻转这一步就不需要了。 对应的几个术语： kernel_size 表示卷积核的大小；stride 表示一次卷积之后，卷积核平移几个像素；pad 是对原图的边缘进行像素扩充。 池化/降采样层（Pooling）选定一个区域进行降采样就是留下一个值，主要用于减少数据量。 降采样的方法通常有取区域中的最大值留下，或者对区域中的像素求均值留下（滑动平均？）等等。 激活函数层额，就是应用一个激活函数… 常用的有 sigmoid、tanh、ReLU 等等函数，要看需要把值控制在什么区间范围内，然后反向求导的时候是不是好算……（话说选什么激活函数完全是经验+玄学吧？数学原理在哪里？） 标准化/归一化层（Normalization）有的实现里面能看到这个，有的没有，一般就是用个公式对数据进行归一化一遍。 全连接层（Fully Connected）接上一个深度神经网络，没啥好说的。 遗忘层（Dropout）以一定概率随机（纯随机哦…）扔掉一些数据，不加很容易造成过拟合。 据说是为了模拟人脑的过程（？？？） 反正在我这里看来怎么看怎么玄学，是不是换个随机数生成器都可能有不同的效果？ Alexnet 结构详解第一阶段输入的是一些 227*227 大小的图像，RGB 三个通道，这是初始数据。 第一个卷积层用的是 96 个大小为 11*11 的卷积核，stride=4 表示每次卷积完了之后移动 4 格，所以一张 227*227 的图经过一次卷积之后就会变成 $\frac{227-11}{4}+1=55$ 边长的图，3 个通道做完之后数值相加。96 个卷积核所以得到了 55*55*96 这样的数据。 中间过一遍激活函数，ReLU。 第一个降采样层用了 3*3 的采样核，stride 为 2，则每张图通过之后得到了 $\frac{55-3}{2}+1=27$ 边长的数据，一共为 27*27*96 个像素点。 第二阶段输入从 27*27*96 开始。 第二个卷积层用的是 256 个大小为 5*5 的卷积核，stride 这里是 1，pad=2 是扩充边缘，即对前面 27*27 的图像的上下左右四个方向各扩充 2 个像素变成 31*31 的图像，这是为了防止卷积操作使得图像大小缩小得过快。所以由于 pad 参数的存在，经过卷积之后图像的大小是 $\frac{27+2*2-5}{1}+1=27$ 没有发生改变。group=2 指定的是把前面 96 个通道分成 2 个子集，数据的累加在各自的子集内完成。输出是 27*27*256。 过一遍激活函数。 然后第二个降采样层与第一次一样，得到了 $\frac{27-3}{2}+1=13$ 边长的数据，一共 13*13*256。 第三阶段输入从 13*13*256 开始。 第三个卷积层用了 384 个大小为 3*3 的卷积核，pad=1，则得到了 $\frac{13+2*1-3}{1}+1=13$ 边长的图，一共 13*13*384。 过一遍激活函数。 第四阶段输入从 13*13*384 开始。 第四个卷积层用了 384 个大小为 3*3 的卷积核，pad=1，则得到了$\frac{13+2*1-3}{1}+1=13$ 边长的图，一共 13*13*384。 过一遍激活函数。 第五阶段输入从 13*13*384 开始。 第五个卷积层用了 256 个大小为 3*3 的卷积核，pad=1，则得到了$\frac{13+2*1-3}{1}+1=13$ 边长的图，一共 13*13*256。 过一遍激活函数。 第五个降采样层用了 3*3 的采样核，stride=2，则得到了 $\frac{13-3}{2}+1=6$ 边长的图，一共 6*6*256。 第六阶段输入从 6*6*256 开始。 连接了 4096 个隐层节点。 开始时这一部分我不是很明白，以为 6*6*256 的数据怎么就变成 4096 个了，后来看了代码的实现才知道是把前面的 $6*6*256=9216$ 个像素作为输入层的 9216 个节点，跟接下来的 4096 个隐层节点全连接起来。所以这里的权值 W 是个 9216*4096 的矩阵，偏置 B 是长度为 4096 的向量。 过一遍激活函数。 再接一个 Dropout，随机把这 4096 个数据中的某些丢掉置为 0。 第七阶段输入从 4096*1 开始。 继续连接 4096 个隐层节点。 权值 W 是个 4096*4096 的矩阵，偏置 B 是长度为 4096 的向量。 过一遍激活函数。 继续 Dropout。 第八阶段输入从 4096*1 开始。 连接 1000 个输出层节点。 权值 W 是个 4096*1000 的矩阵，偏置 B 是长度为 1000 的向量。 输出的 1000 个数据对应的就是分在 1000 个类别的概率了。 To be continued.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习初探]]></title>
    <url>%2F2017%2F03%2F22%2F2017-03-22-deeplearning%2F</url>
    <content type="text"><![CDATA[Talk is cheap, show me the code. 开始接触深度学习。 虽然 TensorFlow 是很早就开始折腾了，不过做的基本是研究它的底层实现方面的工作，真正上层应用方面的原理了解的很少。但是之后的工作需要用到这方面的知识了，因此学习一下。 看了不少教程，觉得最好的方法还是跟着个 C 的裸程序走一遍会印象深刻些~ 深度学习如何入门？ CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？ Basic Knowledge深度学习只是机器学习这个大类中的一种，这种算法其实很早就有了，只是受限于以前机器的计算能力跟不上，因而由于近几年计算力的飞速提升而重新受到关注。 神经网络的算法最早起源于感知机，深度神经网络即隐层层次比较多的神经网络，其实本质就是多层感知机。 然后整个神经网络的计算其实就有点像是在用大量数据加上大计算量去暴力拟合一个多维函数的过程（在多维函数空间中搜索最小值？）。 基本过程是数据进来，沿着各层神经网络算一遍，走到输出层输出结果，这个结果跟实际值肯定存在误差，然后把误差反向沿着神经网络传播一遍，逐层用梯度下降法修正神经网络中的各个参数，这样就是一次样本训练了。 网络越深，参数成倍增长，计算量也是成倍增长。 用大量样本训练出参数之后，就可以用这些练好的参数来进行预测了。 Show me the Code下面是一个 C++ 写的 1 层隐层的全连接网络，数据来源于这里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207/* ***********************************************MYID : Chen FanLANG : G++ --std=c++11PROG : BP Neural Network************************************************ */#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cstdlib&gt;#include &lt;cmath&gt;#include &lt;iostream&gt;#include &lt;functional&gt;#include &lt;random&gt;#define INP 4#define HID 10#define OUT 3#define LR 0.1// Learning Rate/*Iris DataBase4 input neuronsHID hidden neurons3 output neurons*/using namespace std;default_random_engine engine;uniform_real_distribution&lt;float&gt; distr(0, 1);function&lt;float()&gt; rnd = bind(distr, engine);float train_data[3][25][4];float test_data[3][25][4];class nodeclass&#123; public: int num_input; vector&lt;float&gt; weight_input; float value; function&lt;float(float)&gt; f; nodeclass(int n, function&lt;float(float)&gt; ff): num_input(n), f(ff) &#123; init_weight(); &#125; ~nodeclass()&#123;&#125; void init_weight() &#123; for (int i=0;i&lt;num_input;i++) weight_input.push_back(rnd()); &#125; void calcf(float x) &#123; value = f(x); &#125;&#125;;vector&lt;nodeclass*&gt; inpnode, hidnode, outnode;float sigmoid(float x)&#123; if (x &lt; -45.0) //too big and too small return 0.0; else if (x &gt; 45.0) return 1.0; else return 1.0 / (1.0 + exp(-x));&#125;void init_nodes()&#123; for (int i=0;i&lt;INP;i++) inpnode.push_back(new nodeclass(0, NULL)); for (int i=0;i&lt;HID;i++) hidnode.push_back(new nodeclass(INP, sigmoid)); for (int i=0;i&lt;OUT;i++) outnode.push_back(new nodeclass(HID, sigmoid));&#125;void init_dataset()&#123; FILE * fp = fopen("iris.data", "r"); for (int j=0; j&lt;3; j++) &#123; char temp[100]; for (int i=0; i&lt;25; i++) fscanf(fp, "%f,%f,%f,%f,%s", &amp;train_data[j][i][0], &amp;train_data[j][i][1], &amp;train_data[j][i][2], &amp;train_data[j][i][3], temp); for (int i=0; i&lt;25; i++) fscanf(fp, "%f,%f,%f,%f,%s", &amp;test_data[j][i][0], &amp;test_data[j][i][1], &amp;test_data[j][i][2], &amp;test_data[j][i][3], temp); &#125; fclose(fp);&#125;void inference(int type, int clas, int index)&#123; // prepare the input data if (type) //type 0 means train data, 1 means test data &#123; int i = 0; for (auto now : inpnode) now-&gt;value = test_data[clas][index][i++]; &#125; else &#123; int i = 0; for (auto now : inpnode) now-&gt;value = train_data[clas][index][i++]; &#125; // calculate the hidden layer for (auto now : hidnode) &#123; float temp = 0.0; int i = 0; for (auto front : inpnode) temp += front-&gt;value * now-&gt;weight_input[i++]; now-&gt;calcf(temp); &#125; // calculate the output layer for (auto now : outnode) &#123; float temp = 0.0; int i = 0; for (auto front : hidnode) temp += front-&gt;value * now-&gt;weight_input[i++]; now-&gt;calcf(temp); &#125;&#125;void train()&#123; for (int j=0;j&lt;3;j++) &#123; float expected[] = &#123;0.0, 0.0, 0.0&#125;; expected[j] = 1.0; for (int index = 0; index &lt; 25; ++index) &#123; inference(0, j, index); //train output -&gt; hidden int i = 0; for (auto now : outnode) &#123; float delta = (expected[i++] - now-&gt;value) * (1 - now-&gt;value) * now-&gt;value; int k = 0; for (auto front : hidnode) now-&gt;weight_input[k++] += LR * delta * front-&gt;value; &#125; //train hidden -&gt; input i = 0; for (auto now : hidnode) &#123; float delta = 0.0; int k = 0; for (auto back : outnode) delta += (expected[k++] - back-&gt;value) * (1 - back-&gt;value) * back-&gt;value * back-&gt;weight_input[i] * (1 - now-&gt;value) * now-&gt;value; k = 0; for (auto front : inpnode) now-&gt;weight_input[k++] += LR * delta * front-&gt;value; i++; &#125; &#125; &#125;&#125;int select(vector&lt;nodeclass*&gt; arr)&#123; int max = (arr[1]-&gt;value &gt; arr[0]-&gt;value) ? 1 : 0; if (arr[2]-&gt;value &gt; arr[max]-&gt;value) max = 2; return max;&#125;int test()&#123; int count = 0; for (int clas = 0; clas &lt; 3; ++clas) &#123; for (int index = 0; index &lt; 25; ++index) &#123; inference(0, clas, index); if (select(outnode) == clas) &#123; ++count; &#125; &#125; &#125; printf("Accuracy rate: %lf\n", (double)count / 75); return count;&#125;int main()&#123; //init the nodes &amp; the weights of NN init_nodes(); //init the train data and test data init_dataset(); //start training while (test() &lt; 75) &#123; train(); &#125; //test test(); return 0;&#125; Notes上面这个神经网络非常简单，其他还有诸如 CNN（卷积神经网络）、RNN（循环神经网络）、FCN（全卷积网络）等等等等各种算法。 作为一个数学本来就不太好的人，其中深层的数学原理我也不想再深究了（毕竟也不一定能懂），其实光是反向推回来的求导这里就有点爆炸了，高数忘了很多，更不用说之后还可能有多维矩阵的求导。 幸而这部分到时候都有 TensorFlow 这样的框架直接搞定了，更多的可能还是要动更底层的内容。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11 及之上的一些新东西]]></title>
    <url>%2F2017%2F02%2F17%2F2017-02-17-cppnew%2F</url>
    <content type="text"><![CDATA[读代码的时候遇到了一些新东西，以前从没见过的语句和使用方式，惊觉 C++ 标准都扩展到 C++17 了，然而我以前写 ACM 的时候甚至连 STL 都很少用。 后来倒是陆续用过 queue 和 bitset 这样的黑科技。 这里陆续补充一点新东西的学习记录吧。 std::function std::function - cppreference std::function 类模板是一种通用的函数封装，这个模版的实例可以对任何可以调用的目标进行存储、复制、和调用操作，包括函数、lambda 表达式、绑定表达式、以及其它函数对象等。 直接看代码示例吧，这一段来自上面的 cppreference。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/* ***********************************************MYID : Chen FanLANG : G++ -std=c++11PROG : function test************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;functional&gt;void print_num(int n)&#123; std::cout &lt;&lt; n &lt;&lt; std::endl;&#125;struct foo&#123; foo(int n) : num(n) &#123;&#125; void print_add(int n) const &#123;std::cout &lt;&lt; num + n &lt;&lt; std::endl;&#125; int num;&#125;;struct Print_num&#123; void operator()(int n) const &#123; std::cout &lt;&lt; n &lt;&lt; std::endl; &#125;&#125;;int main()&#123; std::function&lt;void(int)&gt; f_display = print_num; f_display(100); std::function&lt;void(int)&gt; f_display_lambda = [](int n) &#123; print_num(n);&#125;; f_display_lambda(200); std::function&lt;void()&gt; f_display_bind = std::bind(print_num, 300); f_display_bind(); std::function&lt;void(const foo&amp;, int)&gt; f_add_display = &amp;foo::print_add; foo new_foo(400); f_add_display(new_foo, 12); std::function&lt;void(int)&gt; f_add_display2 = std::bind(&amp;foo::print_add, new_foo, std::placeholders::_1); f_add_display2(123); std::function&lt;void(int)&gt; f_add_display3 = std::bind(&amp;foo::print_add, &amp;new_foo, std::placeholders::_1); f_add_display3(234); std::function&lt;void(int)&gt; f_display_obj = Print_num(); f_display_obj(700); return 0;&#125; Lambda 表达式 Lambda expressions - cppreference 遇见 C++ Lambda - cnblogs 这个东西的官方定义是能构造一个闭包：是能够捕获作用域中变量的无名函数。 简单地说可以看成是个匿名函数，写法就是函数的结构，只是没有名字。 详细的语法： 1234[ capture-list ] ( params ) mutable(可选) constexpr(可选)(C++17) exception attribute -&gt; ret &#123; body &#125;[ capture-list ] ( params ) -&gt; ret &#123; body &#125;[ capture-list ] ( params ) &#123; body &#125;[ capture-list ] &#123; body &#125; 基本上是三个部分组成，[] 中是捕获列表，() 中是函数的输入参数（跟普通的函数是一个意思，空的就是没有输入参数），-&gt; 后面可以跟函数返回值的类型，不加的话可以自动推断，{} 中是函数体（跟正常函数一样）。 这个结构其实跟普通的函数基本是一致的。 具体的看示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394/* ***********************************************MYID : Chen FanLANG : G++ -std=c++11PROG : Lambda************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;ctime&gt;int main()&#123; srand(time(NULL)); std::vector&lt;int&gt; a(10); /* 这里是一个 std::function 跟 Lambda 表达式的例子 */ std::function&lt;void()&gt; aout = // Lambda 函数体中用到了 a 这个 vector，需要“从外界捕获到闭包内”，大致可以这么理解。 // a 前面加 &amp; 是引用捕获，闭包内的 a 和闭包外的 a 在内存里面是一样的。 // 如果不加就是值捕获，也就是把外面的 a 在当前位置的值传进去。 // 注意当前位置这个概念很重要，用下面那个不带 &amp; 的来测试会发现后面输出的全都是 0。 // 把这个 Lambda 表达式的定义换个位置试试就明白了。 //[a]() [&amp;a]() &#123; for (auto i : a) std::cout &lt;&lt; i &lt;&lt; std::endl; &#125;; //----------- std::generate(a.begin(), a.end(), // 一个不需要捕获也没有输入参数的函数。 []() &#123; return rand() % 100; &#125;); // std::function 的调用例子，函数模版加上 Lambda 表达式之后其实可以看成是个普通的函数。 aout(); int sum = 0; std::for_each(a.begin(), a.end(), // 需要引用捕获 sum，并且要把 vector 中每个元素作为参数传进去。 // 如果不加 &amp; 但是又在函数体中改了捕获变量的内容会发生 read-only 错误。 [&amp;sum](int i) &#123; sum += i; &#125;); std::cout &lt;&lt; &quot;Sum: &quot; &lt;&lt; sum &lt;&lt; std::endl; //----------- int step = 2; int i = 0; std::generate(a.begin(), a.end(), // 后两个都是对的，捕获列表里面可以加多个，不同变量可以用不同的捕获方式。 // 直接写 &amp; 则 Lambda 表达式会自动推断把函数体中出现的变量全用引用捕获抓进来。 //[i, step]() //[&amp;i, step]() [&amp;, step]() &#123; i += step; return i; &#125;); aout(); std::cout &lt;&lt; &quot;i: &quot; &lt;&lt; i &lt;&lt; std::endl; step = 3; i = 0; std::generate(a.begin(), a.end(), // 直接写 = 则 Lambda 表达式会自动推断把函数体中的变量用值捕获抓进来。 // mutable 的意思是让捕获过来的值引用变量在 Lambda 表达式的范围内可变。 // 但是在全局范围内无影响，因为毕竟还是值引用，相当于在 Lambda 的作用范围内加了个成员变量。 // 对比下面输出来的 i 的值就明白了。 //[i, step]() mutable [=]() mutable &#123; i += step; return i; &#125;); aout(); std::cout &lt;&lt; &quot;i: &quot; &lt;&lt; i &lt;&lt; std::endl; //----------- step = 4; i = 0; std::function&lt;int()&gt; addstep = [&amp;]() &#123; i += step; return i; &#125;; std::generate(a.begin(), a.end(), addstep); aout(); return 0;&#125; 我刚开始遇到 Lambda 表达式是在 TensorFlow 的代码里面，弄明白上面这一串之后，回到 TensorFlow 的代码里，抽出来了这样一段看上去有点诡异的代码： 12345678910111213141516171819202122232425262728293031323334/* ***********************************************MYID : Chen FanLANG : G++ -std=c++11PROG : special Lambda************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;// Closure 是一个函数模版，表示一个返回值为空的函数typedef std::function&lt;void()&gt; Closure;// Runner 也是一个函数模版，返回值为空，输入参数是上面那个 Closuretypedef std::function&lt;void(Closure)&gt; Runner;Closure closure_;Runner runner_;int main()&#123; // closure_ 可以直接赋值成一个返回值和输入参数都为空的 Lambda 表达式 closure_ = []()&#123; printf(&quot;aaa\n&quot;);&#125;; // runner_ 赋值的是一个输入参数为 Closure 的 Lambda 表达式 runner_ = [](Closure f)&#123; f();&#125;; // runner_ 本身是一个函数，可以直接调用，输入参数是一个 Closure // 这里的 Closure 可以直接是一个符合 Closure 格式的 Lambda 表达式，因为 Closure 本身就是个函数模版 runner_([]()&#123; printf(&quot;.....\n&quot;);&#125;); // 也可以像这样输个 Closure 类型的参数进去 runner_(closure_); return 0;&#125; std::bindbind 用于把一些参数跟一个调用进行绑定起来，包括函数对象，函数指针，引用函数，成员函数的指针，指针到成员数据等等。 主要是看代码的时候看到了个占位符，不太明白，所以记一下。 还是先上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/* ***********************************************MYID : Chen FanLANG : G++PROG : Bind_test************************************************ */#include &lt;random&gt;#include &lt;iostream&gt;#include &lt;functional&gt;void f(int n1, int n2, int n3, const int&amp; n4, int n5)&#123; std::cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; ' ' &lt;&lt; n4 &lt;&lt; ' ' &lt;&lt; n5 &lt;&lt; '\n';&#125;int g(int n1)&#123; return n1;&#125;struct Foo &#123; void print_sum(int n1, int n2) &#123; std::cout &lt;&lt; n1+n2 &lt;&lt; '\n'; &#125; int data = 10;&#125;;int main()&#123; using namespace std::placeholders; // demonstrates argument reordering and pass-by-reference int n = 7; auto f1 = std::bind(f, _2, _1, 42, std::cref(n), n); n = 10; f1(1, 2, 1001); // 1 is bound by _1, 2 is bound by _2, 1001 is unused f(2, 1, 42, std::cref(n), n); std::cout &lt;&lt; "---" &lt;&lt; std::endl; // nested bind subexpressions share the placeholders auto f2 = std::bind(f, _3, std::bind(g, _3), _3, 4, 5); f2(10, 11, 12); f(12, g(12), 12, 4, 5); std::cout &lt;&lt; "---" &lt;&lt; std::endl; // common use case: binding a RNG with a distribution std::default_random_engine e; std::uniform_int_distribution&lt;&gt; d(0, 10); std::function&lt;int()&gt; rnd = std::bind(d, e); for(int n=0; n&lt;10; ++n) std::cout &lt;&lt; rnd() &lt;&lt; ' '; std::cout &lt;&lt; std::endl &lt;&lt; "---" &lt;&lt; std::endl; // bind to a member function Foo foo; auto f3 = std::bind(&amp;Foo::print_sum, foo, 95, _1); f3(5); foo.print_sum(95, 5); std::cout &lt;&lt; "---" &lt;&lt; std::endl; // bind to member data auto f4 = std::bind(&amp;Foo::data, _1); std::cout &lt;&lt; f4(foo) &lt;&lt; '\n'; std::cout &lt;&lt; foo.data &lt;&lt; '\n';&#125; 第一段的 f1 是 f 这个函数加上一些参数的封装。_1 和 _2 都是占位符，分别表示等一下输入的第一个和第二个参数。所以后面的例子中 f1(1, 2, ...) 就等于 f(2, 1, ...)，bind 封装里面没有用到第三个占位符，所以 f1 中输入的 1001 是没有用的。 第二段是说在 bind 中再加个 bind，占位符是共享最外层的那个。f2 中输入的 10 和 11 无效，因为没有用到 _1 和 _2。 To be continued.]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>C++11</tag>
        <tag>vector</tag>
        <tag>function</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-SWIG 初探以及其 gdb 调试]]></title>
    <url>%2F2017%2F02%2F06%2F2017-02-06-swigdebug%2F</url>
    <content type="text"><![CDATA[SWIG is used with different types of target languages including common scripting languages such as Javascript, Perl, PHP, Python, Tcl and Ruby. The list of supported languages also includes non-scripting languages such as C#, Common Lisp (CLISP, Allegro CL, CFFI, UFFI), D, Go language, Java including Android, Lua, Modula-3, OCAML, Octave, Scilab and R. 上面那段是 SWIG 的官网介绍，简单地说，SWIG 是一个用于 C/C++ 和高层脚本语言交互的工具。脚本语言在 SWIG 的辅助下可以直接调 C/C++ 的程序。 简单试了一下 SWIG 在 Python 里面的用法，然后测试了一下 gdb 以及 VS 的远程调试（红红火火恍恍惚惚，其实想记录的重点是这个）。 以下内容均在 WSL 里面完成。 Python-SWIG直接 apt-get 把 SWIG 装上，然后就可以开始瞎搞了。 1. 首先随便写个 C 的函数官方的文档里面用的是个简单的阶乘的例子，这里也用这个好了。 cfact.h 1int fact(int n); cfact.c 12345678#include "cfact.h"int fact(int n)&#123; if (n &lt; 0) return 0; else if (n == 0) return 1; else return n * fact(n-1);&#125; 2. 写 SWIG 部分的内容，并且生成对应的封装代码创建一个 SWIG 的文件： cfact.i 12345678%module cfact%&#123;#define SWIG_FILE_WITH_INIT#include &quot;cfact.h&quot;%&#125;int fact(int n); 然后用 swig 命令生成封装代码： 1$ swig -python cfact.i 调用完了之后会生成两个新的文件 cfact.py 以及 cfact_wrap.c。 3. 编译生成 Python 模块Python 与 C/C++ 的交互是以模块的方式进行的，C/C++ 代码编译生成运行库之后，以模块的方式链接到 Python 的运行时中。 Python 自带一个 distutils 工具用于创建扩展模块，使用这个工具也很简单，只要写一个配置文件即可： setup.py 12345678910111213from distutils.core import setup, Extensioncfact_module = Extension('_cfact', sources=['cfact_wrap.c', 'cfact.c'], )setup (name = 'cfact', version = '0.1', author = "Chen Fan", description = """Simple swig test""", ext_modules = [cfact_module], py_modules = ["cfact"], ) 在目录下执行： 1$ python setup.py build 之后即可编译得到完整的 Python 模块。 123456789101112$ python setup.py buildrunning buildrunning build_pycreating buildcreating build/lib.linux-x86_64-2.7copying cfact.py -&gt; build/lib.linux-x86_64-2.7running build_extbuilding '_cfact' extensioncreating build/temp.linux-x86_64-2.7x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c cfact_wrap.c -o build/temp.linux-x86_64-2.7/cfact_wrap.ox86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c cfact.c -o build/temp.linux-x86_64-2.7/cfact.ox86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/cfact_wrap.o build/temp.linux-x86_64-2.7/cfact.o -o build/lib.linux-x86_64-2.7/_cfact.so 可以看到 cfact_wrap.c 和 cfact.c 首先被编译成 build/temp.linux-x86_64-2.7/ 目录下的 cfact_wrap.o 和 cfact.o 两个文件。 之后再编译成一个完整的 _cfact.so 运行库。 切到 build/lib.linux-x86_64-2.7/ 目录下就能用 Python 测试啦： 123456789101112# jcf @ J-CF-YOGA in ~/swig_test [21:36:17]$ cd build/lib.linux-x86_64-2.7# jcf @ J-CF-YOGA in ~/swig_test/build/lib.linux-x86_64-2.7 [21:41:45]$ pythonPython 2.7.6 (default, Oct 26 2016, 20:30:19)[GCC 4.8.4] on linux2Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import cfact&gt;&gt;&gt; cfact.fact(5)120&gt;&gt;&gt; 上 gdb 调试Python 进程是已经在运行了的，要用 gdb 调就只能附加上进程号来启动，可以在 Python 里面 import os 然后用 os.getpid() 获取进程号，或者用 top/htop 啥的直接看一下 Python 的进程号，然后在 gdb 里面启动即可。 1$ gdb -p pid gdb 正常启动，然后这时候可能会报找不到库的调试信息的错误，直接添加 fact 的断点会加不上。用 info sharedlibrary 命令看一下现在追踪到的库信息： 1234567891011121314(gdb) info sharedlibraryFrom To Syms Read Shared Object Library0x00007fa4d2be59f0 0x00007fa4d2bf2471 Yes /lib/x86_64-linux-gnu/libpthread.so.00x00007fa4d282f520 0x00007fa4d2974183 Yes /lib/x86_64-linux-gnu/libc.so.60x00007fa4d2600ed0 0x00007fa4d26019ce Yes /lib/x86_64-linux-gnu/libdl.so.20x00007fa4d23f0f10 0x00007fa4d23f1804 Yes /lib/x86_64-linux-gnu/libutil.so.10x00007fa4d21d1e00 0x00007fa4d21e1bf8 Yes (*) /lib/x86_64-linux-gnu/libz.so.10x00007fa4d1ec5610 0x00007fa4d1f34056 Yes /lib/x86_64-linux-gnu/libm.so.60x00007fa4d2e00ae0 0x00007fa4d2e1b490 Yes /lib64/ld-linux-x86-64.so.20x00007fa4d1b32720 0x00007fa4d1b33ef6 Yes (*) /usr/lib/python2.7/lib-dynload/readline.x86_64-linux-gnu.so0x00007fa4d18f30d0 0x00007fa4d1913ee5 Yes (*) /lib/x86_64-linux-gnu/libreadline.so.60x00007fa4d16bc3d0 0x00007fa4d16c8028 Yes (*) /lib/x86_64-linux-gnu/libtinfo.so.5 No ./_cfact.so(*): Shared library is missing debugging information. _cfact.so 由于不在 gdb 的默认库搜索路径下，所以找不到，需要手动加上路径。 关于 gdb 的库搜索策略，参见这篇博文。 这里简单地在 solib-search-path 里面加上 _cfact.so 的位置就好啦： 123456(gdb) show solib-search-pathThe search path for loading non-absolute shared library symbol files is .(gdb) set solib-search-path :/home/jcf/swig_test/build/lib.linux-x86_64-2.7/Reading symbols from /home/jcf/swig_test/build/lib.linux-x86_64-2.7/_cfact.so...done.Loaded symbols for /home/jcf/swig_test/build/lib.linux-x86_64-2.7/_cfact.so(gdb) 可以看到 set 完之后 _cfact.so 马上就被自动被载入完成了。 这时候在 fact 这个函数上设断点就能够正常找到了： 1234(gdb) break factBreakpoint 1 at 0x7fa4d14a3020: file cfact.c, line 5.(gdb) cContinuing. 在 Python 窗口里面再调一次 cfact.fact 函数，gdb 成功断在了这里： 1234567891011Breakpoint 1, fact (n=n@entry=6) at cfact.c:55 if (n &lt; 0) return 0;(gdb) l1 #include "cfact.h"23 int fact(int n)4 &#123;5 if (n &lt; 0) return 0;6 else if (n == 0) return 1;7 else return n * fact(n-1);8 &#125;(gdb) 调用栈也是比较清晰的，Python 通过生成的封装文件 cfact_wrap.c 最后才调用到最初写的 cfact.c 文件中去。 1234567891011121314(gdb) bt#0 fact (n=n@entry=6) at cfact.c:5#1 0x00007fa4d14a1c45 in _wrap_fact (self=&lt;optimized out&gt;, args=&lt;optimized out&gt;) at cfact_wrap.c:3119#2 0x0000000000523f6d in PyEval_EvalFrameEx ()#3 0x0000000000567d14 in ?? ()#4 0x0000000000465a2d in PyRun_InteractiveOneFlags ()#5 0x0000000000465b49 in PyRun_InteractiveLoopFlags ()#6 0x00000000004661fe in PyRun_AnyFileExFlags ()#7 0x0000000000466d92 in Py_Main ()#8 0x00007fa4d2831f45 in __libc_start_main (main=0x466e50 &lt;main&gt;, argc=1, argv=0x7fffcd30af38, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7fffcd30af28) at libc-start.c:287#9 0x0000000000577c2e in _start () 接下来是 VS 的瞎搞时间~ Visual Studio 中的远程进程调试新版 VS 中加入了看上去吊炸天的远程 ssh 调试功能，自然是情不自禁地想试试。 在 WSL 中开好 ssh，在 VS 里面 调试 -&gt; 附加到进程 -&gt; SSH -&gt; 选择之前添加好的 WSL 的 ssh 信息 -&gt; 选择附加到 Native(GDB)代码 -&gt; 在下面刷新出来的列表里面直接选前面正在运行的 Python 进程即可。 调试启动！ 然后打开的调试界面基本是空的，什么都没有，首先也会有与前面 gdb 相同的加不上断点的问题，原因一样，因为这里调试的原理就是 ssh 过去开一个 gdb。 开始时我在 VS 里面找了很久都没找到哪里可以添加额外 gdb 信息的地方，后再在 VS 的官方文档里面终于翻到了 gdb 调试的命令交互方法。 在 VS 的 命令窗口 中输入 Debug.MIDebugExec 后面再跟 gdb 命令即可直接发送给 ssh 那一端的服务器 gdb 来执行。 一开始直接调用可能会报错： 12&gt;Debug.MIDebugExec info sharedlibraryError: Commands are only accepted when the process is stopped. 在调试中给上一个 中断 先进入 gdb 的交互中即可，在 Debug.MIDebugExec 后面跟上正常的 gdb 调试命令，具体的方法跟上面一致。 1234567891011121314151617&gt;Debug.MIDebugExec info sharedlibraryFrom To Syms Read Shared Object Library0x00007fa4d2be59f0 0x00007fa4d2bf2471 Yes /lib/x86_64-linux-gnu/libpthread.so.00x00007fa4d282f520 0x00007fa4d2974183 Yes /lib/x86_64-linux-gnu/libc.so.60x00007fa4d2600ed0 0x00007fa4d26019ce Yes /lib/x86_64-linux-gnu/libdl.so.20x00007fa4d23f0f10 0x00007fa4d23f1804 Yes /lib/x86_64-linux-gnu/libutil.so.10x00007fa4d21d1e00 0x00007fa4d21e1bf8 Yes /lib/x86_64-linux-gnu/libz.so.10x00007fa4d1ec5610 0x00007fa4d1f34056 Yes /lib/x86_64-linux-gnu/libm.so.60x00007fa4d2e00ae0 0x00007fa4d2e1b490 Yes /lib64/ld-linux-x86-64.so.20x00007fa4d1b32720 0x00007fa4d1b33ef6 Yes /usr/lib/python2.7/lib-dynload/readline.x86_64-linux-gnu.so0x00007fa4d18f30d0 0x00007fa4d1913ee5 Yes /lib/x86_64-linux-gnu/libreadline.so.60x00007fa4d16bc3d0 0x00007fa4d16c8028 Yes /lib/x86_64-linux-gnu/libtinfo.so.5 No ./_cfact.so&gt;Debug.MIDebugExec set solib-search-path :/home/jcf/swig_test/build/lib.linux-x86_64-2.7/Reading symbols from /home/jcf/swig_test/build/lib.linux-x86_64-2.7/_cfact.so...done.Loaded symbols for /home/jcf/swig_test/build/lib.linux-x86_64-2.7/_cfact.so=cmd-param-changed,param=&quot;solib-search-path&quot;,value=&quot;:/home/jcf/swig_test/build/lib.linux-x86_64-2.7/&quot; 这时候在 断点 窗口里面已经可以成功加上 fact 函数的断点了。 加好断点之后 VS 继续，然后 Python 窗口调一次函数。 VS 的断点会马上生效，然后弹出一个窗口让选择源代码文件的位置，毕竟是远程机器上 ssh 得到的，这里的代码位置只能手动选择。 然后可以看到完整的各项信息。 用起来还是相当不错的。 我觉得 VS 调试的意义在于，Debug.MIDebugExec 命令就等于 gdb，所以能用 gdb 做的，VS 都能够做到，关键是能在这样一个方便的图形界面下调试才是最爽的。 以上。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Visual-Studio</tag>
        <tag>wsl</tag>
        <tag>gdb</tag>
        <tag>Python</tag>
        <tag>SWIG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow]]></title>
    <url>%2F2017%2F02%2F04%2F2017-02-04-tensorflow%2F</url>
    <content type="text"><![CDATA[把去年 7 月的那篇 TF 删掉了，重新开一篇。 2016.07.10 初次接触 TensorFlow，当时主要是面临 RDMA 竞赛，题目的目标是 TensorFlow 的 RDMA 优化。 分析之后发现这玩意有点大，不确定是否能够来得及整个剖析一遍，只好关注于 TensorFlow 里面的通信模块（gRPC）的 RDMA 化了。 只是对 TensorFlow 的分布式版做了一些很小的测试，更多的还不太了解。 2016.11.07 写了个： 未来可能要花很长时间在这玩意上了。 初步确定之后有个大方向是围绕 TensorFlow 展开。 2016 学年末 TensorFlow 这艘大船搭上了实验室的很多人，主要是实验室看上了 TensorFlow 的数据流框架： 深度学习是一方面，另外的想法是是否可以以 TensorFlow 为基础，用数据流来支持其他类型的计算，如并行科学计算等等。 按之前 RDMA 优化下来的感觉，TensorFlow 的并行性能还是受 gRPC 限制，我的任务方向是着手考虑是否能够把整个 gRPC 都拿掉，用例如 MPI 等等的办法来替代其中的通信交互。 期间也见过一些其他人改出来的 “MPI 版 TensorFlow”，都是在外面跑一个 MPI 的调度框架，然后里面跑单节点版的来避开 gRPC 的影响。 Example其他的介绍啥的暂时不写了，直接记一些干货吧。 以下是一个来自 github 的例子。 该例做的是一个简单的线性回归，即给出一些已知点来拟合一条直线。在 TensorFlow 里面可以直接视作一个训练的过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import tensorflow as tfimport numpyimport matplotlib.pyplot as pltrng = numpy.random# 设定后面需要的一些参数learning_rate = 0.01training_epochs = 2000display_step = 50# 给出训练数据，就是两个坐标数组了train_X = numpy.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])train_Y = numpy.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])# 坐标点的个数n_samples = train_X.shape[0]# tf Graph 的输入部分，这里创建了两个占位符X = tf.placeholder("float")Y = tf.placeholder("float")# 建立模型# 模型的权重和偏值W = tf.Variable(rng.randn(), name="weight")b = tf.Variable(rng.randn(), name="bias")# 激活函数是一个乘加的线性模型，A = X * W + bactivation = tf.add(tf.mul(X, W), b)# 最小化方差# 这里调用了 TensorFlow 里面内置的优化函数，每次都算出方差 cost，再调用内置的梯度下降算法在给定的学习率的条件下优化方差到最小# ... 说白了整个线性回归就 optimizer 这里是有用的，然而直接用的 TF 的内置函数来完成了cost = tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples) #L2 lossoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) #Gradient descent# 变量的初始化器init = tf.initialize_all_variables()# 启动 TensorFlow 的流图计算with tf.Session() as sess: # 首先初始化所有变量 sess.run(init) # 训练 training_epochs 次 for epoch in range(training_epochs): # 把输入的两个序列 train_X 和 train_Y 中对应位置的数打包成 (x, y) 的结构 for (x, y) in zip(train_X, train_Y): # 运行 optimizer 就是训练过程了，每次训练都使得 W 和 b 朝着使方差 cost 最小的方向更近一步 sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;) # 过某特定步输出一次 log if epoch % display_step == 0: print("Epoch:", '%04d' % (epoch+1), "cost=", \ "&#123;:.9f&#125;".format(sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)), \ "W=", sess.run(W), "b=", sess.run(b)) # 上面循环完之后整个线性回归的过程就结束了 print("Optimization Finished!") print("cost=", sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), \ "W=", sess.run(W), "b=", sess.run(b)) # 把结果显示出来，这里用了 matplotlib 这个库 plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() 这个例子得益于优化算法完全由 TensorFlow 里面自己来实现了，所以写起来非常简单。 TensorFlow 在深度学习中的使用方法就这么简单，类似上面这个例子： 建立数据流图 在 Session 中启动数据流图，把数据喂进去 然后数据跑完一遍，留下的参数就离最优解更近一步。 之后的重点就是看看 Session.run() 以及内置的这些 tf.train.xxx 函数在代码里面是怎么实现的了。 TensorFlow Unpacking 2018年新年启动 开始 TensorFlow 比较完整的源码分析记录： TensorFlow 拆包（一）：Session.Run() TensorFlow 拆包（二）：TF 的数据流模型实现 TensorFlow 拆包（三）：Graph 和 Node TensorFlow 拆包（四）：Device TensorFlow 拆包（五）：Distributed TensorFlow 拆包（六）：RDMA]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio 与 Linux]]></title>
    <url>%2F2017%2F01%2F26%2F2017-01-26-vs%2F</url>
    <content type="text"><![CDATA[曾经我是比较排斥 VS 的，尽管不得不承认这玩意大概是世界上最好的 IDE 无疑。 最新的 2017RC 发布后，观望了半天还是装上了，然后深陷其中无法自拔…oh 自从微软拥抱开源之后，大动作不少，例如 Win10 中的 Linux 子系统，例如 Visual Studio Code，例如微软在 Github 上的贡献年度第一。 我也逐渐把 Sublime Text 换成了 VS Code，nodejs 应用感觉要开始爆炸了，哪哪都是 nodejs 写的东西。 要说当初为什么不想用 VS 那一系的东西，原因不少，可能其中有一些是没啥用的强迫症： 我平时习惯的编译系统是 GNU 的，VS 一系的 C 语言标准略有区别，虽然其实也能在 VS 里面改用 GCC 编，不过以前历史遗留下来给我的印象就不太好。 VS 的包太大，作为一个平时只是写点小程序的人来说，即使采取了最精简的安装，可能里面 90% 以上的都是我不会用到的内容，几十 M 的例如 DevC++，或者再稍大点的 Code::Blockes 足以满足我的需求。 运行起来不太顺畅，可能也跟以前包太大这个原因有关，要么就是心理作用了，总觉得用起来很不爽。 新版的 2017RC 采用了一些新的设计，例如模块化的安装器： 然后最重要的就是它原生支持 Linux 开发了，这也是我阔别多年重新装回 VS 的一个重要原因。 VS 可以本地编辑，通过 ssh 把文件远程传输到 Linux 服务器上运行和调试，这里的由于用的是 ssh，其实本地的 WSL 子系统也是可以用的，不知道以后微软会不会加上与 WSL 交互的一些更方便的接口。 WSL 上的 ssh这其实也是个坑。 没加 Insider Preview 所以不知道目前 WSL 的开发到了哪个程度了，正式版系统中的 WSL 反正是还有不少的问题需要解决。 例如现在只有在管理员模式下运行 cmd 然后再开 bash 才能用 ping 等等的网络命令，然后其他还有各种奇怪的问题。 默认的 WSL 下（用的 Ubuntu 14.04）需要重新设置一下 ssh 才能用。 装上 openssh-server 之后，sshd_config 中有几个比较重要的需要改的有： 123456789Port 2222ListenAddress 0.0.0.0UsePrivilegeSeparation noPasswordAuthentication yesPermitRootLogin yes 端口号是因为 Win10 系统自带一个 ssh … -_-/// 忘了是什么时候出现的了，某次更新之后，发现可以 ssh 自己，账号是 Win10 的本地账户，密码是微软账号的密码。 ssh 进去之后是一个 cmd，当时知道的时候真是觉得我了个大去，微软偷偷摸摸就把这功能加上了。 所以为了避开，就只能另外选个端口了。 另外 ListenAddress 亲测不加不行，不知道什么原理。 切到 root 下 service ssh restart 就好了，ssh 可以正常使用，然后过程中这个 bash 窗口不能关，关掉就切断了子系统中的所有任务。 我看网上有其他教程有弄开机启动脚本的，后台一直跑着一个 sshd，有需要可以另外再找。 VS 的远程调试首先在设置中的 Cross Platform 里面把远程 Linux 的 ssh 加进去。 在 VS 中创建一个 Linux 项目，然后正常写代码，远程 gdb 调试就 OK 啦。 详细的项目配置里面有 gdb 和 gdbserver 两个选项，默认用的应该是 gdbserver，需要 Linux 服务器上也装上，如果选用 gdb 选项的话，用的是 ssh 到服务器上之后调用 gdb --interpreter mi 即调用 Machine Interface 接口。 目前 VS 上的这部分似乎有 BUG，调试完之后 Linux 服务器上的进程不会自己结束，估计后面几个版本应该会修复。 调试中可以查看内存块、调用栈等等等等等，话说 VS 的调试器确实是好用啊，各种强大，具体的以后再挖掘。 Attach to Processgdb 能调，理论上服务器上跑的任何程序都应该是能调的，对 gdb 来说只是加个 -p pid 选项而已。 果然在调试里面找到了 附加到进程 这个命令，可以选择 ssh，显示出远程服务器上的进程之后再选择调试即可，不过目前的版本上也有 BUG。 坐等后面的更新： https://developercommunity.visualstudio.com/content/problem/11345/attach-to-process-over-ssh-fails.html 今天补充的时候，VS 的版本已经更新了，至少我在 root 下登进去已经可以用“附加到进程”进行调试了，爽！！！ 目前遇到的然后解决了的 BUG 添加 ssh 服务器的时候卡死 在配两台服务器的时候遇到的这个问题，其中一台很顺利，另一台一添加 ssh 服务器就卡死了，中间反复检查过各种配置，两台服务器是一模一样的。 一开始还以为是网关、路由表什么的影响，改了之后仍然未果。 更坑爹的是我新建了个账户测试完全正常，只有我自己那个用户账户是不对的。 最后找到的原因是：两台用的都是 zsh，但是卡住那台的默认 shell 用的是 bash，是在 .bashrc 最后加了个 exec zsh 才跳转到 zsh 的。因为当时有些环境变量的脚本跟 zsh 有冲突，所以用 bash 调完脚本加好环境之后才调 zsh。 似乎 VS 的登录到了这一步就卡死了，删掉这部分之后正常。但是我在另外的地方测试了又是好的（例如我的 WSL 就是在 .bashrc 里面写 zsh 跳转，然而在 VS 里面可以正常用）。 Anyway，玄学。 反正一般情况下还是最好不要用这种方式改 shell 了。 附加到进程失败的问题 参见上面那个网址吧，是这个版本的一个 BUG，下个版本能修复，要么就把默认 shell 改成 bash，或者试试在 VS 里面添加 root 的账号。 用 VS 进行远程调试的例子参见 Python-SWIG 初探以及其 gdb 调试。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Visual-Studio</tag>
        <tag>wsl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在冷风中凌乱着]]></title>
    <url>%2F2017%2F01%2F24%2F2017-01-24-lost%2F</url>
    <content type="text"><![CDATA[前天晚上发生了一件巨坑爹的事情——具体的不再详细讲了，简单地说就是： 银泰人太多，只能把车停在了一条比较远的偏僻的小路上然后走过去。回来的时候放在车里的电脑丢了。 然后就呵呵呵呵呵呵了。 这种事情事实上已经是第二次发生在我身上了。 上一次是我高中的时候，一家人出去吃饭，饭店附近没地方停车只能停的比较偏比较远。回来发现车窗被人整个砸碎了，书包被人拿走了。 记得当时好像哭的很惨（捂脸…），书包里其实没啥，就几本书，然后还有周天作业要交的几张试卷，真正其实车前面的储物箱里面还有老爸刚收过来的一笔钱，万幸小偷砸完拿了包就跑了，没翻储物箱。 我们觉得小偷拿走之后应该没啥用，估计会随手扔在附近，然后一家三口在旁边找了好久。 最后老妈打电话给班主任说了下，然后回去之后还是找脑袋复印了卷子重新写了。 自从那次之后后来都不敢在车上放包了，都在身边带走，要么就放到后备箱里，反正不能放在车窗望进来看得到的地方。 我想说我真是SB啊，停车的时候也没有多想想，甚至现在我都不确定当时是不是锁了车门了？万一当时遥控按错键了，车门根本没锁上呢？也没有重新拉一下门看看。 最后让我觉得搞笑的是包里就丢了个笔记本，其他东西都还在，甚至电源线和鼠标都没拿走。 ？？？黑人问号.jpg？？？ 我要说万幸没有带书包整个给我拿走吗？包里有身份证学生证银行卡回学校的车票等等，或许人家打开包之后看到个觉得是最贵的东西就抱了直接跑了，也没空去管包里其他的东西？又或是我真得谢谢这位小偷同志把对他没什么用的东西都留着了，要不也就放假这么几天，鬼知道补这些东西要有多麻烦。 丢了的笔记本暂时不打算再买了，工作的话反正平时主要是要ssh到实验室的服务器上干活，手上还有个Yoga在，实验室也有分配下来的台式机可以用。 也算是给自己个教训，把游戏给戒掉吧。 （本来还买了个手柄准备回家把笔记本接到客厅电视上玩的来着） 好好学习，吸取教训。 最后真的要再提醒路过的各位，出门在外一定一定一定要把东西看好，放在车里也不一定安全，最好不要把车停在阴暗偏僻的地方，贵重物品能带在身上的一定要带着，带不了的也不要放在显眼的地方，包包什么的不要放在能从车窗往里看到的地方，停好车真的要拉一下车门看看，万一就没锁上呢。 都是活生生的教训啊。 在传闻中的“贼城”西安待了4年，除了有一次在公交车上差点被人摸走手机被我发现以外，算是平平静静地躲过了无数魔爪，没想到在自己家乡被人黑了。 ╮(╯_╰)╭ 我还能说什么呢？]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[October]]></title>
    <url>%2F2016%2F10%2F16%2F2016-10-16-october%2F</url>
    <content type="text"><![CDATA[开学已一月有余，距离在科大开始正式的学习生活也有近三个月了。可能还是没把握好研究生新生活的节奏，感觉这一个月以来活得赶了点。 虽然以前写过的代码量不算少了，但相对来说我的工程项目经验还是少的可怜，如此完整地阅读一个比较大的开源案例还是第一次。也幸而还有以前的写代码经验在，让我能磕磕绊绊把任务做下去。 搞 RDMA 期间有很多东西是现查现学现用的，当时也来不及额外花太多时间去好好研究，然后这三个月的博客也是——一直在挖坑。。。 后面会稍微多花点时间把这些东西都好好消化一下，也把前面的坑都填一填。 Something New &amp; Left继续挖坑以及列一下还有啥坑要填 Iptables 前面留下来的坑了… Linux 里面的防火墙，花时间搞搞清楚。 （另外…再花时间研究一下 firewalld 和 SELinux ？） 填坑先从这里开始。 gRPC &amp; TensorFlow 好歹花了3个月时间在这俩上面，稍微总结一下，搞完就再也不想再看到这两个了。 RDMA 前面留过两篇：【RDMA Warming Up】、【Simple usage of RDMA ibv】，简单地记了下 RDMA 的架构和 IB Verbs 的用法，后面可以看看再补充一下。 File Descriptor Linux 中的文件描述符这个东西以前就见过，一直没花时间好好弄明白过。 幸好百度出来的 Pipe fd 和 FIFO fd 直接改进代码里面去没出问题… poll()/epoll()/… 用于轮询文件描述符事件的几个 Linux 系统函数，配合文件描述符使用。 到这里又让我想起来当年多次想啃掉结果总是中途放下的《Linux 内核完全剖析》…我发现内核的东西确实是有点意思的 pthread 多线程编程，这玩意也是之前没研究过的，强行百度了一波直接上马。 函数指针 由于函数指针的存在，读 gRPC 代码的过程中经历了生死般的折磨（后来想想可能跟我 gdb 没用好也有关系）。 虽然总说指针是C的灵魂，但平时总是极力避免使用，原因也是灵魂很难驾驭啊。 函数指针这个东西虽然难搞，但不得不说很好用，尤其是在一些比较大的项目中需要保证代码灵活性的时候。 gdb 非常有用的工具，大项目调试时也是，以前在比赛时见过边上各种神校的巨巨 vim+gdb 调得飞起，值得好好整理一波。 例如 dir 导入源码目录等等用法，挺多可以整理的。 gprof 这次是没用到这玩意啦…当时曾经想过用 gprof 跑个函数调用图出来，最后还是 gdb 查调用栈然后手写整理了。 先补这么多，加上前面已经挖好的几个坑，后面已经是一大堆需要整理的东西了，感觉可以写个“填坑系列”出来。 Something to be written down 有人问我：技术博客不是应该只留技术的东西在里面吗？ 这里还有这么多其他的... 其实还是挺庆幸自己能把这个博客一直坚持写下来的，曾经好多次想要不要把之前瞎写的随笔啥的都给删了，最后还是没舍得，不想让这个小站太冷得像机械。有时候往回翻翻以前写的东西，或是嘲笑/反思那时候为什么会有这样的想法，或是怀念那段精彩的/艰难的/开心的/难过的人生经历。 其实这些东西也不在乎有没有人看，主要还是写给自己看的，看过我写过的心情，走过我去过的地方的人，有一两个就够了。 刚刚看了看，这个域名我居然直接买到了 2018 年… 0.0 …看来当时拿了不少优惠券啊 刚搭起来的时候还想着跟着原 po 一起改改博客主题或者干脆自己写一个，博客存放的地方也是，从 Github 到 Gitcafe 到自己的 VPS 最后又回到 Github，现在大概是过了那段瞎折腾的劲了，只想着干净清爽能写东西就好 回到科大生活的话题，有时候想想我原来理想中的研究生生活到底应该是什么样的？ 并不是怕在电脑前一坐一天（毕竟以前刷题的时候这种日子早就习惯了），就是觉得这样的日子太过无趣。大概是之前自己被 Deadline 压得太厉害，原来想好的好多事情都没有机会去做，现在稍微从这样的 push 中解放出来一些了，也就心有余悸想着不要再回到这样的恶性循环里了。 有时候还是应该浪费点时间到实验室以外的生活中去的。 15号这天晚上刚洗完澡就跟着舍友一起出去浪了… 酒吧的名字叫 驴圈慢生活，一家很有悠闲氛围的清吧。除了驻唱唱歌以外还算清净，适合悠闲下来慢慢坐着谈天说地。 键盘+吉他以及驻唱。 哦，还有惊喜地发现他们用的播放器是 网易云音乐 …我现在对用 网易云 的都很有好感 你敢信我们四个人逛在酒吧街上聊的居然是数据挖掘和机器学习…（捂脸，已沉迷学术无法自拔）…然后聊到人生，聊到社会发展… 都是一群热爱学术并且很有生活的人啊~ 然后去了这个曾战斗过好多年的战场~ 到今天为止 参赛队员、裁判、教练 我算是都经历过了哈哈，说实话坐在观众席上看他们打比赛甚至有点比自己在赛场上还要紧张的感觉。 Life Continues 生活仍要继续，不管面对什么，总要微笑地继续走下去。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simple usage of RDMA ibv]]></title>
    <url>%2F2016%2F09%2F07%2F2016-09-07-rdmaibv%2F</url>
    <content type="text"><![CDATA[记录一些 ibv 函数的用法 Refer to: RDMAmojo ibv functions ibv_poll_cq()1int ibv_poll_cq(struct ibv_cq *cq, int num_entries, struct ibv_wc *wc); 用于从 Completion Queue 中查询已完成的 Work Request。 所有的 Receive Request、signaled Send Request 和出错的 Send Request 在完成之后都会产生一个 Work Completion，Work Completion 就被放入完成队列（Completion Queue）中。 完成队列是 FIFO 的，ibv_poll_cq() 检查是否有 Work Completion 在完成队列中，如果是那么就将队首弹出，并返回那个 Work Completion 到 *wc 中。 ibv_wc 的结构如下，描述了一个 Work Completion 的情况。 123456789101112131415struct ibv_wc &#123; uint64_t wr_id; enum ibv_wc_status status; enum ibv_wc_opcode opcode; uint32_t vendor_err; uint32_t byte_len; uint32_t imm_data; uint32_t qp_num; uint32_t src_qp; int wc_flags; uint16_t pkey_index; uint16_t slid; uint8_t sl; uint8_t dlid_path_bits;&#125;; wr_id 由产生 Work Completion 的 Request 决定 status 是操作的状态，通常为 IBV_WC_SUCCESS 表示 Work Completion 成功完成，其他还有一些错误信息 opcode 表示当前的 Work Competition 是怎么产生的 bute_len 表示传输的字节数 参数说明： |Name|Direction|Description||-||cq|in|用于存放 Work Completion 的完成队列||num_entries|in|表示最大从完成队列中读取多少个 Work Completion||wc|out|将读取到的 Work Completion 返回出来，如果有多个则返回的是数组| 函数的返回值：成功则返回读取到的 Work Completion 数量，为 0 表示未读取到 Work Completion，可认为是完成队列为空，为负值则表示读取出错。 ibv_req_notify_cq()1int ibv_req_notify_cq(struct ibv_cq *cq, int solicited_only); 用于在完成队列中请求一个完成通知。 调用 ibv_req_notify_cq() 之后，下一个被加到 CQ 中的请求（发送请求或者接收请求）会被加上通知标记，当请求完成产生一个 Work Completion 之后就会产生通知，完成通知将被 ibv_get_cq_event() 函数读取出来。 传入的参数中： solicited_only 为 0 时表示无论下一个加入 CQ 的请求是哪种类型的都会产生通知，否则只有 Solicited 或者出错的 Work Completion 才会产生通知 ibv_get_cq_event()12int ibv_get_cq_event(struct ibv_comp_channel *channel, struct ibv_cq **cq, void **cq_context); 用于等待某一 channel 中的下一个通知产生。 ibv_get_cq_event() 默认是一个阻塞函数，调用之后会将当前程序阻塞在这里，直到下一个通知事件产生。 当 ibv_get_cq_event() 收到完成事件的通知之后，需要调用 ibv_get_cq_event() 来确认事件。 典型用法： Stage I：准备阶段 创建一个 CQ，并且将它与一个 Completion Event Channel 相关联； 用 ibv_req_notify_cq() 对一个 Completion Work 调用通知请求； Stage II：运行中 等待事件产生； 产生之后处理事件，并且调用 ibv_ack_cq_events() 来确认事件； 对下一个 Completion Work 调用 ibv_req_notify_cq() 的通知请求； 参数说明： |Name|Direction|Description||-||channel|in|关联在 CQ 上的 Completion Event Channel||*cq|out|从 Completion 事件中得到的一个 CQ||**cq_context|out|从 Completion 事件中得到的 CQ context| 返回值：为 0 表示成功，-1 在非阻塞模式下表示当前没有事件可读，阻塞模式则表示出错。 这种机制用于避免 CPU 反复读取 Work Completion ，若不采用事件的方法则只能通过不断地 ibv_poll_cq() 来轮询是否有事件完成 ibv_ack_cq_events()1void ibv_ack_cq_events(struct ibv_cq *cq, unsigned int nevents); 用于确认已完成 Completion events。 ErrorsIBV_WC_WR_FLUSH_ERR(5/0x5)Work Request Flushed Error 当 QP 的传送状态处于 Error 状态时，任何操作都会引发该错误。 IBV_WC_RNR_RETRY_EXC_ERR(13/0xd)Receiver-Not-Ready Retry Error 当接收端没有准备好 Recv Request 时发送端产生了一个 Send Request 就会发生 RNR_RETRY 错误。 要求 ibv_post_recv() 必须在 ibv_post_send 之前完成，所以一种基本的思路就是一开始就 Post 一堆 Recv Request 到队列中去，然后检查当队列中的 Recv Request 少于一定数量时补充，保证不管发送端什么时候 Post Send Request 时，接收端都有足够的 Recv Request 来接收。 问题是如果发送端毫无顾忌地可以任意发送数据，尤其是在 RDMA_WRITE 方式，接收端这边会不会来不及取走数据，就被发送端传过来的新数据覆盖掉了？ 或者设置 ibv_modify_qp() 参数中的 min_rnr_timer 以及 rnr_retry，前者是重试间隔时间，后者是重试次数，当 rnr_retry 为 7 时表示重试无限次。这种方法可用于重试直到接收端确认取走数据，并且准备好下一次的 Recv Request，然后发送端再进行发送。 当发送端发生 RNR_RETRY 错误时，重新调用 ibv_post_send() 是没用的，因为此时 QP 已经进入错误状态，接下来不管什么样的操作都会继续引发 IBV_WC_WR_FLUSH_ERR 错误。 除非另外使用一种流控制的方式，不然上面的两种解决方案都总会存在一定的局限性。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>RDMA</tag>
        <tag>InfiniBand</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[填坑系列 - Iptables]]></title>
    <url>%2F2016%2F08%2F30%2F2016-08-30-iptables%2F</url>
    <content type="text"><![CDATA[iptables 是 Linux 中一个非常重要的安全工具，用于配置内核防火墙。 参考自：Arch Wiki - iptables 12345678910111213141516171819202122232425262728293031323334 XXXXXXXXXXXXXXXXXX XXX Network XXX XXXXXXXXXXXXXXXXXX + | v +-------------+ +------------------+ |table: filter| &lt;---+ | table: nat | |chain: INPUT | | | chain: PREROUTING| +-----+-------+ | +--------+---------+ | | | v | v [local process] | **************** +--------------+ | +---------+ Routing decision +------&gt; |table: filter | v **************** |chain: FORWARD|**************** +------+-------+Routing decision |**************** | | | v **************** |+-------------+ +------&gt; Routing decision &lt;---------------+|table: nat | | ****************|chain: OUTPUT| | ++-----+-------+ | | | | v v | +-------------------++--------------+ | | table: nat ||table: filter | +----+ | chain: POSTROUTING||chain: OUTPUT | +--------+----------++--------------+ | v XXXXXXXXXXXXXXXXXX XXX Network XXX XXXXXXXXXXXXXXXXXX Basiciptables 用于检查、修改、转发、丢弃 IP 数据包，其中每一步的详细工作通过 tables 和 chains 这两个结构来完成。 上面那张图的完整情况是这个样子的： 每个节点上的小写字母表示 table ，下面的大写字母表示 chain ，可以把每个节点理解成很多设定好的规则。每一个 IP 数据包都要从上到下经过这样一整个工作流的检查（有的数据包从外面进来到 Local Process 就结束了，有的是从 Local Process 里面产生然后发送到外面去）。 Tablesiptables 中包含5个 tables： raw 用于配置数据包，其中的数据包不会被系统跟踪 filter 存放了所有与防火墙相关的操作 nat 用于网络地址转换 mangle 用于对特定数据包的修改 security 用于强制访问控制（…不懂…暂时跳过） 通常情况下常用的是 filter 和 nat ，其他的用于更复杂的情况，所以上面手画的流图里面只画出了这两个 tables 。 Chainstables 由 chains 组成，例如 filter 表中就有 INPUT 、 OUTPUT 、 FORWARD 3条内建的 chains ，这三条 chains 作用在数据包过滤过程的不同时间点上（参见上面的流程图）。 nat 表包含 PREROUTING 、 POSTROUTING 、 OUTPUT 这三条 chains 。 Configuration在 Arch Linux 和 CentOS 上 iptables 的启动和关闭都在服务里配置（话说 CentOS 7 已经改用 firewalld 来管理防火墙了）： 12# systemctl start iptables# systemctl stop iptables 然后不同的发行版设置的 iptables 的记录文件的位置还不一定一样，比如 Arch 里面是在 /etc/iptables/iptables.rules ， CentOS 就是直接放在 /etc/sysconfig/iptables 这个文件里了， Ubuntu … 恕我没找到。 管理和配置 iptables 可以在 root 权限下用 iptables 命令，或者直接修改配置文件，然后重启 iptables 也行。一般我比较喜欢改文件，一目了然，不容易出错，，，不过像 Ubuntu 这种我没找到写在哪的就坑了。 具体的 iptables 命令还是用的时候 man iptables 慢慢查吧，这里简单记一下几个常用命令的用法： 12345678910111213141516171819# iptables -A chain rule向 chain 中附加一条规则 rule# iptables -C chain rule检查 chain 中是否存在规则 rule# iptables -D chain rule从 chain 中删除规则 rule# iptables -I chain [rulenum] rule向 chain 中插入一条规则 rule这个跟 -A 的区别是 -A 是加入到 chain 的末尾-I 如果不指定插入的序号的话则默认序号为1，即插入到 chain 的开头# iptables -S [chain]打印 chain 中的所有规则，不指定 chain 的话就是所有 chain# iptables -L [chain]列出 chain 中的所有规则，不指定 chain 的话就是所有 chain 上面的每条命令还可以加 -t table 来指定当前的操作对某个特定的表 table 进行。 -A 和 -I 的不同在于， iptables 检查规则时是从上往下按 rules 的顺序依次进行，一旦匹配到一条 rule 那就直接跳出当前 chain 了，因此同样的一些 rules 插入到 iptables 里之后可能因为顺序的不同会有不同的结果。 Rules比较坑的是我在好几个 wiki 上查了 iptables ，但是上面很少有讲 rules 具体怎么写的， man iptables 返回的信息是在 PARAMETERS 这段中有讲。 1234567891011121314151617181920212223[!] -p, --protocol protocol指定协议，可以是 tcp/udp/udplite/icmp/icmpv6/esp/ah/sctp/mh/all加了 ! 则表示除了该协议以外，下同[!] -s, --source address[/mask][,...]指定源地址[!] -d, --destination address[/mask][m...]指定目标地址[!] -i, --in-interface name指定数据包进入的网卡设备[!] -o, --out-interface name指定数据包流出的网卡设备-j, --jump target指定当前 rule 要跳转到的目标，可以是 ACCEPT/DROP/REJECT/LOGACCEPT 和 LOG 都是让数据包通过当前的 chain ，区别是 LOG 会记录日志信息DROP 直接把包从这里丢弃不做任何操作， REJECT 丢弃包并且向源地址发送拒绝响应-g, --goto chain跳到某条 chain 中继续执行 Example列个示例吧，下面是一台 VPN 服务器 iptables 文件的一部分内容： 123456789101112131415161718192021222324252627*filter # filter 表:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0] # 设置默认规则都是 ACCEPT-A INPUT -i enp7s0 -j ACCEPT-A INPUT -i ppp+ -j ACCEPT-A INPUT -i lo -j ACCEPT # 设置各块网卡都能通过-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT-A INPUT -p tcp -m tcp --dport 1723 -j ACCEPT-A INPUT -p udp -m udp --dport 1194 -j ACCEPT-A INPUT -p udp -m udp --dport 4500 -j ACCEPT-A INPUT -p esp -j ACCEPT-A INPUT -p gre -j ACCEPT-A INPUT -p icmp -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -d 202.38.64.59/32 -i ppp+ -j DROP # 有个特定的地址不想让它转发COMMIT*nat # nat 表:PREROUTING ACCEPT [0:0]:INPUT ACCEPT [0:0]:OUTPUT ACCEPT [0:0]:POSTROUTING ACCEPT [0:0]-A POSTROUTING -o enp7s0 -j MASQUERADE # 设置输出数据包地址伪装COMMIT 默认规则后面的 [0:0] 是用 iptables-save 命令保存当前的 iptables 配置到文件后写的，表示当前收发了多少数据包。我这里是手写的所以就直接设成 0 了，其实没什么用。 数据包进入某个 table 之后是从上往下遍历所有规则，最后再走默认规则。 事实上上面这个文件写的是有问题的，所有输入的数据包的 INPUT 链在网卡这里就直接被 ACCPET 了，所以后面写的一堆规则其实没啥用… 比较好的写法是把默认规则设成 DROP ，然后再写一些需要用的 ACCPET 规则，让数据包从上到下一条一条匹配，如果没有匹配到任何一条规则，那就说明这个包不是我们想要的，默认 DROP 掉。 filter 表的 FORWARD 链这里这条规则表示：从 ppp+ 这些网卡（PPTP 服务生成的虚拟网卡，对应了每个 VPN 的连接）来的，要到 202.38.64.59 这个地址去的数据包全部丢掉，这样 VPN 进来的连接就不能通过这台机子的转发来访问这个地址了。 下面的 nat 表， MASQUERADE 表示动态地址伪装，即 VPN 链接过来的数据从服务器出去的时候把数据包的源地址改成服务器的地址，用于 NAT 转换。 话说其实应该找时间开个虚拟机好好测试一下的… To be continued…]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>填坑系列</tag>
        <tag>Linux</tag>
        <tag>firewall</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[填坑系列 - gRPC]]></title>
    <url>%2F2016%2F07%2F20%2F2016-07-20-grpc%2F</url>
    <content type="text"><![CDATA[gRPC 是谷歌写的一个开源 RPC 框架，官网在这里。 RPC 刷题外传之深入浅出 RPC 谁能用通俗的语言解释一下什么是 RPC 框架？ 通常我们写代码的时候，要么调用的函数就在同一个文件里，要么在同一个工程里，要么函数在本地的其他某个库里，编译的时候加点选项就 OK 了。 1234567void functionA(...)&#123;...&#125;int main()&#123; functionA(...);&#125; 然而实际的生产环境中可能遇到的很多都是另一台机器上的 Server 提供服务，然后 Client 要去得到服务的过程。 我们当然可以说这就是本地调用通信函数跟远程的通信进程交互嘛，如果把这个过程再作一点点的抽象，RPC（Remote Procedure Call）提供了一种很有意思的思路： 即把远程的服务调用抽象成为一个类似本地函数调用的过程，我写代码的时候不管这个函数在什么地方，还是直接写： 12345678910111213void remoteFunctionA(...)&#123; ... ... tcp connect ... ... send/recv message ... ... return data ... ...&#125;int main()&#123; remoteFunctionA(...)&#125; 对于写这段代码的我来说，我的目标要的只是一个能工作的函数，并不关心这个函数是在什么地方，如果它不在本机上，如果有人能给我一个实现好的框架来实现调用，那么我也并不关心里面的通信部分到底是怎么写的。 RPC 框架就是完成了这个任务，高度封装之后，直接体现在工作代码里面就是一个简单的函数调用，背后的通信等等实现都被隐藏在框架里面了。 大概来说，一般的 RPC 都是这么个结构： Server 和 Client 的应用中都有一个叫 Stub 的结构，这个结构用于找到需要调用的函数在哪台机器上或者对远程向本地的服务申请作出响应。简单地说就是靠它来完成所有工作细节的封装，向上只要给出一个类似本地函数调用一样的接口（当然实际中肯定还需要一些别的东西），向下它会完成网络通信部分的任务。 中间通常还会有个序列化/反序列化的过程，则完整的 RPC 过程是类似这个样子的： 底层的数据传输用的是二进制序列的形式， Client 需要调用 RPC 服务的时候，首先将方法名、各种参数等等序列化为一串二进制数据，然后将其发送给 Server Server 收到二进制数据之后反序列化得到需要调用的详细内容，交给上层处理 上层处理完成之后，将返回值等等信息重新序列化为二进制数据，发送给 Client Client 收到二进制数据，反序列化得到想要的结果，返回给上层 gRPC回到这篇日志的主体内容。 谷歌之前一直用的 RPC 服务随着时代发展已经越来越不适合他们自己的使用了，于是人家在一堆原则和诉求下新写了一个 gRPC 用于支持他们的各种服务。 GRPC的产生动机和设计原则 不愧是谷歌的东西，事实上 gRPC 实现得非常好，性能强，上层支持十多种不同的编程语言混合通信，还支持各种平台…满满的都是优点。 怨念时间：但是这玩意让你拿去用当然都是的优点，需要你去读代码和改代码的时候就成了巨大的坑点。为了支持多语言、多平台， gRPC 的层次结构非常复杂，而且代码部分缺乏足够的文档，读代码真的是把人折磨得死去活来啊。 坑死爹了！ 之前的手上的任务是把这玩意从 TCP 改成 RDMA ，啃代码花了不少时间。 具体关于代码部分，之后看情况要不要再补写到这里吧。 To be continued…]]></content>
      <categories>
        <category>Master-of-Science</category>
      </categories>
      <tags>
        <tag>填坑系列</tag>
        <tag>Google</tag>
        <tag>RPC</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Dark Room]]></title>
    <url>%2F2016%2F07%2F09%2F2016-07-09-adarkroom%2F</url>
    <content type="text"><![CDATA[空间里找到的这么个简单有趣的文字游戏： 小黑屋 在Github上找到了源码，是用JS来进行后台的计算和简单的动画效果处理（静态博客可用Hoho~），顺手fork了过来： A Dark Room - jcf94 上面那个地址的版本似乎是被人改过的，反而是我 fork 的原版在某些中文显示上有一点问题，只能留坑等着以后再看看能不能修正了。]]></content>
      <categories>
        <category>玩玩玩玩</category>
      </categories>
      <tags>
        <tag>Text Game</tag>
        <tag>JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDMA Warming Up]]></title>
    <url>%2F2016%2F06%2F27%2F2016-06-27-rdma%2F</url>
    <content type="text"><![CDATA[RDMA(Remote Direct Memory Access) is a technique for computers to access memory on a remote machine without interrupting the processing of its own CPU(s). Refer to: RDMAmojo Wikepedia - RDMA Basic OpenFabrics Alliance A non-profit organization that promotes RDMA technologies for server and storage connectivity. It aims to develop open-source software that supports the three major RDMA fabric technologies: IB(InfiniBand), RoCE(RDMA over Converged Ethernet) and iWARP(Internet Wide Area RDMA Protocol). The software includes two packages, one that runs on Linux and FreeBSD and one that runs on Microsoft Windows. OFED(OpenFabrics Enterprise Distribution) It was released by the OpenFabrics Alliance. The OFED stack includes software drivers, core kernel-code, middleware, and user-level interfaces. It offers a range of standard protocols. IB(InfiniBand) A new generation network protocol which supports RDMA natively from the beginning. RoCE(RDMA over Converged Ethernet) It allows performing RDMA over Ethernet network. Its lower network headers are Ethernet headers and its upper network headers (including the data) are InfiniBand headers. This allows using RDMA over standard Ethernet infrastructure (switches). iWARP(Internet Wide Area RDMA Protocol) It allows performing RDMA over TCP which means RDMA can be used over standard Ethernet infrastructure. However, some features that exist in IB and RoCE may not be supported in iWARP, as well as loosing some of the RDMA performance advantages. Advantages Direct user-level access to HW Zero-copy: Applications can perform data transfer without the network software stack involvement and data is being send /received directly to the buffers without being copied between the network layers. Kernel bypass in the fast path: Applications can perform data transfer directly from userspace without the need to perform context switches. No CPU involvement: Applications can access remote memory without consuming any CPU in the remote machine. The caches in the remote CPU(s) won’t be filled with the accessed memory content. Asynchronous Communication Computation and communication overlap. Hardware managed transport Software deals with buffers, not packets. InfinibandA pervasive, low-latency, high-bandwidth interconnect which requires low processing overhead and is ideal to carry multiple traffic types (clustering, communications, storage, management, etc) over a single connection. IB mainly consists of 5 layers. |No|Layers|Functions||-||1st|Software Transport Verbs and Upper Layer Protocols|Interface between applications and hardwaresDefine methodology for management functions||2nd|Transport|Delivers packets to the appropriate QP nodeMessage Assembly/De-assemblyAccess Right||3rd|Network|Route packets between different partitions/subnets||4th|Data Link(Symbols and framing)|Route packets on the same partition/subnet||5th|Physical|Signal levels/Media/Connections| In an IB net, there are devices: HCA(Host Channel Adapter): Like a Ethernet NIC. Connects the InfiniBand Cable to the PCI Express bus. It is the end node of the InfiniBand network, executing transport-level functions as well as supporting the InfiniBand verbs interface. TCA(Target Channel Adapter): Similar to HCA, TestUse lsmod to show the loaded kernel modules. Check for ‘rdma‘ and ‘ib‘. 12345678910111213141516171819202122jcf@node1:~&gt; lsmod | grep ibib_ucm 18507 0ib_ipoib 140788 0ib_cm 47822 3 ib_ucm,rdma_cm,ib_ipoibib_uverbs 75527 2 rdma_ucm,ib_ucmib_umad 22476 6mlx5_ib 188056 0mlx5_core 397618 1 mlx5_ibmlx4_ib 209139 0ib_sa 33470 5 rdma_ucm,rdma_cm,ib_ipoib,ib_cm,mlx4_ibib_mad 56507 4 ib_cm,ib_umad,mlx4_ib,ib_saib_core 138343 12 rdma_ucm,ib_ucm,rdma_cm,iw_cm,ib_ipoib,ib_cm,ib_uverbs,ib_umad,mlx5_ib,mlx4_ib,ib_sa,ib_madib_addr 18889 3 rdma_ucm,rdma_cm,ib_coremlx4_core 370626 2 mlx4_en,mlx4_ibmlx_compat 30364 17 rdma_ucm,ib_ucm,rdma_cm,iw_cm,ib_ipoib,ib_cm,ib_uverbs,ib_umad,mlx5_ib,mlx5_core,mlx4_en,mlx4_ib,ib_sa,ib_mad,ib_core,ib_addr,mlx4_corelibcrc32c 12644 0ipv6_lib 344914 187 ib_ipoib,ib_core,ib_addr,ipv6libsas 88001 1 isciscsi_transport_sas 40887 2 isci,libsaslibahci 35044 1 ahcilibata 230626 5 libsas,ata_generic,ata_piix,ahci,libahciscsi_mod 235785 13 usb_storage,sr_mod,sg,sd_mod,isci,libsas,scsi_transport_sas,scsi_dh_emc,scsi_dh_alua,scsi_dh_hp_sw,scsi_dh_rdac,scsi_dh,libata ib_uverbs and low-level driver of the hardware shoule be checked. USERSPACE VERBS ACCESS The ib_uverbs module, built by enabling CONFIG_INFINIBAND_USER_VERBS, enables direct userspace access to IB hardware via “verbs”. See Kernel Documentation - Infiniband 1234567891011jcf@node1:~&gt; lsmod | grep rdmardma_ucm 22630 0rdma_cm 54692 1 rdma_ucmiw_cm 36675 1 rdma_cmconfigfs 35817 2 rdma_cmib_cm 47822 3 ib_ucm,rdma_cm,ib_ipoibib_uverbs 75527 2 rdma_ucm,ib_ucmib_sa 33470 5 rdma_ucm,rdma_cm,ib_ipoib,ib_cm,mlx4_ibib_core 138343 12 rdma_ucm,ib_ucm,rdma_cm,iw_cm,ib_ipoib,ib_cm,ib_uverbs,ib_umad,mlx5_ib,mlx4_ib,ib_sa,ib_madib_addr 18889 3 rdma_ucm,rdma_cm,ib_coremlx_compat 30364 17 rdma_ucm,ib_ucm,rdma_cm,iw_cm,ib_ipoib,ib_cm,ib_uverbs,ib_umad,mlx5_ib,mlx5_core,mlx4_en,mlx4_ib,ib_sa,ib_mad,ib_core,ib_addr,mlx4_core Then use ibv_devices to show the available RDMA devices in the local machine. 1234jcf@node1:~&gt; ibv_devices device node GUID ------ ---------------- mlx4_0 10d2c91000006bd0 Use ibv_devinfo to check for more information about the IB device. 1234567891011121314151617181920jcf@node1:~&gt; ibv_devinfo -d mlx4_0hca_id: mlx4_0 transport: InfiniBand (0) fw_ver: 2.9.1000 node_guid: 10d2:c910:0000:6bd0 sys_image_guid: 10d2:c910:0000:6bd3 vendor_id: 0x02c9 vendor_part_id: 26428 hw_ver: 0xB0 board_id: MT_0D90110009 phys_port_cnt: 1 Device ports: port: 1 state: PORT_ACTIVE (4) max_mtu: 4096 (5) active_mtu: 4096 (5) sm_lid: 1 port_lid: 1 port_lmc: 0x00 link_layer: InfiniBand Make sure at least one port is in PORT_ACTIVE state, which means that the port is available for working. Next, use ibv_*_pingpong to test the connection between machines. There have to be a server process and a client process. Server: 12345jcf@node1:~&gt; ibv_rc_pingpong -g 0 -d mlx4_0 -i 1 local address: LID 0x0001, QPN 0x000241, PSN 0x20e9d5, GID fe80::10d2:c910:0:6bd1 remote address: LID 0x0001, QPN 0x000242, PSN 0x3bef44, GID fe80::10d2:c910:0:6bd18192000 bytes in 0.01 seconds = 8998.49 Mbit/sec1000 iters in 0.01 seconds = 7.28 usec/iter &amp; Client: 12345jcf@node1:~&gt; ibv_rc_pingpong -g 0 -d mlx4_0 -i 1 localhost local address: LID 0x0001, QPN 0x000242, PSN 0x3bef44, GID fe80::10d2:c910:0:6bd1 remote address: LID 0x0001, QPN 0x000241, PSN 0x20e9d5, GID fe80::10d2:c910:0:6bd18192000 bytes in 0.01 seconds = 9322.33 Mbit/sec1000 iters in 0.01 seconds = 7.03 usec/iter Two nodes, Server: 12345jcf@node1:~&gt; ibv_rc_pingpong -g 0 -d mlx4_0 -i 1 local address: LID 0x0001, QPN 0x000240, PSN 0x490d6c, GID fe80::10d2:c910:0:6bd1 remote address: LID 0x0009, QPN 0x00006a, PSN 0xf5086d, GID fe80::10d2:c910:0:6c618192000 bytes in 0.01 seconds = 7793.55 Mbit/sec1000 iters in 0.01 seconds = 8.41 usec/iter &amp; Client: 12345jcf@node4:~&gt; ibv_rc_pingpong -g 0 -d mlx4_0 -i 1 node1 local address: LID 0x0009, QPN 0x00006a, PSN 0xf5086d, GID fe80::10d2:c910:0:6c61 remote address: LID 0x0001, QPN 0x000240, PSN 0x490d6c, GID fe80::10d2:c910:0:6bd18192000 bytes in 0.01 seconds = 7809.34 Mbit/sec1000 iters in 0.01 seconds = 8.39 usec/iter Besides ibv_*_pingpong, rping can be used to test the connection too. Server: 1234567891011121314151617181920212223jcf@node1:~&gt; rping -s -v -C 20server ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrserver ping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrsserver ping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstserver ping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuserver ping data: rdma-ping-4: EFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvserver ping data: rdma-ping-5: FGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwserver ping data: rdma-ping-6: GHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxserver ping data: rdma-ping-7: HIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyserver ping data: rdma-ping-8: IJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzserver ping data: rdma-ping-9: JKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzAserver ping data: rdma-ping-10: KLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzAserver ping data: rdma-ping-11: LMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABserver ping data: rdma-ping-12: MNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCserver ping data: rdma-ping-13: NOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDserver ping data: rdma-ping-14: OPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEserver ping data: rdma-ping-15: PQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFserver ping data: rdma-ping-16: QRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGserver ping data: rdma-ping-17: RSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHserver ping data: rdma-ping-18: STUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHIserver ping data: rdma-ping-19: TUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHIJserver DISCONNECT EVENT...wait for RDMA_READ_ADV state 10 &amp; Client: 12345678910111213141516171819202122jcf@node4:~&gt; rping -c -v -a 10.0.0.1 -C 20ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrsping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuping data: rdma-ping-4: EFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvping data: rdma-ping-5: FGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwping data: rdma-ping-6: GHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxping data: rdma-ping-7: HIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyping data: rdma-ping-8: IJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzping data: rdma-ping-9: JKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzAping data: rdma-ping-10: KLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzAping data: rdma-ping-11: LMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABping data: rdma-ping-12: MNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCping data: rdma-ping-13: NOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDping data: rdma-ping-14: OPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEping data: rdma-ping-15: PQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFping data: rdma-ping-16: QRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGping data: rdma-ping-17: RSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHping data: rdma-ping-18: STUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHIping data: rdma-ping-19: TUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHIJclient DISCONNECT EVENT... To be continued at Simple usage of RDMA ibv]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>RDMA</tag>
        <tag>InfiniBand</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五分之三的青海]]></title>
    <url>%2F2016%2F05%2F16%2F2016-05-16-qinghai%2F</url>
    <content type="text"><![CDATA[我的诗和远方。 （多图流量预警） 从科大回到西安的第二天，我踏上了这趟开往西域的列车。 总说： 到不了的地方都是远方。 其实我对青海以及大西北的向往从很久以前就开始了，总觉得那是个神秘纯净的地方，远离喧嚣。 然而一直说想去，一直都没有找到合适的时间，各种事情。后来想想一直都觉得自己放不开，说走就走的旅行怎么就这么难呢？（学生时期尚且如此，我好担心以后读研和工作之后自己会不会更走不动了？） 将青海作为自己毕业前的最后一站也是想好好放松一下，马上就要离开西安了，这可能是我未来的好多年中离青海最近的最后一段日子了，也许未来的机会会更少了吧。 虽然开始时遗憾没有能与原来约好的好多小伙伴一同前往，但此行见识到了太多美如画的景色，认识了许多新朋友。待到返程时，只剩下了不舍和怀恋而已。 Day One 西宁漫行抵达西宁是在早上，从西安过来仍然穿着短袖的我着实被西宁早晨的低温吓了一跳。等到太阳升的比较高了之后气温也终于高了起来，话说这天中午特别热… 其他同行的队员们都还没到，于是在宾馆稍作休整之后，我背上包独自开始了这一天的漫行。 也许是海拔高的原因，这里感觉离天空会更近一点，天也看上去更蓝一点。而西宁作为一个现代化的大都市，与我所走过的其他城市并没有什么太大的不同——高楼林立，忙碌的车辆和行人。只是建筑中偶尔穿插一些别具异域风情的清真寺，然后路上走过的人群中也能看到很多戴着头巾或白帽的穆斯林。 话说虽然这一天在街上看到的年轻穆斯林女性不多，但是真的有见到几个配上头巾特别好看的。 能够感觉到在这里生活着的多个民族都是非常和谐地与整座城市融合在一起。 最先来到的是这个东关大清真寺，这是西北地区的四大清真大寺之一，据说每周五穆斯林礼拜日那天游人是不允许进入的，然后前来参加礼拜的穆斯林可以从寺中的大殿一直坐到寺外的大广场还坐不下，整条街都要封路。 不知道我是该庆幸还好这天不是周五我可以进去参观，还是该遗憾这天为什么不是周五然后我没有机会见识到如此盛大的礼拜场景呢？ 里面是典型的中国殿堂式建筑风格，配上门口大殿的伊斯兰风情，一踏进来就能感受到浓浓的文化氛围。 最中央的礼拜大殿是不允许游人进入的，我进来时左边的小楼上书声琅琅，里面在上经文课，右边的小楼上可以看到晒着许多毯子，这些都是供前来礼拜的信众使用的。 …然后还看到人家这里可以报名学习初级班…汗~~不愧是清真大寺，好专业呐。 离开清真大寺，一路慢慢逛到了西宁的人民广场。 旁边是一条当地非常热闹的小吃街，叫“水井巷”，里面大概有这边的各种小吃，然后工艺品、土特产什么的。门口这家店的手工酸奶特别好吃我的天！只要5块钱一份。后来想想好后悔没有多吃几份。 人民广场上大白天的就有好多人顶着大太阳在跳广场舞…不知道是不是一种民族风俗？其实我不是很能理解的说（笑~~）因为真的好热啊，哈哈，一堆人围着音响汗流浃背然后还跳的很开心。 下一站来到了马步芳公馆，马步芳其人大概是当年西宁这里的一个土皇帝一样的角色，回族，号称“西北王”，是民国时期西北势力最大、控制范围最广、官衔最高的封建军阀，统治青海40多年，曾任国民政府西北军政长官公署长官。 这个公馆就是他的私宅，整个宅子保留下来一共6个院落，占地近3万平方米，值得一提的是里面用了大量玉石作为墙面材料，据说当年共耗资3000万大洋建成，折合今天的人民币大约相当于2个亿吧。 面前那幢小屋白色的墙面就是玉石。 这面墙上也都是玉石。 这所宅子名为“磬庐”，但百度完马步芳的事迹之后我却觉得这个人与这两个字的格调相差太远，而把这里开发成一个4A的旅游景区实在是一大败笔啊。 捡几个主要的说一下：马步芳靠家族势力起家，一生所做的事迹基本就是个土匪了。马步芳当年代理青海省政府主席，被蒋介石任命为西北“剿共”总司令，在解放战争中积极参加反共内战，阻挠红军北上抗日，围剿了当年的红军西路军。然后在西北疯狂聚敛钱财，镇压民族反抗，全面控制青海的贸易、工矿、畜牧、金融，收各种苛捐杂税，抓壮丁挖玉石给他建房子（最后建成的也就是这所宅子）。并且这个人极为荒淫无耻，他部属的妻女，自己家族的胞妹、侄女、兄嫂、弟媳，长得好看的都不放过……变态成这样也是史上罕见的人物啊。 然而马步芳公馆现在却被开发成一座不问来历的纯“建筑艺术品”，景区只字不提马步芳对人民犯下的滔天罪行，其实稍微有一点历史知识的人都会对这个人的所做感到愤怒。 知道了这么多之后，我随便看了看之后也就没有兴趣再接着逛下去了，悻悻地离开了这里。 回到宾馆之后这天晚上终于见到了此行所有的队员。我们这个团队一共17个人，领队跟我一样也是今年大四，其中有10个人是一起来的，最后就是包括我在内的6个年轻人啦（话说除了我之外剩下5个都是女生…0.0）。 见面会上面太匆忙，然后好像最后也没讲什么就匆匆结束了，等待明天开始正式旅程咯！ Day Two 塔尔寺-贵德黄河源 这天天有点阴沉沉的，不是个好天气啊。 塔尔寺是为了纪念藏传佛教中黄教的创始人宗喀巴大师而建。大师佛法学成是在西藏，然后这座寺庙是他的出生地。吧啦吧啦一堆历史渊源我也记不清楚，就不多说了。 中间许多大殿里面都不允许拍照，佛像什么的本来也都是不宜拍照带走的，所以在这里留下的照片比较少。 寺中的各个院内都能看到五体投地朝拜的人，有的是寺中的僧人在做日常的功课，有的是外来的朝拜者远道而来的。解说介绍有的信徒会从家里一路叩拜过来，在寺外租个房子，然后每天就带上垫子、水、食物等等东西，从开寺之后开始行朝拜，渴了饿了自己吃带过来的东西，一直拜到晚上闭寺才走，日日如此下去，可能要持续好几个月直到完成自己的目标数量。 有的是用佛珠计数，有的比较现代化的也有用电子计数器来计数的。 确实是虔诚啊。 酥油花是塔尔寺的一大艺术品，用酥油捏成的花。由于酥油的熔点很低，所以需要在冰天雪地中的冰水里捏，对寺中的僧人来说也是一种修行。 领队小哥还花了20元在寺里祈求我们接下来几天会有好天气~o(￣▽￣)d 这里是在离开塔尔寺去往贵德的路上，停在国家地质公园的门口休息。 来两发小团体合照~~ 封面的跳图也是在这里拍的。 这一片的丹霞地貌真的是特别好看，红色和黄色的山，我都想好了好多个角度和取景可以拍出特别赞的效果，就是可惜了自己手上没有单反或者四旋翼！！！（好恨啊！！） 路过黄河的源段，河水特别清澈，简直难以想象这个居然是黄河！！！ 由于天气是阴着的，所以水色看上去有点绿。 文艺吧~ 一直觉得这张照片抓拍得非常美~~ 这晚我们住在贵德黄河边，宾馆背后就是滔滔流过的黄河。 翻过铁丝栏，就能走到黄河边上。难以想象我在青海找到了江南水边的感觉（笑~）。 河水非常清澈，中间的水流非常湍急，我还尝了一小口…不过其实没啥味道。 这天的晚饭也是在黄河边吃的，不过可惜的是阴天风比较大，如果是大晴天的话，应该更有一番风味。 大晴天再在黄河边吃西瓜的话，oh，想想都爽~~ Day Three Way to 青海湖！ 一大早起来似是下了点小雨，小雨停了之后天空就变得特别干净，随手拍了一下我们昨晚黄河边住处的红房子。蓝天与红顶，甚是好看。 感叹天地之广阔，遗憾手机只能拍出这种程度的照片来。 最好的伙伴都在身边，最美的景色都在路边。 旅行中最开心的事情就是这两件了吧。 公路旅行，一路向着青海湖。但是当我们沿路发现路边的美景时，就会情不自禁地停车下来休息观赏。 翻越一个山头，其实我们已经身处云端了。拍这几张照片的时候都是恰逢风吹开了我们旁边的云雾，抓拍到手的时候都觉得特别幸运，因为几分钟之后马上就飘过来一大团的云，那之后就什么都看不到了。 等到中午休息的时候天又重新开始阴下来了，看来天公并不作美啊。 下午4点多，我们终于抵达了今天的目的地——青海湖边的旦切大叔家，这里也是我们今晚要住的地方。 旦切大叔是当地的原住民，他家的房子前面就是青海湖，后面是山，山上是草原，草原上养着他们自己家的牛和羊。一切都是那么和谐。 湖边也是有铁丝栏围起来的…不过对于我们来说，翻翻翻已经是家常便饭了。 一直都听说青海湖是内陆最大的盐水湖，当真正亲眼看到青海湖之后…这哪是湖啊？根本就是海了啊！ 玛尼堆是藏民用大小不等的石块、石板和卵石垒成的“祭坛”，也被称为“神堆”。代表了藏民们的祈愿。 此时天空已经开始下雨了，我们在湖边也捡起了好多石头，祈求一定给我们个好天气啊啊啊！！ 从湖边回房间休息时，就看到旦切大叔家的羊吃完草自己从山上下来排队进羊圈了，根本没有人在管它们啊~觉得特别神奇。我们讨论了一下大概是有训练一只头羊，到时间了会把其他的羊带回来︿(￣︶￣)︿ 旦切大叔家的羊肉做得特别好吃，这一晚也吃到了纯正统的奶茶！！味道跟街上卖的原味奶茶一样，但是更香~~原来最原始的奶茶真的是用奶做得啊。然后还有个叫藏粑的当地美食，是用青稞和一些谷物炒出来捏成的，特别赞！ 吃完饭再出来的时候天已经暗下来了，青海湖的湖色随着天空和光线的影响也在发生着改变。 湖边的风浪这时候已经开始大起来了。 再晚一点雨也开始下起来，最后居然……下雪了！！！ 看来第二天能看到青海湖的雪景了吗~~(≧▽≦)/ 旦切大叔是个左撇子，写得一手好藏文~~ 虽然我这次没有寄明信片的准备，但还是觉得这些特别好看，拍了人家写的好多张下来。 Day Four Way to 祁连青海湖边的第二天，原本是想早起看日出的，然而看这天气显然我们看不到日出了…但是打开窗看到的景象却是我这辈子都难以忘记的！！ 本来是想来这里看日出的，却惊喜看到了青海湖的雪景！！ 特地跑到湖边又看了看昨天堆得玛尼堆，也许是受到了青海湖的庇佑吧，虽然没有日出，这样的雪景也是幸运了。 青海湖 我要毕业了 下楼时看到车上的雪堆得太漂亮了，忍不住画了个笑脸。 早饭后玩够了雪，继续上路！今天的目的地是有着天境之称的祁连山脉。 青海湖边的路上也是到处能够看到经幡。 身为一个盐水湖，据说青海湖中唯一的鱼是黄鱼。 沿着一条小道深入祁连腹地，我们从雪地来到了草原。 …这些羊都太警觉了，一看我们下车过去就都跑了╮(╯_╰)╭ 一路上也是美不胜收，几乎是每一眼看到的景色都能拿来做桌面背景，只是遗憾…我手上没有单反没有四旋翼啊啊啊啊！！！（恨！） 回去之后一定要攒钱买其中一个！！ 辽阔草原上的狂欢~ 地势越来越高，这是即将深入雪山之前停留的最后一站。 这是此行最高的一个点，足足四千多米的海拔，站在垭口边上风特别大。 停车在这里休息时看到有个喇嘛在向天空播撒什么祭纸之类的东西吧，马上拿手机拍了一下，居然抓拍到了一点点~ 越过这个最高点之后，接下来就要下山直入祁连山脉了。 穿行在雪山之中，两侧看到的特别像巧克力雪糕上面撒上糖霜…好看又好吃…T_T （单反啊！四旋翼啊！再次怨念！） 抵达住宿的酒店时天色已经较晚了。 本来准备住青海湖的那晚去看星星的，可惜天气问题没有看成，这晚是我离开青海前的最后一晚了，说什么也得出来浪一浪~ 最好是能拍到星星~~ 机智如我们，我和舍友首先在房间里试了下光绘！！！ 固定好相机，然后设置长点的曝光时间，再由另一个人拿手电筒画就好啦~~O(∩_∩)O 我们出来看星星的时候已经是10点多了，可惜祁连县城内灯光还是比较亮，于是我们一路向着暗处走，走到山脚下的阴影处时抬头终于能看到满天繁星了。 满眼的星星，北斗七星什么的都是一眼就能看出来了。满天都是惊喜！ 两个人都是激动得不行，赶紧拿手机查拍星星的方法，然后又是各种测试，终于让我们新技能get了！！！！ 最后把手机和单反的电都用完了才回到宾馆~ 关键在于架好相机，焦距调到无限远，调节ISO到合适，光圈开到最大，然后调节曝光时间在1020s，曝光完成之后就能留下星空图啦╰(￣▽￣)╭ 具有手动模式的手机使用同样的远离也可以完成星空的拍摄。 最为激动的是下面这组图： 那一年我们望着星空 有那么多的灿烂的梦 五月天《星空》 单反在同一个位置拍出来的白天与星空两张图，据说这也是VIVA所有领队中第一次有人拍出来祁连圣山的星空景～(￣▽￣～)(～￣▽￣)～ 然后意外收获是MEIZU手机上面自带的滤镜简直是太逆天啦~ 事实上我上面发出来的好多图都是经过了那个滤镜的处理… Day Five 卓尔山-返程西宁 卓尔山在藏语称为”宗穆玛釉玛”，意为美丽的红润皇后。这也是我们此次行程的最后一站啦。昨天晚上拍到的阿咪东索山就是正与卓尔山隔河相望。 我们在这里停留了相当久的时间。合着身后蓝牙音箱的乐点，一路慢慢向山顶走去，美景尽收眼底。 卓尔山附近还是属于丹霞地貌，我们所在山的一边是雪山的远景，另一边则是红色和黄色相间的壮观景象！！！（再一次怨念四旋翼…） 如此景色，当然要拍拍拍啦！！(ˉ▽￣～) 一路上留下了太多记忆。 几位kind的姐姐妹妹一人送了我一张亲密合照~~ 虽然当时拍照时我是把自己理解成一个背景，然后身边的人一个一个换过去~~还是心跳得不行… 深夜千万不能看系列…肉肉肉肉肉肉！！ 把这几天吃饭时拍的都一起放在这里了。青海出城后到处可以看到草原与牧场，牛羊肉自然也会是当地的主菜了。]]></content>
      <categories>
        <category>玩玩玩玩</category>
      </categories>
      <tags>
        <tag>Gallery</tag>
        <tag>青海</tag>
        <tag>VIVA旅行家</tag>
        <tag>毕业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】功耗对处理器的限制究竟有多大？]]></title>
    <url>%2F2016%2F05%2F07%2F2016-05-07-icpower%2F</url>
    <content type="text"><![CDATA[在知乎上看到一篇文章，讲CPU功耗的，感觉这样的干货还是很不错的。 以下是文章内容： 首先需要澄清的是，这篇文章的作者其实是IC之神-rabaey。rabaey之名无需赘述，上过微电子课程的童鞋想必都被这本《数字集成电路——电路、系统与设计》折磨过，你们的苦难就是拜他所赐。 大神写了新书《low power design essentials》（也不算新，09年的），讲低功耗设计的，今天讲的内容出自该书第一章。 之所以讲第一章，是因为后面的内容没人看得懂。 我们都知道，低功耗设计很重要，尤其是对于移动端处理器以及嵌入式系统。但是功耗对设计究竟影响到了什么程度，相信很多人并没有明确的定量的概念。而rabeay在第一章用一系列数据告诉了我们Power是怎么影响芯片设计的。 上图是统计了近年来主流微处理器（MPU）和DSP的平均功耗。可以看到95年之前，每3年功耗增加4倍；95年之后，每3年增加1.4倍。同时95年前后，移动端应用的快速发展，出现了低功耗处理器的新分支（向下的灰箭头）。 为什么在95年会有功耗增长率减半这个突然变化？因为95年开始，工业界放弃了5V固定电压的设计模式，开始采用等比降压的设计模式。什么叫5V固定电压设计？就是说，早先的处理器供电电压是5V，随着工艺尺寸不断减小，晶体管的阈值电压降低，理论上供电电压也可以减小。但是电压减小意味着晶体管开关速度变慢，IC厂商为了性能考虑，在设计时，即便采用更小的工艺尺寸，照样会保持5V供电电压，带来的后果就是功耗增大很多。什么叫等比降压？就是随着工艺尺寸的缩小，等比例的缩小chip电压。95年左右就是该方案的开始。 上面这张图展示了90年开始芯片电压是怎么变化的。95年之后，在0.35um的工艺中开始采用3.3V供电，此后随着工艺尺寸的不断缩小，供电电压也开始不断降低。在180nm的时候，电压降为1.8V。在130nm的时候，电压降为1.3V。当然了，工艺尺寸2016年已经到达14nm，chip电压可没有降到0.14V。所以，实际情况是随着工艺尺寸进一步减小，chip电压再往下降已经非常困难了。因此，最近几年再次出现了工艺尺寸不断缩小，但是供电电压基本不变的趋势。 为什么工艺尺寸缩小，就一定要降电压？这个就得说到功耗密度（每c㎡的功率）。 上图展示了功耗密度的变化趋势。可以看到，95年之前，chip上的功耗密度与k成3次方正比，95年之后，随着chip供电电压降低，功耗密度开始与k成0.7次方正比。k是工艺尺寸缩小因子，典型值是1.41。为什么是1.41？这其实是摩尔定律决定的参数，反映了工艺尺寸的演进的跨度。180nm的下一代工艺是130nm，180/130就大约接近1.41。 说了这么多，目前我们知道的是，尽管芯片工艺尺寸不断减小，chip的电压也在不断降低，但是功耗密度仍然在不断增加，但是究竟会达到什么程度？ 上图给出了直观的对比。当然了，这是一个很惊悚的图，大概是说按照目前功耗密度的趋势，大概2004年功耗密度将跟核反应一样的程度。2008年达到火箭尾焰的程度。看到这里，我不由得掐指算了算，咦，今年好像2016年了。 上图当然是危言耸听。但是也说明了一个问题，那就是从2004年开始，如果不遏制持续增长的功耗密度，芯片设计将变得不太可能。至少，封装将不太可能。现在的设计普遍认为，功耗密度高于150W/c㎡是应该极力避免的，除非说你完全不在乎封装的成本。 150W/c㎡是个很大的数了，一个2c㎡的chip 能允许的是300W。想象一下指甲壳大小的chip是个300W的热源，有多烫简直不可想象。当然实际的chip远远不到这个功耗就已经烫的不行了。 上面这张图终于给出了我们最关心的主题。根据估算，在20nm工艺下，以前的45nm处理器如果采用1.2V供电，不考虑散热的话，其实运行频率理论上可以达到30GHZ。但是带来的问题就是功耗密度达到惊人的20KW/c㎡，远高于太阳表面的功耗密度。即便运行频率达到10GHZ，功耗密度也达到5KW/c㎡，比火箭的尾焰还热。所以从散热考虑，实际的处理器运行频率都被限制在10GHZ以内，即便晶体管的速度允许其达到10GHZ。 咦？好像又有哪里不太对。我记得Intel的CPU都可以上4G的频率的。按照10G就是5KW/c㎡，4G也妥妥是800W/c㎡，依然远高于我们的阈值：150W/c㎡。那现在的处理器是怎么上4G频率的呢？ 这张图给出了解释。对于2cmx2cm的处理器，供电电压假设为0.6V，系统频率假设可以上10GHZ，那么将会有一个极大的功耗密度。但是我们把条件放松一点，对功耗密度除以5（够仁至义尽了），最终总功耗也有4kW。如果要将chip的总功耗限制在200W，结果就是：整个chip将在任何时候只能有5%的门电路翻转。其余的95%必须不消耗任何功耗，连漏电流也不能有。 这就解释了要满足功耗密度限制，如果想做高性能，你必须牺牲什么。那就是只有很少的一部分逻辑能够翻转。 基于以上我们看到了功耗密度对处理器的限制究竟有多大。由于阈值摆在那里：150W/c㎡，所以处理器的频率以及晶体管翻转率就受到了极大的限制。即便晶体管速度允许达到10GHZ，带来的功耗密度也完全不可容忍。想要提高频率，在总功耗一定的情况下，就只能减少晶体管翻转率。 我们经常可以看到新闻报道，某科研机构研制出来了新的晶体管，速度达到几十G乃至上T赫兹，有望改变计算机前景，而对其功耗只字不提。事实上，根据上述介绍，在不改变性能/功耗比的前提下，这些都是扯淡。未来的工艺技术，需要的绝对不仅仅是更快的开关，还需要在更低功耗下的开关。 当然，还需要更好的封装技术。 比较坑爹的是，封装是有成本的。对于处理器来讲，普通的塑料封装，成本可能只有几美元，但是总功率必须低于3W；高性能的封装，可以允许功率密度达到100W/c㎡，但是需要十几美元的成本。再往上，封装成本越高。 因此，现在的处理器设计其实是一个悖论：想要处理器的性能更高，就必须降低某一频率下的整体功耗。因为只有整体功耗更低，才能允许核心运行更高的频率，才能允许更高的性能。IC设计早已经过了那种性能提高功耗也提高的粗放式增长年代了，在总功耗一定的情况下，性能就是看谁对功耗运用的更合理、更节省。同样的工艺，intel的CPU能上4G散热照样hold住，别人家的CPU则只能上2G或者3G，弱势就很明显。 换句话说，芯片设计者不得不面对的事实是：芯片性能要稳定提高，但是功耗却不能更高，这可真是难啊！在摩尔定律尚未终结的时候，工艺尺寸的不断缩减带来的福利使得这个目标或许不难达成。但是假如工艺尺寸无法继续缩减，漏电流也无法进一步改善，芯片性能还能有提高吗？未来的CPU发展很有可能会是下图的情况，由于总功耗的限制，CPU的性能在有限范围内不断小幅升级，但是终至枯竭，急需新的封装工艺，加工工艺，电池工艺和材料物理的突破，再来一次革命。关于这图，有疑问的只是摩尔定律终结究竟是哪一年，新工艺元年又是哪一年，两者中间有多大的latency而已。 作者：龚黎明链接：https://zhuanlan.zhihu.com/p/20808891来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 本文也转自微信公众号：IC免费课。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>转载</tag>
        <tag>IC免费课</tag>
        <tag>CPU</tag>
        <tag>功耗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS地图插件jvectormap]]></title>
    <url>%2F2016%2F05%2F01%2F2016-05-01-jvectormap%2F</url>
    <content type="text"><![CDATA[我的About页面上面有一张Footmark，用于记录自己走过的地方。 其实我最早的想法并不仅仅是挂个图片而已的，只是除了能按教程弄个博客以外，我没有系统地学过html、css那些东西，就不了了之了。 昨天看到个博客： 翁天信 博主是个不走寻常路的人，他的经历也曾经火过一段时间：仅仅上过2星期小学就退学回家，一直homeschool至今。比较厉害的是他在homeschool期间并没有荒废时间，旅行了很多地方，通过自学在摄影、平面设计、前端领域取得了非常不错的成果。 在他博客上面看到一张足迹地图，正是我一直想要的那种效果。F12了一下，发现是用一个开源的JS插件写的，叫jvectormap。 正好今天是假期，弄着玩吧。 发现 上图来自翁天信同学的网站截图。这个95年出生的年轻人只比我小1岁而已，他所经历的人生轨迹、所设计的东西却都令我感到惊叹。 他从10年开始写自己的个人网站，之后每一年都会用这一年所学的新知识把它重写一遍。14年的这版看上去是最炫的，用了大量动画效果，最后15年这版又重归极简。 从设计理念上可能也能看出来一点他人生思维的变化吧。 扯回到jvectormap这个东西，用chrome的F12工具扫了一下他博客上的地图，可以找到jvectormap-container这个类，猜想这张地图大概是用jvectormap来实现的。 只是可惜直接查看网页的源代码找不到其他有用的信息，地图的位置只能看到一个id为maps的div标签而已。 123&lt;div id="maps"&gt; &lt;h3&gt;我在中国的旅行足迹&lt;/h3&gt;&lt;/div&gt; 没办法从网站上直接找到参考，之后只好通过搜索引擎来进行学习了。 jvectormap 找到如下有用的资料： jVectorMap Home U396的jvectormap专题页 Jvectormap中文帮助文档（API） 第一个是jvectormap的官方页面，里面非常详细地介绍了这一JS插件的功能、文档等等内容，还有全球的地图数据可供下载。 后两个是两个开发着为jvectormap写的中文专题，参考这些资料写出自己需要的足迹效果还是非常容易的。 Hexo对html和js的支持Hexo虽然是个主打轻量的静态博客框架，却不意味着一点动态的玩意都实现不了。 Hexo框架渲染Markdown文件时能够直接兼容里面的html代码这一点我很早就发现了，之前也利用这个特性直接在文章中插进去了音乐和视频等等内容。然后许多非常漂亮的Hexo主题也都是通过JS来实现各种动画效果。 这个是在博客中插入jvectormap的基础条件。 创建一个基本的地图jvectormap是基于jquery来实现的，其实只需要3个步骤就能完成插件的添加： 引用渲染地图所需要的css文件和JavaScript文件css文件用于设定地图的外观，js文件里面则是各种API接口和地图数据 在页面中加入div标签，生成一个地图容器，这个就是上面看到的jvectormap-container 初始化地图数据，指定各种渲染地图所需要的参数 需要引入的文件有4个： 1234&lt;link href="/jquery-jvectormap.css" rel="stylesheet" type="text/css"&gt;&lt;script src="/jquery.min.js"&gt;&lt;/script&gt;&lt;script src="/jquery-jvectormap.min.js"&gt;&lt;/script&gt;&lt;script src="/jquery-jvectormap-cn-merc.js"&gt;&lt;/script&gt; 第一个是jvectormap的css文件，第二个是基础的jquery，第三个是jvectormap，最后一个是地图数据。 第二个可以在jquery的官方网站上下到，其他三个都在jvectormap的官网上可以找到。 然后在页面中需要放地图的地方加入div标签： 1&lt;div id="map" style="width: 600px; height: 400px"&gt;&lt;/div&gt; 高度和宽度不能为0，这样一个地图容器控件就生成了。 接下来需要对地图数据进行初始化，就可以在页面中直接看到生成好的地图了。 可能是我下的jquery的版本问题，网上找到的资料上调用初始化函数的方法都是： 1234567&lt;script&gt; $(function()&#123; $('#map').vectorMap(&#123; map: 'cn_merc_en' &#125;); &#125;);&lt;/script&gt; 但是我这么写就会报错，需要去掉外面那个function的调用即可： 12345&lt;script&gt; $('#map').vectorMap(&#123; map: 'cn_merc_en' &#125;);&lt;/script&gt; map项后面是需要引入的地图数据，这个需要打开包含地图数据的js文件来看里面是怎么写的。 进阶应用完成初始地图的生成之后，接下来可以使用许多参数来改变地图的外观，设定一些响应动作等等，在上面提到的2篇中文资料中已经介绍得非常详细了。 戳右边About页面可以看到我这里的实现方式，我的代码是直接写在Markdown文件里面的，所以查看网页源代码可以直接看到详细的代码情况。 其他额，逛翁天信同学网站的时候还发现个有趣的东西：]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>jvectormap</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPS 的折腾之路]]></title>
    <url>%2F2016%2F04%2F30%2F2016-04-30-qcloud%2F</url>
    <content type="text"><![CDATA[果然经常跟着实验室的师兄混，总能发现点好东西。 以前玩Ingress的时候其实就想买一台自己的VPS了，因为平时用的一些简易的梯子不太稳，很想自己搭一个用，还能往上面挂博客什么的。 后来忙了，Ingress上的也比较少了，平时对科学上网的需求也少了，博客还直接用Hexo+Github搞定了，于是就再也没想过买台VPS能用来干嘛了。 来到科大以后，我发现我们自己实验室里的服务器就多得都没人用了，然后貌似等正式入学之后还能拿学号在科大的学校云上申请服务器空间用╮(╯_╰)╭，巨大的幸福感袭来。 Anyway，今天的主题是这个： 【云+校园计划】 1元=免费域名+专享服务器 VPS腾讯在15年10月启动的这个校园优惠计划，简单来讲就是在腾讯云上面创建最低配置的VPS是65元/月，通过腾讯校园认证之后，腾讯云会每个月发一张64块钱的优惠券，这样就相当于能每月1元使用云主机了。 另外还会每年送一张38元的域名优惠券，应该是在腾讯云上申请域名也是每年38元，这样就相当于白送了个域名。 认证需要通过学信网来完成，优惠只在大学生在校期间有效。 其实国内的VPS服务感觉相对国外来说还是偏贵的，从下面这个知乎提问的回答上来看： 有哪些便宜稳定，速度也不错的Linux VPS 推荐？ 国外好多VPS动辄5$一个月，19.99$一年的价格，相比腾讯云最便宜的也要65元/月，阿里云我看了下也不比腾讯便宜。相较国外的有的差不多能有10倍的差价了。 更重要的是国内的VPS不止贵，而且节点还在墙内。。拿来搭梯子都不行。。-_- 额，写到这里突然发现BAT三家里面两家都做了云服务，没听说过百度的不太应该啊。查了一下果然百度也有个开放云，不过似乎知名度不高的样子，以前我都没怎么听说过这个。 也真是坑啊~~ 不过从另一个角度来看，国外的服务毕竟远，说不定什么时候倒掉了也不一定，流量还有被大墙杀掉的危险性。国内的VPS至少在访问上还是相当稳定的，速度也比较快，而且如果是用来搭网站，经过备案之后还可以开CDN加速，个人感觉体验上会更好一点。 校园计划阿里云也针对在校大学生推出过校园计划，9.9元/月的价格，可惜想要得到这个资格貌似需要抢购？每天只有限量30台的样子。 腾讯云的1元服务器还送域名已经是非常良心了，我之前注册域名的地方是DNSPOD，在14年8月已经直接并入腾讯云了，从域名的解析体验上来看，还是相当不错的，这也让我对腾讯云的服务有着不错的信心。 这份安利我吃了！！ 开始折腾昨天在腾讯云上完成了注册和学生认证（现在用的还是本科的身份认证的，下半年应该需要拿研究生的身份重新认证一次），今天早上就收到了发过来的优惠券，然后果断开始动手了。 搭站的过程其实没什么可说的，整个流程非常简单。 选好服务器节点的位置、配置、操作系统等等参数，再选上优惠券刷个一元钱，就能在腾讯云的管理界面上看到VPS正在生成了。 创建完成的VPS会分配一个公网的IP，可以用之前设定的密钥来登录，如果有多台VPS选在同一个服务器大区里面的话，还能通过内网地址来互相访问。 由于实验室里面的服务器用CentOS的比较多，也相对稳定一些，在师兄的建议下我还是选装了CentOS7.1。 装一些基本的软件，创建个非root账户，然后再配一下自己常用的环境，之后就可以把LAMP环境给搭起来用了。MySql和Php我暂时还没想好要用来做什么，所以准备先装个Apache，考虑可以把Github上的静态博客迁移到腾讯云上去。 Apache比较神奇的是，Apache在CentOS软件库里面不叫Apache，叫httpd。除了Ubuntu Server我没在其他发行版下装过这个，不知道别的是不是也这样。 简单地使用yum指令装上即可。 1$ yum install httpd 然后启动httpd的服务，并添加开机启动。 12$ systemctl start httpd$ systemctl enable httpd 这样就可以了。Apache会自动把/var/www/html/目录映射给80端口，访问服务器的IP地址即可看到其中的页面。 然后要在腾讯云的管理台上打开这台VPS的80端口，我在创建时默认只打开了用来ssh登录的22端口。 在安全组里面添加一个80端口的配置文件，然后把这台VPS的安全组加上即可。 可以直接在浏览器里面输入服务器IP来测试Apache是否生效。 Swap空间云服务器给的1G内存有可能会不够用，例如要在上面跑某些大内存需求的服务时。 配置swap分区/swap文件可以防止当内存用满时操作系统强杀进程的情况。 这里有一篇写得不错的教程： How To Add Swap on CentOS 7 使用这两个命令可以查看当前系统中的Swap空间和内存情况： 123$ swapon -s$$ free -m 不想额外分一个Swap区的话，就简单地创建Swap文件好了。 首先创建一个文件空间： 1$ sudo fallocate -l 2G /swapfile fallocate命令用于在硬盘上预留一部分空间，正常情况下命令运行完了会在根目录生成一个名为swapfile的文件夹，使用ls -lh命令可以查看。 我在服务器上运行的时候不知道是什么问题给我报错了： 1$ fallocate: /swapfile: fallocate failed: Operation not supported google了好久也没有搞明白原因，只好换个方法： 1$ sudo dd if=/dev/zero of=/swapfile bs=512M count=4 dd命令是用于直接按照物理参数来复制文件，if是输入，of是输出，bs参数表示每次写入的大小，count参数表示写入的次数。 创建完成之后改一下权限： 123$ sudo chmod 600 /swapfile$ ls -lh /swapfile-rw------- 1 root root 2.0G May 4 18:12 /swapfile 然后就可以设定Swap空间了： 12$ sudo mkswap /swapfile$ sudo swapon /swapfile 再使用swapon -s和free -m命令查看，swapfile文件应该已经生效了。 要使swapfile永久生效还需要把这个写进fstab挂载文件中，在fstab最后加上： 1/swapfile swap swap sw 0 0 重启机器即可。 Gitlab咳咳，上面搞Swap空间主要是为了弄这个。Gitlab要求安装需要2G内存，以支持至少100个用户同时协作。如果内存不足的话，可以用Swap空间来补充，但是性能应该会比较受影响，当然我目前是准备自己一个人用着玩，应该还是可以承受的。 Gitlab是一个开源的git服务器软件，提供完整的网站页面和git托管功能，简单地说就是个私人定制的Github。 官网上的安装教程非常简单，Gitlab已经集成好了rpm包，只需要添加一下Gitlab的源站网址，然后yum install gitlab-ce即可。 Gitlab的CE版意思为Community Edition，免费但功能不完全，Enterprise Edition则支持得更为完整，例如像Github一样的静态pages功能都有。 如果是手动编译安装的话，可以指定数据库使用MySQL或者PostgreSQL，网页服务器也可以选择Apache或者Nginx，但是直接用rpm包安装的话就没有办法自己定制了，默认使用的是PostgreSQL+Nginx。 Nginx默认用的也是80端口，安装Gitlab的时候需要把Apache（在这里也就是httpd服务）关掉，不然可能影响到成功安装和测试。 不出意外的话，输入地址就能成功打开页面了。首次登录需要设置root的密码。 为了使Apache服务能正常使用（毕竟我还是想把博客移上去），我选择改掉Nginx的服务端口。 找到/etc/gitlab/gitlab.rb文件，其中记录了gitlab的配置信息。需要修改的地方有两个： 123external_url 'http://localhost:16666'unicorn['port'] = 16667 第一个是Gitlab对外服务的地址，默认是80，也就是Nginx的地址；第二个是Gitlab后台使用的Unicorn的服务地址，默认是8080。 重新配置所有Gitlab服务gitlab-ctl reconfigure即可。在腾讯云的安全组上打开16666端口，应该就可以看到配置成功生效了。 其实我原本的想要的结果是多个域名都解析到一台VPS的不同服务上去。比如gitlab.jcf94.com解析给Gitlba，blog.jcf94.com或者jcf94.com解析给博客。 在网址后面加端口号这事看上去就让人觉得不爽，尤其是DNS解析的时候，后面的主机记录没有办法只解析某个ip的端口。DNSPOD倒是提供了一种隐性url的解析方式，可以把gitlab.jcf94.com这样的地址解析给另外一个带端口号的ip，不过这个功能是要收费的。 Google了一下，看来要继续从Nginx和Apache上面下手了。 我用Nginx Apache 共存、Nginx 多域名、Nginx 转发 Apache这样的关键字去找，搜出来一些还算有用的结果，然后综合了一下，最终想出了这样的方法： Nginx监听80端口，httpd监听16666端口，Nginx发现80端口的目标地址是qcloud.jcf94.com时就把服务转发给16666端口，交给Apache来处理，如果80端口的目标地址是gitlab.jcf94.com，那么就自己解决。 好，首先是改端口，Nginx的端口修改方法在上面已经写了，Nginx改成80，unicorn还是16667吧，改完gitlab-ctl reconfigure一下，会重新生成包括Nginx在内的配置文件。 然后是Apache的端口，修改/etc/httpd/conf/httpd.conf文件，找到Listen 80改成Listen 16666即可。 然后在/var/opt/gitlab/nginx/conf下新增一个配置文件apache.conf： 12345678910111213141516171819202122server &#123; listen *:80; server_name qcloud.jcf94.com; server_tokens off; location / &#123; proxy_read_timeout 300; proxy_connect_timeout 300; proxy_http_version 1.1; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; proxy_pass http://localhost:16666/; proxy_redirect default; &#125;&#125; 主体是我直接拷的原来的gitlab-http.conf然后改的，关键的内容在于server_name和后面的proxy_转发配置上。 然后对gitlab-http.conf也需要作一下修改，找到server_name加上gitlab.jcf94.com即可。 我直接重启了Nginx服务，发现没有生效… 原来Nginx加载时其实读的是这个目录下的nginx.conf，在这个配置文件中有一句引用gitlab-http.conf的话，那么在下面仿照这个样子把新创建的apache.conf也引用进去就好了。 gitlab-ctl restart nginx单独重启Nginx的服务，然后测试gitlab.jcf94.com和qcloud.jcf94.com的情况。 完美！！ 注：每次gitlab-ctl reconfigure之后，gitlab-http.conf和nginx.conf都会重写，这里修改的内容都要重新改一遍。想要永久生效应该是需要改掉/etc/gitlab/gitlab.rb里面的某些东西，不过我还没找到合适的修改方法。 话说其实没事一般也不会reconfigure，所以这个问题应该不会成为太大的困扰吧。 转移博客开始着手把博客移到VPS上。 其实想了一下，只用Nginx已经足够支持我原来的静态博客了，非要上Apache主要是为了以后能往上面放一点php的东西上去。（至于未来到底会不会真的去折腾php就要以后再看啦~）。 网上推荐的教程都是在服务器上创一个git的用户，然后在git用户下建立远程库地址，但是我这台VPS由于前面装过Gitlab，结果git帐号已经被改掉了，我也不知道这玩意默认的密码是什么，于是决定直接放到我自己的用户目录下好了。 1234$ cd ~$ mkdir blog.git$ cd blog.git$ git init --bare 后面的--bare选项必须要加上，否则创建的不是远端仓库的格式，没有办法接受客户端传上来的数据。 然后可以测试一下上传是否成功，打开hexo的站点配置文件_config.yml，把服务器的仓库地址加到depoly列表里面去： 1gitlab: jcf@qcloud.jcf94.com:blog.git,master hexo d上传成功即可。 然后由于这只是在我的用户目录下，接下来要做的是把上传上去的文件复制到Apache的网页目录下，这个可以交给Git Hooks来自动完成。Git Hooks是一些运行脚本，可以设定在git服务器和客户端交互的多个阶段自动执行。 打开~/blog.git/hooks目录，里面应该有一些写好的sample，不过对我们来说暂时没什么用，创建一个名为post-receive的文件： 12345678910#!/bin/bash -lGIT_REPO=/home/jcf/blog.gitTMP_GIT_CLONE=/tmp/blogWWW=/var/www/htmlrm -rf $&#123;TMP_GIT_CLONE&#125;git clone $&#123;GIT_REPO&#125; $&#123;TMP_GIT_CLONE&#125;rm -rf $&#123;WWW&#125;/*cp -rf $&#123;TMP_GIT_CLONE&#125;/* $&#123;WWW&#125; 脚本的内容非常简单，即首先从blog.git中clone出能够正常读写的文件，然后把文件复制到Apache的网页目录下。 处理一下权限问题： 123$ chmod +x post-receive$$ sudo chmod 777 -R /var/www/html 一个是post-receive这个文件的运行权限，另外就是对html目录的读写权限，为了省事我直接777了。。。简单粗暴。这样就完成了全部的设置。 可以sh post-receive测试一下脚本是否可用，之后每次hexo d上传成功之后，都会执行这个脚本的内容。 当然，博客移到VPS之后，后面还需要处理一下域名解析等等其他的小问题，这个就不详细记录了。 例如做一下博客站双机备份切换，原来用的是Github+Gitcafe，现在用VPS作主站，Github Pages仍然可以作为备份站来使用。 VPN跳板有时候需要在校外远程vpn到实验室的机器上，实验室本来是有搭好了vpn可以直接用的，但是坑爹的是当地的网络可能会出现各种问题：例如路由器没开vpn穿透、本地存在重名ip等等。然后就连不上。 试了下腾讯云跟实验室之间是通的。 最后想到个办法就是ssh连到腾讯云上去，腾讯云连实验室vpn，然后再ssh到机器上工作，相当于把腾讯云做了个跳板。 首先确定软件都已经装上了： 1$ sudo yum install pptp pptp-setup 需要用到的主要是这个pptpsetup，设置的方式很简单： 1$ sudo pptpsetup --create VPN_Name --server VPN_IP --username VPN_Username --password VPN_Password --encrypt --start 最后两个选项--encrypt是采用加密，--start是立即启动，只是创建配置文件的话可以不加--start。 创建好文件之后用： 1$ sudo pppd call ACSA 即可开启vpn，然后可以用ifconfig看看出现了ppp0就连接成功了。 不过这样还不能直接用，因为路由没走vpn，虽然连上了但是流量还是按照原来的路由走，需要手动加上路由。 1$ sudo ip route add xxx.xxx.xxx.xxx dev ppp0 把实验室那几台机器的ip加上即可。 添加default和0.0.0.0的话就是全局用vpn，我测试的时候发现全局转完之后我电脑和腾讯云的ssh就断了，还好在腾讯云网页上的终端还能进，赶紧把网络重置了，差点就悲剧了 顺手写了两个用于开关VPN的脚本： 开 1234567891011121314151617181920212223#! /bin/bash# Start The VPNVPN=`ifconfig|grep ppp0`if [ -z "$VPN" ]then sudo pppd call ACSAfiVPN=`ifconfig|grep ppp0`while [ -z "$VPN" ]doVPN=`ifconfig|grep ppp0`done# NODE 7sudo ip route add 114.214.166.246 dev ppp0# SUGONsudo ip route add 202.38.79.157 dev ppp0 关 123456789#! /bin/bash# Stop The VPNsudo killall pppd# Reset The Networksudo systemctl restart network 后话从ping值上来看，腾讯云的地址响应速度比Github的地址要快上很多： 这张图是在我迁移博客之前截的。 腾讯域名解析虽然我原来已经在DNSPOD上面买过域名了，本着不用白不用的原则，迅速建了个域名。 腾讯云上的域名解析虽然写着说是基于DNSPOD，但是感觉这个功能特别简陋，几乎没有任何进阶的设置功能。然后解析能够支持的记录类型也不如DNSPOD多。 不知道是不是这个域名的实名审核还没结束的问题，添加一条CNAME记录之后一直没有生效。在DNSPOD上面加完几乎是刷新几遍的时间就能正常使用了。 似乎腾讯云上面注册的域名不备案真的解析不了…完成实名审核之后还是没有生效。只好把这个域名放到DNSPOD里面去解析了。所有权还是在腾讯云上，解析交给DNSPOD来完成，完美解决。 如果这台VPS只是拿来挂个博客，感觉确实是有点没意思哈哈。 VPS有什么有趣的用途？ 再后话本科毕业前夕，我问过客服升学之后能不能继续，他们说到时候重新验证一下就行了。所以我在中间补上了2个月的费用，结果后来系统上就更新了，升学不能重新认证。 然后就呵呵了。 最后觉得，有需求还是花点钱上个国外的 VPS 吧，现在拿了 Github 的学生优惠，在 DigitalOcean 上开了一个。 再再后话 2017 年更新 Google Cloud Platform 上面提供一个一年期 300 刀的试用服务，在控制台的 Compute Engine 里面，貌似每个新的谷歌账号都能享受。 嗯，上面那个 Google 云的控制台地址在国内是被墙了的。 east-asia 的台湾机房速度快的飞起！！！ 目前我的服务已经全面迁移到这个上面。 Apache 单 ip 多域名除了这个博客以外，还放了其他的一些页面在 VPS 上，幸好 Apache 本身就是支持这种多域名的解析的，而且配置起来很方便，这个功能在 Apache 里面叫 VirtualHost。 在/etc/httpd/ 目录下面新建一个 conf.vhost.d 的目录，里面就用来放不同域名的 VirtualHost 配置文件。 在/etc/httpd/conf/httpd.conf末尾加上： 1IncludeOptional conf.vhost.d/*.conf 这样 conf.vhost.d 目录下的所有配置文件就能起效了。 然后对每个域名写个配置文件就好了，例如jcf94.conf： 123456789101112&lt;VirtualHost *:80&gt; Serveradmin jcf94@outlook.com ServerName jcf94.com DocumentRoot /var/www/html &lt;Directory "/var/www/html"&gt; Options FollowSymLinks AllowOverride All #Require all denied Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt; 别的域名： 1234567891011&lt;VirtualHost *:80&gt; ServerName www.xxxx.com DocumentRoot /var/www/xxxx &lt;Directory "/var/www/xxxx"&gt; Options FollowSymLinks AllowOverride All #Require all denied Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt; 再再再后话 2018-03-04 更新 Google 云快到期了，所以最近正在着手找下家。 DigitalOcean 以前用了倒是还不错啦，但是以前开过的旧金山 1 号机房没货了，据传 SFO2 速度不太好，于是现在在尝试 Vultr 的日本机房。 话说 Google 云的台湾机房真的是很满意的，ping 值低的飞起，网速快的飞起，就是流量要单独收费，算下来一个月平均要 7、8 刀左右了，稍微多用点可能就要 10 刀了，想想还是算了。 现在开了 Vultr 的东京机房，比较坑爹的是日本 ip 好像没了，刷了几次刷出来的都是加拿大的，解析的时候要绕一圈，不过速度上测了一下还是要比其他地方稍微好点。 准备之后长期用了。 每月 5 刀，25G 的 SSD，1G 内存，1 T 流量，还是挺便宜的了。部分地区能买到 2.5 刀每月的机器。（这时候只有纽约和迈阿密机房有货，但是试了下这两个地方给我的 ip 似乎都被墙了） 最后扔个注册的推广链接好了 Seafile从 Google Cloud 移出来之后其实还有个好处，1 T 流量啊！1 T 流量！ 平时很少看 YTB，科学上网对我来说主要还是在 Google 上搜搜资料，Google Play 更新一下软件，然后科学领个 Steam 游戏什么的，基本上流量用的特别少。 于是顺手用 Seafile 建个网盘，平时稍微传传东西。 官方的中文手册写的有的地方不太清楚，不过基本上搭这个还算容易。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>云服务</tag>
        <tag>腾讯云</tag>
        <tag>qcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSYS2-Windows下的类Unix环境]]></title>
    <url>%2F2016%2F04%2F25%2F2016-04-25-msys2%2F</url>
    <content type="text"><![CDATA[生命不息，折腾不止。 最近感觉Linux越用越顺手了，尤其是Linux下的shell（此处可以特指一下目前正在用的zsh…）甩了Windows的cmd好多条街。 我平时在Windows下也经常用命令行，之前装了个Git for Windows之后，里面自带的ls/ssh/grep等等基本上已经成了我Windows下常用的命令了。 以前没怎么注意，昨天看了下Git for Windows本身就是用MSYS环境搭的，想想干脆装一个完整的MSYS2好了。 介绍MSYS是基于MinGW开发的一个类Unix环境，MinGW可以认为是Windows下的GNU环境，也是一直以来我就在Dev和Code::Blocks上面用的东西。感觉是相当于在Windows下面用MinGW把Linux下GNU写的东西再实现一遍。 相对的，还有个Unix环境叫cygwin，比起MSYS来说cygwin更接近正常的Linux，它是依靠一个中间层把Posix调用转换成Windows的API，就像一个虚拟机一样。可能运行速度会慢一点，不过已经跟普通的Linux没什么区别了，而MSYS可能会弱很多。 问题是MinGW和MSYS更新得比较慢，然后就有人开发了MSYS2，它是fork了cygwin比较新的开发版，然后用MinGW重新实现了一部分内容而成，保持了MSYS的轻巧，同时功能上也不比cygwin弱多少。 更加重要的是，MSYS2用pacman作为包管理器！这个对于一个刚转到Arch Linux，然后逐渐开始习惯Arch的环境的人来说简直就是送上门来的好事。 更多详细介绍，详见知乎上的这个问题： Cygwin和MinGW的区别与联系是怎样的？ 开始折腾MSYS2的官网居然是搭在github pages上面的，跟我的博客一样，一看这地址就觉得亲切了。 下载安装包，然后装上。 打开msys的shell之后首先升级一下pacman，然后就可以愉快地Syu了。 12$ pacman -Sy pacman$ pacman -Syu 看到pacman这个熟悉的指令，感觉更亲切了~ Syu可能需要多次，第一次的时候升级了文件系统等等工具软件，结束后需要直接关闭shell软件然后重启继续更新。 然后把常用的一些东西都装上： 1$ pacman -S git zsh vim 把MSYS目录中的usr/bin加到环境变量里面去，在正常的cmd下面就也能跑其中的软件了。 装好vim之后可以把软件源改成科大的。 zshMSYS自带的几个exe文件都是生成好的，默认用的是bash，我试了下正常打开之后找不到Linux下chsh等等命令，因此需要在启动mintty的时候直接加上使用zsh的参数。 MSYS目录下提供了几个bat和cmd的脚本，用于添加环境和启动shell。 直接修改start_shell.cmd，找到startmintty下面启动的参数，把bash改成zsh即可。之后要使用msys2_shell.bat这个脚本来启动MSYS的环境。 为了能在win+R窗口中直接打开MSYS，我给msys2_shell.bat脚本做了个软链接，win+X+a管理员权限启动cmd窗口，然后切到MSYS的usr/bin目录里面： 1c:\msys64\usr\bin&gt; mklink msys.bat c:\msys64\msys2_shell.bat 这样就创建了一个名字是msys.bat的软链接，由于上面已经把usr/bin加到系统环境变量中了，现在可以在win+R的窗口中直接输入msys就能正常打开MSYS的shell了。 有个问题是在cmd下输入msys的话，会跳出来： 1'"c:\msys64\usr\bin\start_shell.cmd"' 不是内部或外部命令，也不是可运行的程序或批处理文件。 这样的东西。看来这两个的执行机制稍微有点区别啊，感觉还是怪怪的，让人不太舒服，等等想办法解决一下。 或者直接在cmd里面输入zsh --login即可打开加载了完整环境内容的zsh。 SHELL变量打开MSYS时，想用echo $SHELL输出shell信息时是空的，需要改改把信息加上去。编辑/etc/profile文件，找到： 1234elif [ ! "x$&#123;ZSH_VERSION&#125;" = "x" ]; then HOSTNAME="$(/usr/bin/hostname)" profile_d zsh PS1='(%n@%m)[%h] %~ %% ' 在这个if语句里面加上SHELL=\which zsh``即可。 Linux中的软链接“ln -s”然后是MSYS因为受到Windows机制的影响，默认使用ln -s做软链接的时候直接就变成复制了，也需要在/etc/profile文件里面加上一个关键变量： 1export MSYS="winsymlinks:lnk" 之后ln -s就正常了。 访问其他目录在MSYS里面查看当前的文件系统是这样的： 1234$ df -hFilesystem Size Used Avail Use% Mounted onC:/msys64 100G 76G 24G 77% /D: 832G 715G 117G 86% /d MSYS的安装目录作为当前Linux环境的根目录，然后我的D盘挂在/d下面。在这样的环境中，Linux的挂载命令就非常好用了，建个文件夹然后直接用mount把目标目录挂上去就可以了，比如C盘： 12$ mkdir ~/c$ mount c:/ ~/c 即可。 刚才作了个死，吓出一身冷汗。测试挂载的时候刚好也在试软链接，然后一下忘了c这个文件夹是C盘的挂载点了，还以为是软链接就直接rm -rf下去了…还好C盘里面的东西MSYS没有权限，所以它只删掉了自己的安装目录…删掉了自己的安装目录…自己的安装目录…安装目录！！！！只好卸了重装了一遍MSYS 以后用rm命令一定要特别特别特别小心！！！ 右键添加“在此打开MSYS2”选项win+R输入regedit打开注册表管理器，在[HKEY_CLASSES_ROOT\Directory\Background\shell]下新建一个项，命名为msys2，值的内容是“Open MSYS2 Here”； 在[HKEY_CLASSES_ROOT\Directory\Background\shell\msys2]下新建一个项，命名为command，值的内容是“c:\msys64\msys2_shell.bat” shift+右键菜单里面本来也就有一个打开cmd的选项，用那个感觉更方便一点，把需要加载的环境变量内容直接写在.zshrc里面，然后在cmd中不加--login选项运行zsh就好。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>MSYS2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[USTC Spring Outing 2016]]></title>
    <url>%2F2016%2F04%2F18%2F2016-04-18-spring%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Master-of-Science</category>
      </categories>
      <tags>
        <tag>Gallery</tag>
        <tag>USTC</tag>
        <tag>春游</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一段差点被Gcc逼疯的历程]]></title>
    <url>%2F2016%2F04%2F15%2F2016-04-15-gcc%2F</url>
    <content type="text"><![CDATA[在Windows下，或是像Ubuntu、Arch这些拥有出色的软件包管理器的Linux发行版下，似乎从来都是不需要怎么考虑如何安装Gcc的问题。几个键就好了。 有一天，需要在没有root权限、然后还不能用包管理器的情况下安装Gcc时，就可能遇上各种奇怪的问题。 我需要在多个Linux服务器集群上测试OpenFOAM这个软件，要用Gcc来编译安装。OpenFOAM要求编译用的Gcc版本要高于4.5.0，而测试用的几个服务器上装的是稳定性比较好的比如CentOS和SUSE这样的系统，尤其CentOS中默认的最高版本Gcc也只有4.4.7。因此需要手动编译安装更高版本的Gcc。 之前在2个服务器上使用OpenFOAM提供的安装脚本装过Gcc，中间虽然也不太顺利，最后还是成功装上了。今天又有在一台新系统上测试的工作需要，上面装的是CentOS 6.7，而且似乎系统上的环境不太完整，折腾了安装脚本快一整天都没把Gcc装上…-_-。 最后让师兄捣鼓了一下，似乎还是有问题，只好决定放弃OpenFOAM提供的脚本，自己手动安装Gcc，成功之后再好好分析一下脚本里面到底有什么问题。 写这篇博客的时候，还连着ssh在服务器上编译中。 依赖问题Gcc官网的说明文档中详细地介绍了编译Gcc需要的依赖工具、库等等。参考目前常见的Linux系统来看，需要准备的最基本的东西有gcc、mpc、mpfr、gmp的源码包、一个低版本的Gcc，以及一些其他需要的依赖库例如glibc、zlib等。 通常一些基本的工具，例如flex、bison等等基础编译工具包，应该在大部分的系统环境中都是有的，没有它们Gcc仍然可以编译，但是之后使用Gcc时可能因为缺少它们而产生各种奇怪的错误，不知道编译时是不是会对它们添加一些依赖什么的，最好还是装上。 如果安装时还发现缺了其他什么东西的话，只能一个一个补上了。 在编译或者安装时做好日志记录非常重要，这样一旦发生错误，可以在结束后马上从log文件中查到报错的原因，然后针对错误来进行修复。养成加log的习惯，在运行命令后面加上： 1$ xxxx 2&gt;&amp;1 | tee log 即可，2&gt;&amp;1是把错误输出跟正常输出导到一起，然后通过tee写入log文件，同时在屏幕上继续输出。 编译安装Gcc的三大件前面已经提到过了，而它们在安装时也存在依赖关系，mpfr需要依赖gmp，mpc则是依赖gmp和mpfr，因此安装的顺序是gmp，mpfr，最后才是mpc。 这部分的安装一般不会有什么问题，按照顺序编译安装、并且注意指定上一个依赖的位置即可。 gmp 12345$ tar jxf gmp-5.1.2.tar.bz2$ cd gmp-5.1.2$ ./configure --prefix=$HOME/opt/gmp$ make$ make install mpfr 12345$ tar jxf mpfr-3.1.2.tar.bz2$ cd mpfr-3.1.2$ ./configure --prefix=$HOME/opt/mpfr --with-gmp-lib=$HOME/opt/gmp/lib --with-gmp-include=$HOME/opt/gmp/include$ make$ make install mpc 12345$ tar zxf mpc-1.0.1.tar.gz$ cd mpc-1.0.1$ ./configure --prefix=$HOME/opt/mpc --with-gmp-lib=$HOME/opt/gmp/lib --with-gmp-include=$HOME/opt/gmp/include --with-mpfr-lib=$HOME/opt/mpfr/lib --with-mpfr-include=$HOME/opt/mpfr/include$ make$ make install 然后需要在LD_LIBRARY_PATH的路径中把gmp、mpfr、mpc的动态库地址加上，虽然下面gcc的configure里面也需要指定3个依赖软件的位置，但是不加的话会出现各种奇怪的错误。 1$ export LD_LIBRARY_PATH=$HOME/opt/gmp/lib:$HOME/opt/mpfr/lib:$HOME/opt/mpc/lib:$LD_LIBRARY_PATH gcc 12345$ tar zxf gcc-4.8.4-tar.gz$ cd gcc-4.8.4$ ./configure --prefix=$HOME/opt/gcc --with-gmp=$HOME/opt/gmp --with-mpfr=$HOME/opt/mpfr --with-mpc=$HOME/opt/mpc --enable-languages=c,c++ --disable-multilib --enable-threads=posix --disable-checking$ make -j4 # Gcc相对比较大，最好采用并行编译，否则会需要很长时间来完成$ make install 如果没有错误的话，之后将gcc的目录加到环境变量中即可使用。 编译过程中可能报的各种错最常见的大概是找不到-lz、找不到libc等等，需要根据情况补装zlib和glibc的库。 有时候zlib的64位和32位库都可能会缺，因此都需要补上。 虽然没有root权限无法调用yum来进行快速安装，但是yum库还是可以使用的，比如要找一个zlib的64位库： 1$ yumdownloader zlib.x86_64 会将yum软件库中的zlib-1.2.3-29.el6.x86_64.rpm包下载到当前目录，然后使用rpm2cpio命令可以将这个包直接解压到当前目录下： 1$ rpm2cpio zlib-1.2.3-29.el6.x86_64.rpm | cpio -idvm 将解出来的例如lib64、usr/lib64等等目录添加到环境变量中即可。 OpenFOAM中的Gcc安装脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546## Build GCC# might need 32-bit glibc-devel headers to compile# E.g. on ubuntu install g++-multilib#echo "---------------"if [ -d $GCC_ARCH_PATH ]then echo "Already built: $gccPACKAGE"else echo "Starting build: $gccPACKAGE" echo( sourceDIR=$WM_THIRD_PARTY_DIR/$gccPACKAGE buildDIR=$buildBASE/$gccPACKAGE cd $sourceDIR || exit 1 make distclean 2&gt;/dev/null unset withMpc [ -d "$MPC_ARCH_PATH" ] &amp;&amp; withMpc="--with-mpc=$MPC_ARCH_PATH" rm -rf $buildDIR mkdir -p $buildDIR cd $buildDIR set -x $sourceDIR/configure \ --prefix=$GCC_ARCH_PATH \ --with-gmp=$GMP_ARCH_PATH \ --with-mpfr=$MPFR_ARCH_PATH \ $withMpc \ --with-pkgversion=OpenFOAM \ --enable-languages=c,c++ \ --enable-__cxa_atexit \ --enable-libstdcxx-allocator=new \ --with-system-zlib \ MAKEINFO=missing \ &amp;&amp; make -j $WM_NCOMPPROCS \ &amp;&amp; make install \ &amp;&amp; echo "Built: $gccPACKAGE") || &#123; echo "Error building: $gccPACKAGE" exit 1&#125;fi 看上去应该没有什么特别的问题，大概关键就在configure那里的几个我没见过的参数上面吧，下次详细分析一下Gcc安装目录下面的configure文件再补充。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Gcc</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arch Linux over Yoga 3 11]]></title>
    <url>%2F2016%2F04%2F10%2F2016-04-10-arch%2F</url>
    <content type="text"><![CDATA[“你最喜欢的Linux发行版是什么？不要跟我说是Ubuntu。” “额…用过的不多，就是Ubuntu了。” 然后就被安利成功了…… 简单介绍一下Arch是一个相当简单和轻量级的发行版，官方主打的目标也就是想要把它保持得简单点。 但是Arch中的软件库管理却做得非常出色，秒杀apt-get和yum等等好大一截，以至于成功被安利并且使用了一段时间之后，真心不想再切回其他的了。 Arch除了官方的软件库之外，还提供了很多社区维护的软件甚至还有很多用户提供的安装脚本。比如很多本身还在Github上面处于开发阶段的软件，只要Arch的库中有人给它写过安装脚本，都能通过Arch的软件库命令一键安装，中间缺少的各种依赖都会自动补全。堪称神器。 Arch给我的感觉相当极客，它的发行版采用滚动更新的方式，不像其他发行版会有版本号什么之类的，而且始终保持里面的软件是最新的。（CentOS默认的gcc版本一直在4.5以下没变过好像，其他的发行版也有类似的情况，都是保证某个版本稳定适用。Oh，装完Arch，下完gcc，习惯性地看一下“–version”，返回来个5.3吓我一跳。） 然后就是系统特别干净，除了Linux最基础的东西以外，其他的几乎什么都没有加进去，由用户根据自己的需求来定制。 安装过程在虚拟机上测试了一下之后，果断在Yoga上装上了Arch。 记录这个是方便以后自己再装的时候可以不用再去网上找别的教程。 在空U盘上刻好Arch的镜像，在Yoga的BIOS里面关掉SecurityBoot，然后就可以引导进安装盘了。 相对Ubuntu这些发行版来说，Arch的精简和轻量级简直令人发指。它安装的时候只给了一个root登录的shell，然后需要通过命令来完成整个安装过程。 shell登录之后目录下只有一个install.md的文件，其中记录了安装流程，可供安装过程中阅读使用，这些内容跟Arch官网Wiki中的Installation Guide是一样的。额，然而我觉得还是另外开个电脑上看Installation Guide比较好，上面有其他详细内容的页面链接。 按照教程上的提示安装过程大致是以下几个部分： 1 Pre-installation1.1 Set the keyboard layout1.2 Connect to the Internet1.3 Update the system clock1.4 Partition the disks1.5 Format the partitions1.6 Mount the partitions 2 Installation2.1 Select the mirrors2.2 Install the base packages2.3 Configure the system2.4 Install a boot loader2.5 Reboot 3 Post-installation Pre-installation这部分主要是安装前的准备工作，比如检查硬盘情况，设置网络等等。 键盘和时间设置可以跳过，到后面安装的过程中会另外设置。 首先要检查网络，Arch的安装对网络的要求非常高，因为需要在安装过程中直接下载/更新一些软件，如果网络不太好，那… 还是不要装了吧。。。 最简单的情况是连在路由器上的有线网，最好是不需要什么特殊的登录等等的要求，直接插上网线就能用的情况。 以下介绍的部分都是建立在网卡能直接被安装shell驱动的情况，如果驱不起来…那就over了，查查wiki怎么解决吧 在shell中调用： 1$ ip link 可以查看当前的网络硬件情况，比如网口在里面显示的是eth0，那么： 1$ ip link set eth0 up 应该就能启动了，然后 1$ dhcpcd 自动获取完地址之后应该就能ping通外网了。 Yoga上面没有自带网口，我也没有USB网卡可以用，幸好Arch的安装shell中也包含了无线网的连接工具。 首先使用： 1$ wifi-menu -o 这个工具会扫描区域内的无线网信息，设置好连接的SSID和密码等信息之后生成连接网络用的配置文件。使用时会弹出一个对话框界面，非常方便。 生成的配置文件会被写在/etc/netctl/目录下，以网卡编号命名的一个文件中。把这个文件复制一份或者直接改名成wireless-wpa。 之后，运行： 1$ netctl start wireless-wpa 就能连接之前设置好的无线网了。 之后是对硬盘的检查和分区。 1$ fdisk -l 可以显示当前硬盘上各个分区的情况，然后根据需要把分区格式化成ext4格式，主要用到的几个指令是： 1234567$ cfdisk 磁盘分区$ mkfs.ext4 xxxx 把分区格式化成ext4格式，后面跟上分区表，一般是/dev/sda1、/dev/sda2这些$ mkswap xxxx 把分区格式化成交换分区，后面跟上分区表$ swapon xxxx 启用交换分区，后面跟上分区表 分区结束之后，开始挂载各个分区到系统中： 1$ mount xxxx yyyy 把硬盘上的xxxx挂载到yyyy目录下 首先挂载”/“目录，然后在目录下创建其他目录，并继续挂载其他的部分，比如我的Yoga下： 1234$ mount /dev/sda8 /mnt$ cd /mnt$ mkdir home$ mount /dev/sda7 /mnt/home 然后开始正式安装的过程。 Installation为了后面能够更快地安装，在正式安装的时候可以修改一下/etc/pacman.d/mirrorlist文件中记录的安装源地址，删掉国外的，留一个速度最快的国内源就好…比如科大源。 然后输入： 1$ pacstrap /mnt base 命令，开始安装基础的Linux。命令执行结束之后可以看到/mnt目录下面已经出现了常见的Linux根目录结构了。 调用： 1$ genfstab -p /mnt &gt;&gt; /mnt/etc/fstab 生成分区的挂载表。 至此，在安装shell上的工作全部完成，并且已经安装到硬盘上的Linux系统也已经初具雏形了，调用以下指令将根目录切换到硬盘上去，之后所有的工作都是在已经安装好的新Linux系统中完成： 1$ arch-chroot /mnt /bin/bash 最后的bash也可以不加，这样切换过去之后默认用的是sh作为shell。 设置计算机的名字： 1$ echo Yoga-arch &gt; /etc/hostname 设置时区： 1$ ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 用vi打开/etc/locale.gen文件，把需要用到的语言从里面注释掉，一般常用的是en_US和zh_CN，选择UTF-8的就行。 然后调用： 1$ locale-gen 生成语言信息。 还要在配置文件中加一下： 1$ echo LANG=your_locale &gt; /etc/locale.conf 这些全部设置好之后就可以开始最后的系统安装了： 1$ mkinitcpio -p linux 至此，然后可以改一下root的密码，至此，Arch linux系统安装结束。 然而，系统虽然是装好了，引导还没添加。之后可以选择自己常用的引导工具来引导Arch。 当然这些在Arch里面是没有的，需要自己安装： 1$ pacman -S grub efibootmgr os-prober grub是linux下最常用的引导软件，efibootmgr用于efi模式的支持，os-prober用于处理多系统的引导。 123456$ cd /boot$ mkdir efi$ mount /dev/sda2 /boot/efi$$ grub-mkconfig -o /boot/grub/grub.cfg$ grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=grub 首先将efi分区挂载到系统中，然后使用grub-mkconfig将整个硬盘上的系统信息写入配置文件，然后使用grub-install安装引导。 重启即可引导进入Arch！！！ ！！！ 然而这时候得等一下 ！！！ 要是什么都不注意直接重启可能就会悲剧 前面说过新装好的Arch里面什么都没有，可能连网络连接的工具都是不完整的，这个时候最好检查一下网络管理软件netctl等等是不是已经装上了，否则重启之后可能没有办法连上网。 Post-installation装好Arch之后就是各种驱动，图形界面等等的安装了，记录一下通常需要做的事情： 1234$ pacman -S alsa-utils # 声卡驱动$ pacman -S xorg xorg-xinit # X图形环境$ pacman -S gdm gnome # gnome图形界面以及gdm启动器$ systemctl enable gdm # 设置gdm开机启动 然后重启一遍，进入系统就OK了。 后话Yoga对Arch的支持还算可以，基本的设备都是能够驱动起来的，几个Fn键也都是正常的，目前还没有解决的问题是触摸屏不支持多点触控，触控板也不太好用，总体上还是比较满意的。 记录一下Arch下常装的几个工具： yaourtArch默认的软件库管理工具是pacman，yaourt是在pacman的基础上做的一个辅助工具，安装完之后语法与原始的pacman相同，但是功能更为强大。 安装方式是在/etc/pacman.conf中加入yaourt的源 123[archlinuxfr]SigLevel = NeverServer = http://repo.archlinux.fr/$arch 然后同步一下源信息并安装即可： 12$ pacman -Sy$ pacman -S yaourt zsh通常bash是最为常用的shell，zsh搭配“oh my zsh”的配置脚本…只能说非常强大。 回头可以专门写一篇讲zsh的。 xorg/wayland+gdm+gnome图形界面。 file-rollergnome下解压用的插件。 在Yoga上的Bug记录触摸屏的多点触控问题装了不少驱动，最好还是发现Linux对触摸屏的支持不算太好，其实多点触控已经驱起来了，但是没有对应的动作可以执行。 Wiki上写了一个叫Touchegg的软件可以配置多点触控的动作。 搞了半天装上之后发现还是不能用，虽然成功地识别了多点，然后在一个文档里面找到了一句话大意是说pinch（捏）等等动作支持没完成什么的… 坑爹呢这是！ 论坛上有人说gnome和某些应用已经支持了多点触控，回头找找看官方的说明。 4月15号更新 Oh，我的天！ 晚上回来yaourt -Syu一下更新了一堆东西，比较重要的东西有xinput的输入驱动，然后chrome也更新到了版本号50。打开chrome之后惊喜地发现多点触控能用了，双指放上去可以成功缩放页面了。 不知道是驱动更新了还是chrome的功能。 应该是chrome的功能，因为我现在在vs code下面开了双栏，然后左右不能够独立触摸滑动，我记得windows下是可以的。 之前在chrome下做过一些设置，可惜49版没有什么效果，在chrome地址栏输入： 1chrome://flags 然后跳出来的页面中开启Enable touch events项。 NetworkManager在gnome下显示故障4月9号还是10号左右的时候gnome有过一次更新，新版本后NetworkManager在设置里面打开就出问题了，各种显示不正常。原本以为是我的Arch装了什么奇怪的东西起冲突了，后来发现实验室的台式机也是这个状况，找到原因是gnome更新带来的bug。 4月13号gnome再次有个小版本更新，故障消失，不得不赞一下Arch滚动更新之迅速，一有bug应该马上就会有人上报。 gdm下触控板只能移动不能点击gdm下触摸屏能点，但是触控板不能点，然后触摸屏还不太好用，所以挺不舒服的。 在Arch的gdm Wiki页上找到了解决方案： Enabling tap-to-click 触控板点击的这个功能在gdm下是默认关闭的（话说我不明白为啥要关？） 直接在命令行下： 1$ sudo -u gdm gsettings set org.gnome.desktop.peripherals.touchpad tap-to-click true 即可。 然而我这里输这个居然dbus报错了，不知道什么情况，还好还有GUI下的解决方法： 12$ xhost +SI:localuser:gdm$ sudo -u gdm dconf-editor 首先处理一下gdm的访问权限，然后在dconf-editor的图形界面下面把tap-to-click改过来。 重启gdm即可。 Arch和Windows时间不一致问题发现一个非常蛋疼的事情，Arch和Windows下如果都开了自动同步网络时间的话，同步的结果会差8个小时。而且它们都会修改bios里面的硬件时间，在Arch下设置好时区，把时间改到正常了，然后切到Windows下面就不正常了；如果在Windows下面改过来了，切到Arch下又会不正常。 刚装的时候未果，只好把Arch的自动同步时间关掉，然后时区选到英国那边的零时区，这样才保证两边不会出问题。 今天回来好好研究了一下，发现这是因为两个系统默认读取硬件时间之后的设定不同。Windows会把bios上的硬件时间直接作为localtime，而Arch会把硬件时间认为是UCT的标准时间，然后加上由于时区产生的8小时算出来计算机所在的当地时间。 据Arch的Wiki上说，Windows这样做是不太好的，会有夏令时计算上的bug，也建议用户不要使用localtime，而采用Arch的标准。 幸运的是，我们国家貌似没有夏令时的制度，那么果断把Arch的UCT改成localtime即可。 1timedatectl set-local-rtc true 然后用status可以检查配置： 1timedatectl status 打开之后，下面会有个warning，提示说localtime怎么不好啊，什么的，无视之就好，重启一下检查bios和Windows时间是不是都是一致的。 给Arch安装微软字体Arch中有一些编写好的aur包比如ttf-ms-win10、ttf-ms-win8等等，支持快速地给系统装上windows对应版本上面自带的那些字体，比如我还是非常喜欢微软雅黑的，而且默认gnome下面的中文偶尔在某些应用下面显示得怪怪的，看起来非常地不舒服。 于是直接yaourt -S ttf-ms-win10，结果安装给我报错了，说是需要下载的文件找不到。 这我就呵呵了，想着是不是这个aur包的PKGBUILD文件写的有问题啊，需要下载的网址写错了什么的。 重新yaourt装了一下，然后查看PKGBUILD，原来这里面写得很清楚：微软的字体是有license限制的，不是开源随便用的，因此字体以及license文件都需要自己想办法弄到，放到PKGBUILD的目录下面再执行安装即可。 来源则比较直观，就是windows的安装镜像或者一个现成的windows系统。这个时候就体现出双系统的好处了，直接mount把C盘挂载过来，然后准备把字体直接复制过来就好了。 但是在这里直接复制的时候发生了点问题，显示出来中间有很多文件是软链接，而且系统无法定位到是链接到哪里去了，遂失败。 于是重启系统切到windows下面，特意查看了一下，这时候并没有发现里面的文件有链接的迹象啊？ Anyway，把C盘的字体文件重新复制了一份到D盘。复制快结束的时候资源管理器报告有重复文件，问我是否替换，没怎么注意结果切回Arch安装的时候发现少了文件。试图在PKGBUILD里面直接删掉那个文件的记录，结果下面居然还有个sha256sums校验，我了个去。遂失败。（至今不明白为什么复制会出问题，C盘这个字体文件夹有点神奇） 于是再重启切回到windows下面，这次我在命令行里面复制，再切回去的时候文件终于都正常了。 把D盘挂载上，然后把字体复制到/tmp/yaourt-xxx的对应目录中去，继续安装，完成。 IPv6 2018-03-30 重新把实验室配的台式机也装成了 Arch，发现 IPv6 虽然能够获取到地址，但是就是死活 ping 不通任何一个。 解决方案是 Arch Wiki 上提到的启用 Privacy Extension。 暂时不明具体的原理，但是改完 sysctl.d 以及 NetworkManager 的设置之后，重启生效。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Arch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[USTC]]></title>
    <url>%2F2016%2F03%2F06%2F2016-03-06-ustc%2F</url>
    <content type="text"><![CDATA[开始在USTC求学的生活。 少年你想要力量吗？]]></content>
      <categories>
        <category>Master-of-Science</category>
      </categories>
      <tags>
        <tag>Gallery</tag>
        <tag>USTC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理课设尝试（二）——PL/0编译器改写]]></title>
    <url>%2F2016%2F02%2F22%2F2016-02-22-pl0change%2F</url>
    <content type="text"><![CDATA[花了点时间，终于把PL/0整个编译器的源码分析了一遍，接下来就是上手对这个编译器进行改造的过程了。 我决定首先尝试将PL/0编译器的源码从C转到C++上，然后划分成多文件，再准备开始改。先从课设作业要求开始，最终的目标是完成PL/0的完整文法支持。 已经完成的部分我用删除线在下面标出来。 在github上开了个仓库，准备存放改写后的代码，以及用于记录整个过程：https://github.com/jcf94/pl0_compiler 下面先看一下拓展后的PL/0文法。 PL/0可拓展的完整文法 Program → Block . Block → [ConstDecl][TypeDecl][VarDecl][FuncDecl] begin Stmt {; Stmt } end ConstDecl → const ConstDef {, ConstDef} ; ConstDef → ident = number TypeDecl → type TypeDef {TypeDef } TypeDef → ident = TypeExp ; TypeExp → integer | real | Boolean | array ‘[’number .. number‘]’ of TypeExp VarDecl → var VarDec {VarDec } VarDec → ident {, ident} : Type ; Type → integer | real | boolean | ident FuncDecl → FuncDec { FuncDec } FuncDec → procedure ident [ ( ForParal ) ]; Block ; | function ident [ ( ForParal ) ] : Type ; Block ; ForParal → ident : Type {; ident : Type } Stmt → IdentRef := Exp | if Exp then Stmt | if Exp then Stmt else Stmt | begin Stmt {; Stmt } end | while Exp do Stmt | exit | ε | call ident [ ( ActParal ) ] | write ( Exp {, Exp } ) | read (IdentRef {, IdentRef } ) IdentRef → ident [ ‘[’Exp‘]’ { ‘[’Exp‘]’ } ] ActParal → Exp {, Exp } Exp → SimpExp RelOp SimpExp | SimpExp RelOp → = | &lt;&gt; | &lt; | &gt; | &lt;= | &gt;= SimpExp → [+ | - ] Term {+ Term | - Term | or Term} Term → Factor {* Factor | / Factor | div Factor | mod Factor | and Factor} Factor → IdentRef | number | ( Exp ) | not Factor | ident [ ( ActParal ) ] | odd (SimpExp) | true | false 课设要求课设要求上给出了8个题目： 给PL/0语言增加C语言形式的/∗ …… ∗/的注释。 给PL/0语言增加带else子句的条件语句和exit语句。exit语句作为while语句的非正常出口语句。若处于多层while语句中，则它只作为最内层while语句的非正常出口。若它没有处于任何while语句中，则是一个错误。 给PL/0语言增加输入输出语句。 给PL/0语言增加带参数的过程，参数传递按值调用方式。 给PL/0语言增加布尔类型，并且布尔类型的表达式按短路方式计算。 给PL/0语言增加数组类型。 给PL/0语言增加函数类型。 给PL/0语言增加实数类型。 改写过程记录 多文件改写 添加C语言形式的注释功能，支持//注释一行和/* */的段注释 修改词法分析器的getsym()部分即可，要注意的是除法也是/符号 添加else子句 这里有个比较容易出错的问题，即PL/0中对分号的处理与C中稍微有些区别，并不是所有的语句后面都会有分号。查看文法可知，block后跟点或分号，常量、变量定义的最后要有分号，出现多条并列的stmt时需要用分号来分隔（而最后一条stmt可能是不需要带分号的）。 这一点从文法分析中对分号semicolon的处理上也可以看出来。 因此else子句只要继续按照文法进行扩展即可： if Exp then Stmt | if Exp then Stmt else Stmt else前面应该是没有分号的。 首先将else加入保留字，norw、word、wsym都要作相应修改，注意原代码中用了二分查找，所以保留字的顺序也要作相应调整。 在文法分析中if语句部分的后面加上对else的判定即可。 最后还要处理一下若遇到一个语句开头部分直接是else的错误反馈。 添加exit语句 同样先将exit加入保留字，然后在stmt中增加对exit的支持。 一个exit语句其实就是jmp跳转到while的末尾即可，一个while中可能出现多个exit，而且while本身也是可以嵌套多层的，因此这里可以采用栈的结构exitlist保存下每个exit产生的jmp语句的位置。 等到一个while块结束时，再根据exitlist把所有jmp语句缺少的地址给补上。 添加输入输出语句 首先看一下输入输出部分的文法： write ( Exp {, Exp } ) | read (IdentRef {, IdentRef } ) write是括号中跟一个或多个exp，read是括号中跟一个或多个IdentRef。 由于还没有作其他的拓展，这里的IdentRef只是变量名，后面拓展成数组等等之后就有更多的东西了，比较麻烦…我在想是不是要先把IdentRef单独先分出来，以便于以后的工作。 首先继续添加关键字，原本的sym用的是unsigned long类型，大小有点不够，所以把所有的sym都改成unsigned long long类型。 在statbegsys中把read和write都加上。 然后在stmt中完成对read和write的处理即可。 有了read和write之后，接下来对PL/0编译器的各种测试和调试都可以先通过这两个来完成啦，简直是赞。 添加true和false支持 为后面添加boolean类型作准备 添加not/and/or/div/mod支持 事实上由于类型还没改，到这里为止编译器还只能支持整数，这里的div和原本的除法是一样的。 添加integer/boolean类型定义 在enum object这个枚举类型中把integer和boolean都加上。 添加类型定义需要涉及到的修改的地方比较多，比如VarDec的定义： VarDec → ident {, ident} : Type ; PL/0不像C语言那样先写下类型，后面再跟变量名，而是刚好反过来，因此这里新加一个enterlist的结构，识别出ident之后先把对应的符号表位置存入enterlist，然后等类型识别完毕后再改上去。 各个位置涉及到变量值的都需要修改，另外在变量的使用过程中还需要增加运算类型是否匹配的检查。我增加了一个运算操作用来整理boolean型变量的数据，即非零的全变成1。 这个编译器中运行时直接用的是C的数组作为数据栈，所以对于不同类型变量的存储其实是大大简化了的，因为随便怎么写都能存下来，大不了最后我把运行栈整个改成double数组就能同时支持整数和浮点数了，而实际要生成真正的汇编语言的话，需要考虑到的东西很多，还要看是不是跟机器也相关等等。 这里还是仅作实验学习吧… 额，各种问题都是相当麻烦，详细的就不记录了，改着改着都改忘了。 boolean运算的短路方式 or前面是1，则后面部分不再计算；and前面是0，则后面部分不再计算。 本来想是不是生成中间代码时就直接跳了，但是值的计算是到运行时才完成的，编译阶段根本不知道结果，所以要修改的部分应该是在中间代码生成时加上判断跳转的指令。 这里遇到点小麻烦，本来以为已经完成了这部分，结果调试时不对。 翻解释器的代码出来看之后惊奇地发现jpc结束后居然是要栈顶减1的…这样我原来想的就有问题了。明天再想想怎么解决吧。 待续…]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
        <tag>学习笔记</tag>
        <tag>PL/0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理课设尝试（一）——PL/0编译器分析]]></title>
    <url>%2F2016%2F02%2F21%2F2016-02-21-pl0%2F</url>
    <content type="text"><![CDATA[半路起步，继续学习编译原理ing，龙书还在继续看。我向来推崇学习时理论与实践结合，这本书现在带给我最大的感受是：我急于尽快掌握构建技术，而书上大篇幅介绍的还是词法、文法等理论知识，感觉我静心学习的耐心正在耗完中。 唉，这就是没系统上过课的悲剧，虽然知道不打好基础贻害无穷，但是还是决定上手做点实践。希望在实践中能够对编译原理有更多的理解。 这里要感谢挚友Bigballon同学以前分享给我的一份学习资料。他是软件系科班出身，对编程有着自己的理解。于是决定参考他写的一篇课程设计心得《从PL/0到Flex》尝试PL/0编译器的改写。 还记得当年我最早开始接触编程，学习的就是Pascal语言。 PL/0是Pascal语言的一个子集，它只有整数类型，但它是相当完全的可嵌套的分程序（block）的程序结构，分程序中可以有常量定义、变量声明和无参过程声明，过程体又是分程序。PL/0有赋值语句、条件语句、循环语句、过程调用语句、复合语句和空语句。 PL/0的文法定义 Program → Block . Block → [ConstDecl][VarDecl][ProcDecl] Stmt程序块的基本结构是：常量定义、变量定义、过程定义、语句 ConstDecl → const ConstDef {, ConstDef} ;常量定义 ConstDef → ident = number VarDecl → var ident {, ident} ;变量定义 ProcDecl → procedure ident ; Block ; {procedure ident ; Block ;}过程定义 Stmt → ident := Exp | call ident | begin Stmt {; Stmt} end | if Cond then Stmt | while Cond do Stmt | ε赋值语句、调用语句、begin-end块语句、判断语句、循环语句 Cond → odd Exp | Exp RelOp Exp条件表达式 RelOp → = | &lt;&gt; | &lt; | &gt; | &lt;= | &gt;=关系运算符 Exp → [+ | − ] Term {+ Term | − Term}表达式 Term → Factor {∗ Factor | / Factor} Factor → ident | number | ( Exp ) ident 字母开头的字母/数字串 number 无符号整数 PL/0的指令集定义PL/0的目标代码放在一个固定的存储数组中，而其中所需的数据组织成一个栈的形式存放。 它的中间语言是一种栈机器代码，其指令集结构如下： |指令/F|含义|L|A||-||LIT|将常数置于栈顶|—|常量||LOD|将变量的值置于栈顶|层次差|数据地址||STO|将栈顶的值赋予某变量|层次差|数据地址||CAL|过程调用|层次差|程序地址||INT|在数据栈中分配空间|—|常量||JPC,JMP|条件/无条件转移|—|程序地址||OPR|一组算术或逻辑运算指令|—|运算类别| PL/0编译器的执行过程 编译器启动 编译器初始化包括：对保留字表( word )、保留字表中每一个保留字对应的 symbol 类型( wsym )、部分符号对应的 symbol 类型表( ssym )、类 PCODE 指令助记符表( mnemonic )、声明开始集合( declbegsys )、表达式开始集合( statbegsys )、项开始符号集合( facbegsys )以及一些全局变量的初始化 首次调用getsym()进行词法分析 调用block()过程，包括词法分析和语法分析 调用interpret()解释执行目标程序 PL/0编译器源码注释pl0.h123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144#include &lt;stdio.h&gt;#define norw 11 // 保留字的数量#define txmax 100 // 标识符表的长度#define nmax 14 // 允许的最长数字位数#define al 10 // 标识符的最大长度#define amax 2047 // 寻址空间#define levmax 3 // 最大嵌套层数#define cxmax 2000 // 代码数组的长度#define nul 0x1 // 空#define ident 0x2 // 标识符#define number 0x4 // 数值#define plus 0x8 // +#define minus 0x10 // -#define times 0x20 // *#define slash 0x40 // /#define oddsym 0x80 // 奇数判断#define eql 0x100 // =#define neq 0x200 // &lt;&gt;#define lss 0x400 // &lt;#define leq 0x800 // &lt;=#define gtr 0x1000 // &gt;#define geq 0x2000 // &gt;=#define lparen 0x4000 // (#define rparen 0x8000 // )#define comma 0x10000 // ,#define semicolon 0x20000 // ;#define period 0x40000 // .#define becomes 0x80000 // :=#define beginsym 0x100000 // 保留字：begin#define endsym 0x200000 // 保留字：end#define ifsym 0x400000 // 保留字：if#define thensym 0x800000 // 保留字：then#define whilesym 0x1000000 // 保留字：while#define dosym 0x2000000 // 保留字：do#define callsym 0x4000000 // 保留字：call#define constsym 0x8000000 // 保留字：const#define varsym 0x10000000 // 保留字：var#define procsym 0x20000000 // 保留字：procedureenum object // 三种标识符的类型&#123; constant, variable, proc&#125;;enum fct // 中间代码指令集&#123; lit, opr, lod, sto, cal, Int, jmp, jpc&#125;;/* lit 0, a : load constant a opr 0, a : execute operation a lod l, a : load variable l, a sto l, a : store variable l, a cal l, a : call procedure a at level l Int 0, a : increment t-register by a jmp 0, a : jump to a jpc 0, a : jump conditional to a */typedef struct // 指令类型&#123; enum fct f; // 功能 long l; // 层次差 long a; // 数值/地址&#125; instruction;char* err_msg[] = // 错误信息&#123;/* 0 */ "",/* 1 */ "Found ':=' when expecting '='.",/* 2 */ "There must be a number to follow '='.",/* 3 */ "There must be an '=' to follow the identifier.",/* 4 */ "There must be an identifier to follow 'const', 'var', or 'procedure'.",/* 5 */ "Missing ',' or ';'.",/* 6 */ "Incorrect procedure name.",/* 7 */ "Statement expected.",/* 8 */ "Follow the statement is an incorrect symbol.",/* 9 */ "'.' expected.",/* 10 */ "';' expected.",/* 11 */ "Undeclared identifier.",/* 12 */ "Illegal assignment.",/* 13 */ "':=' expected.",/* 14 */ "There must be an identifier to follow the 'call'.",/* 15 */ "A constant or variable can not be called.",/* 16 */ "'then' expected.",/* 17 */ "';' or 'end' expected.",/* 18 */ "'do' expected.",/* 19 */ "Incorrect symbol.",/* 20 */ "Relative operators expected.",/* 21 */ "Procedure identifier can not be in an expression.",/* 22 */ "Missing ')'.",/* 23 */ "The symbol can not be followed by a factor.",/* 24 */ "The symbol can not be as the beginning of an expression.",/* 25 */ "",/* 26 */ "",/* 27 */ "",/* 28 */ "",/* 29 */ "",/* 30 */ "",/* 31 */ "The number is too great.",/* 32 */ "There are too many levels."&#125;;char ch; // 用于词法分析，存放最近一次从文件中读出的字符unsigned long sym; // 存放最近一次识别出来的token类型char id[al+1]; // 存放最近一次识别出来的标识符的名字long num; // 存放最近一次识别出来的数字long cc; // 字母计数（列指针）long ll; // 行长度long kk, err;long cx; // 代码分配指针，代码生成模块总在cx所指的位置生成新代码char line[81]; // 行缓冲区char a[al+1]; // 词法分析器中用于存放正在分析的词instruction code[cxmax+1]; // 指令表，存放生成的中间代码char word[norw][al+1]; // 保留字表unsigned long wsym[norw]; // 保留字中每一个保留字对应的symbol类型表unsigned long ssym[256]; // 符号对应的symbol类型表char mnemonic[8][3+1]; // 中间代码助记符表unsigned long declbegsys, statbegsys, facbegsys; //声明开始、表达式开始、factor开始符号集合struct // 符号表&#123; char name[al+1]; // 符号名 enum object kind; // 符号类型 long val; // 值 long level; // 层差/偏移地址 long addr; // 地址&#125;table[txmax+1];char infilename[80];FILE* infile;// the following variables for blocklong dx; // 数据分配指针long lev; // 当前的块深度long tx; // 当前的符号表指针// the following array space for interpreter#define stacksize 50000long s[stacksize]; // 数据栈 pl0.c123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056// pl/0 compiler with code generation#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include "pl0.h"void error(long n) // 打印错误信息&#123; long i; printf("Error=&gt;"); for (i = 1; i &lt;= cc-1; i++) &#123; printf(" "); &#125; printf("|%s(%d)\n", err_msg[n], n); err++;&#125;void getch() // 读取下一个字符&#123; if(cc == ll) // 判断是否读完一行 &#123; if(feof(infile)) // 文件结束 &#123; printf("************************************\n"); printf(" program incomplete\n"); printf("************************************\n"); exit(1); &#125; ll = 0; cc = 0; printf("%5d ", cx); while((!feof(infile)) &amp;&amp; ((ch=getc(infile))!='\n')) &#123; printf("%c", ch); ll = ll + 1; line[ll] = ch; &#125; printf("\n"); ll = ll + 1; line[ll] = ' '; &#125; cc = cc + 1; ch = line[cc];&#125;//---------------------------------//-------------词法分析//---------------------------------//从源文件中读出若干有效字符，组成一个 token 串，识别它的类型为保留字/标识//符/数字或是其它符号。如果是保留字，把 sym 置成相应的保留字类型，如果是标//识符，把 sym 置成 ident 表示是标识符，于此同时，id 变量中存放的即为保留//字字符串或标识符名字。如果是数字，把 sym 置为 number,同时 num 变量中存//放该数字的值。如果是其它的操作符，则直接把 sym 置成相应类型。经过本过程后//ch 变量中存放的是下一个即将被识别的字符void getsym()&#123; long i, j, k; while(ch == ' ' || ch == '\t') // 删除前导空格 &#123; getch(); &#125; if(isalpha(ch)) // 当前token的第一个字符为字母，可能是标识符或保留字 &#123; k = 0; // 用于记录token长度 do // 读完一个token &#123; if(k &lt; al) // 保证token长度不超过标识符最大长度 &#123; a[k] = ch; k = k + 1; &#125; getch(); &#125; while(isalpha(ch) || isdigit(ch)); if(k &gt;= kk) // 保证a数组的末尾都是空格 &#123; kk = k; &#125; else &#123; do &#123; kk = kk-1; a[kk] = ' '; &#125; while(k &lt; kk); &#125; strcpy(id, a); i = 0; j = norw - 1; //存放当前token的名字 do // 在保留字表中二分查找该串是否存在 &#123; k = (i+j)/2; if(strcmp(id, word[k]) &lt;= 0) &#123; j = k - 1; &#125; if(strcmp(id, word[k]) &gt;=0) &#123; i = k + 1; &#125; &#125; while(i &lt;= j); if(i-1 &gt; j) // 找到则标记保留字 &#123; sym = wsym[k]; &#125; else // 否则标记为标识符 &#123; sym = ident; &#125; &#125; else if(isdigit(ch)) // 首字符为数字，则将其解析为数值 &#123; k = 0; num = 0; sym = number; do // 不断读取、保存数字 &#123; num = num * 10 + (ch - '0'); k = k + 1; getch(); &#125; while(isdigit(ch)); if(k &gt; nmax) // 位数超限 &#123; error(31); &#125; &#125; else if(ch == ':') // 首字符为“:”，则可能是赋值 &#123; getch(); if(ch == '=') &#123; sym = becomes; getch(); &#125; else &#123; sym = nul; &#125; &#125; else if(ch == '&lt;') // 首字符为“&lt;” &#123; getch(); if(ch == '=') // &lt;= &#123; sym = leq; getch(); &#125; else if(ch == '&gt;') // &lt;&gt; &#123; sym=neq; getch(); &#125; else // &lt; &#123; sym = lss; &#125; &#125; else if(ch == '&gt;') // 首字符为“&gt;” &#123; getch(); if(ch == '=') // &gt;= &#123; sym=geq; getch(); &#125; else // &gt; &#123; sym=gtr; &#125; &#125; else // 均不是上述情况，则查询符号表，赋值 &#123; sym = ssym[(unsigned char)ch]; getch(); &#125;&#125;//---------------------------------//-------------中间代码生成//---------------------------------//将生成的中间代码写入中间代码数组，供后面的解释器运行void gen(enum fct x, long y, long z)&#123; if(cx &gt; cxmax) // 代码长度超限 &#123; printf("program too long\n"); exit(1); &#125; code[cx].f = x; code[cx].l = y; code[cx].a = z; cx = cx + 1;&#125;//---------------------------------//-------------测试当前单词是否合法//---------------------------------// 参数：s1:当语法分析进入或退出某一语法单元时当前单词符合应属于的集合// s2:在某一出错状态下，可恢复语法分析正常工作的补充单词集合// n :出错信息编号，当当前符号不属于合法的 s1 集合时发出的出错信息void test(unsigned long s1, unsigned long s2, long n)&#123; if (!(sym &amp; s1)) // 当前符号不在s1中 &#123; error(n); s1 = s1 | s2; // 把s2补充进s1 while(!(sym &amp; s1)) // 循环找到下一个合法的符号 &#123; getsym(); // 继续词法分析 &#125; &#125;&#125;//---------------------------------//-------------符号表//---------------------------------void enter(enum object k)&#123; tx = tx + 1; strcpy(table[tx].name, id); // 保存符号名字 table[tx].kind = k; // 保存符号类型 switch(k) &#123; case constant: // 常量 if(num &gt; amax) &#123; error(31); num = 0; &#125; table[tx].val = num; break; case variable: // 变量 table[tx].level = lev; // 记录当前层次 table[tx].addr = dx; // 记录层次中的偏移地址 dx = dx + 1; // 只有变量登录时需要用dx在数据栈中预留空间 break; case proc: // 过程 table[tx].level = lev; break; &#125;&#125;//---------------------------------//-------------查找符号在符号表中的位置//---------------------------------long position(char* id)&#123; long i; strcpy(table[0].name, id); i=tx; while(strcmp(table[i].name, id) != 0) &#123; i = i - 1; &#125; return i;&#125;//---------------------------------//-------------常量声明//---------------------------------void constdeclaration()&#123; if(sym == ident) &#123; getsym(); if(sym == eql || sym == becomes) // 出现等号或赋值号 &#123; if(sym == becomes) // 赋值号报错 &#123; error(1); &#125; getsym(); if(sym == number) // 将数字登录到符号表 &#123; enter(constant); getsym(); &#125; else &#123; error(2); &#125; &#125; else &#123; error(3); &#125; &#125; else &#123; error(4); &#125;&#125;//---------------------------------//-------------变量声明//---------------------------------void vardeclaration()&#123; if(sym == ident) &#123; enter(variable); // 将标识符登录到符号表中 getsym(); &#125; else &#123; error(4); &#125;&#125;//---------------------------------//-------------输出当前代码块的中间代码//---------------------------------void listcode(long cx0)&#123; long i; for(i=cx0; i&lt;=cx-1; i++) &#123; printf("%10d%5s%3d%5d\n", i, mnemonic[code[i].f], code[i].l, code[i].a); &#125;&#125;void expression(unsigned long);//---------------------------------//-------------factor处理//---------------------------------//fsys: 如果出错可用来恢复语法分析的符号集合void factor(unsigned long fsys)&#123; long i; test(facbegsys, fsys, 24); // 开始因子处理前，先检查当前 token 是否在 facbegsys 集合中 // 如果不是合法的 token，抛 24 号错误，并通过 fsys 集恢复使语法处理可以继续进行 while(sym &amp; facbegsys) &#123; if(sym == ident) // 遇到标识符 &#123; i = position(id); // 查符号表 if(i==0) // 标识符未定义 &#123; error(11); &#125; else &#123; switch(table[i].kind) &#123; case constant: //常量 gen(lit, 0, table[i].val); break; case variable: //变量 gen(lod, lev-table[i].level, table[i].addr); break; case proc: //过程 error(21); break; &#125; &#125; getsym(); &#125; else if(sym == number) // 遇到数字 &#123; if(num&gt;amax) &#123; error(31); num=0; &#125; gen(lit,0,num); getsym(); &#125; else if(sym == lparen) // 遇到左括号 &#123; getsym(); expression(rparen|fsys); // 后面是一个exp if(sym==rparen) // 子表达式结束，应该遇到右括号 &#123; getsym(); &#125; else &#123; error(22); &#125; &#125; test(fsys,lparen,23); // 一个因子处理完毕，遇到的 token 应在 fsys 集合中 // 如果不是，抛 23 号错，并找到下一个因子的开始，使语法分析可以继续运行下去 &#125;&#125;//---------------------------------//-------------term处理//---------------------------------void term(unsigned long fsys)&#123; unsigned long mulop; factor(fsys|times|slash); // 每个term都应该从factor开始 while(sym==times || sym==slash)// 处理乘除 &#123; mulop = sym; // 保存当前运算符 getsym(); factor(fsys|times|slash); // 运算符之后是一个factor if(mulop == times) &#123; gen(opr,0,4); // 乘法 &#125; else&#123; gen(opr,0,5); // 除法 &#125; &#125;&#125;//---------------------------------//-------------exp处理//---------------------------------void expression(unsigned long fsys)&#123; unsigned long addop; if(sym==plus || sym==minus) // 处理正负号 &#123; addop=sym; // 保存正负号 getsym(); term(fsys|plus|minus); // 正负号后面是一个term if(addop==minus) &#123; gen(opr,0,1); // 负号，取反运算 &#125; &#125; else &#123; term(fsys|plus|minus); &#125; while(sym==plus || sym==minus) // 处理加减 &#123; addop=sym; // 保存运算符 getsym(); term(fsys|plus|minus); // 运算符后是一个term if(addop==plus) &#123; gen(opr,0,2); // 加 &#125; else &#123; gen(opr,0,3); // 减 &#125; &#125;&#125;//---------------------------------//-------------cond处理//---------------------------------void condition(unsigned long fsys)&#123; unsigned long relop; if(sym==oddsym) // odd运算符（一元） &#123; getsym(); expression(fsys); // 后面是一个exp gen(opr, 0, 6); &#125; else // 否则是个二元运算符 &#123; expression(fsys|eql|neq|lss|gtr|leq|geq); // 后面是一个exp if(!(sym&amp;(eql|neq|lss|gtr|leq|geq))) &#123; error(20); &#125; else &#123; relop=sym; // 保存当前运算符 getsym(); expression(fsys); // 处理表达式右边 switch(relop) &#123; case eql: gen(opr, 0, 8); break; case neq: gen(opr, 0, 9); break; case lss: gen(opr, 0, 10); break; case geq: gen(opr, 0, 11); break; case gtr: gen(opr, 0, 12); break; case leq: gen(opr, 0, 13); break; &#125; &#125; &#125;&#125;//---------------------------------//-------------stmt处理//---------------------------------void statement(unsigned long fsys)&#123; long i,cx1,cx2; if(sym==ident) // 标识符 &#123; i=position(id); // 查符号表 if(i==0) &#123; error(11); // 未定义 &#125; else if(table[i].kind!=variable) &#123; error(12); // 非变量 i=0; &#125; getsym(); if(sym==becomes) // 赋值 &#123; getsym(); &#125; else &#123; error(13); &#125; expression(fsys); // 后面应该是一个exp if(i!=0) // 若未出错，则产生一个sto代码 &#123; gen(sto,lev-table[i].level,table[i].addr); &#125; &#125; else if(sym==callsym) // call语句 &#123; getsym(); if(sym!=ident) &#123; error(14); &#125; else &#123; i=position(id); if(i==0) &#123; error(11); &#125; else if(table[i].kind==proc) &#123; gen(cal,lev-table[i].level,table[i].addr); &#125; else &#123; error(15); &#125; getsym(); &#125; &#125; else if(sym==ifsym) // if语句 &#123; getsym(); condition(fsys|thensym|dosym);// 后面是一个cond if(sym==thensym) &#123; getsym(); &#125; else &#123; error(16); &#125; cx1=cx; gen(jpc,0,0); statement(fsys); // 后面是一个stmt code[cx1].a=cx; &#125; else if(sym==beginsym) // begin语句 &#123; getsym(); statement(fsys|semicolon|endsym); // 后面是stmt while(sym==semicolon||(sym&amp;statbegsys)) // 处理分号和语句 &#123; if(sym==semicolon) // 分号 &#123; getsym(); &#125; else &#123; error(10); &#125; statement(fsys|semicolon|endsym); &#125; if(sym==endsym) &#123; getsym(); &#125; else &#123; error(17); &#125; &#125; else if(sym==whilesym) // while语句 &#123; cx1=cx; // 记录中间代码起始指针 getsym(); condition(fsys|dosym); // 后面是一个cond cx2=cx; // 记录中间代码位置，要放退出地址 gen(jpc,0,0); if(sym==dosym) // do语句 &#123; getsym(); &#125; else &#123; error(18); &#125; statement(fsys); // 后面是stmt gen(jmp,0,cx1); // 循环跳转 code[cx2].a=cx; // 把退出地址补上 &#125; test(fsys,0,19);&#125;//---------------------------------//-------------block处理//---------------------------------void block(unsigned long fsys)&#123; long tx0; long cx0; long tx1; long dx1; dx=3; // 地址寄存器给出每层局部量当前已分配到的相对位置 // 置初始值为 3 的原因是：每一层最开始的位置有三个空间用于存放 // 静态链 SL、动态链 DL 和 返回地址 RA tx0=tx; // 记录本层开始时符号表的位置 table[tx].addr=cx; // 符号表记下当前层代码的开始地址 gen(jmp,0,0); // block开始时首先写下一句跳转指令，地址到后面再补 if(lev&gt;levmax) // 嵌套层数太大 &#123; error(32); &#125; do // 开始处理声明部分 &#123; if(sym==constsym) // 常量 &#123; getsym(); do &#123; constdeclaration();// 常量声明 while(sym==comma) // 逗号 &#123; getsym(); constdeclaration();// 逗号后面继续常量声明 &#125; if(sym==semicolon) // 分号 &#123; getsym(); &#125; else &#123; error(5); &#125; &#125; while(sym==ident); &#125; if(sym==varsym) // 变量 &#123; getsym(); do &#123; vardeclaration(); // 变量声明 while(sym==comma) // 逗号 &#123; getsym(); vardeclaration();// 逗号后面继续声明变量 &#125; if(sym==semicolon) // 分号 &#123; getsym(); &#125; else &#123; error(5); &#125; &#125; while(sym==ident); &#125; while(sym==procsym) // 过程 &#123; getsym(); if(sym==ident) // 标识符 &#123; enter(proc); // 将过程登记进符号表 getsym(); &#125; else &#123; error(4); &#125; if(sym==semicolon) // 分号 &#123; getsym(); &#125; else &#123; error(5); &#125; lev=lev+1; // 层级加1 tx1=tx; // 记录当前层级 dx1=dx; // 记录当前数据指针 block(fsys|semicolon); // 处理block lev=lev-1; tx=tx1; dx=dx1; // 恢复 if(sym==semicolon) // 分号 &#123; getsym(); test(statbegsys|ident|procsym,fsys,6); // 检查当前 token 是否合法，不合法 则用 fsys 恢复语法分析同时抛 6 号错 &#125; else &#123; error(5); &#125; &#125; test(statbegsys|ident,declbegsys,7); // 检查当前状态是否合法，不合法则用声明开 始符号作出错恢复、抛 7 号错 &#125; while(sym&amp;declbegsys); code[table[tx0].addr].a=cx; // 把block开头写下的跳转指令的地址补上 table[tx0].addr=cx; // tx0的符号表存的是当前block的参数 cx0=cx; gen(Int,0,dx); statement(fsys|semicolon|endsym); gen(opr,0,0); // return test(fsys,0,8); //listcode(cx0);&#125;//---------------------------------//-------------通过静态链求出数据区基地址//---------------------------------long base(long b, long l)&#123; long b1; b1=b; while (l&gt;0) // 根据层级差来找到基地址 &#123; b1=s[b1]; l=l-1; &#125; return b1;&#125;//---------------------------------//-------------指令集解释器//---------------------------------void interpret()&#123; long p,b,t; // 程序寄存器PC、基地址寄存器、栈顶寄存器 instruction i; // 指令寄存器 printf("start PL/0\n"); t=0; b=1; p=0; s[1]=0; s[2]=0; s[3]=0; do &#123; i=code[p]; p=p+1; // 每次在code表中读取一条指令 switch(i.f) &#123; case lit: // 常数指令 t=t+1; s[t]=i.a; break; case opr: // 运算指令 switch(i.a) &#123; case 0: // 返回指令 t=b-1; p=s[t+3]; b=s[t+2]; break; case 1: // 负号 s[t]=-s[t]; break; case 2: // 加法 t=t-1; s[t]=s[t]+s[t+1]; break; case 3: // 减法 t=t-1; s[t]=s[t]-s[t+1]; break; case 4: // 乘法 t=t-1; s[t]=s[t]*s[t+1]; break; case 5: // 除法 t=t-1; s[t]=s[t]/s[t+1]; break; case 6: // odd s[t]=s[t]%2; break; case 8: // == t=t-1; s[t]=(s[t]==s[t+1]); break; case 9: // != t=t-1; s[t]=(s[t]!=s[t+1]); break; case 10: // &lt; t=t-1; s[t]=(s[t]&lt;s[t+1]); break; case 11: // &gt;= t=t-1; s[t]=(s[t]&gt;=s[t+1]); break; case 12: // &gt; t=t-1; s[t]=(s[t]&gt;s[t+1]); break; case 13: // &lt;= t=t-1; s[t]=(s[t]&lt;=s[t+1]); &#125; break; case lod: // 调用变量值指令 t=t+1; s[t]=s[base(b,i.l)+i.a]; break; case sto: // 将值存入变量指令 s[base(b,i.l)+i.a]=s[t]; printf("%10d\n", s[t]); t=t-1; break; case cal: // 过程调用，产生新的块标记 s[t+1]=base(b,i.l); s[t+2]=b; s[t+3]=p; // 记录返回地址等参数 b=t+1; p=i.a; break; case Int: // 开内存空间 t=t+i.a; break; case jmp: // 无条件跳转指令 p=i.a; break; case jpc: // 栈顶为0跳转 if(s[t]==0) &#123; p=i.a; &#125; t=t-1; &#125; &#125; while(p!=0); printf("end PL/0\n");&#125;//---------------------------------//-------------PL/0编译器主函数//---------------------------------int main()&#123; long i; //-------------初始化 for(i=0; i&lt;256; i++) &#123; ssym[i]=nul; &#125; strcpy(word[0], "begin ");// 保留字 strcpy(word[1], "call "); strcpy(word[2], "const "); strcpy(word[3], "do "); strcpy(word[4], "end "); strcpy(word[5], "if "); strcpy(word[6], "odd "); strcpy(word[7], "procedure "); strcpy(word[8], "then "); strcpy(word[9], "var "); strcpy(word[10], "while "); wsym[0]=beginsym; // 保留字对应的symbol类型表 wsym[1]=callsym; wsym[2]=constsym; wsym[3]=dosym; wsym[4]=endsym; wsym[5]=ifsym; wsym[6]=oddsym; wsym[7]=procsym; wsym[8]=thensym; wsym[9]=varsym; wsym[10]=whilesym; ssym['+']=plus; // 符号对应的symbol类型表 ssym['-']=minus; ssym['*']=times; ssym['/']=slash; ssym['(']=lparen; ssym[')']=rparen; ssym['=']=eql; ssym[',']=comma; ssym['.']=period; ssym[';']=semicolon; strcpy(mnemonic[lit],"LIT"); // 中间代码助记符表 strcpy(mnemonic[opr],"OPR"); strcpy(mnemonic[lod],"LOD"); strcpy(mnemonic[sto],"STO"); strcpy(mnemonic[cal],"CAL"); strcpy(mnemonic[Int],"INT"); strcpy(mnemonic[jmp],"JMP"); strcpy(mnemonic[jpc],"JPC"); declbegsys=constsym|varsym|procsym;// 声明开始符号集合 statbegsys=beginsym|callsym|ifsym|whilesym;// 表达式开始符号集合 facbegsys=ident|number|lparen; // factor开始符号集合 printf("please input source program file name: "); scanf("%s",infilename); printf("\n"); if((infile=fopen(infilename,"r"))==NULL) &#123; printf("File %s can't be opened.\n", infilename); exit(1); &#125; //-------------开始语法分析 err=0; cc=0; cx=0; ll=0; // 各个计数器初始化 ch=' '; // 为第一次读取字符作初始化 kk=al; getsym(); lev=0; tx=0; block(declbegsys|statbegsys|period); if(sym!=period) // 点 &#123; error(9); &#125; //-------------语法分析完成 if(err==0) // 程序无错误则开始解释执行 &#123; // interpret(); listcode(0); &#125; else &#123; printf("errors in PL/0 program\n"); &#125; fclose(infile); return (0);&#125; PL/0编译器源码分析对上面的源码进行编译，再用一个示例程序进行测试可得到如下结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859please input source program file name: test1.pl0 0 const m=7, n=85; 1 var x,y,z,q,r; 1 procedure multiply; 1 var a,b; 2 begin 3 a:=x; b:=y; z:=0; 9 while b&gt;0 do 13 begin 13 if odd b then z:=z+a; 20 a:=2*a; b:=b/2; 28 end 28 end; 30 begin 31 x:=m; y:=n; 35 call multiply; 36 end.中间代码： 0 JMP 0 30以下为procedure multiply部分的中间代码： 1 JMP 0 2 2 INT 0 5 3 LOD 1 3 4 STO 0 3 5 LOD 1 4 6 STO 0 4 7 LIT 0 0 8 STO 1 5 9 LOD 0 4 10 LIT 0 0 11 OPR 0 12 12 JPC 0 29 13 LOD 0 4 14 OPR 0 6 15 JPC 0 20 16 LOD 1 5 17 LOD 0 3 18 OPR 0 2 19 STO 1 5 20 LIT 0 2 21 LOD 0 3 22 OPR 0 4 23 STO 0 3 24 LOD 0 4 25 LIT 0 2 26 OPR 0 5 27 STO 0 4 28 JMP 0 9 29 OPR 0 0以下为主程序部分的中间代码： 30 INT 0 8 31 LIT 0 7 32 STO 0 3 33 LIT 0 85 34 STO 0 4 35 CAL 0 2 36 OPR 0 0 把源码完整地过了一遍之后，感觉确实对编译的整个过程有了更清晰的理解。 整个编译器的编译过程是从上到下，一次完成。 首先程序的开始部分对各项参数进行了初始化，完成之后即根据PL/0语言的文法定义开始语法分析。仔细阅读这里的这段代码可以比较深刻地感受到写编译器之前有个清晰明确的文法定义是多么重要。 写编译器代码时可以把每一条文法定义都写成一个函数，以方便递归调用来完成语法分析。 语法分析的基础是词法分析，词法分析即程序中的getsym()，每次完成一段字符的读取，处理得到一个token。根据token的不同属性判断，将其登录进符号表或完成不同的语法功能。 这个编译器的中间代码生成是在语法分析的过程中完成的： 在factor中完成常量的lit或变量的lod，在term中完成乘除的opr，exp中完成加减和负号的opr，cond中完成逻辑运算的opr，stmt中完成赋值sto、过程调用cal、if以及while的跳转jmp、jpc，block中完成程序运行跳转jmp和开栈空间int。 中间代码使用层级差而不是层级的原因是数据栈中需要一级一级向底部找。 中间代码生成结束后，这段代码还给出了一个解释器用来运行得到的中间代码。 用到的几个特殊结构： 中间代码存储数组code以及代码分配指针cx 12345678typedef struct&#123; enum fct f; long l; long a;&#125; instruction;instruction code[cxmax+1];long cx; cx用来表示下一条生成的中间代码要放到存储数组的哪个位置。 符号表table以及符号表指针tx 12345678910111213enum object&#123; constant, variable, proc&#125;;struct&#123; char name[al+1]; enum object kind; long val; long level; long addr;&#125;table[txmax+1];long tx; 符号表中存放三种类型的token，常量、变量以及代码块。 常量直接记录下val即可，处理成中间代码是直接用lit调取数值的； 变量需要记录下level和addr，处理成中间代码时需要用到lit、sto、lod； 代码块是进入时addr中先存下当前的cx，代码块的第一条指令都是jmp，之后可能会有常量定义或者其他代码块先生成了代码，结束后需要把jmp指令的跳转地址给补上（即此时的cx），然后addr再重新存一遍cx，这个时候表示存下当前代码块的执行起始位置。 数据分配指针dx dx在语法分析的过程中是用来记录下当前代码块需要开辟的栈空间数，以在生成block代码时，用int开辟栈空间。 这些的具体作用还要再看一下解释器的运行过程。 本文生成的中间代码就是一种类似汇编的语言，用到的运行寄存器有4个： 1234long p; // 程序指针寄存器，即汇编中的PC，标识当前执行到哪条指令long b; // 基地址寄存器，保存的是当前程序块在栈中占用空间的最低位的地址long t; // 栈顶指针寄存器，即汇编中的SPinstruction i; // 指令寄存器，根据p读出来的一条指令会首先放在i中 jmp：直接改变p寄存器的内容即可 jpc：判断栈顶数据是否为零，再决定是否改变p的内容 lit：栈顶指针+1，将常数放到栈顶 int：栈顶指针+要开的空间数，即先把栈顶指针上移，空出需要的变量空间来 cal：过程调用，调用时首先要把栈顶的3个空间用来存放跳转参数，s[t+1]中存放前一个层级的基地址位置，s[t+2]中存放当前的静态基地址b，s[t+3]中存放当前的程序指针p。s[t+1]这个位置是像链表一样指向前一个层级的基地址位置，以应用层级差来找到变量的存储位置。 lod、sto：这里主要是需要用s[base(b,i.l)+i.a]动态基地址以及层级差来计算出对应变量在整个栈中的位置，然后与栈顶元素进行交互。 opr：运算符操作，对栈顶1个或2个元素进行操作，结果仍然存回栈顶。 opr 0 0：这个操作是过程调用的返回操作，需要将3个参数恢复回去，首先把t恢复为b-1即原来的栈顶，p恢复为s[t+3]，b恢复为s[t+2]。 过程调用这部分感觉还可以作一些改进。 PL/0编译器的拓展花了好大的精力终于是把PL/0整个编译器的源码解读了一遍，接下来就是上手对这个编译器进行拓展的过程了。 另开一篇写拓展：编译原理课设尝试（二）——PL/0编译器改写]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
        <tag>学习笔记</tag>
        <tag>PL/0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX学习]]></title>
    <url>%2F2016%2F02%2F13%2F2016-02-13-latex%2F</url>
    <content type="text"><![CDATA[一套强大的文档排版系统。 日常生活中，Office一般足以完成各类文档的处理，后来再到记事、写博客这样需要一些格式化而又方便的排版功能时，Markdown+Mathjax就足以搞定一切了。然而当遇到大型论文排版的需求时，则需要一套更为专业的文档排版系统 —— $\LaTeX$。 历史最早Donald Ervin Knuth写了一套叫$\TeX$的排版系统，很容易用它来生成高质量排版的文档。 $\TeX$大概就是类似机器语言or汇编语言一样的存在吧（大雾），而它也支持使用宏来简化操作，后人为让人们了更方便地使用它，在原始$\TeX$宏集的基础上增加了许多功能，定义了许多命令和模板，将其作了扩展，于是$\LaTeX$出现了。 $\LaTeX$的使用者不需要知道每一条$\TeX$命令是怎么写的，只要按照$\LaTeX$的标准来写文章，就能够得到非常漂亮的效果。 尤其是$\LaTeX$中的公式显示非常完美，Mathjax支持的也就是$\LaTeX$的公式格式。 装完$\LaTeX$之后，在命令行中输入 1texdoc tex-overview 可以看到$\TeX$发展多年的黑历史。 有个有趣的地方$\TeX$是用$\pi$来作为版本号的，每完善发布一个新版本，就多一位小数，表示版本在不断趋近完美（$\pi$）。 值得一提的是，$\TeX$发布四十多年，只更新了九次，确实是“完美的系统啊”。 更值得一提的是，Knuth写的另外一个东西Metafont是用自然对数e来作为版本号的，也是每更新一个版本，就往上面加一位小数。-_-/// 开始使用$\LaTeX$$\LaTeX$与Word这种所见即所得的文档软件不同，用它写文章出来会有一种写代码的感觉，这一点与Markdown非常像。而要得到完成的文章，则需要在写完文章的源代码之后，将其编译成PDF等固定格式。 因此通常要使用$\LaTeX$，我们需要一个文本编辑器或者IDE，还需要一个$\LaTeX$编译器。（这个跟编程的过程是一致的。） 与Linux类似，$\LaTeX$有很多种引擎（编译器），也有很多种发行版，选择一种自己常用的发行版装起来就好了。 比如TeX Live就是一套跨平台的$\TeX$发行版。 整个TeX Live 2015的安装包的大小大概2G左右，在线安装会很慢，可以找个镜像站把镜像下过来之后手动装上。 装完$\LaTeX$之后，生成PDF的流程是这样的： 首先文章的源代码写在.tex的文件中，通过latex xxx或者latex xxx.tex可以得到一个xxx.dvi的文件（Device Independent，即与设备无关，用这个文件拿到能够识别的设备上就能直接打印成文章），然后dvips xxx或者dvips xxx.dvi生成xxx.ps，即打印字体文件，最后ps2pdf xxx或者ps2pdf xxx.ps即生成了xxx.pdf文件。 xxx.dvi也可以直接使用dvipdfm xxx或dvipdfm xxx.dvi得到xxx.pdf。 再简便一些，可以使用pdflatex xxx或pdflatex xxx.tex一次得到xxx.pdf。 明白了这些编译指令之后，使用Sublime Text再加一些配置即可不依靠其他IDE来完成$\LaTeX$文档的编写和生成。 中文的实现中文字符与英文字符在排版时有很大的不同，例如英文是以词为单位而中文是以字为单位，英文的字母宽度与标点宽度与中文也有一定的区别等等。所以默认的$\LaTeX$是不支持中文的。 这里就要再提到XeTeX，这也是一套在原基础上扩展而来的引擎，用XeLaTeX来完成.tex的编译，它的特点是原生支持Unicode和OpenType，配合ctex的宏包即可比较完美地支持中文了。 配置SublimeText2又到了Sublime时间，TexLive自带的那个编辑器感觉没啥功能，用着也很不舒服，于是准备继续使用Sublime来写和编译。 Sublime默认能够自动识别.tex的文本为LaTex文件，因此只需要稍微做一些配置即可。 首先还是要确保cmd下能够正常运行xelatex命令。一开始的时候我这里只能运行xetex、tex等命令，但是就是运行不了xelatex命令，而在TexLive自带的那个控制台中是正常的。 后来查询TexLive控制台的启动命令，发现它在启动时会把TexLive的运行目录放在系统环境变量的第一个，于是我调整了系统环境变量，遂成功运行。 我不明白为什么需要这样，也不知道这样会不会出现别的什么问题？ 接下来在Sublime中Tools &gt; Build System中新建一个Build System，填入： 12345678910111213&#123; "cmd": ["xelatex", "$&#123;file&#125;"], "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$", "working_dir": "$&#123;file_path&#125;", "selector": "text.tex.latex", "variants": [ &#123; "name": "Run", "cmd": ["cmd", "/c", "start", "$&#123;file_base_name&#125;.pdf"] &#125; ]&#125; 即配置完成。 之后Ctrl+B进行编译，Ctrl+Shift+B查看生成的Pdf文档。 其他具体的$\LaTeX$使用还是得花时间学习的，或者像我用Markdown一样，常见的一些操作多用就熟了，遇到解决不了的再去搜使用方法。 这里有一份简单的教程 网上还推荐参照模板来学习： 这个是国外一个不错的$\LaTeX$模板网站 比如这里这份通用论文模板： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159%----------------------------------------------------------------------------------------% 宏包以及其他文档配置信息%----------------------------------------------------------------------------------------\documentclass[twoside]&#123;article&#125;\usepackage&#123;lipsum&#125; % 用于生成本模板中虚拟内容的宏包\usepackage[sc]&#123;mathpazo&#125; % 使用Palatino字体\usepackage[T1]&#123;fontenc&#125; % 使用8位编码256个字形\linespread&#123;1.05&#125; % Palatino字体需要更多行距\usepackage&#123;microtype&#125; % 另外一种字体\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]&#123;geometry&#125; % 文章边距\usepackage&#123;multicol&#125; % 使用多栏布局\usepackage[hang, small,labelfont=bf,up,textfont=it,up]&#123;caption&#125; % 设定表格和图片的题注格式\usepackage&#123;booktabs&#125; % 表格水平对齐\usepackage&#123;float&#125; % 在多栏布局中使用表格和图片时需要的宏包(e.g. \begin&#123;table&#125;[H])\usepackage&#123;hyperref&#125; % 超链接\usepackage&#123;lettrine&#125; % 首字大写\usepackage&#123;paralist&#125; % Used for the compactitem environment which makes bullet points with less space between them\usepackage&#123;abstract&#125; % 允许定制摘要\renewcommand&#123;\abstractnamefont&#125;&#123;\normalfont\bfseries&#125; % 摘要设为黑体\renewcommand&#123;\abstracttextfont&#125;&#123;\normalfont\small\itshape&#125; % 摘要设为小斜体\usepackage&#123;titlesec&#125; % 允许定制标题\renewcommand\thesection&#123;\Roman&#123;section&#125;&#125; % 节标题使用罗马字母\renewcommand\thesubsection&#123;\Roman&#123;subsection&#125;&#125; % 第二级节标题使用罗马字母\titleformat&#123;\section&#125;[block]&#123;\large\scshape\centering&#125;&#123;\thesection.&#125;&#123;1em&#125;&#123;&#125; % 改变节标题的外观\titleformat&#123;\subsection&#125;[block]&#123;\large&#125;&#123;\thesubsection.&#125;&#123;1em&#125;&#123;&#125; % 改变第二级节标题的外观\usepackage&#123;fancyhdr&#125; % 页眉和页脚\pagestyle&#123;fancy&#125; % 所有页都有页眉和页脚\fancyhead&#123;&#125; % 取消默认的页眉\fancyfoot&#123;&#125; % 取消默认的页脚\fancyhead[C]&#123;Running title $\bullet$ November 2012 $\bullet$ Vol. XXI, No. 1&#125; % 修改页眉文字\fancyfoot[RO,LE]&#123;\thepage&#125; % 修改页脚文字%----------------------------------------------------------------------------------------% 标题部分%----------------------------------------------------------------------------------------\title&#123;\vspace&#123;-15mm&#125;\fontsize&#123;24pt&#125;&#123;10pt&#125;\selectfont\textbf&#123;Article Title&#125;&#125; % 文章标题\author&#123;\large\textsc&#123;John Smith&#125;\thanks&#123;A thank you or further information&#125;\\[2mm] % 名字\normalsize University of California \\ % 来源\normalsize \href&#123;mailto:john@smith.com&#125;&#123;john@smith.com&#125; % Email\vspace&#123;-5mm&#125;&#125;\date&#123;&#125;%----------------------------------------------------------------------------------------\begin&#123;document&#125;\maketitle % 插入标题\thispagestyle&#123;fancy&#125; % 所有页都有页眉和页脚%----------------------------------------------------------------------------------------% 摘要部分%----------------------------------------------------------------------------------------\begin&#123;abstract&#125;\noindent \lipsum[1] % 虚拟一段摘要文本\end&#123;abstract&#125;%----------------------------------------------------------------------------------------% 文章内容%----------------------------------------------------------------------------------------\begin&#123;multicols&#125;&#123;2&#125; % 双栏布局\section&#123;Introduction&#125;\lettrine[nindent=0em,lines=3]&#123;L&#125; orem ipsum dolor sit amet, consectetur adipiscing elit. % 一段文字，首字母L大写\lipsum[2-3] % 虚拟文字%------------------------------------------------\section&#123;Methods&#125;Maecenas sed ultricies felis. Sed imperdiet dictum arcu a egestas.\begin&#123;compactitem&#125;\item Donec dolor arcu, rutrum id molestie in, viverra sed diam\item Curabitur feugiat\item turpis sed auctor facilisis\item arcu eros accumsan lorem, at posuere mi diam sit amet tortor\item Fusce fermentum, mi sit amet euismod rutrum\item sem lorem molestie diam, iaculis aliquet sapien tortor non nisi\item Pellentesque bibendum pretium aliquet\end&#123;compactitem&#125;\lipsum[4] % 虚拟文字%------------------------------------------------\section&#123;Results&#125;\begin&#123;table&#125;[H]\caption&#123;Example table&#125;\centering\begin&#123;tabular&#125;&#123;llr&#125;\toprule\multicolumn&#123;2&#125;&#123;c&#125;&#123;Name&#125; \\\cmidrule(r)&#123;1-2&#125;First name &amp; Last Name &amp; Grade \\\midruleJohn &amp; Doe &amp; $7.5$ \\Richard &amp; Miles &amp; $2$ \\\bottomrule\end&#123;tabular&#125;\end&#123;table&#125;\lipsum[5] % 虚拟文字\begin&#123;equation&#125;\label&#123;eq:emc&#125;e = mc^2\end&#123;equation&#125;\lipsum[6] % 虚拟文字%------------------------------------------------\section&#123;Discussion&#125;\subsection&#123;Subsection One&#125;\lipsum[7] % 虚拟文字\subsection&#123;Subsection Two&#125;\lipsum[8] % 虚拟文字%----------------------------------------------------------------------------------------% 引用列表%----------------------------------------------------------------------------------------\begin&#123;thebibliography&#125;&#123;99&#125; % 参考书目\bibitem[Figueredo and Wolf, 2009]&#123;Figueredo:2009dg&#125;Figueredo, A.~J. and Wolf, P. S.~A. (2009).\newblock Assortative pairing and life history strategy - a cross-cultural study.\newblock &#123;\em Human Nature&#125;, 20:317--330.\end&#123;thebibliography&#125;%----------------------------------------------------------------------------------------\end&#123;multicols&#125;\end&#123;document&#125; 附上一个简单的$\LaTeX$测试文本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263\documentclass&#123;article&#125;\usepackage&#123;ctex&#125;\author&#123;Jcf94&#125;\title&#123;这是标题Title&#125;\begin&#123;document&#125;\maketitle\tableofcontentsHello, World!\section&#123;中国&#125; China is in East Asia.\subsection&#123;北京&#125; Beijing is the capital of China.\subsubsection&#123;东城区&#125;\paragraph&#123;Tian'anmen Square&#125;is in the center of Beijing\subparagraph&#123;毛泽东纪念堂&#125; is in the center of Tian'anmen Square\subsection&#123;广州&#125;\paragraph&#123;Sun Yat-sen University&#125; is the best university in Guangzhou.\section&#123;段落和换行测试&#125;Beijing isthe capitalof China.New York isthe capitalof America.Amsterdam is \\ the capital \\of Netherlands.\section&#123;表格测试&#125;\begin&#123;tabular&#125;&#123;|c|c|&#125;a &amp; b \\c &amp; d\\\end&#123;tabular&#125;\begin&#123;tabular&#125;&#123;|c|c|&#125;\hlinea &amp; b \\\hlinec &amp; d\\\hline\end&#123;tabular&#125;\begin&#123;center&#125;\begin&#123;tabular&#125;&#123;|c|c|&#125;\hlinea &amp; b \\ \hlinec &amp; d\\\hline\end&#123;tabular&#125;\end&#123;center&#125;\end&#123;document&#125; 很多特性其实感觉跟Markdown非常像。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>排版</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理 龙书]]></title>
    <url>%2F2016%2F01%2F22%2F2016-01-22-dragonbook%2F</url>
    <content type="text"><![CDATA[拖了快半年了，终于把《硬件/软件接口》过完了，第一遍看的比较粗略，也深深地意识到深入学CPU的难度之大T_T，回来先把编译原理的坑给填了吧Q_Q。 课后答案：http://dragon-book.jcf94.com/官方主页：http://dragonbook.stanford.edu/ 看了几章，感觉《龙书》里面废话很多啊，很多东西都重复讲了好多遍，而且感觉第二章很多余。 不知道是不是我太肤浅了。 1. Introduction引论 1.1 language processors语言处理器 区别一下编译器（Compiler）和解释器（Interpreter）： 高级语言的源程序经过编译器编译后产生目标程序，然后将数据输入目标程序得到输出结果；解释器则是同时将数据和源程序输入，利用输入的数据来执行源程序中指定的操作，得到输出。 通常，一个完整的语言处理系统（高级语言编译器）由以下几部分组成： 高级语言通过一系列步骤，最终生成能够在机器上执行的机器码。 1.2 the structure of a compiler编译器的结构 编译器整体上可以分为两个部分：分析部分（前端）和综合（后端）。 分析是将源程序分解成多个组成要素，收集有关的信息放在一个符号表（Symbol Table）里，并产生一个中间表示形式。 综合则是根据上一步得到的符号表和中间表示形式进一步生成机器码。 常把编译的过程分成5个阶段： Lexical Analysis 词法分析 Parsing 语法分析 Semantic Analysis 语义分析 Optimization 中间代码生成和优化 Code Generation 代码生成 1.3 the evolution of programming languages程序设计语言的发展历程 1.4 the science of building a compiler构建编译器的科学 1.5 applications of compiler technology编译技术的应用 1.6 programming language basics程序设计语言基础 1.7 summary of chapter 11.8 references for chapter 1 2. A Simple Syntax-directed Translator一个简单的语法制导翻译器 2.1 introduction引言 2.2 syntax definition语法定义 本节定义了一种上下文无关文法（Context-free Grammar），其由四个元素组成： 一个终结符号集合：文法中语言基本符号的集合 一个非终结符号集合：每个非终结符号表示一个终结符号串的集合 一个产生式集合： 箭头左边称为产生式头，右边称为产生式体。用于表示某个构造的某种书写形式。意为：产生式头 可以具有 产生式体 的形式 指定一个非终结符号为开始符号 有了明确的定义之后，文法推导就是：从开始符号出发，不断将某个非终结符号替换为该非终结符号的某个产生式的体。 则可以从开始符号推导出来的所有终结符号串的集合，即称为该文法定义的语言。 语法分析（Parsing）的任务是：接受一个终结符号串作为输入，找出从文法开始符号推导出这个串的方法。 如果不能推导出来，则报告该符号串中包含语法错误。 2.3 syntax-directed translation语法制导翻译 2.4 parsing语法分析 2.5 a translator for simple expressions简单表达式的翻译器 本章是以一个简单的文法制导翻译器为例，介绍编译的过程。 前几节是对中缀转后缀表达式翻译器文法、语法方面的分析。 书中在这里给出了一个极其简单的中缀转后缀的表达式翻译器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/* ***********************************************MYID : Chen FanLANG : JAVAPROG : postfix************************************************ */import java.io.*;class Parser&#123; static int lookahead; public Parser() throws IOException &#123; lookahead = System.in.read(); &#125; void expr() throws IOException &#123; term(); while(true) &#123; if (lookahead == '+') &#123; match('+'); term(); System.out.write('+'); &#125; else if (lookahead == '-') &#123; match('-'); term(); System.out.write('-'); &#125; else return ; &#125; &#125; void term() throws IOException &#123; if (Character.isDigit((char)lookahead)) &#123; System.out.write((char)lookahead); match(lookahead); &#125; else throw new Error("Syntax error"); &#125; void match(int t) throws IOException &#123; if (lookahead == t) lookahead = System.in.read(); else throw new Error("Syntax error"); &#125;&#125;public class Postfix&#123; public static void main(String[] args) throws IOException &#123; Parser parse = new Parser(); parse.expr(); System.out.write('\n'); &#125;&#125; 平时JAVA用的比较少，正好这时候多熟悉下 2.6 lexical analysis词法分析 前一节中的翻译器功能比较弱，为了在不改变语法的情况下扩充其功能，在词法分析这一步加入更多内容。 2.7 symbol tables符号表 词法分析的目的是按照文法从源程序中提取出各种词素，存在符号表中，然后将分析完的词素信息交给语法分析器。 在这里还有个作用域的问题，因此符号表的整体结构是一个栈的形态，每进到一个新的代码块中，就应该多建立一个符号表，查找变量定义时，则是从内层向外层逐层查询。 2.8 intermediate code generation生成中间代码 2.9 summary of chapter 2 3. Lexical Analysis词法分析 3.1 the role of the lexical analyzer词法分析器的作用 3.2 input buffering输入缓冲 3.3 specification of tokens词法单元的规约 介绍了正则表达式的相关规则。 3.4 recognition of tokens词法单元的识别 介绍了使用状态转换图来分析词法单元的方法。 3.5 the lexical-analyzer generator lex词法分析器生成工具Lex 3.6 finite automata有限自动机 有限自动机是一种识别器，它们只能对每个可能的输入串回答是或者否。 有限自动机可以分为两类： 不确定的有限自动机（Nondeterministic Finite Automata）对边上的标号没有限制，同一个标号可能有多条离开一个状态的边 确定的有限自动机（Deterministic Finite Automata）每个状态下对于每个独立的符号，只有一条边 正则表达式 $(a|b)^*abb$ 的NFA和DFA示例： NFA抽象地表示了用来识别某个语言串的算法，DFA则是一个具体可以实现该语言串的算法。 3.7 from regular expressions to automata从正则表达式到自动机 通常有了正则表达式就很容易得到它的NFA，而模拟NFA对于软件来说肯定是没有模拟DFA来的方便。因此常要将NFA转化为DFA，再交给软件去模拟。（当然，有时候从NFA到DFA的转化代价可能要大于直接模拟NFA，这时候就不会考虑再转化了） 书中介绍了一种广搜算法来完成NFA到DFA的转化。 子集构造法 123456789101112initially, t-closure(so) is the only state in Dstates, and it is unmarked;while ( there is an unmarked state T in Dstates )&#123; mark T; for ( each input symbol a ) &#123; U = t-closure( move(T, a) ); if ( U is not in Dstates ) add U as an unmarked state to Dstates; Dtran[T, a] = U; &#125;&#125; 计算t-closure(T) 123456789101112push all states of T onto stack;initialize t-closure(T) to T;while ( stack is not empty )&#123; pop t, the top element, off stack; for ( each state u 'with an edge from t to u labeled 空集) if ( u is not in t-closure(T) ) &#123; add u to t-closure(T); push u onto stack; &#125;&#125; 3.8 design of a lexical-analyzer generator词法分析器生成工具的设计 3.9 optimization of dfa-based pattern matchers基于DFA模式匹配器的优化 3.10 summary of chapter 33.11 references for chapter 3 4. Syntax Analysis语法分析 4.1 introduction引论 4.2 context-free grammars上下文无关文法 4.3 writing a grammar设计文法 4.4 top-down parsing自顶向下的语法分析 4.5 bottom-up parsing自底向上的语法分析 4.6 introduction to lr parsing: simple lrLR语法分析技术介绍：简单LR 4.7 more powerful lr parsers更强大的LR语法分析器 4.8 using ambiguous grammars使用二义性文法 4.9 parser generators语法分析器生成工具 4.10 summary of chapter 44.11 references for chapter 4 5. Syntax-directed Translation语法制导的翻译 5.1 syntax-directed definitions语法制导定义 5.2 evaluation orders for sdd’sSDD的求值顺序 5.3 applications of syntax-directed translation语法制导翻译的应用 5.4 syntax-directed translation schemes语法制导的翻译方案 5.5 hnplementing l-attributed sdd’s实现L属性的SDD 5.6 summary of chapter 55.7 references for chapter 5 6. Intermediate-code Generation中间代码生成 6.1 variants of syntax trees语法树的变体 6.2 three-address code三地址代码 6.3 types and declarations类型和声明 6.4 translation of expressions表达式的翻译 6.5 type checking类型检查 6.6 control flow控制流 6.7 backpatching回填 6.8 switch-statementsswitch语句 6.9 intermediate code for procedures过程的中间代码 6.10 summary of chapter 66.11 references for chapter 6 7. Run-time Environments运行时环境 7.1 storage organization存储组织 7.2 stack allocation of space空间的栈式分配 7.3 access to nonlocal data on the stack栈中非局部数据的访问 7.4 heap management堆管理 7.5 introduction to garbage collection垃圾回收概述 7.6 introduction to trace-based collection基于跟踪的回收概述 7.7 short-pause garbage collection短停顿垃圾回收 7.8 advanced topics in garbage collection垃圾回收中的高级论题 7.9 summary of chapter 77.10 references for chapter 7 8. Code Generation代码生成 8.1 issues m the design of a code generator代码生成器设计中的问题 8.2 the target language目标语言 8.3 addresses in the target code目标代码中的地址 8.4 basic blocks and flow graphs基本块和流图 8.5 optimization of basic blocks基本块的优化 8.6 a simple code generator一个简单的代码生成器 8.7 peephole optimization窥孔优化 8.8 register allocation and assignment寄存器分配和指派 8.9 instruction selection by tree rewriting通过树重写来选择指令 8.10 optimal code generation for expressions表达式的优化代码的生成 8.11 dynamic programming code-generation动态规划代码生成 8.12 summary of chapter 88.13 references for chapter 8 9. Machine-independent Optimizations机器无关优化 9.1 the principal sources of optimization优化的主要来源 9.2 introduction to data-flow analysis数据流分析简介 9.3 foundations of data-flow analysis数据流分析基础 9.4 constant propagation常量传播 9.5 partial-redundancy elimination部分冗余的消除 9.6 loops in flow graphs流图中的循环 9.7 region-based analysis基于区域的分析 9.8 symbolic analysis符号分析 9.9 summary of chapter 99.10 references for chapter 9 10. Instruction-level Parallelism指令级并行 10.1 processor architectures处理器体系结构 10.2 code-scheduling constraints代码调度约束 10.3 basic-block scheduling基本块调度 10.4 global code scheduling全局代码调度 10.5 software pipelining软件流水线化 10.6 summary of chapter 1010.7 references for chapter 10 11. Optimizing for Parallelism and Locality并行性和局部性优化 11.1 basic concepts基本概念 11.2 matrix multiply: an in-depth example矩阵乘法：一个深入的例子 11.3 iteration spaces迭代空间 11.4 aftlne array indexes仿射的数组下标 11.5 data reuse数据复用 11.6 array data-dependence analysis数组数据依赖关系分析 11.7 finding synchronization-free parallelism寻找无同步的并行性 11.8 synchronization between parallel loops并行循环之间的同步 11.9 pipelining流水线 11.10 locality optimizations局部性优化 11.11 other uses of affine transforms仿射转换的其他用途 11.12 summarv of chapter 1111.13 references for chapter 11 12. Interprocedural Analysis过程间分析 12.1 basic concepts基本概念 12.2 why interprocedural analysis?为什么需要过程间分析 12.3 a logical representation of data flow数据流的一种逻辑表示方式 12.4 a simple pointer-analysis algorithm一个简单的指针分析算法 12.5 context-insensitive interprocedural analysis上下文无关的过程间分析 12.6 context-sensitive pointer analysis上下文相关指针分析 12.7 datalog implementation by bdd’s使用BDD的Datalog实现 12.8 summary of chapter 1212.9 references for chapter 12 a. A Complete Front End一个完整的编译器前端 a.1 the source language源语言 a.2 mainMain a.3 lexical analyzer词法分析器 a.4 symbol tables and types符号表和类型 a.5 intermediate code for expressions表达式的中间代码 a.6 jumping code for boolean expressions布尔表达式的跳转代码 a.7 intermediate code for statements语句的中间代码 a.8 parser语法分析器 a.9 creating the front end创建前端 b. Finding Linearly Independent Solutions Index寻找线性独立解 待续…]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡马克快速平方根倒数算法]]></title>
    <url>%2F2016%2F01%2F14%2F2016-01-14-carmack%2F</url>
    <content type="text"><![CDATA[逛知乎的时候看到这么个问题： 游戏设计中有哪些经典的计算公式？ 其中有个答案提到了一种计算浮点数的平方根倒数的快速算法，其实我以前就曾经思考过大数平方根的计算方法，然后…就没有然后了。 神奇的地方在于这个快速算法中有个诡异的常数，据说至今没有人知道它最早是怎么来的。 代码示例下面这段代码来源于上面那个答案。 123456789float InvSqrt(float x)&#123; float xhalf = 0.5f*x; int i = *(int*)&amp;x; i = 0x5f3759df - (i &gt;&gt; 1); x = *(float*)&amp;i; x = x*(1.5f - xhalf*x*x); return x;&#125; 这个算法说到头只有这么几行而已，而恰恰神奇的就是这么几行代码可以非常快速地实现平方根倒数。 整个算法的关键就是第5行的那个诡异常数以及第7行的牛顿迭代了。 算法原理要理解这个算法，首先要明白牛顿迭代，而牛顿迭代法的基础是泰勒级数。 泰勒级数定义：如果$f(x)$在点$x=x_0$处具有任意阶导数，则幂级数： $$\begin{align}f(x)&amp;=\sum^{\infty}_{n=0}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n\&amp;=f(x_0)+f’(x_0)(x-x_0)+\frac{f’’(x_0)}{2!}(x-x_0)^2+…+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+…\end{align}$$ 称为$f(x)$在$x=x_0$处的泰勒级数。 泰勒展开在数学上还有很多的应用…然而，高数学过去那么多年了，有啥用我早忘了…0.0 这里只要知道泰勒级数可以用来近似计算函数值就行了。 后面部分明显是越往后越小的，收敛性应该是要有条件来严格证明的…这个我也忘了。 牛顿迭代牛顿迭代解线性方程，就是把线性方程$f(x)=0$线性化的一种近似方法。把$f(x)$在$x_0$的某个邻域内展开成泰勒级数，取其线性部分（前两项）： $$f(x)=f(x_0)+f’(x_0)(x-x_0)$$ 令其为零，就可以作为原本$f(x)=0$方程的近似方程了。只要$f’(x_0)\neq0$，方程的解就可以写成： $$x_1=x_0-\frac{f(x_0)}{f’(x_0)}$$ 这样，就得到牛顿迭代法的迭代关系式了： $$x_{n+1}=x_n-\frac{f(x_n)}{f’(x_n)}$$ 理论上来说，是不是我任意取一个满足$f’(x_0)\neq0$的数，都可以把它作为第1个近似解，然后通过不断地进行牛顿迭代，直到精度达到要求即解出了比较靠谱的方程解？ 回到目标算法上面来，假设已知的数是$a$，要求它的平方根的倒数，就是解方程： $$(\frac{1}{x})^2=a$$ 即解方程： $$f(x)=\frac{1}{x^2}-a=0$$ 那么： $$f’(x)=-\frac{2}{x^3}$$ 代回到牛顿迭代式中去： $$\begin{align}x_{n+1}&amp;=x_n-\frac{f(x_n)}{f’(x_n)}\&amp;=x_n-\frac{\frac{1}{x^2_n}-a}{-\frac{2}{x^3_n}}\&amp;=x_n+\frac{x_n-ax^3_n}{2}\&amp;=\frac{3}{2}x_n-\frac{a}{2}x^3_n \end{align}$$ 这个就是上面程序第7行的那个牛顿迭代式了。上面那个程序段只迭代了一次，其实精度是不够的，一般迭代2次，跟标准值的误差就不大了。 当然，迭代次数越多，精度越高，相应地耗时也是越长。 诡异的常数最后，还有个问题没解决……0x5f3759df这个常数是什么鬼啊？ 上面已经讨论过了，通过牛顿迭代法，我们可以在第n个近似解的基础上推出第n+1个精度更高的近似解。这里唯一的漏洞就是：我们必须得到第1个近似解！这样才能够往下递推！ 而上面第5行的这段代码： 1i = 0x5f3759df - (i &gt;&gt; 1); 就是用来计算第1个近似解的。 好吧，还是不明白这个数是怎么来的… 据史料记载： 浮点数的平方根倒数常用于计算正规化矢量。3D图形程序需要使用正规化矢量来实现光照和投影效果，因此每秒都需做上百万次平方根倒数运算，而在处理坐标转换与光源的专用硬件设备出现前，这些计算都由软件完成，计算速度亦相当之慢；在1990年代这段代码开发出来之时，多数浮点数操作的速度更是远远滞后于整数操作，因而针对正规化矢量算法的优化就显得尤为重要。 1999年的《雷神之锤III竞技场》就借助了这个神奇的算法。 《雷神之锤III》的代码直到QuakeCon 2005才正式放出，但早在2002年（或2003年）时平方根倒数速算法的代码就已经出现在Usenet与其他论坛上了。最初人们猜测是卡马克写下了这段代码，但他在询问邮件的回复中否定了这个观点，并猜测可能是先前曾帮id Software优化雷神之锤的资深汇编程序员Terje Mathisen写下了这段代码；而在Mathisen的邮件里他表示在1990年代初他只曾作过类似的实现，确切来说这段代码亦非他所作。现在所知的最早实现是由Gary Tarilli在SGI Indigo中实现的，但他亦坦承他仅对常数R的取值做了一定的改进，实际上他也不是作者。Rys Sommefeldt则在向以发明MATLAB而闻名的Cleve Moler查证后认为原始的算法是Ardent Computer公司的Greg Walsh所发明，但他也没有任何决定性的证据能证明这一点。 目前不仅该算法的原作者不明，人们也仍无法明确当初选择这个“魔术数字”的方法。Chris Lomont在研究中曾做了个试验：他编写了一个函数，以在一个范围内遍历选取R值的方式将逼近误差降到最小，以此方法他计算出了线性近似的最优R值0x5f37642f（与代码中使用的0x5f3759df相当接近），但以之代入算法计算并进行一次牛顿迭代后，所得近似值与代入0x5f3759df的结果相比精度却仍略微更低；而后Lomont将目标改为遍历选取在进行1－2次牛顿迭代后能得到最大精度的R值，并由此算出最优R值为0x5f375a86，以此值代入算法并进行牛顿迭代后所得的结果都比代入原始值（0x5f3759df）更精确，于是他的论文最后以“原始常数是以数学推导还是以反复试错的方式求得”的问题作结。在论文中Lomont亦指出64位的IEEE754浮点数（即双精度类型）所对应的魔术数字是0x5fe6ec85e7de30da，但后来的研究表明代入0x5fe6eb50c7aa19f9的结果精确度更高（McEniry得出的结果则是0x5FE6EB50C7B537AA，精度介于两者之间）。在Charles McEniry的论文中，他使用了一种类似Lomont但更复杂的方法来优化R值：他最开始使用穷举搜索法，所得结果与Lomont相同；而后他尝试用带权二分法寻找最优值，所得结果恰是代码中所使用的魔术数字0x5f3759df，因此McEniry确信这一常数或许最初便是以“在可容忍误差范围内使用二分法”的方式求得。 先复习一下计算机中浮点数的表示法 然后回到上面讨论过的第1个近似解，第1个近似解越接近正确答案，则显然牛顿迭代法的效果会越好。 考虑已知的浮点数$a$在计算机中的实际表示为： $$(1+F)*2^E$$ 那么它的平方根倒数为： $$(1+F)^{-\frac{1}{2}}*2^{-\frac{E}{2}}$$ 猜测一下：第5行代码中，将i右移1位，再用常数减掉，可以看成是将指数除2之后再取反。至于尾数…早都变得不知道哪去了。 所以，还是没明白这个常数是怎么来的，我只能猜测到这里了。 这里把Chris Lomont写的那篇论文也一并贴出来：FAST INVERSE SQUARE ROOT]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>平方根</tag>
        <tag>泰勒级数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具软件整理]]></title>
    <url>%2F2016%2F01%2F11%2F2016-01-11-tools%2F</url>
    <content type="text"><![CDATA[2016年的第一篇博文，记录一下自己平常使用的一些软件。 IDEDev-C++ 平台：Windows 短小精悍，即写即编。 整个软件很小，也没有什么复杂的设置，基本的编译调试功能都有了，缺点是调试功能不太强。 CodeBlocks 平台：Windows、Linux、OS X 跨平台，支持多种编程语言，甚至可以开发QT。 重点是调试功能非常强大，以前只觉得VC的调试能力很强，可惜编译系统不是GNU一系的，没法用啊，好在发现了CodeBlocks。查看内存、调汇编什么的完全不虚。 文本编辑器Sublime Text 平台：Windows、Linux、OS X 原本只是个简单的文本编辑器，但其扩展能力非常强大，因而现在已经打造成我平常最常用的IDE了。唯一的缺点是没法断点调试。 并且自带vim模式哦。 Github上面出的Atom也用过一段时间，最后还是跑回来用回这个了。 最近微软出了个VS的精简版编辑器，不知道效果怎么样。 Haroopad 平台：Windows、Linux、OS X 最早我用过像“作业部落”那种在线的编辑器，处理本地文件比较不方便。 Sublime里面也有能够实时显示Markdown的插件，或者有人是在Sublime里面写，然后另外用一些nodejs的插件在网页中实时预览。 多方对比之后终于让我找到了这么个软件，支持多种格式多种主题，效果非常不错。 也是自带vim模式哦。 Visual Studio Code 平台：Windows、Linux、OS X 原本以为VS Code这个东西是从VS里面精简出来的，结果知道真相的我当时就傻了。 VS Code可以说跟Github的Atom师出同源，都是源自Electron这个开源框架（电子-&gt;原子，Github也真是会起名），也是微软相当少见的出的开源的软件。 其实直观地看VS Code跟Atom有几分相像，类似风格的界面，同样的关注重点在极强的可扩展性上，同样拥有大量插件。虽然Atom的官方页面上吹得很厉害，然而不得不承认微软就是厉害啊。自己在电脑上装一下试试就能明显感觉到，Atom卡得要死，然后VS Code则是相当顺滑。 VS Code上面的Markdown插件可以很方便地生成预览，虽然目前好像还没有做预览和编辑的同步滚动，然后貌似也还不支持Mathjax，但是用起来已经相当舒服了。因为我主要用Markdown还是记录Hexo博客，Hexo的很多扩展功能本来就是一般的Markdown编辑器显示不了的，所以少点功能对我来说影响不太大，用hexo s直接本地查看博客效果作为预览就好。 所以Haroopad已经被我卸掉了…0.0 然后VS Code上面还做了个很厉害的C/C++调试器，几乎能做到跟VS差不多的调试功能，只是目前还在开发中，可以期待一下。如果这个完成的话，可能我也会把Sublime Text给卸掉。界面方面，习惯了Sublime之后，对VS Code确实不是特别满意，不过还可以接受。 浏览器IE11 平台：Windows 感觉微软家族里面最不受待见的就是这个了吧，响应速度慢、各种各种问题。 但是不得不说这还是日常最常用的。 尤其是一些一直没有更新过的旧网页，是用很老的IE内核写的，新版的浏览器根本兼容不了上面的Js（长大的信息门户教务系统就是这样，非IE兼容模式不可正常使用啊）。 对我来说，IE最重要的功能是同步，两台Win10的笔记本，微软账户的同步功能很有用。 Microsoft Edge 平台：Windows 以前电脑上除了IE以外，我还会装个Chrome。 自从更了Win10以后，我发现Edge很好用啊。 后来的版本也能跟随微软账户一起同步了。 唯一的问题是收藏夹管理TM谁做的？？？整理收藏夹简直反人类！！！稍微动一下它就自己又乱了。oh…已弃疗。 Chrome 平台：Windows、Linux、OS X、Android 这个没啥说的，最强大的浏览器不为过吧？ 也是用了好多年了，然后装了卸，卸了装。很大一部分原因是因为不太喜欢电脑上装太多浏览器，然后就是发现普遍大家不太接受的微软系浏览器我用着觉得还行，再有个原因是Chrome的好多功能都被墙掉了，也是反而会带来更多的麻烦。 现在设备多起来了，平时要工作的平台也多了，于是就又把Chrome下回来了。。。-_- Chrome的同步能力真心好用，Windows、Linux、Android几乎可以做到无缝切换。 媒体播放器Windows Media Player + K-Lite Codec Pack 平台：Windows 极力推荐这个插件！大概是Win7时代发现的这个东西，一直用到现在。 其实系统自带的WMP很好用的啊，紧随系统，并且在智能手机还没现在这么火的时候，像DLAN这些功能很早版本的WMP上面就已经有了（还记得4、5年前用MX一代随意控制电脑放歌的时候，现在都出到MX5了）。 K-Lite Codec Pack这个插件完全弥补了WMP的短板，即支持的格式不多的问题。装完之后我是没发现还有什么格式是WMP打不开的了。 以前暴风这些都是干干净净就是个本地播放器，现在一打开就是各种广告、各种在线资源。然而我什么都不想要啊！！我从网上下好了资源，只想找个干净的本地播放器看就好了啊！！ So，WMP+KCP就是最好的选择了。 网易云音乐 平台：Windows、Android 啥都不说了，最完美的在线音乐播放器，没有之一。 最近出了Win10应用商店版，可以完全替代PC版的了，满分！！！ 同步软件OneDrive 平台：Windows 因为有两台电脑的同步需求，同步文件用的软件也是我一直都在找的。 以前感觉用过的最满意的是DropBox，可惜后来好像是被墙了…坑。 后来Win8开始用系统自带的SkyDrive，也就是现在的OneDrive。 当年SkyDrive也是各种坑啊，文件稍微一多就出错，而且对网络要求比较高，网差了基本就传不动了。 现在更新完了之后最新版的OneDrive非常好用，上传下载也是非常快。 坚果云 平台：Windows、Linux、OS X、Android、iOS 全平台支持，而且可以自己选定文件夹进行同步，支持增量同步。 没有了DropBox之后，一直都不知道该去哪找有这样功能的同步软件，师兄推荐了一下，果断用上了。 自从Yoga上面装了Arch之后，在Yoga上开Windows的次数就少了很多了，大部分工作能在Arch下完成的都不太想切到Windows里面去。 我需要的多平台同步功能主要是博客，现在本地的hexo是整个目录都在OneDeive上面，两台Win10笔记本同步的效果倒是还不错。因为考虑到同步冲突的问题，我只把hexo的source目录扔到了坚果云上进行同步，这样稍微产生冲突的时候，出问题的文件也比较少，可以手动调整过来。 Linux和Android上主要改一下source中的Markdown文章，然后生成博客和上传的工作还是换到Windows下完成，这样可以保证hexo的稳定性。 待续…]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>整理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的保研路——致所有不懈努力着的长大学子们]]></title>
    <url>%2F2015%2F11%2F24%2F2015-11-24-baoyan%2F</url>
    <content type="text"><![CDATA[你想要的，时间都会给你。 距离推免志愿确认完毕已经过去差不多两个多月了，也是很早就想过要记下些东西，结果一直没有去写。 回头看看这一年做过的准备，走过的路，写过的材料，发过的邮件，翻了下当初留下的那几篇随笔（【tags：保研】），随着时间慢慢过去，心态一直在变，也真是感触良多，幸而最后的结果是好的。 之前在学校准备外推的时候，有不少学弟学妹问过我保研的事情，今天顺便好好理理吧。 我的保研路要说最早的时候问我要不要读研？其实我是没什么想法的。 一开始只是想好好读完大学，以后的事情以后再说，可能大四毕业顺利找个工作就可以回家了。 这是大一后来越来越发现，通信并不是我以后真正想要从事的行业，不知道当初为什么铁了心的所有高考志愿都把通信写在第一个。怎么办？也许再读个研，研究生小跨一下专业？ 期间给系主任吴老大发过几次邮件，问他研究生如果想要跨去计算机怎么样？他说应该不存在太大的跨专业问题，毕竟都是信息这个大行业中的专业。 然后又想起10级的Psw学长离开学校去复旦之前跟我聊过的很多，总结一下就是： 好好搞ACM！然后好好保研！ 开始有了读研的打算了众所周知我们专业学霸一堆一堆的，这学期最后统计所有人成绩的时候，绩点从90多分排到80分的大概能排到接近40名左右（我们一共69个人…0.0）。 要说我虽然大一拿了国奖，我自己都明白其实我的成绩并不是最顶尖的那一批，只是我恰好对所有跟计算机相关的东西都有一种敏感，恰好擅长敲代码，恰好参加的课外竞赛比较多而已。 后来虽然大二也是总评第一，然而纯绩点已经不够国奖线了。大三参加的比赛少了，掉的更惨。 而有时候为了冲刺比赛，放弃一下课堂上的学习也是常有的事。但是我从不后悔选择ACM（就像从不后悔当初面临高考依然坚持NOIP），也是深深地庆幸当初没有为了其他的原因而放弃它，我从这里收获到的，远远超出所得的成绩本身。 那时候对自己的学习成绩本身也是没有信心的，一旦有要读研的想法之后，谁不想保研啊？但是也要担心自己的成绩到底够不够上？ 后来开始关注保研论坛，偶尔上去刷刷帖，刷刷积分，读一下前人留下的心路历程，收藏一下他们放出来的材料。 也开始关注一些计算机的论坛，找找自己真正想做的是什么，挑专业，挑学校。 大二这一年推掉了很多事情，专心搞竞赛。 大三首先很惊讶地得知从这一年开始国家的保研政策变了！原本只有专业前几个可以保到外面去，剩下的是只能保在本校的，现在开始全部放开，不再区分保内保外。 政策一出，第一年的这一批没有早作准备的大四学生就吃了很大的亏，很多人都没有想过自己能有机会保到外面去，而错过了很多夏令营的机会。 论坛上也有人预言第二年暑假夏令营的风向会有很大的变动，不知道对我们这一批恰好赶上了政策变革的人来说，是好事还是坏事。 也许是去年的现在？ 在论坛上发现中山大学的超算学院开始招收寒假优秀大学生冬令营的营员，目标针对的就是第二年即将决定未来归宿的大三学生！！ 之前只听说过夏令营，为了推免而举办的冬令营却是第一次听说。 于是我早了半年就开始准备材料，早了半年跑本部把打印成绩单、排名证明等材料的流程走了一遍（本部打成绩的老师当时也是觉得惊讶…-_-…说我去的太早了点吧），早了半年就寄出了我的第一份申请材料。 你问后来怎么样了？后来，后来结果就是果断被刷掉了… 也许是因为这是那年寒假唯一的冬令营，竞争的人有点多，或者还是因为自己的材料实力不够吧。 这一年，家里生意不太好，结果又凑钱准备买房，真是把能贷的东西都给贷出去了。 我都想是不是本科毕业直接出来工作算了？也许凭我这些年本科还可以称得上辉煌的经历和编码的能力，找个10~20k的工作应该也是比较容易的（…话说我都联系过MEIZU了…）。 要不还得再被家里养3年。 大三的第二学期提前看好了很多个学校，材料也准备的差不多了，开学之后几乎是每天都会刷几次保研论坛，看看有没有出什么新消息。就等各个学校开放之后开始海投了。 然后这个时候我又开始算自己的成绩了。 大一我把自己弄得很忙，大二算是稍微清闲了一点，大三却可能是我最忙的一年。 跟Lw一起创立了A协，刚创的新社团有不少事情要操心，然后要给新生上课，一边准备自己出去参加比赛，另一边又要准备在学校给别人办比赛… 我们专业，大三也是课多、实验多。几乎每门课都要跑本部做实验，后期天天要早起赶校车去本部，然后花上一整天时间。 一堆课设。 班里以及其他还有很多琐碎的小事。 然后就是为了保研还要有很多东西要准备。除了关注各处的信息以外，毕竟我的目标还是跨了专业了，也必须花时间补一下计算机的核心课程。 这一年真正留给课堂内容学习的时间其实很少很少… 看了下自己大三上学期的成绩并不算好，算下来也就是在第一梯队的末尾了，风险很大。我只能靠自己唯一拿的出手的那些竞赛成绩来加点分了。 我们学校计算保研资格时的政策是纯绩点加上一点点的附加分，幸而那些对于其他人来说很难凑的附加分我却是很早以前就足够给自己加满了。但是之前上限5分的附加分在新政策之后被下调到了3分。眼看着到手的分数又少下去2分，感觉不确定性又加大了，只有希望今年不要再有政策上的变动了。 如果保不上，考研吗？那时候确实是这么想的。 我查过计算机考研要考的是408，然后上网买了自己缺的那些课的教材。学校里面理学院开了个高数辅导班，然后我也去报了个名每周上课去了（虽然最后一段时间课没上完就没去了-_-）。 如果现在问我这个问题的话… 保不上我还是去找工作算了。(⊙o⊙)，真心觉着考研是很累的，要我考肯定考不上，哈哈。 终于到了夏令营报名通知井喷的时候查学校、查通知、发邮件、写材料、找老师写推荐信、寄材料… 每天就都是这么过来了。 然后就到了四处碰壁的时候。 看上的不少学校夏令营给的都只有直博的名额，邮件过去： 我说：我现在还不想直博，至少现在还不知道自己读完硕士以后是不是有毅力继续在象牙塔里面待下去。 回复：哦，那就拜拜吧。 其实心里最想去的还是浙大。想回家嘛。 但是浙大计算机学院的夏令营是时间长达一个月的暑期实习，虽然能增进与导师和实验室的了解，但是占用这么长的时间就意味着没有机会参加其他学校的夏令营了。而且我的目标实验室居然没有在这次实习单位的名单里面。 只有准备10月份的时候再另外尝试一下浙大了。 最后我挑好的“海投”列表中，要么是我最后考虑了还是不想去的，要么是不愿意直博邮件过去询问被人家直接拒绝掉的。 真正最后投了材料的只有三家，中科大计算机、北大信科、中山卡内基联培。 系里面一次读研/工作指导会上，付悦学姐：为什么不投一下清华啊？男生为什么不想读博啊？我：…╮(╯_╰)╭ 也是很感谢学姐的好多帮助和指导。 然后中间刷屏了好长时间转发锦鲤，哈哈。 中科大是最早投出去的，也是最早给我确认入选并且给出了A档资格的。（后来知道中科大也是这几家里面投入最多，最用心的一家） 然后是北大的信科院，也是唯一一个人家说的只要直博，然而我还是投了的，毕竟那是北大。审核完了，确认是否能参加时我发现时间跟中科大的重了。后来在我决定放弃时，又有个北大的师姐打电话过来说看过我的材料后，她们信科院下的系统结构研究所可以招直硕，让我考虑一下过去参加夏令营。 毕竟那是北大啊！而且坊间流传说很难的北大机试题对我们这些ACM选手来说都是小菜一碟啊！ 但是后面面试被刷的风险还是很大啊… 毕竟那是北大。心里猜测，也许本科出身对于他们来说会更加看重吧。各方面的担心更加多了。 投材料时心惊胆战担心审核过不了，到了手里拿了几张入场券的时候，地位突然就反转过来了。 最终两难之下，我还是选择了中科大。 毕竟他们已经给了最好的待遇，毕竟之前中科大跟我确认时我已经答应了，毕竟相对来说风险还是要小一些。 不管怎么选择最后都有可能会后悔。最终的决定还是性格使然吧，是激进地冲一下？还是保守一些？任何选择都是最好的选择。 最后是中山大学和美国卡内基梅隆大学的联合培养项目。国内一年，美国一年，入学时是通过保研通道提前确定名额，入学后算做卡内基的正式学生。 CMU可是全美国计算机排名前五的神校，呵呵。 他们来西安开宣讲会的时候我还淡定地混进西交去听了一场学术报告。 人家开出的条件确实很心动啊，那段时间脑子里面比较狂热的一个想法就是肉身翻墙出去，在美国的IT界捞金几年，然后风光回国… 还特地为了申请，跑本部档案室开了英文的各种材料（翻译一份成绩单要了我100啊，真是坑）。 该投的都已经投了，结果也都拿到了于是暑假就去了合肥和广州，辗转在祖国跑了一个大圈。 【中科大in合肥】 【中山卡内基in广州】 到最后回到家的时候，其实心里已经确定好了要去哪了，也是累的不行，不想再到处跑了。 不过事情还没结束大四开学回到学校的时候，感觉气氛比原来更加紧张了，offer已经在手，接下来要确定的就是自己能出得去，否则这一年来所做的一切都要付之一炬了。 我和隔壁班的班长关注的比较早，成绩和附加分我们是已经提前很久就按照前一年的规则算好了，只是不确定今年会不会再有什么变动，就等学院拿出正式通知和政策文件，一切都能尘埃落定。 一等又是好久。 等到科大已经开始催着确认每个人的推免资格，距离推免系统开放注册也没剩下几天的时候，学院的文件终于下来了。 于是匆匆忙忙地开会、算分、查分。就怕中间出现任何的岔子，3年的努力都系于此！ 推免名单公示时，终于松了一口气。 赶紧跟科大研究生教务办和导师又确认了一遍。 后话处理完所有的事情已经快到9月底了，却又突然接到老爸的电话说老妈生病了要在上海动手术，于是匆忙买了票直接赶赴上海。 也是在上海医院里最后完成了推免的报名手续。 最早是打算弄完所有事情，最后可能还有机会再冲一把下半年的ACM区域赛的。然后想想自己这么久没练了，水平早都退化到不知道哪去了，再上也是拖后腿，只有把希望寄托在下一届身上了。…Syh带队果然拿回来个银牌！也是圆了我们学校多年的ACM梦！ 看到推免系统显示回来的： 已录取 的那一秒，脑子里面是空白的。 结束了，终于结束了。 现在终于除了我长之外，又多了一个可以亲切称呼的： 我科 除了一群学弟学妹之外，又有了一群： 师兄 师姐 准备推免的一年中，发生过很多事，碰过很多壁，有很多开心的事情，也有很多灰心失落的时候，幸而这一年安然度过了。 我想感谢的人很多，只是很多话到了嘴边又不知该如何提起了，唯有将此历程与大家共勉了。 写给长大的学弟学妹们好，废话聊完了，下面是干货： 首先是很多人问我到底能保几个？ 我们学院一个年级总人数大约400人左右，推免的名额，去年是41个，今年是37个，每年都可能会有变动，以后只能说估计人数差不多也是这个数，不能确定。分摊到每个专业大概10%不到吧，比如我们专业去年是7个，今年是6个。新几届的卓越工程师班这些应该比例会高一些。 然后是推免的时间点，从什么时候开始准备？ 要我说，如果想要考虑保研，很早之前就要开始考虑了。因为最后系里面比的是所有人这三年以来的成绩和积累，当然大一成绩差点，大二还来得及。真正要开始准备材料这些的话，大三上学期结束的那个寒假就可以开始了，大三下学期一来就要到处跑开证明、开成绩单、找老师写推荐信什么的。到5月份左右，各个学校的夏令营通知都会发出来，需要提前关注好，然后提前联系导师，报名，审核。暑假，全国各地到处跑，到处笔试面试。10月左右，拿到学校里面的推免名额，推免系统开放。大多数学校还会有第二波的推免面试，夏令营没有去的，这时候还来得及再去试一下。在推免系统中及时报完名，然后就结束了。 夏令营的内容？ 每个学校都有自己的特色，每个学校的每个学院/每个系都可能是不一样的。一般都是参观、学术交流、笔试（机试）、面试…等等具体的还是要找自己的目标学校单独了解比较好。面试是一定会有的。通过之后会发预录取的offer。 面试的内容和准备？ 个人觉得重点应该放在：科研经历、竞赛经历、项目经历上。一般的面试的教授可能会问一些专业课的内容，不过应该对你在课外学术方面的工作更加感兴趣，因为那些才是能体现出你个人能力的地方。（当然也有例外，如果是外国的教授，感觉对专业课的基础是否扎实非常看重！中山卡内基的面试真是被虐惨了…T_T…） 除了成绩特好，没什么其他的特长，也没做过什么比赛、项目等等这些怎么办？ 额，我觉得，还是要有的吧。走出学校到了夏令营这个更大的平台上，你会看到很多牛人。拿了一堆国家奖的、发过好多篇SCI的、手上做过的项目可以直接出书的…比比皆是。成绩很重要，是敲门砖。但是敲开门之后怎么让人家对你有印象，想要留住你，这就是单纯的成绩做不到的了。好吧，我属于成绩没那么好的那种…对这个问题没什么发言权。 其他的一些话（嗯，就是俗称的鸡汤时间） 没有人能够决定你的未来，未来掌握在自己的手里。对于自己到底是不是适合读研，选择什么方向，还是要自己考虑清楚再说。都这时候了，不能没有一点自己的主见啊。 眼界还是要开阔点，在学校风生水起的，出去可能什么也不是。学校外面的世界很大，牛人也多了去了。“我的征途是星辰大海！” 合理地预估自己的水平。不要妄自菲薄，也不要骄傲自大。之前在论坛上老是能看到“求定位”的帖子：我的情况怎么样怎么样，大家帮忙看看能上什么学校啊？想进哪哪哪有没有希望？说实话挺不喜欢看这类帖子的，下面的回复无非就是“楼主加油”、“顶”、“啊，我的情况是怎么样怎么样，大家也帮我看看”，感觉毫无帮助。而且别人说能怎么样就是怎么样了吗？ 认真地挑选目标。仔细地考虑自己最想去的、最适合自己的是哪里。如果有机会，能冲就冲一下（之前好像从来没听说过我们学院有保去清北的？…我是10月份身边发生的事情太多了，心累，而且科大当时找的也是我最满意的导师，选的最满意的方向了，所以懒得再到处再跑了…希望未来能听说后面几届有开这个先河的）。 ..待续]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>保研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共享内存系统并行 OpenMP]]></title>
    <url>%2F2015%2F11%2F20%2F2015-11-20-openmp%2F</url>
    <content type="text"><![CDATA[OpenMP是一个针对共享内存并行编程的API。 与之前的MPI不同的是，OpenMP是线程级并行，比MPI的进程级并行要更轻量化一些。在 更重要的一个特点是，MPI的并行需要完全重写整个程序，而将一个串行程序改造成OpenMP的并行则有可能只要进行少量的改动即可。 而且gcc原生支持OpenMP，不需要像MPI一样另外要装个运行环境和运行库。 用gcc编译时加上-fopenmp开关即可： 123$ gcc -fopenmp &lt;source.c&gt; -o &lt;exec&gt;$ g++ -fopenmp &lt;source.cpp&gt; -o &lt;exec&gt; 示例12345678910111213141516171819202122232425262728293031/* ***********************************************MYID : Chen FanLANG : G++PROG : openmp_test************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;omp.h&gt;using namespace std;void hello()&#123; int my_rank = omp_get_thread_num(); int thread_count = omp_get_num_threads(); printf("Hello from thread %d of %d\n", my_rank, thread_count);&#125;int main()&#123; int thread_count = 4;# pragma omp parallel num_threads(thread_count) hello(); return 0;&#125; 上面这段示例代码中首先增加了一个omp.h头文件，然后就是主函数中多出来一句串行代码中没有的预处理器指令，其他的跟正常的串行程序没什么区别。 # pragma是C/C++中用以允许非C语言规范部分的行为，如果编译器不支持预处理器指令，那么编译时这句话就会被忽略掉。 OpenMP就依靠这些以# pragma omp开头的预处理器指令来进行线程级并行。 预处理器指令后面加的是一些子句，用来附加额外的控制信息。比如说num_threads()子句是用来控制接下来的代码块中需要用多少个线程进行并行。 让我比较头疼的是…虽然说并行的几个线程可以认为是同时在运行，当他们都要占用屏幕进行内容输出时会发生抢占。然而MPI的进程在相互抢占时，至少还能保证一个进程输出的东西是在一起的，只不过每次运行时，进程输出的顺序可能会不同。但是OpenMP中，上面那个程序输出来的东西就几乎是完全混乱的了…0.0…应该是其中一个线程只输出了几个字符就被另一个线程抢占了。 OpenMP的并行模式在MPI中，程序编译完成之后如果直接打开是无法运行的，需要用mpiexec来调用生成好的可执行文件，mpiexec会首先得到运行的目标机器、进程数等等情况，然后开始启动多个进程，等到多进程全部开起来之后，并行就开始了。 而在OpenMP中，编译完成之后的可执行文件可以直接运行，程序在一开始是串行运行，到了需要并行的时候，单进程单线程会分裂成单进程多线程（其实是除了主线程以外，又启动了几个新的线程同时执行），执行完毕之后又回到单线程的串行。而且每次并行的线程数是可以在运行时指定的。 比如说像这张图： MPI和OpenMP的区别还是比较大的。 所以相对来说，OpenMP可以只把其中的一部分作并行处理，而且并行的时候共享的内存、变量等等都是在一起的，从串行程序的基础上改造过来非常容易，可能只要加几段预处理器指令就可以了，剩下的交给编译器和处理器去解决就可以了。 冲突解决不同于MPI需要依靠进程间通信来完成协作，既然OpenMP是内存共享的，很多操作只需要解决掉对同一块内存的访问冲突就可以多线程协作了。 OpenMP中的冲突解决主要有四种方法： Crirical指令/归约指令 例如： 1234567 int sum = 0;# pragma omp parallel for num_threads(100) for (int i=0;i&lt;100;i++) &#123; sum += i; &#125; printf("%d\n", sum); 直接运行的结果是每次运行，sum最终给出来的结果都有可能是不同的。因为运行时多个线程同时访问了sum这个变量，可能前一个线程写上去的内容马上被下一个线程给覆盖掉了，即出现了数据冲突。 12345678 int sum = 0;# pragma omp parallel for num_threads(100) for (int i=0;i&lt;100;i++) &#123;# pragma omp critical sum += i; &#125; printf("%d\n", sum); 加上# pragma omp critical指令即告诉编译器需要安排线程对下面执行的代码进行互斥访问，即每次只能够有一个线程执行下面的这一句代码。 或者采用： 1234567 int sum = 0;# pragma omp parallel for num_threads(100) reduction(+: sum) for (int i=0;i&lt;100;i++) &#123; sum += i; &#125; printf("%d\n", sum); reduction(+: sum)是归约子句，加上这一句之后，执行下面的并行任务时，sum本身是共享的，但每个线程在执行时都会产生一个私有变量，当并行块运算结束之后再将私有变量的值整合回共享变量。 带命名的critical指令 可用# pragma omp critical(name)来命名不同的临界区 对同一个临界区的访问还是跟上面一样，一次只有一个进程能够进行操作，而对不同的临界区则可以有不同的进程进行同时访问。 atomic指令 用# pragma omp atomic作用在形式为： 12345x &lt;op&gt;= &lt;expression&gt;;x++;++x;x--;--x; 的指令中。 这些语句可以用CPU中的某些特殊硬件指令来实现。 简单锁 123omp_set_lock(&amp;lock);critical sectionomp_unset_lock(&amp;lock); 锁住的区域只允许单个线程进行访问]]></content>
      <categories>
        <category>Parallel</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>并行编程</tag>
        <tag>OpenMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lightbot 一个有意思的编程解谜游戏]]></title>
    <url>%2F2015%2F11%2F15%2F2015-11-15-lightbot%2F</url>
    <content type="text"><![CDATA[帖个图： 首先把各种指令动作输入到右边的指令框中，然后机器人会依序按照框中的指令来完成动作的执行。 目标就是点亮图中所有的蓝色方格。 刚看到这个游戏的时候就觉得眼前一亮，好赞的画风，好赞的游戏创意！ 这个是Lightbot的官方网站：http://lightbot.com/ 然后有个网页可以打开的Flash的Demo。原来准备把这个Demo扒过来的，结果人家做了防盗链，Flash的swf文件倒是扒下来了，然而一旦挂到网站上去它就会自己检查域名，不是他的网站不让运行…（怨念中） Swf里面是3个示例大关，可以下过去之后直接用Flash播放器或者拉到普通的浏览器里面打开：点此下载Swf（约4M） 当然还有最重要的Android客户端，完整版6个大关，最后一个比较有挑战性：点此下载1.6.3版的Apk（约41M） 基本介绍基本的指令有：前进、点灯、左转、右转、跳跃。用以控制机器人的动作。 然后更重要的是这个游戏里面出现了函数/过程的概念：点完运行后，机器人会执行主函数（MAIN）中的指令，然后下面最多会提供2个子函数可以调用，子函数的调用可以用来构成递归循环，循环也是完整版中第4大关的主旨。 第5关还开始出现红绿两色的条件指令。 最基础和最重要的一些编程思想都已经包含在里面了，而且其中的有些关卡还会要求用规定的指令数量来完成，更增加了挑战性。 前5大关每一大关会增加一些新的指令，有新的功能。第6大关就是综合了，个人感觉…真的是相当难啊啊啊啊！！！ 我的游戏记录把打星的几个关卡记录一下，这种游戏的特点就是方案不唯一，求交流哈哈~~ 2-9 思路：合理地利用好P1和P2的调用，我是把点亮三个块作为一个基础模块，然后反复调用。 3-2 思路：P1上去，P2回来，然后开始下一个相似的步骤。 3-9 思路：点亮5个块的方式一样，都是上前一步，点亮，转圈回来，写在P1里。然后P2用于处理右上方的4个块。 4-8 这关当时卡了很久……怎么推都不对。 思路：原来一直尝试的是把整个区域按照2*3分块，然后思考怎么让它走完2*3的格子之后能继续开始下一个循环，算是有点思维上的误区吧。最后的方案是循环完了，虽然多走了几步，但是能够花很少的步数把它补上。 4-9 思路：没啥说的，四个区域一个相同的循环。 5-6 思路：绿色块的处理是相同的，用红色块退出即可。 5-7 思路：红蓝区分一下即可。 6-1 第六大关开始，每个都很坑…… 思路：开始的时候试的想法是上一排纵向走完再回来，结果没成功，后来改成横向走。两组阶梯是一样的，主要要注意的是怎么从中间走回来。 6-2 思路：看上去下面部分跟前面的4-8是一样的，但是尝试用4-8的方法走失败了，改走1*6的格子。 6-3 6-4 6-5]]></content>
      <categories>
        <category>玩玩玩玩</category>
      </categories>
      <tags>
        <tag>Flash</tag>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPI小结]]></title>
    <url>%2F2015%2F11%2F14%2F2015-11-14-mpi%2F</url>
    <content type="text"><![CDATA[这段时间刚好赶上这学期的考试期，之前MPI只是稍微看了一点，在这里：并行编程 MPI初探 考完一个阶段稍微轻松些了，回来把总结补上。 Message Passing Interface并行编程的结构大致有两种： 共享内存式系统：运算核心有多个，但是多核心通过访问一个公共的内存区域来进行协作； 分布式内存系统：运算核心有多个，每个核心有它自己的内存，多个核心之间通过通信来协作，通信可以是通过网络实现的，因此分布式系统可以做到多机并行。 MPI就是一种分布式内存系统。 它的全称叫做消息传递接口，就是在C、C++、Fortran的基础上又做了一套并行的函数库接口，然后运行时通过多进程之间的消息传递机制来协作完成并行任务。 MPI程序框架首先在主函数开始时调用MPI_Init()来建立MPI进程，如果程序不需要额外的输入，则直接传入MPI_Init(NULL, NULL)。 中间的代码段则是各个进程同时都会运行的部分，所以需要注意的是，MPI程序与一般的串行程序应该是完全不一样的，因为相同的代码段会被不同的进程同时运行，而需要程序员在写代码的时候就考虑到不同进程的运行情况，写出特殊的代码，进程与进程之间通过通信子来进行相互通信交换数据。 如果只是简单地将原本的串行程序放上去，那只不过是同时在多个核上跑了很多遍一模一样的代码而已，完全达不到加速的效果。 关键的MPI语句运行完毕之后，如果中间有创建过另外的通信子或者组的话，还需要单独将它们释放掉。 最后，调用MPI_Finalize()来结束MPI程序。 基本函数123MPI_Comm_size( MPI_Comm comm, int* comm_size) 返回comm通信子中的进程总数。 123MPI_Comm_rank( MPI_Comm comm, int* comm_rank) 返回当前进程在comm通信子中的序号。 点对点通信 1234567MPI_Send( void* msg_buf, int msg_size, MPI_Datatype msg_type, int dest, int tag, MPI_Comm comm) 向dest进程发送信息。 12345678MPI_Recv( void* msg_buf, int buf_size, MPI_Datatype buf_type, int source, int tag, MPI_Comm comm, MPI_Status* status) 从source进程接收信息。 12345MPI_Probe( int source, int tag, MPI_Comm comm, MPI_Status* status) 与MPI_Recv()很像，除了不能接收消息，其他都一样，主要是为了将下一条消息的各项属性保存到status里面来。 集合通信 12MPI_Barrier( MPI_Comm comm) 将comm通信子中的所有进程都同步到当前位置。 123456MPI_Bcast( void* data_buf, int count, MPI_Datatype datatype, int source, MPI_Comm comm) 从source进程向通信子中的所有其他进程发送消息。 MPI_Bcast()用的是树形结构的广播，会比单纯的从source进程开个for循环要高效的多。 然而我在单台电脑上实测的时候貌似没发现MPI_Bcast()能快多少…反而有时候还慢很多，不知道是不是我发送的东西太少了，还是电脑本身的处理速度太快了。 123456789MPI_Scatter( void* send_buf, int send_count, MPI_Datatype send_type, void* recv_buf, int recv_count, MPI_Datatype recv_type, int source, MPI_Comm comm) 将source进程中的send_buf均分成多份，分发给通信子中的所有进程。需要注意的是send_count和recv_count的值是一样的，都等于每个通信子将收到的数据数量。 123456789MPI_Gather( void* send_buf, int send_count, MPI_Datatype send_type, void* recv_buf, int recv_count, MPI_Datatype recv_type, int dest, MPI_Comm comm) 与上面的MPI_Scatter()非常相似，这个的作用是从多个进程中接收一定数量的数据到dest的recv_buf中。同样send_count和recv_count的值是一样的，都等于每个通信子需要发送的数据量。 这里有完整的MPI函数说明。 后话前段时间忙着准备期末考试，原本准备好的好多个文档都没看，然后之前买的书也都没怎么看。我发现我就是擅长挖坑，然后挖了坑又填不上 o(￣▽￣)d。 虽然看到现在，基本的MPI操作已经明白了，但是还是比较缺少实践。 接下来准备再看下OpenMP和CUDA，然后找点小项目来练练手吧。]]></content>
      <categories>
        <category>Parallel</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>并行编程</tag>
        <tag>MPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跑跑跑]]></title>
    <url>%2F2015%2F11%2F08%2F2015-11-08-run%2F</url>
    <content type="text"><![CDATA[当一只胖纸开始喜欢上跑步之后……就变成了一只会跑步的胖纸……Q_Q 前几个学期开始，偶尔晚上会出去跑跑步，但总是坚持不了太久就又慢慢放下了。有时候是因为跑了一段时间就被其他的事情打断（要考试啊、要管社团啊、要弄活动啊，吧啦吧啦），有时候就真的是自己太懒了。 这学期重新开始跑步还是在10月多回校以后了。记得那时候留了一篇 修身记，到现在差不多刚好一个月吧。 当你有一天真的闲下来，每天都能够随便地有足够的时间来做一件事情的时候，把跑步变成习惯其实是一件很简单的事情。 这学期以前，跑个5、6圈操场就到极限了，跑标准400米跑道的外圈也不过3公里。 后来开始偶尔能绕学校跑一次，一圈4公里多点。 这一个月，开始计时跑： 一开始，40分钟只能跑5公里。 再后来越跑越多，40分钟能跑过6.5公里了。 再后来又加10分钟，能到8公里了。 有一天心想说要不8公里以后再跑几步试试看？然后终于突破了10公里。 我和舍友涛的日常： 我：日常打卡5公里。涛：日常打卡10公里。我：→_→…（阵亡）… 我：今天突破了一下，跑了10公里，好开心！涛：今天突破了一下，跑了个半马，好开心！我：→_→…（阵亡）… 我：我离中级跑者还差三次10公里了，接下来几个星期内把它拿下涛：我算一下，我还差xxx个10公里，xxx个半马……接下来几个星期内把它拿下我：啥！！升高级要这么多啊？（…好可怕的软件…这还让不让人活了Q_Q）涛：哦不是，我想凑个500公里总里程，还差100多公里，凑凑就有了我：→_→…（阵亡）… 虽然以上经过艺术加工，o(￣▽￣)d，不过我们涛就是这么厉害~ 正好软件里面那天跳出来个上马线上赛的预告，就顺手报上名了。 今天全国一共有6场马拉松同时进行。 上海马拉松、西昌邛海湿地马拉松、武夷山马拉松、北京鸟巢半马、湖南昭山半程、酒泉戈壁超级马拉松。大约一共有7.5万人参加。 再加上线上这个不知道多大赛场，也许总人数会超过10万？而我终于有幸也能成为其中的一个了。 曾经马拉松这几个字都是只能远远眺望的，真的没有想到有一天自己能够真的参与进来，虽然现在能参加的只是其中等级最低的10公里级。 再多跑跑，半马指日可待！！！ 全马…这个目前想想都觉得太遥远了，以后再想吧。 有人问我跑多了能瘦下来吗？ 咳咳……这个问题很尴尬吖 要知道： 一只胖纸去练瑜伽，就会变成一只柔软的胖纸…一只胖纸多跑多跳，就会变成一只灵巧的胖纸…]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>悦跑圈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的书单整理]]></title>
    <url>%2F2015%2F11%2F04%2F2015-11-04-book%2F</url>
    <content type="text"><![CDATA[电子书再怎么发展，都没有一本能实实在在拿在手里翻的纸质书来的亲切。 把自己这几年买过的书都整理一下。 包括以前读过的书、现在在读的书和以后要读的书。 Linux相关《LINUX权威指南》【豆瓣链接】 我看的第一本Linux的书，也是很老的一本书了，以至于现在能买到的只有影印版的了。 内容对于现在来说是有一些过时，不过系统性还是不错的。 再有同学想找入门书的话，推荐下面这本。 《Unix &amp; Linux 大学教程》 【豆瓣链接】 非常赞的一本Linux入门教程书，书中穿插了很多历史知识和小故事等等。 事实上《权威指南》我最后还是没有看完，想找点比较新的于是买了这一本。 本书内容非常详细，包括了Linux的各个方面，比如日常使用、Vi的介绍等等。个人觉得即使看完了也很适合作为工具书使用，需要查阅相关部分的使用说明的时候肯定能够从本书中得到帮助。 然而这本也没看完…0.0…很多都是到直接上手应用时遇到不懂的直接去百度了 《Linux内核完全剖析 —— 基于0.12内核》 【豆瓣链接】 相对上一个版本，增加了有关x86保护模式的详细分析之类的。 内容超详细！把内核源码的每个部分都做了注释。 ……然而，因为各种原因最后还是没看多少……T_T……我很想努力把它啃完的啊 《深入理解LINUX内核》 【豆瓣链接】 其中讲的内容其实是比较近的内核版本中的特性了，看本书需要对内核的机制有一定的了解。 数据结构与算法《算法导论》 【豆瓣链接】 高中OI时代买的书，一直看到大学毕业，其中的很多内容都翻来覆去地看过好多遍了。 《算导》一直都算是算法界的神书了，豆瓣的评分也非常高。 我对它的最深的印象就是严谨，几乎所有定理和算法都给出了数学上的严格证明。当初最早看的时候不太能看懂证明，都是跳过直接用结论的，后来再回来看第二遍、第三遍的时候，证明能看懂了，对某一种算法就有了更深的理解。 其他比较好的比如：伪代码很规范、时间/空间复杂度都整理得很好、课后的思考题也很有发展的深度等等。 《算导》的确是一本值得反复看上很多遍的书。 我买的时候是第二版，后来听说已经出到第三版了。 计算机系统结构《计算机组成与设计 硬件/软件接口》 【豆瓣链接】【学习笔记】 体系结构圣经，之一。 也算是我自己买的第一本纯英语的书吧。 10086个赞！！！ 从来没有这么透彻地了解过CPU，看过这本书之后，真心觉得甩了国内的组成原理/微机原理的教材不知道多少条街。 这本书从最基础的原理开始，一步步搭建数据通路，流水线优化，内存管理…看完会对从上到下的一整个体系都能有个直观的轮廓了。 内容方面，这本书也是非常全面的，第五版的第六章已经介绍到云了，里面举出来的实例也都是非常近期的芯片（实例里面以Intel Core i7为主），最后的附录有介绍天河二号超算的结构。 粗略地把整本书过了一遍，脑子里大概有一些计算机结构的全貌了。本书较大的篇幅是在细致地讲CPU，感觉自己第一遍过的还是有点水，值得日后反复读上几遍T_T。 书后面的习题做了几章，感觉跟网上官方找的标准答案稍微有点出入，不知道是我有些地方没明白还是标准答案有误。 原来想着翻完这本就去看《量化研究方法》的，现在想想还是太年轻…里面的东西需要花上不少时间来消化，而且目前我只看了这么点理论，理解的还不是特别透彻，最好找时间用VHDL实现一下一些简单的芯片功能之后再继续吧。 《计算机体系结构 量化研究方法》 【豆瓣链接】 体系结构圣经，之二。 编译原理《编译原理》（龙书） 【豆瓣链接】【学习笔记】 传说中的龙书。 连买三本英文原版书确实有点累，不过收获也很大。 并行《并行程序设计导论》 【豆瓣链接】【并行编程 MPI初探】【MPI小结】【共享内存系统并行 OpenMP】 《算导》虽然叫导论，然而算法、定理、证明等等各种内容都详细得不像话。这本导论就真的只是导论了。 本书主要以介绍为主，从并行编程的思想开始，然后介绍到分布式内存系统、共享内存式系统，最后介绍了三种并行编程框架：分布式的MPI、共享式的Pthreads和OpenMP。 对从来没有接触过并行的人来说（比如说买这本书的我），是一本非常好的入门书籍，其实看起来也非常快，用不了多久就看完了。当然，语言这种东西实践还是第一位的，每章后面的习题感觉出的还挺不错。 当然，无论是MPI还是OpenMP本身都是一大块很复杂的系统，这本书里面只是讲解了基础的知识和一些简单的接口函数，如果想要深入学习，还需要另外找东西看。 其他《编程之美》【豆瓣链接】 《编程珠玑》 【豆瓣链接】 《信息简史》 【豆瓣链接】 非常赞！非常赞！非常赞！重要的事情说三遍！ 本书讲述了很多与信息有关的历史和故事，时间跨度是从不知道多久前的非洲鼓声开始，一直到现代。 还差一点点看完，看完之后回来把这个写完整。 《奇点临近》 【豆瓣链接】 《数学之美》 【豆瓣链接】 《黑客与画家》 【豆瓣链接】 作者的很多看法都非常有意思。 《禅与摩托车维修艺术》 【豆瓣链接】 看了不少了，描写了主人公在摩托车骑行穿越美国时的历程，重要的是作者的思考。 说实话有的地方在我看来有点深刻，不太容易读懂，我准备慢慢来，最后肯定能有收获。 《哥德尔 艾舍尔 巴赫 —— 集异璧之大成》 【豆瓣链接】 《无言的宇宙》 【豆瓣链接】 《天才在左 疯子在右》 【豆瓣链接】 很早以前看过电子版的，刚刚遇上双十一有减价就顺便凑单买回来了，买的是新出的版本，大概比原版多了10个故事。 内容的真实性不置可否（尤其里面涉及到的量子力学部分，虽然很有趣，但是感觉还是有点问题的，读者玩就好了），作为一本简单的闲暇读物还是不错的。 启发：尝试从另外的角度来看待这个世界；强迫症和偏激每个人都会有；享受生活。 有空单独写一篇博客记录一下我比较喜欢的几个故事。 网络教材Freely available programming books【Github上的入口在此】 想了想还是把这个作为书单的一部分加上吧，好多人整理的各种编程语言以及工具软件等等的教程，不知道内容到底有多少……只能说，非常非常多！！ 需要用到某种工具或者语言的时候可以直接来这里找下，比如说当时想看nodejs的、vim的、MPI的、OpenMP的…在上面都能找到。 最完整的应该是英文版的List，中文版的似乎缺了一些。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>整理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并行编程 MPI初探]]></title>
    <url>%2F2015%2F11%2F03%2F2015-11-03-mpi%2F</url>
    <content type="text"><![CDATA[刚刚跟实验室的一个博士生师兄联系上了，研究生入学前本来漫无目的的乱学过程终于稍微找到点方向。虽然还没决定以后具体要做体系结构方向的哪一块内容，至少先走一步学一步吧。 师兄是主要做并行程序优化的，估计老师也是看我以前编程方面还可以才给我推荐的这个方向。 下一阶段开始学习并行编程：mpi和openmp 先从mpi开始吧。 一些资料 一个MPI的教程站：http://mpitutorial.com/因为是托管在Github上的，顺手fork过来了：http://mpi.jcf94.com/ MPI的完整API：https://computing.llnl.gov/tutorials/mpi/ MPICH，一个MPI的实现机制，包括编译器和运行器等：http://www.mpich.org/以及MPICH的各个版本资源 MPICH在Windows下的安装MPICH官网上提供了整个MPICH2的源码，一般是需要下过来然后在本地进行完整编译，这个等下次移到Ubuntu下面的时候再看一下编译的全过程吧。现在先找现成的安装包用着先。 最新的版本大概出到了3.x，不过已经编译好了的Windows安装文件似乎最高只到1.4.1。下之，然后安装。 安装过程中要求输入一串smpd的底层密码，这个应该是在整个MPICH系统中用来传递消息的一个服务。 装好之后，安装目录里面有用的几个exe有： mpiexec.exe MPI运行器smpd.exewmpiconfig.exe 图形界面下的MPICH配置查看器wmpiexec.exe 图形界面下的MPI运行器wmpiregister.exe 图形界面下的MPICH服务注册器 首先还是把bin这个目录加到系统环境变量里面去，这是一般装开发环境时肯定要做的事。 然后测试smpd服务是否在运行了，没有的话就需要把它安装上并启动起来： 1smpd -install -phrase behappy -phrase后面的这一串就是安装过程中输入的密码串。 确认已启动后继续。 启动wmpiregister.exe，这里需要把此时登录Windows系统的用户账户和密码输进去，因为MPICH需要调用管理员的底层权限。 比较坑的是当时在这里以及后面的测试这一步中卡了很久… 我现在用的是win10，从win8开始就使用了在线的微软账号进行登录。所以我把微软账号输入到register里面去，结果却死活没办法通过系统的验证。 后来才知道，如果采用微软账号登录系统，Windows在本地是还有另一个本地账号的用户名来对应的。一般是把登录微软账号前的最后一个本地账号直接作为对应。 我当时装机的时候应该是没有经过本地账号这一步，所以不知道它写在那里的本地用户名是什么。 于是切到本地账户，创建了一个本地账户的用户名，再切回微软账号。 然后用之前的本地账户用户名+微软账号的密码注册到register里面去。 注册完之后，可以用wmpiexec.exe测试一下，安装目录下example文件夹中有一个测试用的圆周率计算程序cpi.exe。 测试成功能运行即可。 不成功的话就需要检查smpd服务是不是正常运行，然后网络有没有被防火墙挡了，用户账户有没有注册到register里面去。 编译网上提到MPI编译需要用到mpigcc之类的东西，然而我在MPICH2的安装目录下根本找不到这玩意的迹象。 官方文档中建议使用Visual Studio来作为MPI的IDE…然而，这货这么大，我是真心不太想用的感觉（尤其是VC6.0给我留下的印象太差了，不了解现在新版的VS套件的编译标准是不是跟GNU的一样）… 再仔细搜索之后，原来mpigcc就是调用了gcc的编译指令，这样事情就简单了。 看一下安装目录下面的文档，果然找到了gcc的说明：README.winbin.rtf 123456789...For gcc/g771) create a makefile2) add –I…mpich2\include3) add –L…mpich2\lib4) add –lmpi (for g77: -lfmpich2g)5) add the rules for your source files6) same as 6,7,8 above... 整个过程很简单，只要在gcc的编译指令后面加上-I开关，把mpi的头文件include包括进来；-L开关，把mpi的函数库lib包括进来；最后指定-lmpi，使用mpi方式进行编译即可。 这样直接调用gcc和g++就可以对整个完成编译了，只不过运行必须调用mpiexec或者wmpiexec。 既然gcc的编译这一部分OK了，可以直接把编译命令绑进文本编辑神器Sublime2里面去。 Sublime2配置配置部分跟以前已经记录过的一样：Sublime 2 配置 在sublime2中找到编译命令的配置文件： Preference -&gt; Bowser Packages -&gt; /C++/C++.sublime-build 123456789101112131415161718192021222324252627&#123; "cmd": ["g++", "$&#123;file&#125;", "-o", "$&#123;file_base_name&#125;"], "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$", "working_dir": "$&#123;file_path&#125;", "selector": "source.c, source.c++", "encoding": "gbk", "variants": [ &#123; "name": "Run", "cmd": ["$&#123;file_base_name&#125;"] //"cmd": ["cmd", "/c", "g++", "$&#123;file&#125;", "-o", "$&#123;file_path&#125;/$&#123;file_base_name&#125;", "&amp;&amp;", "cmd", "/c", "$&#123;file_path&#125;/$&#123;file_base_name&#125;"] &#125;, &#123; "name": "RunInCommand", "cmd": ["cmd", "/c", "start", "cmd", "/c", "$&#123;file_base_name&#125; &amp; pause"] &#125;, &#123; "name": "BuildAndRun", "cmd": ["cmd", "/c", "g++", "$&#123;file&#125;", "-o", "$&#123;file_base_name&#125;", "&amp;&amp;", "start", "cmd", "/c", "$&#123;file_base_name&#125; &amp; pause"] &#125;, &#123; "name": "BuildWithMPI", "cmd": ["g++", "$&#123;file&#125;", "-o", "$&#123;file_base_name&#125;", "-I", "D:\\Program Files\\MPICH2\\include", "-L", "D:\\Program Files\\MPICH2\\lib", "-lmpi"] &#125; ]&#125; 前面部分保持不变，最后加上一条命令，命名成BuildWithMPI。 然后选一个没有被使用过的快捷键，加到快捷键注册文件里面去： Preference -&gt; Key Bindings – User 1234[ &#123; "keys": ["f10"], "command": "build", "args": &#123;"variant": "RunInCommand"&#125; &#125;, &#123; "keys": ["alt+b"], "command": "build", "args": &#123;"variant": "BuildWithMPI"&#125; &#125;] 保存好，之后就可以用alt+b来调用附加了MPI编译命令的g++指令了。 Hello World~！写个测试程序试试先： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/* ***********************************************MYID : Chen FanLANG : G++PROG : mpitest************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;mpi.h&gt;#include &lt;ctime&gt;using namespace std;int main()&#123; printf("test start:\n"); int test=0; time_t t; t= time(NULL); printf("start time:%ld\n",t); //MPI环境初始化 MPI_Init(NULL, NULL); //获取进程数量 int world_size; MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size); //获取进程号 int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank); //获取处理器名 char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, &amp;name_len); //Hello world! printf("Hello world from processor %s, rank %d out of %d processors\n",processor_name,world_rank,world_size); test++; printf("%d\n",test); t= time(NULL); printf("end time:%ld\n",t); printf("\n"); //关闭MPI环境 MPI_Finalize(); return 0;&#125; 此时普通的编译指令会返回找不到头文件mpi.h，就算找到了也是编译失败。alt+b可正常通过编译。 然后用wmpiexec来运行之，开了4个线程： 结果： MPI我还没有真正开始学，不过也可以从这个简单的测试程序中稍微看出一些特点来： 从test这个变量的输出结果来看，4个线程的变量应该是完全独立的，并没有出现同一个变量反复加了4次这种事情。 我还特别调用了一下时间函数。从输出结果上来看，四个进程的时间居然真的是同步的！！结束时间和起始时间一样应该是因为中间运行的过程太少了。 关闭之后再运行，4个进程的输出的rank先后顺序可能会有些不同。 MPI初探： 把同一个过程分给多个线程独立执行，相互之间通过一些特殊的调用来进行通信，进程与进程之间是独立的。 并行编程与普通的编程之间看来应该是会有挺大的差别的，这一点上来看，编程的思路也需要发生挺多变化。 多机并行MPICH除了可以在本地进行多线程并行的运算，还可以通过网络来进行远程多机多线程并行运算。 这个地方也挺坑的… 要求多台计算机上register的用户名和密码必须一致，要不然连不上。 我的两台win10都用的是微软账户，为了测试这个多机并行改了好久的本地账户。 再然后是要求如果是直接执行的本机的文件的话，要求对方的同样目录下必须有同样的文件，就是说文件也必须是同步的。 完成上面所有的设置之后，然后用wmpiexec调用： 或者用命令行调用： 1mpiexec -hosts 2 j-cf-yoga 2 j-cf-pc 2 "D:\mpitest.exe" 如果网络和设置都是正常的，应该能得到这样的结果： 多机并行完成。 并行的效率问题这里测试了一下多个线程之间的效率比较情况： 可以比较明显地看到同样对于9999999999精度的求圆周率计算： 单线程花了10.7s，2线程5.41秒，4线程2.78秒，8线程以上效率就不会再怎么增加了 不知道是不是我的CPU只有4核的原因，还是与这个算圆周率的程序本身有关…进行到这里，已经非常能够体会到并行编程与普通程序之间的差别了： 并行程序必须要能够考虑到任务的划分、多线程之间的通讯等等，只是把原本的单线程程序拿来扔给多线程执行的话，效率根本不能增加，只是把相同的代码重复执行好多遍而已。 要想做到多线程，需要学的东西还很多。 然后是双机并行进行圆周率计算，两台电脑（Y480 i5-3210 以及 Yoga3_11 5Y10），双机8线程的时候能在1.7s左右完成上面的那个运算。 这里要再记一下的是，win10上面的防火墙太坑了…明明设了例外，还是会把通信线程给挡掉，发出指令的那一台机子需要关闭所有防火墙。回头再好好研究下。]]></content>
      <categories>
        <category>Parallel</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>并行编程</tag>
        <tag>MPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊特斯拉]]></title>
    <url>%2F2015%2F10%2F31%2F2015-10-31-tesla%2F</url>
    <content type="text"><![CDATA[偶然逛ZEALER官网，原来想找找手机评测视频的来着。 然后发现了视频版块下有个叫科技相对论的标签，是ZEALER跟爱奇艺合作的小节目，每周四更新一期，聊聊科技界的一些趣事。 一口气把前面6期都看完了，感觉还不错，以后可以跟上更新看看。 第二期聊的是特斯拉。 君不知，特斯拉这种高大上的东西对于像我这样未来是搞IT的小同志来说有多么大的吸引力啊~！ 这篇随笔建的很早，结果等到有空想起来回来补坑已经是2个月以后了。 前几天SpaceX成功发射了猎鹰9号火箭，并成功回收，开创了人类历史上首次可重复利用的火箭的先例，这也是NASA等巨头砸了多少钱都没有完成的任务。 虽然最后成本评估、安全性评估等等还不知道怎么样，许多人担心火箭回收技术会不会重走当年航天飞机的老路。不过至少SpaceX真的让人类看到了新的路，如果真的成功，那么以后航天成本就将大大降低，甚至到任何一个公司都可能发的起自己的卫星。 恰巧最近看了电影《火星救援》，当时看着电影，心里面突然冒出来这么个想法：感觉这才是Musk真正想要做的事情！ Elon Musk简直是一个神一样的男人啊！ 火箭、太阳能、电动车！！ 这三大元素正是电影中最重要的部分，而Musk手下的三大公司分别做的就是这三大部分！！ 当别人还在为自己的生活打拼时，人家早就有了改变历史、改变人类的野心，也早就开始了改变人类未来的路。]]></content>
      <categories>
        <category>有趣的</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>Tesla</tag>
        <tag>科技相对论</tag>
        <tag>ZEALER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机信号 复习笔记]]></title>
    <url>%2F2015%2F10%2F26%2F2015-10-26-suiji%2F</url>
    <content type="text"><![CDATA[上一篇核心课程的复习笔记还是在写在上学期：信号相关专业复习 《随机信号》是我本科阶段最后一门通信的核心课程，主要内容是用概率论的方法分析随机信号的统计特性等特点。虽然马上就要毕业离开通信领域了，还是把这最后一门好好理理。 目前上课只介绍到了连续信号的分析。 主要内容是随机过程的统计特性等等特点，平稳随机过程的谱分析，以及随机信号通过线性系统的分析。 1. 什么是随机信号（随机过程）一组随机变量，结果不仅与每次实验有关，还与时间t有关，这时候的随机变量就叫随机过程。 随机过程就是多条样本函数的集合。 随机过程的定义和一般表征设随机实验的样本空间是$S={\xi}$，对于每个$\xi$，对应有参数$t$的函数$X(\xi,t),t\in T$。那么对于每一个$\xi$，得到一族$t$的函数${X(\xi,t),t\in T}$。 这个$t$的函数族称为随机过程，简记为$X(\xi,t)$，或$X(t)$。族中每个函数称为一个样本，它是随机过程的一次试验的物理实现，是一个确知的时间函数。 对于每个给定的时间$t_i(i=1,2,…),X(\xi,t_i)$都是随机变量。 样本函数集合： $$X(t,\xi)=\{X(t,\xi_i),i=1,2,…\}$$ 样本变量集合： $$X(t,\xi)=\{X(t_i,\xi),i=1,2,…\}$$ 2. 随机信号的研究方法时域分析（1）随机过程的概率特性随机过程$X(t)$在任意时刻$t_1\in T$的取值$X(t_1)$是以为随机变量，则其： 一维概率分布函数： $$F_X(x_1;t_1)=P\{x(t_1)\le x_1\}$$ 一维概率密度函数： $$f_X(x_1;t_1)=\frac{\partial F_X(x_1;t_1)}{\partial x_1}$$ 为了描述在任意两个时刻$t_1$和$t_2$的状态间的内在联系，引入二维随机变量的概率分布： 二维概率分布函数： $$F_X(x_1,x_2;t_1,t_2)=P\{X(t_1)\le x_1,X(t_2)\le x_2\}$$ 二维概率密度函数： $$f_X(x_1,x_2;t_1,t_2)=\frac{\partial F_X(x_1,x_2;t_1,t_2)}{\partial x_1 \partial x_2}$$ （2）随机信号的数字特征均值/数学期望（一阶原点矩）相当于把时间$t$先固定，然后用随机变量的方法来计算当前时刻样本变量集合的数学期望。 $$m_X(t)=E[X(t)]=\int_{-\infty}^{\infty}xf_X(x;t)dx$$ $$m_X(t)$$是一个平均函数，是时间$t$的函数，随机过程的样本在它附近起伏变化。 均方值（二阶原点矩）$$\Psi_X^2(t)=E[X^2(t)]=\int_{-\infty}^{\infty}x^2f_X(x;t)dx$$ 方差（二阶中心矩）$$\sigma_X^2(t)=D[X(t)]=E[(X(t)-m_X(t))^2]$$ 方差和均方值都是$t$的确定函数，描述了随机过程$X(t)$的所有样本函数在$t$时刻的函数值相对于$m_X(t)$的偏离程度/围绕数学期望的分散程度。 相关函数（混合原点矩）$$R_X(t_1,t_2)=E[X(t_1)X(t_2)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x_1x_2f_X(x_1,x_2;t_1,t_2)dx_1dx_2$$ 描述了$X(t)$在任意两个不同时刻的状态之间的相关程度，即整个随机过程中任意两个不同时刻之间的内在关系：线性相关性。 当$t_1=t_2=t$时： $$R_X(t_1,t_2)=R_X(t,t)=E[X(t)X(t)]=E[X^2(t)]$$ 此时$X(t)$的自相关函数等于均方值。 协方差函数（混合中心矩）$$\begin{align}K_X(t_1,t_2)&amp;=E[(X(t_1)-m_X(t_1))(X(t_2)-m_X(t_2))]\&amp;=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x_1-m_X(t_1))(x_2-m_X(t_2))f_X(x_1,x_2;t_1,t_2)dx_1dx_2\&amp;=R_X(t_1,t_2)-m_X(t_1)m_X(t_2)\end{align}$$ 描述了$X(t)$在任意两个不同时刻的起伏值之间的相关程度。 当$t_1=t_2=t$时： $$K_X(t_1,t_2)=K_X(t,t)=E[(X(t)-m_X(t))^2]=D[X(t)]=\sigma^2_X(t)$$ 此时$X(t)$的协方差函数等于方差。 描述两个随机信号之间的联系互相关函数、互协方差函数、互相关系数… 方法类似。 频域分析（1）随机信号样本函数的功率及功率谱平稳过程的任何一个非零样本函数的持续时间都是无限长，因此不满足绝对可积和能量有限的条件，傅里叶变换不存在。 为了能够用频域来分析，首先定义其中一条样本函数的截断函数： $$x_T(t)=\begin{cases}x(t),&amp;|t|\le T\\0,&amp;|t|\gt T\end{cases}$$ 易知，截断函数是符合傅里叶变换存在的充要条件的，有： $$F_x(\omega,T)=\int_{-\infty}^{\infty}x_T(t)e^{-j\omega t}dt=\int_{-T}^{T}x(t)e^{-j\omega t}dt$$ 截断函数的能量（帕萨瓦尔等式）： $$E=\int_{-\infty}^{\infty}x_T^2(t)dt=\frac{1}{2\pi}\int_{-\infty}^{\infty}|F_x(\omega,T)|^2d\omega$$ 把上式右边的被积式称为截断函数的能量谱： $$|F_x(\omega,T)|^2$$ 截断函数的功率： $$\frac{1}{2T}\int_{-T}^{T}x^2(t)dt=\frac{1}{2\pi}\int_{-\infty}^{\infty}\frac{1}{2T}|F_x(\omega,T)|^2d\omega$$ 令$T\to\infty$并交换运算次序，得到$x(t)$在$(-\infty,\infty)$上的平均功率： $$\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}x^2(t)dt=\frac{1}{2\pi}\int_{-\infty}^{\infty}\lim_{T\to\infty}\frac{1}{2T}|F_x(\omega,T)|^2d\omega$$ 把上式右边的被积式称作函数的平均功率谱密度，简称功率谱密度，记作： $$S_x(\omega)=\lim_{T\to\infty}\frac{1}{2T}|F_x(\omega,T)|^2$$ （2）随机信号的功率谱密度把单个样本函数的功率谱推广到整个随机过程$X(t)$ 对于随机过程来说，求各样本函数功率谱密度的统计平均： $$\begin{align}S_X(\omega)=E[S_x(\omega)]&amp;=E[\lim_{T\to\infty}\frac{1}{2T}|F_x(\omega,T)|^2]\&amp;=\lim_{T\to\infty}\frac{1}{2T}E[|F_x(\omega,T)|^2]\end{align}$$ 定义为整个随机过程$X(t)$的功率谱密度。 求完统计平均之后，$S_X(\omega)$是一个$\omega$的确知函数。 随机过程的平均功率： 频域计算 任意样本函数的平均功率： $$Q_x=\frac{1}{2\pi}\int_{-\infty}^{\infty}S_x(\omega)d\omega$$ 随机过程的平均功率： $$Q=E[Q_x]=\frac{1}{2\pi}\int_{-\infty}^{\infty}E[S_x(\omega)]d\omega=\frac{1}{2\pi}S_\color{red}{X}(\omega)d\omega$$ 时域计算 任意样本函数的平均功率： $$Q_x=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}|x(t)|^2dt$$ 随机过程的平均功率： $$Q=E[Q_x]=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}E[x^2(t)]dt$$ 注：这里的$Q$是一个确定的值，不是随机变量了。 平稳随机过程功率谱密度的性质（1）$S_{X}(\omega)$是$\omega$的实的、非负的偶函数 （2）功率谱密度可积 （3）功率谱密度与自相关函数的关系功率谱密度反应了随机信号在频域的统计特性，自相关函数反映了随机信号在时域的统计特性，它们均是描述随机信号的重要数字特征。 维纳辛钦定理：当一个平稳过程满足一定条件的时候，其自相关函数与功率谱密度可以构成一对傅里叶变换对。 满足： $$\int_{-\infty}^{\infty}|\tau R_X(\tau)|d\tau\lt \infty$$ 要求均值为0。 则： $$S_X(\omega)=\int_{-\infty}^{\infty}R_X(\tau)e^{-j\omega\tau}d\tau$$ 满足： $$\int_{-\infty}^{\infty}S_X(\omega)d\omega\lt \infty$$ 平均功率有限。 则： $$R_X(\tau)=\frac{1}{2\pi}\int_{-\infty}^{\infty}S_X(\omega)e^{j\omega\tau}d\omega$$ 3. 两大类随机信号平稳随机过程（平稳性）平稳随机过程$X(t)$是在时间平移下概率性质不变的随机过程。 （1）严平稳随机过程（狭义平稳随机过程）一个随机过程的n维概率密度（或n维分布函数）不随时间起点选择的不同而改变，即： $$f_X(x_1,x_2,…,x_n;t_1,t_2,…,t_n)=f_X(x_1,x_2,…,x_n;t_1+\tau,t_2+\tau,…,t_n+\tau)$$ 则称$X(t)$为严平稳随机过程。 性质： 1.它的一维概率密度与时间无关： $$f_X(x_1;t_1)=f_X(x_1;t_1+\tau)=f_X(x_1;0)=f_X(x_1)$$ 其均值、均方值、方差也同样与时间无关。 2.它的二维概率密度只与$t_1$和$t_2$的时间间隔有关，而与时间起点无关： $$f_X(x_1,x_2;t_1,t_2)=f_X(x_1,x_2;t_1+\tau,t_2+\tau)=f_X(x_1,x_2;0,t_2-t_1)=f_X(x_1,x_2;\eta)$$ 其相关函数、协方差函数同样只与时间间隔有关，与时间起点无关。 （2）宽平稳随机过程（广义平稳随机过程）若随机过程的均值函数和相关函数存在，并且满足： 1.均值与时间无关（常数）；2.相关函数只与时间间隔有关，与时间起点无关。 则称$X(t)$为宽平稳随机过程。 宽平稳过程反映了一个系统处于稳态工作条件下的统计性质。 严平稳一定是宽平稳，反之不一定。当随机过程满足高斯分布时，严平稳和宽平稳是等价的。 若无特别指明，则“平稳随机过程”指的就是宽平稳过程。 假定平稳过程的均值$E[X(t)]=0$，则此时平稳过程的协方差函数$K_X(t_1,t_2)=K_X(\tau)$与相关函数$R_X(t_1,t_2)=E[X(t_1)X(t_2)]=R_X(\tau)$相等。 当均值不为0时，可以通过定义$Y(t)=X(t)-E[X(t)]$来得到均值为0的平稳过程$Y(t)$。 （3）平稳随机过程相关函数的性质自相关函数： （1） $$R_X(0)=E[X^2(t)]=\Psi_X^2\ge0$$ 自相关函数在$\tau=0$处的值给出了平稳过程的均方值，表示了平稳过程的平均功率。 （2） $$R_X(\tau)=R_X(-\tau)$$ 自相关函数具有对称性，是$\tau$的偶函数。 （3） $$R_X(0)\ge|R_X(\tau)|$$ 自相关函数在$\tau=0$处有最大值。 （4） 平稳随机过程的自相关函数对所有的$\omega$满足： $$\int_{-\infty}^{\infty}R_X(\tau)e^{-j\omega\tau}d\tau\ge0$$ 相关系数： $$r_X(\tau)=\frac{K_X(\tau)}{K_X(0)}=\frac{R_X(\tau)-m_X^2}{\sigma^2_X}$$ 又称归一化自相关函数或标准自协方差函数，它确切表征了平稳随机过程在两个不同时刻的起伏值之间的线性关联程度。 相关时间： $r_X(\tau)$从其最大值$r_X(0)=1$下降到$r_X(\tau_0)=0.05$所经历的时间间隔，定义为相关时间。 相关时间越小，意味着相关系数随$\tau$增加/降落得越快，说明随机过程随时间变化得越剧烈。 各态历经随机过程（遍历性）各态历经性（遍历性）：只要观测时间足够长，随机过程的一个样本函数也能够“遍历”整个集合中出现的各种可能的状态。 （1）严遍历性随机过程（狭义遍历性随机过程）如果一个随机过程$X(t)$，它的各种时间平均都依概率1收敛于相应的集合平均，则称该随机过程具有严格的遍历性，称它为严遍历性过程（狭义遍历性过程）。 （2）宽遍历性随机过程（广义遍历性随机过程）定义： 设${X(t),-\infty\lt t\lt\infty}$是均方连续的平稳过程，则分别称： $$A\langle X(t)\rangle=\overline{X(t)}=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}X(t)dt$$ $$\mathfrak{R}_X(t,t+\tau)=\overline{X(t)X(t+\tau)}=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}X(t)X(t+\tau)dt$$ 为该过程的时间均值和时间相关函数。 若： $$A\langle X(t)\rangle=\overline{X(t)}=E[X(t)]=m_X$$ 依概率1成立，则称该平稳过程的均值具有遍历性。 $\overline{X(t)}$与取哪一条样本有关，与时间无关$E[X(t)]$与时间$t$有关，与取哪一条样本无关 均值的各态历经性表明：任何一条样本函数所包含的取值状态与随机过程（任意时刻）所有的状态相同，而且出现的频率与随机过程各状态的概率相同。 若： $$\mathfrak{R}_X(t,t+\tau)=\overline{X(t)X(t+\tau)}=E[X(t)X(t+\tau)]=R_X(\tau)$$ 依概率1成立，则称该平稳过程的相关函数具有遍历性。 若$\tau=0$时上式成立，则称该平稳过程的均方值具有遍历性。 如果均方连续的平稳过程$X(t)$的均值和相关函数都具有遍历性，则称该平稳随机过程为宽遍历性过程（广义遍历性过程）。 实际过程中，要严格验证平稳过程是否满足遍历性条件是比较困难的，最简单的办法是假定平稳过程是遍历性的，由此假设出发，对观测材料进行分析、处理，看其所得结论是否符合实际。 4. 白噪声理想白噪声分布在$(-\infty,\infty)$整个频率区间上，均值为零而功率谱密度为一个非零常数，即： $$S_N(\omega)=\frac{1}{2}N_0$$ 的平稳随机过程称为白噪声过程或简称为白噪声。 其中$N_0$为一个正实常数。 利用维纳辛钦定理，可以得到白噪声的自相关函数： $$R_N(\tau)=\frac{1}{2\pi}\int_{-\infty}^{\infty}S_N(\omega)e^{j\omega\tau}d\omega=\frac{N_0}{4\pi}\int_{-\infty}^{\infty}e^{j\omega\tau}d\omega=\frac{1}{2}N_0\delta(\tau)$$ 自相关系数： $$r_N(\tau)=\frac{R_N(\tau)-m_N^2}{\sigma_N^2}=\frac{R_N(\tau)}{R_N(0)}=\begin{cases}1&amp;\tau=0\\0&amp;\tau\ne0\end{cases}$$ 带限白噪声若一个平稳随机过程的均值为零，而功率谱密度限定在某一个有限的频率范围内均匀分布，而在此范围以外为零，则称这个过程是带限白噪声。 （1）低通白噪声$$S_X(\omega)=\begin{cases}S_0&amp;|\omega|\le W\\0&amp;|\omega|\gt W\end{cases}$$ 自相关函数： $$R_X(\tau)=\frac{1}{2\pi}\int_{-\infty}^{\infty}S_X(\omega)e^{j\omega\tau}d\omega=\frac{1}{2\pi}\int_{-W}^{W}S_0e^{j\omega\tau}d\omega=\frac{WS_0}{\pi}\frac{sinW\tau}{W\tau}$$ （2）带通白噪声$$S_X(\omega)=\begin{cases}S_0&amp;\omega_0-\frac{W}{2}\lt|\omega|\lt\omega_0+\frac{W}{2}\\0&amp;其他\end{cases}$$ 自相关函数： $$R_X(\tau)=\frac{WS_0}{\pi}\frac{\sin\frac{W\tau}{2}}{\frac{W\tau}{2}}\cos\omega_0\tau$$ 5. 随机信号通过连续时间系统的分析在系统给定的条件下，输出信号的某个统计特性只取决于输入信号的相应的统计特性，而不依赖于输入型号其他统计特性。 则根据输入信号已知的均值、相关函数、功率谱密度，再加上已知线性系统的单位冲激响应或者传递函数，就可求出随机信号响应的均值、相关函数和功率谱密度。 3dB带宽如果系统的特性为低通，则3dB带宽定义为幅频特性$|H(\omega)|$值下降到最大值的$\frac{1}{\sqrt{2}}$即$0.707|H(\omega)|_{max}$时的正频率，此时功率谱下降到峰值的一半。 如果系统特性是带通：3dB带宽定义为幅频特性$|H(\omega)|$下降到最大值的$\frac{1}{\sqrt{2}}$即$0.707|H(\omega)|_{max}$时对应正频率的差值。 等效噪声带宽当系统比较复杂时，计算系统输出噪声的统计特性是非常困难的。在实际中为了计算方便，常常用一个幅频响应为矩形的理想系统等效替代实际系统。 等效的原则： 1.理想系统与实际系统在同一白噪声激励下，两个系统的输出平均功率是相等的；2.理想系统的增益等于实际系统的最大增益。 等效噪声带宽：定义为理想系统的带宽。 设一个普通的系统幅频特性为$|H(\omega)|$，则其平均功率为： $$E[Y^2(t)]=\frac{N_0}{2\pi}\int_0^{\infty}|H(\omega)|^2d\omega$$ 取积分限$0\sim\Delta\omega_e$，$|H(\omega)|=K$，则理想线性系统对同一白噪声输入的输出总平均功率为： $$\frac{N_0}{2\pi}\int_0^{\Delta\omega_e}K^2d\omega=\frac{N_0K^2\Delta\omega_e}{2\pi}=\frac{N_0}{2\pi}\int_0^{\infty}|H(\omega)|^2d\omega$$ 则解方程可得： $$\Delta\omega_e=\frac{1}{|H(\omega)|^2_{max}}\int_0^{\infty}|H(\omega)|^2d\omega$$ 对于低通系统来说，$|H(\omega)|$的最大值出现在$\omega=0$处。 中心频率为$\omega_0$的带通系统，$|H(\omega)|$的最大值出现在$\omega=\omega_0$处。 白噪声通过理想线性系统分析希尔伯特变换和解析过程设有一个实值函数$x(t)$，它的希尔伯特变换记作$\hat{x}(t)$或者$H[x(t)]$ $$\hat{x}(t)=H[x(t)]=\frac{1}{\pi}\int_{-\infty}^{\infty}\frac{x(\tau)}{t-\tau}d\tau=x(t)*\frac{1}{\pi t}$$ 易得希尔伯特变换的冲激响应及传递函数： $$h_H(t)=\frac{1}{\pi t}\leftrightarrow H_H(j\omega)=-jsgn(\omega)=\begin{cases}-j,&amp;\omega\ge0\\j,&amp;\omega\lt0\end{cases}$$ 希尔伯特逆变换： $$\begin{align}x(t)=H^{-1}[\hat{x}(t)]&amp;=-\frac{1}{\pi}\int_{-\infty}^{\infty}\frac{\hat{x}(t-\tau)}{\tau}d\tau\&amp;=\frac{1}{\pi}\int_{-\infty}^{\infty}\frac{\hat{x}(t+\tau)}{\tau}d\tau\&amp;=-\frac{1}{\pi}*\hat{x}(t)\end{align}$$ 希尔伯特变换相当于一个正交滤波器/90°移相器。 对于正频率分量，移相-90°；对于负频率分量，移相+90°。 给定任一实随机过程$X(t)$，定义一个复随机过程为$\tilde{X}(t)$为： $$\tilde{X}(t)=X(t)+j\hat{X}(t)$$ 上式中，$\hat{X}(t)$是$X(t)$的希尔伯特变换，它也是一个实随机过程。 称$\tilde{X}(t)$是实随机过程$X(t)$的复解析过程，简称解析过程。 性质： （1）若$X(t)$是实平稳随机过程，则$\hat{X}(t)$也是实平稳过程，且联合平稳； （2）实函数与其希尔伯特变换的相关函数和功率谱相同，即： $$R_{\hat{X}}(\tau)=R_X(\tau)$$ $$S_{\hat{X}}(\omega)=S_X(\omega)$$ （3） $$R_{\hat{X}X}(\tau)=-\hat{R}_X(\tau)$$ $$R_{X\hat{X}}(\tau)=\hat{R}_X(\tau)$$ （4） $$R_{\hat{X}X}(\tau)=-R_{X\hat{X}}(\tau)$$ （5） $$R_{\hat{X}X}(-\tau)=-R_{\hat{X}X}(\tau)$$ （6） $$R_{\hat{X}X}(0)=0$$ 表明，在同一时刻，随机变量$\hat{X}_t$和$X_t$正交，但上式不意味着整个随机过程正交。 （7） $$R_{\tilde{X}}(\tau)=2[R_X(\tau)+jR_{X\hat{X}}(\tau)]=2[R_X(\tau)+j\hat{R}_X(\tau)]$$ （8） $$S_{X\hat{X}}(\omega)=\begin{cases}-jS_X(\omega),&amp;\omega\ge0\\jS_X(\omega),&amp;\omega\lt0\end{cases}$$ （9） $$S_{\tilde{X}}(\omega)=\begin{cases}4S_X(\omega),&amp;\omega\ge0\\0,&amp;\omega\lt0\end{cases}$$]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>复习笔记</tag>
        <tag>信号处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客站双机备份切换]]></title>
    <url>%2F2015%2F10%2F24%2F2015-10-24-dnsswitch%2F</url>
    <content type="text"><![CDATA[B计划还是应该要有的，万一A计划崩了呢。 一大早起来就看到设置的提醒消息发过来： 您好，您的网站宕机啦！！！ 大汗，X_X，赶紧测试了一下，还是能上滴，原来是down了一个站，还好还有另一个地址作了备份切换 我是今年上半年开始迁移Blog到Hexo上的，当时只是架在Github上。偶尔出现的问题就是有时候会被墙，访问速度也间歇性的掉到底，不过总体上来说还算稳定。 还有个问题是自从有一次Github被百度来的流量DDos之后，它就屏蔽掉了百度的蜘蛛，以至于完全没法被度娘收录。（百度任何关键词都找不到自己的Blog；翻出去Google一下，整个站都在上面…） 前段时间终于注册了自己的域名，就顺手把Blog同时部署在Github和Gitcafe上了。 我设的默认的访问地址是Gitcafe上的站，然而今早Gitcafe崩了。。。。。。(⊙o⊙) 前言Gitcafe：网站就在国内，速度快，而且不会被墙，百度的蜘蛛也能正常爬；问题是有时候pages莫名的不更新，稳定性还有待考虑等等 Github的好处是：项目多，因此主要还是得混这个，面向的是全世界，服务相对稳定；缺点就是可能会被墙，访问速度方面要稍微受到影响，然后国内搜索引擎收录不了 两个站都是支持自定义域名的，因此有了域名之后，就可以通过合理地设置CNAME记录来均衡网站的访问情况 然后是域名和域名解析。 我的域名解析用的是dnspod.cn，域名也是顺便在上面买的。 设置CNAME和监控首先在Github上和Gitcafe上分别建好自己的站，按照各自的要求把访问地址都指向自定义域名，然后需要做的就是通过设置域名的CNAME来正常地引导流量。 在dnspod上可以对同一个地址设置多条不同来源的CNAME，还是相当方便的。 Github在墙外访问比较方便，国外流量的目标地址，而且比较稳定，因此把它作为默认站；国内的流量就都链到Gitcafe上。 （下面这张图是刚刚截的，Gitcafe的监控都飘红了，仍然处在宕机中…） 还有更多的站的话，可以分别把来自电信、移动、教育网等等的流量分别链到更适合的地址上去。 其他的域名解析服务我不太清楚，但是对dnspod来说，设置监控是必要的，要不然当一个地址无法访问之后，没有办法自动迁移到另一个上去。 设置好切换规则之后，如果某个地址宕机，就会按照预先的规则切换到另外一个站上。 关于度娘的抓包问题，我发现前段时间即使按照这个样子设置好了，百度蜘蛛还是不爬我的站。 通过百度站长工具测试链接的访问情况之后都是一切正常的，抓包测试也能够通过，但它就是不抓。（我已经不想再隐藏我对度娘的鄙视之情了） Hexo部署方面的设置Hexo的静态系统主要依靠git来把本地生成好的静态页面推送至服务器端，现在有了两个站，如果没有办法解决同时推送的问题，那就太麻烦了。 好在Github和Gitcafe支持的都是标准的Git仓库指令，因此修改一下Hexo下_config.yml中的deploy字段即可： 1234567# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: git repository: github: git@github.com:jcf94/jcf94.github.io.git,master gitcafe: git@gitcafe.com:jcf94/jcf94.git,gitcafe-pages Github上是推送到master分支，Gitcafe要求推到gitcafe-pages分支 这样每次调用hexo d时就会同时部署到两个仓库里面去 后话话说相比Github来说，感觉Gitcafe的稳定性真的不敢说，光页面不更新这一点就很让人头疼，偶尔还会感觉当前显示的pages是前几次推上去的版本。 从速度上来看，D监控反馈给我的是Gitcafe平均访问时间在200ms+，Github平均访问时间在300ms+，事实上感觉应该差不了多少，某些线路上反而是Github更快。主要我现在用的主题刷新的时候带了点延迟动画，有时候不同设备/浏览器上js加载速度不同会让页面看上去像是卡了很久的样子。 额，可以考虑只把百度的流量链到Gitcafe上去抓链接。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>dns</tag>
        <tag>Github</tag>
        <tag>Gitcafe</tag>
        <tag>dnspod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硬件/软件接口 Virtual Memory]]></title>
    <url>%2F2015%2F10%2F23%2F2015-10-23-virtualmemory%2F</url>
    <content type="text"><![CDATA[正在努力攻克《硬件/软件接口》一书。 笔记总帖：计算机组成与设计.硬件/软件接口 学习笔记 读到第五章 Large and Fast: Exploiting Memory Hierarchy 5.7节Virtual Memory时遭遇了较大难度。另外花费了不少功夫来看。 这一节已经开始从CPU层涉及到操作系统的内容了，现在从总帖中另外再开出来一个单独记笔记。 《计算机操作系统》中对虚拟内存的介绍基础的存储器管理方式有一个特点，即要求将一个程序的所有内容都装入内存之后才能够运行，但是还有以下两种情况： 有的程序很大，其要求的内存空间已经超过了物理内存的容量，以至于程序没有办法完全装入内存，以至于无法运行； 有大量程序要求运行，但由于内存容量不足以容纳所有这些内容，因此只能运行一部分，而剩下的只能留在外存上等待。 出现以上两种情况的基本原因都是因为内存容量不够大。一个显而易见的解决方法是从物理内存本身上进行提升，但是这个往往会受到机器本身的限制。另一种方式就是从逻辑上扩充内存容量，这也就是虚拟存储技术要解决的问题。 1.常规存储器管理方式的特征 一次性，作业必须一次性地全部装入内存之后才能够开始运行。事实上，许多程序在运行时，并没有用到全部的数据，其实很多空间被浪费了； 驻留性，当程序被装入内存之后，就会一直留在内存中，其中任何时候都不会被换出，直至完全运行结束。 2.局部性原理程序运行时存在的局部性现象很早就已经被人发现了： 程序执行时，除了少部分的转移和过程调用指令以外，大多数情况下都是顺序执行的； 过程调用会把程序的执行轨迹从一部分区域转到另外一部分区域，但事实上每次调用的深度都是有限的。也就是说，程序将在一段时间之内，都局限在某个范围内运行； 程序中存在许多循环结构，这些结构只有少数指令，但是会被反复多次执行； 程序中还包括对很多数据结构的处理，比如说对数组的操作，这些处理也都是局限在很小的某个范围内。 另外，局限性还表现在下面两个方面： 时间局限性：如果程序中的某条指令被执行，则不久之后改指令可能被再次执行；如果某条数据被访问过，则不久之后该条数据可能被再次访问； 空间局限性：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间之内所访问的地址都集中在某一范围之内。 3.虚拟存储器的基本工作原理根据局部性原理可知，应用程序在运行之前没有必要全部装入内存，而只需要将当前就要用到的少数内容装入就可以，其余部分可以留在外存上。 程序在运行的过程中如果还需要调用更多的内容，则再从外存上调入内存，继续执行下去。 感觉国内的教材就是这个不好，老是总结这个性质、那个性质，除了考试的时候能够拿来背一下，没有特别实际的内容…0.0 回到 5.7. Virtual Memory构造虚拟存储器有两个主要动机： 允许多道程序之间有效而安全地共享存储器，尤其这里需要强调云服务器中多虚拟机之间的虚拟存储； 消除主存容量过小而对程序设计造成的影响。 这里的目标与《操作系统》一书中讲的相似 动机一：多程序运行 为了允许多程序共享同一个主存储器，主存里面只需要存放众多程序中活跃的那部分，就像cache中只存放一个程序的活跃部分一样。这就是局部性原理。 编译时，程序员事先是不知道哪些程序将和其他程序共享存储器。事实上，当程序在运行时，共享主存的情况是动态变化的，因此我们希望将每个程序都编译到它自己的地址空间——就是一连串只能由该程序访问的地址上。 由虚拟存储器来完成程序地址空间到真实的物理地址之间的转换。这也是完成了数据保护。 比如说：写两个程序时，两个程序都访问到了0x0001这个地址，然后同时运行；但两个又其实是独立的程序，不应该相互影响，程序的实际地址就是通过虚拟存储器来转换到主存上。 动机二：程序要求的内容超过了主存的容量 以前，为了解决内存超出限制的问题，程序员需要把整个程序分成很多个片段。每个片段由数据和代码共同构造成模块，运行的过程中由编程控制代码的装载和覆盖。 虚拟存储器的发明可以帮助程序员摆脱这种困境，它的功能就是管理主存储器和辅助存储器之间的关系，就像Cache和主存之间的关系一样。 机制 Cache-主存储器系统 虚拟存储器系统 数据的最终目标 CPU 主存储器（虚拟存储器） 第一级存储器 Cache 主存储器（物理存储器） 第二级存储器 主存储器 辅助存储器（Flash/硬盘） 数据单元 块（Blocks） 页（Page） 数据缺失 缺失（Miss） 缺页（Page Fault） 虚拟地址可以被映射到主存储器上或者辅助存储器上，多个不同的虚拟地址也可以被映射到相同的物理地址上。 虚拟存储器通过重定位技术来简化程序加载，在用地址进行访问主存储器之前，重定位将程序用到的主存虚拟地址映射到不同的主存物理地址上。虚拟存储器可以把程序加载到主存中的任何位置，这样就不一定需要用连续的块来存放程序了，只要整个主存中能够找到足够数量的页，分开映射即可。 另外考虑辅助存储器的访问速度（可见5.2节Memory Technologies中的详表），无论是Flash还是更慢的硬盘，相对内存来说都会有非常长的延时，设计时需要考虑一些关键因素： 访问时间太长，因此页应该足够大，一次获取就能得到足够多的数据，典型的页大小是4KB~16KB 选用减低缺页率的组织结构，比如让页以全相联的方式放置 从软件的角度，用一些更先进的算法来选择替换页 采用写回机制 5.7.1. 页的存放和查找前文提到过，由于缺页的代价实在太高，所以采用全相联的方式来降低缺页率会比较好，但是全文进行顺序检索也是不现实的。因此有了下面这种存储方式： 每一个程序都有自己独立的页表（Page Table）； 页表放置在主存中，用虚拟地址中的页码来进行索引，找到相对应的物理地址页； 为了指出页表在主存中的位置，硬件中有一个页表寄存器，寄存器指向的是每一条页表在主存中的起始位置。 页表、PC和寄存器，这三种东西放在一起就确定了一个程序的状态（State），通常把这种状态叫做一个进程（Process）。当一个进程占用CPU时，它就是活跃的（Active）；当另一个进程需要占用CPU时，就会保存下该进程的当前状态，把它变成非活跃的（Inactive）；等下次再切回来执行的时候，操作系统再重新加载程序的状态，把它激活，进程就会在之前保存的地方继续向下执行。 每个不同的进程可能有相同的虚拟地址（因为编程的时候无需考虑这一点），但是每个进程都有自己的页表，由操作系统负责分配和更新页表（地址的映射状况），每个虚拟地址都会指向不同的物理地址，因此不会产生冲突。 上图中，首先通过页表寄存器查到一个程序页表的起始位置；然后用程序中的虚拟地址页号（虚拟地址高位）来作为索引，定位到虚拟存储器的条目；与Cache非常类似，一条虚拟存储器记录里面有两项：有效位、内容（物理页号）；因为页表里面包含的是所有可能的虚拟页的映射情况，因此不需要Tag位；如果某一条是有效的，则命中，根据记录里面的内容就能找到对应的在主存上的物理页地址；否则就发生缺页，要去硬盘中提取内容到主存中来 注意上图中的设备：一个页的大小是2^12=4KB，而虚拟地址空间一共是2^32=4GB，物理地址只有2^30=1GB（可见虚拟地址系统能够让1GB的实际物理空间处理4GB内存大小的程序，还是相当强大的），然后图里面显示的一个条目宽度只有19位，事实上为了寻址方便，会加入其它保护信息把它扩到32位。 5.7.2. 缺页如果页表中的某条有效位是0，那么就发生了缺页，转由操作系统处理，操作系统需要在下一级存储层中（一般是硬盘）找到这一页，然后把它放到主存中去。 因此除了用页表跟踪虚拟页在主存中的对应位置，如果缺页发生，还需要用一个另外的数据结构来快速找到虚拟页在硬盘中的位置。这个结构可以是页表的一部分（比如说用页表的1表示命中，然后条目内容是虚拟页在主存上的物理地址；用0表示缺页，但是这个时候条目内容是虚拟页在硬盘上的物理地址），或者另外维护一个，然后寻址方式跟页表一样。 操作系统在创建进程的时候也会在硬盘上建好一个专门的区域叫做交换区（Swap Space），把进程中所有页的内容都放进去，然后建好索引。 然后，关于主存上的哪些页可以被替换掉，操作系统中也会有另外一个结构用来维护，当需要替换主存上的内容时，把长时间不用/近期不用的页写入交换区，要用的调出来。这个策略叫做LRU（Least Recently Used）。 要完全准确地执行LRU的代价太高，所以一种简化的方式是在页表中加入一个引用位（Reference Bit）/使用位（Use Bit）。当某一页被访问过时，把它置为1，然后每过一段时间把所有引用位清零。这样就可以判定在某一段特定的时间内哪些页被访问过。 5.7.3. 关于写在虚拟存储器系统中，对于硬盘的操作耗时过大，因此不可能使用写直达方式，必须使用写回机制，对存储器中的页进行单独读写，只有在该页要被替换出存储器时再复制到硬盘中。 并且在磁盘中，复制整页回去比写单个字要高效的多，尽管开销依然大。所以为了追踪读入主存中的页是否被重写过，可以在页表中添加一个重写位（Dirty Bit）。当该页中的任何一个字被修改之后就将其置位。 当操作系统需要替换掉一整页的时候，如果该页需要被重写才将其整页写回硬盘；否则不需要写回。 5.7.4. 加快地址转换：TLB页表存放在主存中，因此程序每次访存实际上需要两次对主存的访问：第一次访问主存上的页表，用虚拟地址得到转换之后的物理地址；然后检查Cache上物理地址的命中情况，如果缺失，那么要用这个物理地址再次访问主存获取数据。 现代的处理器都包含一个特殊的Cache来追踪最近使用过的地址变换。这个特殊的地址变换Cache就是TLB（Translation-lookaside Buffer）。 虚拟存储器这部分多次反复使用到了Cache的思想。TLB与页表的思想区别是，页表有很多个，TLB只有一个。页表是在思想上参照了Cache，TLB则完全就是Cache的翻版。 上图中的页表加上了前几小节中提到的引用位（Reference Bit）和重写位（Dirty Bit）。虚拟地址过来首先跟Cache一样把虚拟地址页号分成两部分，用索引地址找到对应的条目，然后对比Tag，如果命中则直接使用物理地址；否则TLB缺失，则继续看能否在主存中找到，并装载到TLB中；再否则就是发生了主存缺页，进一步跳到硬盘上调取数据。 三个标志位同步更新处理即可。 5.7.5. 集成虚拟存储器、TLB和Cache虚拟存储系统和Cache可以看成是同一级的，下面来看一下TLB、页表、Cache三个缺失和命中的所有情况： TLB 页表 Cache 可能性 发生的情况 Hit Hit Hit 可能 Hit Hit Miss 可能 Miss Hit Hit 可能 TLB缺失，但找到页表，再找Cache Miss Hit Miss 可能 TLB缺失，Cache缺失，找主存 Miss Miss Miss 可能 TLB缺失且缺页，需要找硬盘 Hit Miss Miss 不可能 主存中没有，TLB中就不可能有 Hit Miss Hit 不可能 主存中没有，TLB中就不可能有 Miss Miss Hit 不可能 主存中没有，Cache中也不会有数据 上文的所有情况都是假定Cache是完全物理寻址的，即Cache必须要由虚拟存储器对虚拟地址转换为物理地址之后才能寻址，缺点就是，CPU直接生成的地址都是虚拟地址，所以这是个串行的过程，则访问主存的时间要包括对TLB访问和对Cache访问的时间。 另外有一种方式是Cache的虚拟寻址，可以是完全的或者是部分的，通过查找和对比虚拟标签来找到目标，这样就不需要访问TLB了。当然问题就是，多个程序如果虚拟地址是相同的，那么会产生冲突，需要另外加以限制。 折中解决方案是采用虚拟寻址物理标记，使用页内偏移进行寻址（页内偏移就是地址的低位，是不被转换的，因此实际上这部分虚拟地址就是物理地址），然后高位虚拟地址经过转换之后用来作为Tag。这种方式可以拥有两种方式的优点，并且是并行的（虽然转换的时候可能有更多的延迟）。上面举例的FastMATH采用的就是这种。 5.7.6. 虚拟存储器中的保护为了使得操作系统能够对虚拟存储器系统进行保护，硬件至少要提供3种基本功能：1.至少支持两种模式：用户进程和操作系统进程；2.提供一部分处理器的状态为用户只读。包括读取处理器状态、页表指针以及TLB等。只有操作系统可以通过特殊指令来对它们进行写操作；3.提供用户态和操作系统态的相互切换机制。 页表只能被操作系统修改，用户不能对其进行操作。 一旦开始共享主存，每个用户态的进程只能够在自己的地址空间内进行操作，另外操作系统也通过设置页表上的选项来指定进程的读写能力。 5.7.7. 处理TLB缺失和缺页TLB缺失和缺页都要通过调用系统异常（中断）来解决，主要是通过把异常发生时的状态保存在一些特定的寄存器中，如EPC等，然后调用系统异常处理程序。 TLB操作与页表操作都要通过特殊的指令来完成。 本节到此结束，真是有点累。增加了存储器部分的详细信息，尤其是支持虚拟存储器系统之后，第4章的数据通路与流水线怕是要发生很大的改动了。存储器的访问部分需要耗费的时钟周期远比原来想象的要多。 资料： 什么是TLB和PCID？为什么要有PCID？为什么Linux现在才开始使用它？]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>组成与设计.硬软件接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用小结]]></title>
    <url>%2F2015%2F10%2F16%2F2015-10-16-git%2F</url>
    <content type="text"><![CDATA[Git -- The stupid content tracker. Linus Torvalds 使用git已经有一段时间了，从刚开始的陌生到慢慢熟悉 掌握基本的用法之后，会让人对开发、尤其是多人合作开发有了新的理解……甚至于平时写文档的时候都想用了 脑海中出现这么个画面 毕业论文.doc毕业论文修改版.doc毕业论文完结版.doc毕业论文最终版.doc毕业论文再也不改了版.doc… 这时候要是有git在，一个文件就足够了，多次保存之后commit好即可 把平时常用的一些操作做一下整理。 开始一个项目一般开始项目有2种情况：复制一个现成的git项目或者创建新项目 复制： 1$ git clone [source] [destination] [source]里面是项目的位置，可以是本地/服务器上的某个目录，也可以是远程库（例如github）中项目的地址（HTTPS或者SSH地址） [destination]是拷到本地存放的位置，不加就是默认当前目录下 事实上……这个命令就是复制粘贴，尤其是如果是从本地clone到本地的时候。当然，只有git项目可以clone，对一个普通的文件夹调用clone命令还是不行的 创建: 1$ git init [name] [name]是项目的名字，不加就是把当前目录初始化成git库 初始化之后，目录下面会多出一个隐藏的.git文件夹，里面保存了git库需要的各种信息文件 示例就拿一开始的论文脑洞来举例吧，我这里是装好git之后，配好运行环境，然后就可以在cmd里面操作git了 命令行到桌面，然后： 12C:\Users\lenovo\Desktop&gt;git init testInitialized empty Git repository in C:/Users/lenovo/Desktop/test/.git/ clone一下试试 1234C:\Users\lenovo\Desktop&gt;git clone test lunwenCloning into 'lunwen'...warning: You appear to have cloned an empty repository.done. 然后就可以开始工作啦，接下来的操作都会在这个名字叫做lunwen的目录（库）下面完成 开始工作1$ git status 这条命令是显示当前库中（其实就是当前目录中）需要被跟踪的文件的状态。 文件的状态有三种，一开始新文件的状态是“Untracked”或者“Changes not staged for commit”，表示是刚创建的新文件还没有被git所追踪，或者是库中原本已有的文件发生了修改还没有被git所追踪，添加跟踪需要用到下面的这条add指令： 12$ git add [file]$ git add -A -A 是默认把当前目录下所有需要追踪的文件都添加追踪，或者手动输入文件名进行各个文件的单独追踪 1$ git reset [file] 什么参数都不带表示把刚刚add上去的文件取消追踪，或者手动输入文件名进行各个文件的单独取消 1$ git diff 对库中原本已有的修改文件进行新旧版本对比，显示文件的区别 1$ git diff --staged 跟上一条指令类似，区别是这个是对已add进库中的文件进行新旧版本对比 1$ git commit -m "[descriptive message]" 当把所有文件都add进来之后，文件的状态变成了“Changes to be committed”，表示所有修改都已经被追踪，等待提交 然后就需要上面这条命令，descriptive message是对本次提交的描述，如果不加-m和描述，git会打开一个编辑器要求输入描述 示例现在库里面是空的，新建个文本文档lunwen.txt，里面写上test1，然后查看一下状态： lunwen.txt 1test1 1234567891011C:\Users\lenovo\Desktop\lunwen&gt;git statusOn branch masterInitial commitUntracked files: (use "git add &lt;file&gt;..." to include in what will be committed) lunwen.txtnothing added to commit but untracked files present (use "git add" to track) 显示新的文件还没有被追踪过，先提交一下第一个版本： 123456789101112131415161718192021C:\Users\lenovo\Desktop\lunwen&gt;git add -AC:\Users\lenovo\Desktop\lunwen&gt;git statusOn branch masterInitial commitChanges to be committed: (use "git rm --cached &lt;file&gt;..." to unstage) new file: lunwen.txtC:\Users\lenovo\Desktop\lunwen&gt;git commit -m "first"[master (root-commit) 2561997] first 1 file changed, 1 insertion(+) create mode 100644 lunwen.txtC:\Users\lenovo\Desktop\lunwen&gt;git statusOn branch masternothing to commit, working directory clean 提交完成，接下来对lunwen.txt稍微做点修改，然后再创建个新的文件test.txt： lunwen.txt 12test2test3 test.txt 12test2test3 1234567891011121314C:\Users\lenovo\Desktop\lunwen&gt;git statusOn branch masterChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: lunwen.txtUntracked files: (use "git add &lt;file&gt;..." to include in what will be committed) test.txtno changes added to commit (use "git add" and/or "git commit -a") status里面清楚地显示了库中文件的改变情况即：lunwen.txt是已被追踪但是修改了的，test.txt是还没有被追踪的，接下来试一下diff这个命令： 12345678910111213141516171819202122232425262728293031323334353637383940414243C:\Users\lenovo\Desktop\lunwen&gt;git diffdiff --git a/lunwen.txt b/lunwen.txtindex f079749..66bb064 100644--- a/lunwen.txt+++ b/lunwen.txt@@ -1 +1,2 @@-test1\ No newline at end of file+test2+test3\ No newline at end of fileC:\Users\lenovo\Desktop\lunwen&gt;git add -AC:\Users\lenovo\Desktop\lunwen&gt;git diff --stageddiff --git a/lunwen.txt b/lunwen.txtindex f079749..66bb064 100644--- a/lunwen.txt+++ b/lunwen.txt@@ -1 +1,2 @@-test1\ No newline at end of file+test2+test3\ No newline at end of filediff --git a/test.txt b/test.txtnew file mode 100644index 0000000..66bb064--- /dev/null+++ b/test.txt@@ -0,0 +1,2 @@+test2+test3\ No newline at end of fileC:\Users\lenovo\Desktop\lunwen&gt;git commit -m "second"[master 50e143b] second 2 files changed, 4 insertions(+), 1 deletion(-) create mode 100644 test.txtC:\Users\lenovo\Desktop\lunwen&gt;git statusOn branch masternothing to commit, working directory clean diff显示的只有lunwen.txt这个文件的变化，因为这个文件本来就在库里面，也是一直都被追踪着的。a/显示的是旧版本，b/显示的是新版本 diff –staged显示的则是所有被add过的文件的差异，test.txt由于是新创建的，所有没有旧版本 删除文件1$ git rm [file] 删除文件以及所有信息 1$ git rm --cached [file] 删除文件的追踪信息，但是文件仍然保留 1$ git mv [file-original] [file-renamed] 重命名文件 版本保存和恢复1$ git log 显示所有commit的记录 重点来了，git保存了库中所有文件完整的演变过程，以每一次commit作为一个保存点，如果发生问题，可以直接回滚到之前的任何一个版本，而需要的指令则是上面用过的这一条： 1$ git reset reset的作用是使目录会退到上一次的状态，在前面用来取消掉add，其实它的完整用法是这样的： 1$ git reset [--hard|soft|mixed|merge|keep] [&lt;commit&gt;或HEAD] reset这个命令比较复杂，可以戳这个了解更详细的实例 先整体明确一下整个git的结构： 首先是当前正在操作的内容，就是现在正在修改/编辑的文件，这一块叫做“Working Directory” 通过add之后，追踪的内容被保存到了“Git Index”里面，这一块可以看成是当前目录和版本库之间的暂存区 最后再用commit写上标注，把index里面更新的内容修改到“GIT Directory”即版本库里面 好，那么回到reset commit字段用于选择哪一个版本，如果不加，则默认为HEAD，代表离当前最近的一个版本，也就是上一次最近的commit 简单地说一下常用的几个： –hard 是强制将当前所做的所有修改都清除掉，HEAD、index和working directory全部恢复到目标commit –mixed 是不加参数时的默认选择，将HEAD和index都恢复到目标commit，但是保留目录下面修改了的文件。即此时只要add一下，再commit一下就能把当前修改的版本保存为下一个版本 –soft 程度更弱，只将HEAD指向commit，index和当前的工作文件都保持不变，因此自目标commit以来的所有改变都会显示为“Changes to be committed”。此时只要commit一下，就能将当前修改的版本保存为下一个版本 版本暂存这个功能用来遇到突发状况需要对最后一次的commit进行操作，然后又不想提交当前修改的时候用 1$ git stash 将当前工作文件保存进栈，然后把工作区恢复到上一次的commit 然后这样的操作还可以多次进行，每次的暂存就都会被压进栈 1$ git stash list 显示栈中保存的所有暂存信息 1$ git stash pop 弹出栈顶的保存信息，就是把栈顶的保存信息恢复到工作区 1$ git stash drop 不恢复，直接删除栈顶的保存信息 1$ git stash clean 清除栈中的所有信息 远程操作刚刚的都是针对本地库的操作，如果不需要上传到他地方，这些内容就足够了，而git的一大强大之处就是能方便多人协作，因此远程功能是少不了的 1$ git remote [-v] 不带选项时，remote命令会列出所有远程主机的名字 -v 则会列出远程主机的地址 1$ git remote add &lt;name&gt;&lt;url&gt; 用于添加一台主机 1$ git remote rm &lt;name&gt; 用于删除一台主机 1$ git remote &lt;old-name&gt;&lt;new-name&gt; 重命名一台主机 示例一开始我创建的lunwen库是从test库clone过来的，所以查看lunwen库的远程主机情况就是这样的： 123456C:\Users\lenovo\Desktop\lunwen&gt;git remoteoriginC:\Users\lenovo\Desktop\lunwen&gt;git remote -vorigin C:/Users/lenovo/Desktop/test (fetch)origin C:/Users/lenovo/Desktop/test (push) 默认clone过来的主机名是origin 分支控制这个也是git中相当重要的一个内容 一个库中可以有多个分支，可以用来更方便地管理整个版本信息 1$ git branch [-a|r] 不加参数时，默认列出本地库中所有的分支信息，并在当前分支前面加*号 -r 是列出远程库 -a 列出本地和远程库 1$ git branch [branch-name] 创建一个新的分支 1$ git checkout [branch-name] 切换到某个分支上，如果分支不存在，则会首先创建新分支再切换过去 切换到分支上时，当前工作目录下面的所有文件都会直接变掉（第一次用的时候觉得这个很神奇） 1$ git merge [branch] 将目标分支合并到当前分支里面来 1$ git branch -m [oldname] [newname] 分支重命名 1$ git branch -d [branch-name] 删除某一个特定的分支 1$ git fetch &lt;name&gt; 从某一台远程主机上获取所有的更新 远程分支操作如果 fork 了一个别人的项目，然后对方更新了库，想要保持自己的库跟对方一直，则需要先把源库 fetch 下来，跟自己的 merge 之后，再 push 回自己的库上。 1$ git remote add upstream xxx.git 添加 upstream 的远程库信息。 1$ git fetch upstream fetch upstream 源的新版本到本地 1$ git merge upstream/master 将 upstream 库的 master 分支跟当前分支合并。 之后再 push 回去就行了。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[住院日记]]></title>
    <url>%2F2015%2F09%2F22%2F2015-09-22-back%2F</url>
    <content type="text"><![CDATA[9月22日 周二 In Xi’an本来什么事情都很顺利，下午跟老爸打完电话以后整个人都不好了。 我高中毕业那年在外婆身上查出来过的病又轮到老妈了。明明上周才打完电话一切正常的。 还好是良性的早期，也发现得早。 今晚回上海的票已经买不到了，只好买了明天晚上最快的一班。 什么都比不了健康。 珍惜身边的每一个人，特别是亲人。 9月23日 周三 On The Train原本也是想着学校这边没有什么事情了，推免也是只差最后报名这一步了，准备国庆后回趟家的。 但是做梦也没想到现在提前回去了，却居然会是因为这样一个原因。 老妈明天就要动手术了，不知道是上午还是下午。 现在只希望能在她进手术室前赶到医院。 9月24日 周四 In Shanghai来到医院的第一天。老妈住院第二天。 手术很成功。 谢谢大家。 9月25日 周五 In Shanghai第二天。 住在医院旁边的快捷酒店里。 老爸3点就起来去医院了，只要顺利过了术后的第一个早晨后面应该就好了。 9月26日 周六 In Shanghai第三天。 老妈已经能自己下地了，大医院就是不一样…0.0 要是在家里那些医院，怎么也得天天天天打吊瓶 出门找了一圈，没有发现有卖月饼的，我去。。。 明天下午坐地铁出去找找吧。 9月27日 周日 In Shanghai第四天。 找月饼！ 求助了一圈万能的朋友圈，没有得到太有价值的信息。只好自己又坐地铁去了南京路。 在南京西路找到了一家叫王家沙的糕点店，买鲜肉月饼排了个长队啊啊啊 话说一开始听到鲜肉月饼这个词我是拒绝的，这玩意对我来说听上去就像黑暗料理啊啊啊，月饼里面是一团肉这是什么鬼啊啊啊，肉包吗啊啊啊~~~~ 然后看着人家都抢着买，顺手买了两盒。吃完以后后悔死了啊啊啊啊，买一盒尝尝鲜就好了啊啊啊啊。 Q_Q 我很想吐槽的是，医院的作息时间特别早： 5点：准备起床…6点：发早餐7点半：医生查房10点40：发午餐…这么早是要闹哪样…啊啊啊下午4点：发晚餐…这个时间更加丧心病狂好么…晚上7点以后基本就没人走动了8点左右就准备关灯睡觉了…Q_Q 生物钟已经硬生生地被扯过来了~~0.0 9月28日 周一 In Shanghai第五天。 工作日的早晨，5点多准备去外面KFC买粥的时候就看到医院外面已经全是排队挂号的人了…（-_-///大部分是黄牛） 这几天也是国庆假期之前的最后几个工作日了，此时病房里面正在查房…所有家属都被赶出来了 如果顺利的话，应该这几天就能出院了/要不就得在这里待上一整个国庆。 推免生志愿填报也是今天开放。 我奋斗了十六年，不是为了和你坐在一起喝咖啡 真的是明后天就可以出院了…0_0…好快 隔壁的是今天出院，然后看到护士插了一张新的单子上去，下一个住在那个床位的…36岁 真是… 9月29日 周二 In Shanghai第六天。 下午去办出院手续，明早回家！ 9月30日 周三 Ready to Back Home第七天。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[退役确认以及下阶段学习计划]]></title>
    <url>%2F2015%2F09%2F19%2F2015-09-18-startandnext%2F</url>
    <content type="text"><![CDATA[看着一手建立的ACM协会从初生到慢慢壮大 1年过去了，很高兴看到的是 我们进步得挺好，它也发展得挺好 有关A协记得去年写过一篇弱弱的随笔 浅谈我院学术氛围之我见 记于A协成立之时，也是写的对未来新生们的期许。 转眼间已经是1年过去了，看着当初带出来的一代成员现在都学得非常不错，也都能够独当一面了。 今年新招的新生质量还算不错，然后就看新生见面会上Lw一下没忍住，又上去一番振奋人心的宣讲O(∩_∩)O 希望这些孩子以后都能在ACM中有自己的收获吧~！ 相信长大的ACM事业一定会越来越好！ 有关我的ACM生涯 上学期期末准备夏令营的时候曾经写过一篇 退役贴，原来标题写的是《退役贴 伪》，现在终于回来删掉了那个“伪”字，也算是真的确认退役了。 其实好多想说的话当时已经都写完了，今天也没什么太多要补充的。 很感谢高中3年OI与大学3年ACM经历带给我的一切，虽然我还是没有让自己成长到足以用成绩回报两所母校。 严谨的思维方式、谦逊的生活态度、以及坚持到最后一刻的毅力…… 除了代码能力和专业素养以外，我从这里学到的太多太多。 也认识了很多优秀的朋友。 下周会继续再帮着打今年的网络赛。然后，就要真的告别这个舞台啦。（明年如果我们学校会办省赛的话，可以过来打酱油当当裁判…） 有关未来的准备中午刚跟导师安虹聊了挺久，确定了到时候进她课题组的时候给的是学硕，只是导师名字要挂在韩文廷老师的名下。不过反正整个实验室都是大BOSS带，也就无所谓了。 学院动员会（保本校の挽留会？）上学院院长给我们讲的是： 在自己学校，老师会把你们当宝；一旦出去了，人家只会把他们自己的学生当宝，面临的就将是更大的竞争 其实也无可厚非啊。虽然导师人很好，也一般不会有什么偏见，但是毕竟本科出身不如别人，这是事实（虽然我们学校的名气正在逐年上升）。 想要改变别人对你的看法？靠的只有自己的努力咯！！！ 虽然还没开始报名，然而各方面都已经谈妥，保研的事情终于马上就要告一段落了，下面列一下未来准备要做的事情： 杨俊老师交给我的安卓项目终于被他收回去了（好高兴啊，有木有！一开始就不想做啊，有木有！在我毫不知情的情况下就直接把项目扔过来了啊，有木有！方向完全不对啊，有木有！我本来就不搞软件开发啊，有木有！） 编译原理。龙虎鲸三大神书里面现在买到了并且在看的是《龙书》（坑已经挖好，正在补Q_Q） 计算机系统结构。《组成与设计.硬件/软件接口》（也是已经挖好了坑，还要补）、《量化分析方法》（得等组成与设计看完了再接着看这个）、《计算机组成原理》（标准教科书，也是当时买的，然而还没翻过） 尝试看一下体系结构方向顶级会议的论文，例如ISCA 2014 会议论文整理等等。尝试从写综述开始。（还没发过论文啊啊啊，开始硕士生涯之前希望能够发掉处女文） 继续啃以前啃了好久的Linux内核 学会Python、学会爬虫 学会并行编程 学会HDL编程，目标是用HDL写个简单的CPU（看完体系结构bible之后） （By the Way… 除了上面那一条以外，还想把Minecraft里面的红石电路搞明白/Hoho/） （额，既然要搞Minecraft了，那顺便熟练掌握Besiege的各种玩法吧） 学会nodejs中几个简单库的语法，改或者写一个自己的hexo博客主题出来，拿别人的用总不如完全靠自己来的好。现在已经看上的主题除了这一个以外还有两个，想把它们的特性综合一下 有机会的话，在Github上参与一下别人的开源项目，积累开发经验 继续学习乐理知识，练成Everyone Piano 锻炼… ……剩下的想到再补吧。 很喜欢这首曲子：]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2015 ACM-ICPC Asia Regional Changchun Online]]></title>
    <url>%2F2015%2F09%2F13%2F2015-09-13-Changchun-Online%2F</url>
    <content type="text"><![CDATA[【A】Alisha’s Party 堆模拟【B】Ponds【C】Aggregated Counting【D】Clock Adjusting【E】Travel 并查集【F】Favorite Donut【G】The Water Problem【H】Elven Postman【I】Food Problem【J】Unknown Treasure【K】Good Numbers【L】Marisa’s Cake【M】Robot Dog 题解在此： acminfo 结题报告整理页 然后是这个 长春2015的那一场网络赛 可能是最后一次写题解了，后面的题目也不会再补了。 下半年还是希望能够打出现场赛名额，但是我已经不会再参加了。 【A】HDU 5437 Alisha’s PartyTime Limit: 3000/2000 MS (Java/Others) Memory Limit: 131072/131072 K (Java/Others) Problem DescriptionPrincess Alisha invites her friends to come to her birthday party. Each of her friends will bring a gift of some value v, and all of them will come at a different time. Because the lobby is not large enough, Alisha can only let a few people in at a time. She decides to let the person whose gift has the highest value enter first. Each time when Alisha opens the door, she can decide to let p people enter her castle. If there are less than p people in the lobby, then all of them would enter. And after all of her friends has arrived, Alisha will open the door again and this time every friend who has not entered yet would enter. If there are two friends who bring gifts of the same value, then the one who comes first should enter first. Given a query n Please tell Alisha who the n−th person to enter her castle is. InputThe first line of the input gives the number of test cases, T , where 1≤T≤15. In each test case, the first line contains three numbers k,m and q separated by blanks. k is the number of her friends invited where 1≤k≤150,000. The door would open m times before all Alisha’s friends arrive where 0≤m≤k. Alisha will have q queries where 1≤q≤100. The i−th of the following k lines gives a string Bi, which consists of no more than 200 English characters, and an integer vi, 1≤vi≤10^8, separated by a blank. Bi is the name of the i−th person coming to Alisha’s party and Bi brings a gift of value vi. Each of the following m lines contains two integers t(1≤t≤k) and p(0≤p≤k) separated by a blank. The door will open right after the t−th person arrives, and Alisha will let p friends enter her castle. The last line of each test case will contain q numbers n1,…,nq separated by a space, which means Alisha wants to know who are the n1−th,…,nq−th friends to enter her castle. Note: there will be at most two test cases containing n&gt;10000. OutputFor each test case, output the corresponding name of Alisha’s query, separated by a space. Sample Input 1 5 2 3Sorey 3Rose 3Maltran 3Lailah 5Mikleo 61 14 21 2 3 Sample Output Sorey Lailah Rose 题意所有的人按照一个队列的顺序到达，但是门只有在特定的时间才打开，且每次打开只能有前p个值最大的才能进入。要求输出第n个进门者的名字。 分析刚拿到题就觉得这个按照题意要求直接模拟即可。 然而priority_queue打上去TLE了… 刚分析觉着是不是一个一个从队列里面删掉太慢了，改用splay这样旋转到位一次性全删掉应该会更快点…然后就听旁边另一队手打堆过了。 那么事实证明STL的优先队列效率并没有这么高咯。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142/* ***********************************************MYID : Chen FanLANG : G++PROG : 1001************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct fri&#123; char name[250]; int time,value; friend bool operator &lt; (fri a,fri b) &#123; if (a.value==b.value) return a.time&gt;b.time; else return a.value&lt;b.value; &#125;&#125; friends;friends a[150010];typedef struct ti&#123; int t,p;&#125; tim;bool op_ti(tim a,tim b)&#123; return a.t&lt;b.t;&#125;tim b[150010];int c,outlist[150010];typedef struct heaptyp&#123; int num,key;&#125; heaptype;heaptype heap[150010];int heaptail;void heapadd(int s,int t)&#123; int i,j; heaptail++; heap[heaptail].num=s; heap[heaptail].key=t; i=heaptail; j=i&gt;&gt;1; while (i&gt;1&amp;&amp;(heap[i].num&gt;heap[j].num||(heap[i].num==heap[j].num&amp;&amp;heap[i].key&lt;heap[j].key))) &#123; swap(heap[i],heap[j]); i=j; j=i&gt;&gt;1; &#125;&#125;heaptype heappop()&#123; int i,j; heaptype x,y; y=heap[1]; x=heap[heaptail]; heaptail--; i=1; j=i&lt;&lt;1; while (j&lt;=heaptail) &#123; if (j&lt;heaptail&amp;&amp;(heap[j].num&lt;heap[j+1].num||(heap[j].num==heap[j+1].num&amp;&amp;heap[j].key&gt;heap[j+1].key))) j++; if (x.num&lt;heap[j].num||(x.num==heap[j].num&amp;&amp;(x.key&gt;heap[j].key))) &#123; heap[i]=heap[j]; i=j; j=i&lt;&lt;1; &#125; else j=heaptail+1; &#125; heap[i]=x; return y;&#125;int main()&#123; //freopen("1001.txt","r",stdin); int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int k,m,q; scanf("%d%d%d",&amp;k,&amp;m,&amp;q); for (int i=1;i&lt;=k;i++) &#123; scanf("%s%d",&amp;a[i].name,&amp;a[i].value); a[i].time=i; &#125; for (int i=1;i&lt;=m;i++)scanf("%d%d",&amp;b[i].t,&amp;b[i].p); b[m+1].t=k; b[m+1].p=k; sort(&amp;b[1],&amp;b[m+2],op_ti); int tail=0; heaptail=0; for (int i=0;i&lt;=m;i++) &#123; for (int j=b[i].t+1;j&lt;=b[i+1].t;j++) &#123; heapadd(a[j].value,a[j].time); &#125; int temp=0; while (heaptail&amp;&amp;temp&lt;b[i+1].p) &#123; heaptype now=heappop(); tail++; temp++; outlist[tail]=now.key; &#125; &#125; scanf("%d",&amp;c); printf("%s",a[outlist[c]].name); for (int i=1;i&lt;q;i++) &#123; scanf("%d",&amp;c); printf(" %s",a[outlist[c]].name); &#125; printf("\n"); &#125; return 0;&#125; 【E】HDU 5441 TravelTime Limit: 1500/1000 MS (Java/Others) Memory Limit: 131072/131072 K (Java/Others) Problem DescriptionJack likes to travel around the world, but he doesn’t like to wait. Now, he is traveling in the Undirected Kingdom. There are n cities and m bidirectional roads connecting the cities. Jack hates waiting too long on the bus, but he can rest at every city. Jack can only stand staying on the bus for a limited time and will go berserk after that. Assuming you know the time it takes to go from one city to another and that the time Jack can stand staying on a bus is x minutes, how many pairs of city (a,b) are there that Jack can travel from city a to b without going berserk? InputThe first line contains one integer T,T≤5, which represents the number of test case. For each test case, the first line consists of three integers n,m and q where n≤20000,m≤100000,q≤5000. The Undirected Kingdom has n cities and m bidirectional roads, and there are q queries. Each of the following m lines consists of three integers a,b and d where a,b∈{1,…,n} and d≤100000. It takes Jack d minutes to travel from city a to city b and vice versa. Then q lines follow. Each of them is a query consisting of an integer x where x is the time limit before Jack goes berserk. OutputYou should print q lines for each test case. Each of them contains one integer as the number of pair of cities (a,b) which Jack may travel from a to b within the time limit x. Note that (a,b) and (b,a) are counted as different pairs and a and b must be different cities. Sample Input 1 5 5 32 3 63341 5 157243 5 57054 3 123821 3 2172660001000013000 Sample Output 2 612 题意给了一个无向图。给出一个x作为两点间每一段路径的上限。求从a到b上所有的路径长度都不超过x的（a，b）的点对总数。 分析这样的并查集其实做过很多次了已经。 对所有边按照长度排序，询问也是按照长度先排序，然后从小到大进行。边长满足条件的两端并进一同一个集中，每合并两个集合，总点对数都增加 $左集合的点数/右集合的点数/2$ （a,b和b,a算两个不同点对） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128/* ***********************************************MYID : Chen FanLANG : G++PROG : 1005************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int father[20010],ran[20010]; void clean_father(int n)&#123; for (int i=1;i&lt;=n;i++) &#123; father[i]=i; ran[i]=1; &#125;&#125;int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link_ran(int x,int y)&#123; int xx=getfather(x),yy=getfather(y); if (ran[xx]&gt;ran[yy]) &#123; father[yy]=xx; ran[xx]+=ran[yy]; &#125; else &#123; father[xx]=yy; ran[yy]+=ran[xx]; &#125;&#125;typedef struct nod&#123; int a,b,c;&#125; node ;bool op(node a,node b)&#123; return a.c&lt;b.c;&#125;node a[100010];typedef struct nod1&#123; int limit,no; long long ans;&#125; node1 ;bool op1(node1 a,node1 b)&#123; return a.limit&lt;b.limit;&#125;bool op2(node1 a,node1 b)&#123; return a.no&lt;b.no;&#125;node1 b[5010];bool flag[20010];int main()&#123; //freopen("1005.txt","r",stdin); int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,m,q; scanf("%d%d%d",&amp;n,&amp;m,&amp;q); clean_father(n); for (int i=1;i&lt;=m;i++) scanf("%d%d%d",&amp;a[i].a,&amp;a[i].b,&amp;a[i].c); sort(&amp;a[1],&amp;a[m+1],op); for (int i=1;i&lt;=q;i++) &#123; scanf("%d",&amp;b[i].limit); b[i].no=i; &#125; sort(&amp;b[1],&amp;b[q+1],op1); int j=1; long long ans=0; for (int i=1;i&lt;=q;i++) &#123; for (;j&lt;=m;j++) &#123; if (a[j].c&gt;b[i].limit) break; if (getfather(a[j].a)!=getfather(a[j].b)) &#123; ans+=ran[getfather(a[j].a)]*ran[getfather(a[j].b)]*2; link_ran(a[j].a,a[j].b); &#125; &#125; b[i].ans=ans; &#125; sort(&amp;b[1],&amp;b[q+1],op2); for (int i=1;i&lt;=q;i++) printf("%lld\n",b[i].ans); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成与设计.硬件/软件接口 学习笔记（一）]]></title>
    <url>%2F2015%2F09%2F09%2F2015-09-09-cod%2F</url>
    <content type="text"><![CDATA[《Computer Organization and Design, Hardware/Software Interface》以及《Computer Architecture: A Quantitative Approach》被誉为体系结构界的两本圣经。 最近除了编译原理外，还在补的就是《硬件/软件接口》这一本了。 这本书讲解得非常透彻。原本我只是对体系结构、CPU有个模糊的很难学的概念，，，书中由浅至深详细地解释了其中的工作原理，，，，真的好难啊Q_Q… 2018-02-02 当时还剩下最后一章的一点东西没看，回来把这些内容看完。 算是在这个方面做了这么久的工作，对里面的好多东西也都有了新的理解，顺便重新整理一下。 本书的官方主页：Elsevier Store 笔记的内容基本上按照目录，这一篇记的是前三章的主要内容。 CHAPTER 1. Computer Abstractions and Technology1.1. Instruction1.2. Eight Great Ideas in Computer Architecture计算机系统结构中的八个 Great Ideas 其实系统结构设计发展这么多年了，很多核心的设计思想本质上都是几个基本原理。 Design for Moore’s Law 系统结构设计的周期通常都比较长，由于摩尔定律的存在，很有可能项目开始时和项目结束时能够提供的原件在工艺方面的性能就有很大的差距了，因此做系统结构设计需要考虑到这方面的因素。 话说 Intel 每年的 CPU 升级战略也有一点这个意思，“Tick-Tock”一年升级工艺制程，一年升级架构设计。 然后到了 2017 年，摩尔定律开始唱衰了，各方都开始吹摩尔定律要走到头了，一大波人转而去搞领域专用的 ASIC 芯片设计去了。 Use Abstraction to Simplify Design 分层次进行设计，底层的细节对上层不透明，意思就是上层不用去管底层里面是怎么做的，只要给我想要的接口就好了。 其实也就是模块化。这样做的好处是，比如做某一层工作的人可以专心钻在那层里面，在自己的范围内做改进啊做优化啊，不用去管别人的工作需要做什么。 当然层次分的太多，可能也会引起性能的下降，有的时候我们搞优化的反而就想要层次简单点，一波直通到底。 Make the Common Case Fast 做优化要把工作放在 Common Case 上， 花大力气在 Rare Case 上就浪费了。另一方面要找到最耗时的点，找到瓶颈所在才有用。 那么哪些地方是 Common Case 呢？这就需要做针对性的测试来找出来了（见 Section 1.6）。 Performance via Parallelism 并行也没啥好说的，这几年的发展趋势就是硬件上单核往多核甚至众核走，算法上也都要往并行方面设计。 Performance via Pipelining 流水线是一个非常重要的思想，在硬件和软件的架构设计里面都能用的到。 多级流水可以提高资源利用率、隐藏延迟等等。 Performance via Prediction 这块主要指的是处理器指令预测、指令预取等等。 Hierarchy of Memories 存储器分级最突出的体现还是在 cache 上，现在主流的处理器上 cache 已经做到了 L3，可能过几年就有 L4 了。 Dependability via Redundancy 通过冗余设计来保证系统可靠性，这个主要是容灾备份的思想。 1.3. Below Your Program这里有一句特别有意思的话： Just as the 26 letters of the English alphabet do not limit how much can be written, the two letters of the computer alphabet do not limit what computer can do. 程序运行的过程： 高级语言编译成汇编语言（低级语言），再汇编成机器语言，然后硬件运行 1.4. Under the Covers这一节拿 iPad2 简单介绍了一下硬件实现。在电子元件的层面，无论是屏幕显示还是计算都是 0 和 1，从元件往上开始就涉及到了前面说的抽象思想了，指令集就是对机器码实现的抽象。 1.5. Technologies for Building Processors and Memory电子元件的发展演变： 年份 电子元件技术 单位资源消耗能得到的相对性能 1951 电子管 1 1965 晶体管 35 1975 集成电路 900 1995 （Very）大规模集成电路 2,400,000 2013 （Ultra）超大规模集成电路 250,000,000,000 话说理论上来说从 1965 年一直到现在本质上用的原件都还是晶体管，只不过工艺水平一直在提升，集成度也在不断增加，摩尔定律本质上是对单块芯片里面晶体管数量增长率的估计。 来看下 CPU 这些芯片是怎么造出来的： 首先原料是从硅开始，硅是一种出色的半导体材料，它本身的导电性介于导体和绝缘体之间，但是经过某些化学处理之后，可以变成三种不同性质的结构： 用微观铜线或者铝线（microscopic copper or aluminum wire，不知道这个具体处理过程是啥）处理之后，可以变成良导体 绝缘体 可控进行导电或者不导电，起到开关的作用（晶体管） 把纯的单晶硅熔化了，从里面拉出来硅锭（silicon crystal ingot）；硅锭是圆柱形的，对其进行横向切片，得到了厚度小于 0.1 英寸的晶圆（wafers）；然后经过 2040 道工序的化学处理，就把晶圆上的不同位置变成了前面说的三种结构，要么导电、要么不导电、要么变成晶体管，这个过程也就是电路蚀刻（现在的半导体工艺已经做到了单层晶体管，28 层用绝缘体隔开的导体结构）。 电路蚀刻的过程非常复杂，由于工序太多，而且中间都是各种物理化学反应，很难做到一整个晶圆上的电路都是完美的，所以芯片制造工艺从一开始就考虑了这一点，一个晶圆上蚀刻了很多片（dies）芯片（chips）。这样虽然很难保证整块晶圆上的电路都是完美的，但是切出来的单块芯片是完美的的可能性还是很大的。后续就是对这些芯片做测试，通过测试的再封装上外围电路等等，就生产出来完整的 CPU 了。 封装这件事情上面还有个比较有趣的点。同代的某种型号的 i7、i5、i3 有可能出自同一个晶圆！！！完美的芯片做成 i7，有瑕疵然后测试挂了几个核的就封装的时候关掉几个核做成 i5，瑕疵更多挂了更多个核的就封装成 i3。 一些资料： CPU制造的那些事之一：i7和i5其实是孪生兄弟！？ CPU制造的那些事之二：Die的大小和良品率 为什么晶圆都是圆的不是方的？ 为什么”电路”要铺满整个晶圆？ 1.6. Performance两个需要区别开的概念： wall clock time、response time、elapsed time：指的都是跑一个任务实际花费的时间，包含了 I/O 啊，访存啊，操作系统 overhead 啊等等等等，相当于用秒表记出来的时间； CPU execution time、CPU time：指的是 CPU 跑这个任务消耗的时间，实际上对应的是 CPU 跑了多少个时钟周期，里面不包含其他部分消耗所花费的时间（I/O，线程切换等等其他这些）。 后面说系统性能针对的时间计量方式是前者，而 CPU 性能针对的时间计算方式是后者。 面向某个具体的优化任务时，要考虑清楚要用哪种方式来计量。优化要达到的性能指标跟性能定义是直接相关的，也需要针对具体的指标来找出性能瓶颈所在。 性能的定义： $$性能_X=\frac{1}{运行时间_X}$$ X比Y快n倍： $$\frac{性能_X}{性能_Y}=\frac{运行时间_Y}{运行时间_X}=n$$ CPU运行时间/CPU时间： $$CPU运行时间=CPU时钟周期数*时钟周期时间=\frac{CPU时钟周期数}{时钟频率（主频）}$$ CPI（时钟周期/指令）： $$CPU时钟周期数=指令数*CPI$$ $$CPU运行时间=指令数*CPI*时钟周期时间=\frac{指令数*CPI}{时钟频率}$$ 1.7. The Power Wall功耗与主频几乎是同等上升的，主频越高功耗越高。前期主频提升很快，而最近一个阶段，处理器的主频基本不再提升，因为功耗已经达到了一个相当高的程度，在散热等其他方面还没办法跟上的时候只能限制主频的提升。 动态功耗 CPU 的功耗主要是动态功耗，来源于晶体管的开关切换，即高低电平（0 和 1）之间的翻转，这中间本质上是个充放电的过程，所以这部分能量消耗是无法避免的： $$动态功耗\propto *电容性负载*电压^2*开关频率$$ 动态功耗与主频成正比，与电压的平方成正比。 静态功耗 静态功耗主要来自于晶体管的漏电流，也几乎是无法避免的，只能通过工艺改进来减少。 一些资料： 【转】功耗对处理器的限制究竟有多大？ 为什么CPU的频率止步于4G?我们触到频率天花板了吗？ 1.8. The Sea Change: The Switch from Uniprocessors to Multiprocessors受到工艺、功耗等的限制，单纯地提升主频来提高处理器的性能已经是做不到了，因此转向提高处理器的并行能力来继续发展CPU，即往一个芯片中集成更多的处理核心，而从单核开始向多核转变。 之前由于性能的提升主要在工艺、硬件层面，对于软件来说影响很小，但是多核处理器出现之后，也推动了并行算法的发展，因为只有从算法上进行改进才能够更好地利用上多核处理的优势。 1.9. Real Stuff: Benchmarking the Intel Core i7实例：Intel Core i7的基准测试 1.12. Historial Perspectives and Further ReadingPDF材料 1.13. Exercises答案 CHAPTER 2. Instructions: Language of the Computer第二章主要是以 MIPS 为例，介绍计算机指令。 2.1. Instruction2.2. Operations of the Computer Hardware设计原则1：简单有助于规整 定义一些基本的操作，再用基本的操作来组成复杂操作。 2.3. Operands of the Computer Hardware设计原则2：越少越快 为了保证每个寄存器的读取速度，寄存器数量不能太多。 2.4. Signed and Unsigned Numbers2.5. Representing Instructions in the Computer设计原则3：好的设计需要适宜的折中方案 计算机指令的设计需要考虑到指令长度等等一系列复杂的因素。 2.6. Logical Operations2.7. Instructions for Making Decisions2.8. Supporting Procedures in Computer Hardware函数和过程的实现，主要通过地址跳转和堆栈来完成。 2.9. MIPS Addressing for 32-Bit Immediates and Addresses32位立即数和地址的MIPS编址 说明 op rs rt rd shamt funct address R-format 算术/寄存器操作 6位 5位 5位 5位 5位 6位 I-format 转移/分支/立即数操作 6位 5位 5位 16位 J-format 跳转操作 6位 26位 条件转移范围：16位地址码，但是MIPS中是按4字节寻址，即实际上可以表示18位的地址18位首位为符号位，即可跳转到当前PC位置的$(-32K\sim32K-1)*4=-128K\sim(128K-4)$字节 立即数范围：与上同理，都属于I-format的指令，因此可表示的立即数范围为$-32K\sim32K-1$ 跳转指令范围：26位地址码，4字节寻址，实际可表示28位地址28位为无符号数地址，可跳转到$(0\sim2^{26}-1)*4=0\sim(256M-4)$字节 2.10. Parallelism and Instructions: Synchronization并行编程的指令中最重要的是原子操作，即多个不同进程之间的同步。 这里介绍了一下 ll（load link）和sc（store conditional）指令。 2.11. Translating and Starting a Program高级语言编译成汇编语言，再汇编成机器码的二进制对象，之后链接上一些必要的库文件（也是机器码二进制对象），最终才得到了机器码的二进制可执行文件，然后 load 到内存里运行。 2.12. A C Sort Example to Put It All Together一个C语言的排序示例 2.13. Advanced Material: Compiling C提升材料：编译C语言 PDF材料 2.14. Real Stuff: ARMv7 (32-bit) Instructions实例：ARMv7（32位）指令 2.15. Real Stuff: x86 Instructions实例：x86指令 2.16. Real Stuff: ARMv8 (64-bit) Instructions实例：ARMv8（64位）指令 2.19. Historial Perspectives and Further ReadingPDF材料 2.20. Exercises答案 CHAPTER 3. Arithmetic for Computers计算机中的算术运算 3.1. Instruction3.2. Addition and Subtraction加减的硬件实现考虑数电中的加减电路，原理还是很简单的。 3.3. Multiplication乘法的硬件实现也可以看成是用纸笔列竖式乘法方法的衍生，即移位和加法。 3.4. Division移位加比较，原理类似。 3.5. Floating Point计算机中的实数用科学计数法来表示，将一个存储空间的一部分用于表示指数，另一部分用于表示尾数。 存储空间的总大小是一定的，指数数位越多，则表示的数的范围越大，但是由于尾数数位少了，有效数位下降了；反之，指数数位越少，表示的数有效数位越多，精确度越高，但是能够表示的范围就小了。 以MIPS中常见的32位浮点数表示为例： 首位为符号位，接下来8位是带符号的指数部分，最后23位是尾数，则整个数最后表示的内容是： $$(-1)^S*F*2^E$$ 它能表示的范围在：$2*10^{-38}\sim2*10^{38}$ 64位的浮点数结构则是这样的： 表示的范围在：$2*10^{-308}\sim2*10^{308}$ IEEE754标准对零、无穷与无效数据格式作了规定： 除此之外，对浮点数有两个比较重要的规定： 忽略尾数的前导1 指数带偏阶 标准格式的10进制科学计数法中，整数位一定是个介于1~9之间的数，否则就可进一步进行标准化，把多出来的数位算到指数上去。 那么2进制的科学计数法也是一样的道理。因此尾数第1个数位一定是1，则约定俗成的，若是隐藏这个前导1，实际可以多表示一位数位。 指数的偏阶是为了避免一个看上去似乎是很大的数，因为指数是负数，而结果很小的情况。 32位浮点数中，默认将阶数加上127作为存储的指数。 这样$128_{10}=10000000_2$的指数实际上表示1；而$0_{10}=00000000_2$实际上表示-127。 则最终，实际上浮点数的表示是： $$(-1)^S*(1+F)*2^{E-Bias}$$ 3.6. Parallelism and Computer Arithmetic: Subword Parallelism处理图像和声音等等数据时，可能会有一堆类似的操作一起出现，考虑并行处理。 这里的子字并行（subword parallelism）这个词其实就是平常更常说的向量化或者 SIMD。就是一条指令对一组数据进行相同的操作。 3.7. Real Stuff: Streaming SIMD Extensions and Advanced Vector Extensions in x86实例：x86中的SIMD流扩展和高级向量扩展 3.8. Going Faster: Subword Parallelism and Matrix MultiplyDGEMM: Double precision GEneral Matrix Multiply 对比了x86架构中 scalar double 与 parallel double 两种指令集的性能。其中 parallel double 在运算时执行的是并行的浮点乘法运算（SIMD）。 优化前性能：1.7GFLOPS优化后性能：6.4GFLOPS 3.11. Historial Perspectives and Further ReadingPDF材料 3.12. Exercises答案 后接下一部分： 计算机组成与设计.硬件/软件接口 学习笔记（二）]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>体系结构</tag>
        <tag>组成与设计.硬软件接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[杂记]]></title>
    <url>%2F2015%2F09%2F07%2F2015-09-07-nothing%2F</url>
    <content type="text"><![CDATA[我听过荒芜变成热闹 听过尘埃掩埋城堡 听过天空拒绝飞鸟 没听过你 新生的军训喧闹过后，一切终于又步入正常的轨道。 一样上课，一样看书。 只是看着教室里的人越来越少，一开始是少掉十几个，然后少了一半，终于到了今天偌大的教室中只剩下十几个了。 宿舍里，两个早出晚归复习考研，只有饭后才能看到人；两个每天忙着投简历参加测试，最近还在准备去西电附近租个房子蹲点招聘会…-_-… 每个人都找到了自己的归宿，这样就很好。 回校以后生物钟调得不错，已经习惯11点躺下，尽量在11点半前睡着了。 然后也开始试了一段日子的早起跑步 你问我这是第几次试图坚持跑步了？ 嗯，好多次了。（笑） 为什么不试试更好的学校呢？ 大概是因为我懒吧。。。 很多人都说可以再试试的。 暑假到处跑的心累了，也真的是因为我懒吧。 一开始最想去的是浙大的软硬件协同实验室，结果假期偏偏惊闻实验室BOSS去世的噩耗，顿时心凉了半截 然后是想借着中山大学与CMU的联合招生人肉翻墙去往大洋彼岸。遗憾面试时被问得太惨 北大机试困扰得了别人，却难不住我。遗憾人家只给直博，不要硕士，而我又真心不想再读到Phd。后来一位师姐特地打电话过来要我再投一次信科院下面的微处理器所，那边会要直硕，遗憾面试时间上出现了一些冲突，最后还是选择去了中科大的活动。 曾经走过好多地方，也曾经在好多学校里留下过足迹。 曾经想要么去离家最近的高等学府，要么干脆滚远点，在国外奋斗个几年出人头地了再回来，然后清华园、未名湖也是从小就梦想的地方。 一开始是真的没想到，一路上走的最顺的是中科大，最后真正定下来的地方是中科大。 这里真的已经没有什么可以挑剔的了： 是自己喜欢的方向 找到了一个非常好的导师 承诺了很好的待遇 能提供很好的学习环境 学校也是科研氛围浓厚（夏令营的时候，旁边能听到一顿吃着饭聊着paper的） 离家高铁也就3小时而已 现在也已经开始补导师上次提过的缺的科目了。 虽然《Compilers.Principles,Techniques,&amp;Tools》、《Computer.Organization.and.Design》这两本书看得真的挺头疼的。 准备全身心地开始投入这个新的领域。 暑假实习的时候，跟带队去的老师聊了挺久。 老师说我是我们系这几年里面难得出的一个肯好好钻东西，有好好发展学术的潜力的学生。期望我要么硕博连读，要么读完研考博到国外去深造。 真是汗啊，老师（哭笑不得脸）我真的只是想认真读完硕士，然后赶紧出来工作，赶紧挣钱回家而已。 明天中午开会讲大四之后各种要操心的事情，周末开始给学校拼今年下半年的现场赛名额。 等一切都尘埃落定了之后，这半年就差不多结束咯。 然后下学期估计待在学校的时间会很少了。 我曾经说过我一定要离开西安。 并不是觉得这个城市不好或是怎么样，只是单纯地不想留在这里。 现在原本的愿望终于也要达成了。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>保研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ISCA 2014 会议论文整理]]></title>
    <url>%2F2015%2F08%2F31%2F2015-08-31-isca2014%2F</url>
    <content type="text"><![CDATA[即将踏入 Computer Architecture 领域，准备从今年下半年开始把这个方向相关的国际顶级会议论文都下过来看看。 从 ISCA 2014 开始： ISCA 是由 ACM 与 IEEE 共同主办的体系结构领域顶级会议，全名是 International Symposium on Computer Architecture。 然后搜到了这个..ISCA2000-2011所有论文的摘要 话说，学校买了 IEEE 的刊文版权就是好哇~数字图书馆登上去直接就能下了，怀抱这么一个大金库，真是爽啊！！ Session 1: Machines and Prototypes [238] Unifying on-chip and inter-node switching within the Anton 2 network使用Anton 2网络来统一片上和节点间切换 [195] A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services一种用于加速大型数据中心服务的可重构组织结构 [232] SCORPIO: A 36-Core Research Chip Demonstrating Snoopy Coherence Session 2A: Resilience [200] Avoiding Core’s DUE &amp; SDC via Acoustic Wave Detectors and Tailored Error Containment and Recovery [221] MemGuard: A Low Cost and Energy Efficient Design to Support and Enhance Memory System Reliability [212] GangES: Gang Error Simulation for Hardware Resiliency Evaluation [227] Real-World Design and Evaluation of Compiler-Managed GPU Redundant Multithreading Session 2B: Design Space Exploration [198] ArchRanker: A Ranking Approach to Design Space Exploration [196] Aladdin: A Pre-RTL, Power-Performance Accelerator Simulator Enabling Large Design Space Exploration of Customized Architectures [236] SynFull: Synthetic Traffic Models Capturing Cache Coherent Behaviour [218] Harnessing ISA Diversity: Design of a Heterogeneous-ISA Chip Multiprocessor Session 3A: Caches [203] The Direct-to-Data (D2D) Cache: Navigating the Cache Hierarchy with a Single Lookup [231] SC^2: A Statistical Compression Cache Scheme [204] The Dirty-Block Index [214] Going Vertical in Memory Management: Handling Multiplicity by Multi-policy Session 3B: GPUs and Parallelism [209] Fine-grain Task Aggregation and Coordination on GPUs [208] Enabling Preemptive Multiprogramming on GPUs [234] Single-Graph Multiple Flows: Energy Efficient Design Alternative for GPGPUs [215] HELIX-RC: An Architecture-Compiler Co-Design for Automatic Parallelization of Irregular Programs Session 4: Emerging Technologies [206] Efficient Digital Neurons for Large Scale Cortical Architectures [197] An Examination of the Architecture and System-level Tradeoffs of Employing Steep Slope Devices in 3D CMPs [233] STAG: Spintronic-Tape Architecture for GPGPU Cache Hierarchies Session 5A: NVRAM [222] Memory Persistency [228] Reducing Access Latency of MLC PCMs through Line Striping [216] HIOS: A Host Interface 110 Scheduler for Solid State Disks Session 5B: Datacenters and Cloud [237] Towards Energy Proportionality for Large-Scale Latency-Critical Workloads [235] SleepScale: Runtime Joint Speed Scaling and Sleep States Management for Power Efficient Data Centers [224] Optimizing Virtual Machine Consolidation Performance on NUMA Server Architecture for Cloud Workloads Session 6A: DRAM [230] Row-Buffer Decoupling: A Case for Low-Latency DRAM Microarchitecture [217] Half-DRAM: a High-bandwidth and Low-power DRAM Architecture from the Rethinking of Fine-grained Activation [210] Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors Session 6B: Circuits and Architecture [199] Architecture Implications of Pads as a Scarce Resource [220] Increasing Off-Chip Bandwidth in Multi-Core Processors with Switchable Pins [194] A Low Power and Reliable Charge Pump Design for Phase Change Memories Session 7A: Coherence and Replay [211] Fractal++: Closing the Performance Gap between Fractal and Conventional Coherence [223] OmniOrder: Directory-Based Conflict Serialization of Transactions [225] Pacifier: Record and Replay for Relaxed-Consistency Multiprocessors with Distributed Directory ProtocoL [229] Replay Debugging: Leveraging Record and Replay for Program Debugging Session 7B: Security/OOO Processors [201] The CHERI capability model: Revisiting RISC in an age of risk [202] CODOMs: Protecting Software with Code-centric Memory Domains [205] EOLE: Paving the Way for an Effective Implementation of Value Prediction [219] Improving the Energy Efficiency of Big Cores Session 8: Accelerators [213] General-Purpose Code Acceleration with Limited-Precision Analog Computation [226] Race Logic: A Hardware Acceleration for Dynamic Programming Algorithms [207] Eliminating Redundant Fragment Shader Executions on a Mobile GPU via Hardware Memoization [239] WebCore: Architectural Support for Mobile Web Browsing]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>ISCA</tag>
        <tag>IEEE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Manufactoria 一个好玩的自动机编程游戏]]></title>
    <url>%2F2015%2F08%2F28%2F2015-08-28-manufactoria%2F</url>
    <content type="text"><![CDATA[自从在知乎上发现了这么个有意思的问题： 有哪些很有「工程师情怀」的游戏？ ，就时不时上去关注一下有没有新的回答。 原帖来自 Matrix67 当年刷NOIP的时候经常上的一个博客，好久没上了-_-/// 引用下 Matrix67 的评价： 这是我所见过的程序设计类 Puzzle 游戏中最好玩的一个。它是真正意义上的程序设计游戏，游戏不但提供了完备的读写和流程控制功能，甚至还引入了随机测试数据。游戏很快就会引入算法的思想，因为玩家渐渐会发现，这些谜题并不是单靠模拟就能解决的；后面的谜题则越发困难，需要相当有技巧性的算法设计，对脑力绝对是一个大挑战。如果你热爱算法与程序设计，你一定会爱上这个游戏的。 下面上游戏~这是个写得很简单的Flash游戏，整个swf就3M大小。 然后吐槽一下：谁说静态博客不好用的&gt;_&lt;/ ！！！兼容html格式之后，音乐、视频、Flash游戏……啥都能给你插进来哈哈~ 游戏本体在此 游戏来源：http://jayisgames.com/games/manufactoria/ 点此下载该游戏的swf文件 我的游戏记录（话说不知道页面折叠这种东西能不能用html写出来…） 1.Robotoast！就是从起点到终点。 2.Robocoffee！接收条件：起点是蓝色点。 一个Branch进行判断选择即可。 3.Robolamp！接收条件：至少三个蓝色点。 这里我直接把Branch横着摆，红点则循环，蓝点通过，判断三次结束。 4.1.Robofish！接收条件：无红点。 遇到红点直接结束，蓝点循环即可。 5.1.Robobugs！接收条件：必须是红蓝点交替。 正确的路径只有 红-&gt;蓝-&gt;红-&gt;循环 或者 蓝-&gt;红-&gt;蓝-&gt;循环，其他的都结束。 6.1.Robocats！接收条件：必须以两个蓝点结束。 4.2.RC Cars！从这里开始新增了一种设备：Writer。功能是能够在纸带上打印颜色点。 输出：把第一个点放在最后，输出整个序列。 只需处理第一个点，剩下的保持不变即可。 5.2.Robocars！输出：把蓝点替换成绿点，红点替换成黄点。 6.2.Robostilts！输出：以绿点开始，黄点结束，中间是原始的输入序列。 7.1.Robobears！接收条件：起始点与结束点必须是相同颜色。 7.2.Milidogs！接收条件：蓝点作为1，红点作为0，当纸袋表示的数字是奇数时才接收。 其实就是要求最后一个点是蓝点，思路与 6.1.Robocats！ 类似。 7.3.Androids!接收条件：蓝点在前，后面要跟上相同数量的红点。 这一关真的是想了很久，受网上其他玩家的方案启发： 首先打上一个黄/绿点作为起始标记，然后对于序列中，第一个红/蓝点不打，其余打上相同颜色的点，然后循环。 简单地说就是每一轮减少一组红/蓝点，直到全部结束。 7.4.Roborockets！输出：交换红/蓝点。 跟 5.2.Robocars！ 类似，也是替换点，这里只是加个起始的绿点标记即可。 7.5.Roboplanes!输出：输出全部蓝点，忽略红点。 7.6.Robomecha!输出：把最后一个点放在最前面，输出原始序列。 与 4.2.RC Cars！ 相反。首先打上绿点标记，用于判断最后一个点，找到最后一个点以后剩下的就是原样输出了。 8.1.Soldiers！输出：蓝点作为1，红点作为0，将序列代表的数字乘以8. 乘8就是左移3位，直接在末尾打上3个0即可。 8.2.Robotanks！接收条件：蓝点作为1，红点作为0，当序列代表的数字大于15时接收。 15的二进制是1111，直接判断整个序列的数位是不是大于4即可。当然，一开始要先把先导0给消除掉。 用了最朴素的方法，因为判断太多导致摆放改了很久。 8.3.Robo-children！接收条件：整个序列中蓝点的数量与红点的数量相同，顺序不限。 本题的思路与 7.3.Androids! 完全一致，没有顺序要求，只要把之前空着的另外一边补上即可。（话说当时做 7.3.Androids! 时就是先搭的一整个，最后发现有顺序要求才把左边的删掉的…） 未完待续]]></content>
      <categories>
        <category>玩玩玩玩</category>
      </categories>
      <tags>
        <tag>自动机</tag>
        <tag>Flash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015 多校联合集训 第一场]]></title>
    <url>%2F2015%2F08%2F27%2F2015-08-27-multi1%2F</url>
    <content type="text"><![CDATA[暑假因为出去参加夏令营和各种事情，没有跟留在学校训练的小伙伴们一起打多校，想想还是挺惭愧的。 从大一进校开始，ACM已经打了快4年了，如果所有事情都弄完了，下半年可能还有机会最后收个尾…（咳咳，话说我退役贴都早写过了…） 试看了几题，发现这么长时间没有写题，思路都跟不上了。唉。 开始尝试补一下多校题吧。 HDU 5289 1002 AssignmentTime Limit: 4000/2000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionTom owns a company and he is the boss. There are n staffs which are numbered from 1 to n in this company, and every staff has a ability. Now, Tom is going to assign a special task to some staffs who were in the same group. In a group, the difference of the ability of any two staff is less than k, and their numbers are continuous. Tom want to know the number of groups like this. InputIn the first line a number T indicates the number of test cases. Then for each case the first line contain 2 numbers n, k (1&lt;=n&lt;=100000, 0&lt;k&lt;=10^9),indicate the company has n persons, k means the maximum difference between abilities of staff in a group is less than k. The second line contains n integers:a[1],a[2],…,an,indicate the i-th staff’s ability. OutputFor each test，output the number of groups. Sample Input 2 4 23 1 2 410 50 3 4 5 2 1 6 7 8 9 Sample Output 5 28 HintFirst Sample, the satisfied groups include:[1,1]、[2,2]、[3,3]、[4,4] 、[2,3] 分析真是落后了，看着题解还花了好长时间才想明白。。。 题目求的是连续的最大最小值之差不超过K的子序列的总个数。 因为要求子序列的连续性，本题可以使用单调队列的思想来解决。可以想象成一个滑动窗口从前面开始不断向后移动，需要维护的是两个指针：窗口的左端和右端。 对于窗口的右端：读入新的数据就可以看成是一个窗口右端不断右移的过程，我们需要做的就是对于每一个新加入的点，处理左端指针； 处理左端指针的过程，我们需要用到两个单调队列分别保存当前窗口的最大值和最小值：当当前窗口中的最大值与最小值的差值大于等于K的时候，就是左端指针需要右移的时候。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 5289************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;int a[100010],mi[100010],ma[100010];int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,k; scanf("%d%d",&amp;n,&amp;k); int heada=1,headi=1,taila=0,taili=0; int front=1; long long ans=0; for (int i=1;i&lt;=n;i++) &#123; scanf("%d",&amp;a[i]); while (heada&lt;=taila&amp;&amp;a[ma[taila]]&lt;=a[i]) taila--; while (headi&lt;=taili&amp;&amp;a[mi[taili]]&gt;=a[i]) taili--; taila++; ma[taila]=i; taili++; mi[taili]=i; while (a[ma[heada]]-a[mi[headi]]&gt;=k) &#123; if (front==ma[heada]) heada++; if (front==mi[headi]) headi++; front++; &#125; ans+=i-front+1; &#125; printf("%lld\n",ans); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>Multi-University-Training-Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Treap]]></title>
    <url>%2F2015%2F08%2F23%2F2015-08-23-treap%2F</url>
    <content type="text"><![CDATA[据说平衡树界最经典的三大树是 SBT、Splay、Treap 从一道笔试题开始说起上午帮舍友助攻阿里笔试的时候，遇上这么一道题： 输入是一些数对&lt;a,b&gt;，要求把它们用树结构存储下来，树的一个结点里面要同时保存a、b两个值，然后保证a是二叉搜索树性质，b是堆的性质。这样的树是唯一的。 我心想…平衡树+堆…Tree+Heap…这不就是Treap嘛!!! 然而几大平衡树里面我学了SBT、Splay，没学Treap哇，当时就心凉了半截。 好在仔细看了看题，感觉实现起来不太难，于是试了下。 以下是上面这棵树的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148/* ***********************************************MYID : Chen FanLANG : G++PROG : tree************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct treenod&#123; int left,right,a,b;&#125; treenode;treenode tree[10000];int father[10000];int treetail,root;void rleft(int now)&#123; int fa=father[now],ffa=father[fa]; father[now]=ffa; father[fa]=now; tree[fa].right=tree[now].left; tree[now].left=fa; if (!ffa) &#123; father[now]=0; root=now; &#125; else &#123; father[now]=ffa; if (fa==tree[ffa].left) &#123; tree[ffa].left=now; &#125; else &#123; tree[ffa].right=now; &#125; &#125;&#125;void rright(int now)&#123; int fa=father[now],ffa=father[fa]; father[now]=ffa; father[fa]=now; tree[fa].left=tree[now].right; tree[now].right=fa; if (!ffa) &#123; father[now]=0; root=now; &#125; else &#123; father[now]=ffa; if (fa==tree[ffa].left) &#123; tree[ffa].left=now; &#125; else &#123; tree[ffa].right=now; &#125; &#125;&#125;void check(int now)&#123; while(father[now]&amp;&amp;tree[now].b&gt;tree[father[now]].b) &#123; if (now==tree[father[now]].left) rright(now); else rleft(now); &#125;&#125;void add(int r,int a,int b)&#123; if (!treetail) &#123; treetail++; tree[1].a=a; tree[1].b=b; &#125; else &#123; if (a&lt;tree[r].a) &#123; if (!tree[r].left) &#123; treetail++; tree[r].left=treetail; father[treetail]=r; tree[treetail].a=a; tree[treetail].b=b; check(treetail); &#125; else add(tree[r].left,a,b); &#125; else &#123; if (!tree[r].right) &#123; treetail++; tree[r].right=treetail; father[treetail]=r; tree[treetail].a=a; tree[treetail].b=b; check(treetail); &#125; else add(tree[r].right,a,b); &#125; &#125;&#125;void outp(int now)&#123; printf("&lt;%d,%d&gt;",tree[now].a,tree[now].b); if (tree[now].left) outp(tree[now].left); if (tree[now].right) outp(tree[now].right);&#125;int main()&#123; int a,b; //init memset(tree,0,sizeof(tree)); memset(father,0,sizeof(father)); treetail=0; root=1; //insert while(scanf("%d%d",&amp;a,&amp;b)==2) &#123; add(root,a,b); &#125; //output prefix printf("%d %d\n",tree[root].a,tree[root].b); outp(root); return 0;&#125; 首先按照二叉搜索树的标准插入数据，然后进行堆结构调整。 正常的堆是满二叉树，直接使用子节点2倍和2倍+1的标号来维护，调整时是上下交换。而为了保证二叉搜索树的性质不被破坏，这里的调整需要用到子节点与父节点的左右旋来实现。 代码还是比较简单的。 然后既然今天开了这个头，下面顺手学习一下真正的Treap是怎么实现的。 Treap 真正的Treap其实跟上面那个结构已经非常像了，主体是二叉搜索树，然后在一般的结点之外另外加一个优先级的域，并且使用堆来维护优先级。 为什么要加个堆？我们知道平衡树的出现是为了解决二叉搜索树可能不平衡的问题，因此平衡树越稳定（越平衡），那么进行操作时的渐进复杂度越接近$O(logn)$ Treap通过堆来限定了整棵平衡树的结构： 对于一棵二叉搜索树来说，给定的输入顺序不同，最终生成得到的可能是不同的树结构，而加上堆限定之后，整棵树的形态就固定了。 也就是说： 对于相同的一组数据，输入顺序并不改变Treap的最终形态；数据相同，则Treap的最终形态是唯一的 那么另外一个问题来了，正常情况下我们只会得到a序列，b是哪里来的呢？ 答案是随机数！ 输入数据时，我们对于每一个a都给它生成一个随机数b，然后按照堆的性质去维护b。给a序列加了个b序列之后，剩下的部分就跟上面的笔试题一样了，实现起来也比较简单。 数学上可以证明，随机数+堆的限定可以保证Treap达到$O(logn)$的期望深度。 后话 网上找到的一些资料上说Treap可以完成Splay的所有操作，我还没找到详细的说明。暂时还没有想明白。 如果只是按照上面那种限定死了的树结构的话，完全不明白这货是怎么可能达到像Splay那样灵活的。 上面的代码是上午临时打的，本来想好好改进下。 … 然后，网上搜了下别人的代码 … 找到一些版本的Treap是没有左右旋的！！ 顿时就又吓到了！！ 一个之前听说过，然而一直被吓到的新名词出现了： 可持久化数据结构！ 可持久化Treap！ 没有左右旋，而是使用切分和合并来完成整个结构的维护。 大概只有这种水平才能达到Splay的灵活性吧。 于是瞬间就不想改原来那个左右旋的代码了。待日后有兴趣了，再把可持久化这部分补上吧。]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>平衡树</tag>
        <tag>Treap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络流整理]]></title>
    <url>%2F2015%2F08%2F22%2F2015-08-22-networkflow%2F</url>
    <content type="text"><![CDATA[今天来整理一下网络流的相关内容。 首先贴一个博客：网络流(Network Flow) 关于网络流的介绍，这里都讲得比较详细了。 然后是模板题：HDU 3549 Flow ProblemTime Limit: 5000/5000 MS (Java/Others) Memory Limit: 65535/32768 K (Java/Others) Problem DescriptionNetwork flow is a well-known difficult problem for ACMers. Given a graph, your task is to find out the maximum flow for the weighted directed graph. InputThe first line of input contains an integer T, denoting the number of test cases. For each test case, the first line contains two integers N and M, denoting the number of vertexes and edges in the graph. (2 &lt;= N &lt;= 15, 0 &lt;= M &lt;= 1000) Next M lines, each line contains three integers X, Y and C, there is an edge from X to Y and the capacity of it is C. (1 &lt;= X, Y &lt;= N, 1 &lt;= C &lt;= 1000) OutputFor each test cases, you should output the maximum flow from source 1 to sink N. Sample Input 2 3 21 2 12 3 13 31 2 12 3 11 3 1 Sample Output Case 1: 1Case 2: 2 算法模板最基础的Edmonds-Karp（EK）算法每一次运行都是从源点到汇点，广搜找一遍增广路。如果能够找到，则反向推回去把流加上去，直到找不到任何其他增广路为止，则结束。 复杂度是$O(V*E^2)$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126/* ***********************************************MYID : Chen FanLANG : G++PROG : 3549_EK************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;int start[100],num[100];typedef struct nod &#123; int x,y;&#125; node;node edge[3000];bool op(node a,node b)&#123; if (a.x==b.x) return a.y&lt;b.y; else return a.x&lt;b.x;&#125;int c[100][100];int tail,front[100],minn,next;bool find(int n)&#123; queue&lt;int&gt; q; bool flag[100]; memset(flag,0,sizeof(flag)); int outp[100]; q.push(1); front[1]=0; outp[1]=2147483647; flag[1]=true; while(!q.empty()) &#123; int now=q.front(); q.pop(); for (int i=0;i&lt;num[now];i++) &#123; int next=edge[start[now]+i].y; if (!flag[next]&amp;&amp;c[now][next]&gt;0) &#123; flag[next]=true; q.push(next); front[next]=now; outp[next]=min(outp[now],c[now][next]); if (next==n) &#123; minn=outp[next]; return true; &#125; &#125; &#125; &#125; return false;&#125;int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,m; scanf("%d%d",&amp;n,&amp;m); memset(c,0,sizeof(c)); tail=0; for (int i=1;i&lt;=m;i++) &#123; int x,y,z; scanf("%d%d%d",&amp;x,&amp;y,&amp;z); if (!(c[x][y]||c[y][x])) &#123; tail++; edge[tail].x=x; edge[tail].y=y; tail++; edge[tail].x=y; edge[tail].y=x; &#125; c[x][y]+=z; &#125; memset(num,0,sizeof(num)); int o=0; sort(&amp;edge[1],&amp;edge[1+tail],op); for (int i=1;i&lt;=tail;i++) &#123; if (o!=edge[i].x) &#123; o=edge[i].x; start[o]=i; &#125; num[o]++; &#125; int ans=0; while (find(n)) &#123; int i=n,j=front[n]; while(j) &#123; c[j][i]-=minn; c[i][j]+=minn; i=j; j=front[i]; &#125; ans+=minn; &#125; printf("Case %d: %d\n",tt,ans); &#125; return 0;&#125; ISAP算法引用一下 网络流ISAP算法的简单介绍（zz） 众所周知，在网络流的世界里，存在2类截然不同的求解思想，就是比较著名的预流推进与增广路，两者都需要反向边的小技巧。 其中预流推进的算法思想是以边为单元进行推流操作。具体流程如下:置初始点邻接边满流并用一次反向bfs对每个结点计算反向距离标号，定义除汇点外存量大于出量的结点为活动结点，每次对活动结点按允许边（u-&gt;v:d[u]=d[v]+1）进行推流操作，直到无法推流或者该点存量为0，若u点此时仍为活动结点，则进行重标号，使之等于原图中进行推操作后的邻接结点的最小标号+1，并将u点入队。当队列为空时，算法结束，只有s点和t点存量非0，网络中各顶点无存量，无法找到增广路继续增广，则t点存量为最大流。 而增广路的思想在于每次从源点搜索出一条前往汇点的增广路，并改变路上的边权，直到无法再进行增广，此时汇点的增广量即为最大流。两者最后的理论基础依然是增广路定理，而在理论复杂度上预流推进要显得比较优秀。其中的HLPP高标预流推进的理论复杂度已经达到了另人发指的$O（\sqrt{m}*n^2）$，但是其编程复杂度也是同样的令人发指 于是我们能否在编程复杂度和算法复杂度上找到一个平衡呢，答案是肯定的。我们使用增广路的思想，而且必须进行优化。因为原始的增广路算法（例如EK）是非常悲剧的。于是有人注意到了预流推进中的标号法，在增广路算法中引入允许弧概念，每次反搜残留网络得到结点标号，在正向增广中利用递归进行连续增广，于是产生了基于分层图的Dinic算法。一些人更不满足于常规Dinic所带来的提升，进而加入了多路分流增广的概念，即对同一顶点的流量，分多路同时推进，再加上比较复杂的手工递归，使得Dinic已经满足大部分题目的需要。 然而这样做就是增广路算法优化的极限么？答案永远是不。人们在Dinic中只类比了预流推进的标号技术，而重标号操作却没有发挥得淋漓尽致。于是人们在Dinic的基础上重新引入了重标号的概念，使得算法无须在每次增广后再进行BFS每个顶点进行距离标号，这种主动标号技术使得修正后算法的速度有了不少提高。但这点提高是不足称道的，人们又发现当某个标号的值没有对应的顶点后，即增广路被截断了，于是算法便可以提前结束，这种启发式的优化称为Gap优化。最后人们结合了连续增广，分层图，多路增广，Gap优化，主动标号等穷凶极恶的优化，更甚者在此之上狂搞个手动递归，于是产生了增广路算法的高效算法–ISAP算法。 虽然ISAP算法的理论复杂度仍然不可超越高标预流推进，但其编程复杂度已经简化到发指，如此优化，加上不逊于Dinic的速率（在效率上手工Dinic有时甚至不如递归ISAP），我们没有不选择它的理由。 ISAP是将SAP的预处理广搜得到标号优化进了整个过程中，代码更简洁，而效率也是更快了。 以下还是上面这题的ISAP解法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/* ***********************************************MYID : Chen FanLANG : G++PROG : 3549_ISAP************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;int start[100],num[100];typedef struct nod &#123; int x,y;&#125; node;node edge[3000];bool op(node a,node b)&#123; if (a.x==b.x) return a.y&lt;b.y; else return a.x&lt;b.x;&#125;int c[100][100];int tail,front[100],minn,next,gap[100],h[100],n;int dfs(int now,int flow)&#123; if (now==n) &#123; return flow; &#125; int minh=n-1,lv=flow,d; for (int j=0;j&lt;num[now];j++) &#123; int next=edge[start[now]+j].y; if (c[now][next]&gt;0) &#123; if (h[next]+1==h[now]) &#123; d=min(lv,c[now][next]); d=dfs(next,d); c[now][next]-=d; c[next][now]+=d; lv-=d; if (h[1]&gt;=n) return flow-lv; if (lv==0) break; &#125; if (minh&gt;h[next]) minh=h[next]; &#125; &#125; if (lv==flow) &#123; gap[h[now]]--; if (!gap[h[now]]) h[1]=n; h[now]=minh+1; gap[h[now]]++; &#125; return flow-lv;&#125;int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int m; scanf("%d%d",&amp;n,&amp;m); memset(c,0,sizeof(c)); tail=0; for (int i=1;i&lt;=m;i++) &#123; int x,y,z; scanf("%d%d%d",&amp;x,&amp;y,&amp;z); if (!(c[x][y]||c[y][x])) &#123; tail++; edge[tail].x=x; edge[tail].y=y; tail++; edge[tail].x=y; edge[tail].y=x; &#125; c[x][y]+=z; &#125; memset(num,0,sizeof(num)); int o=0; sort(&amp;edge[1],&amp;edge[1+tail],op); for (int i=1;i&lt;=tail;i++) &#123; if (o!=edge[i].x) &#123; o=edge[i].x; start[o]=i; &#125; num[o]++; &#125; int ans=0; memset(gap,0,sizeof(gap)); memset(h,0,sizeof(h)); gap[1]=n; while (h[1]&lt;n) &#123; ans+=dfs(1,2147483647); &#125; printf("Case %d: %d\n",tt,ans); &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>网络流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BIOS内核载入的方式（引导实验）]]></title>
    <url>%2F2015%2F08%2F19%2F2015-08-19-biosinit%2F</url>
    <content type="text"><![CDATA[之前花了点时间稍微研究了下系统内核载入，重新翻出来整理一下： 内核载入之前的工作 1981年IBM PC刚推出时，系统只带有640K的的RAM，内存寻址范围最大也只有1M。现如今基本的32位机都能寻址到4G，经过CPU的新特性更是可以支持到64G的物理内存。但是为了与原来的PC想兼容，系统1M一下物理内存使用分配上仍然保持与原来一致。ROM BIOS都在物理内存能寻址的最高端位置处，所以在大于1M内存的新计算机上，ROM BIOS最后会被映射到1M末尾的 Shadow 区域中。 计算机上电初始化时，物理内存被设置成从地址0开始的连续区域。除了地址从0xA0000~0xFFFFF（640K~1M）存放显存和ROM BIOS和0xFFFE0000~0xFFFFFFFF（4G的最后64K）存放ROM BIOS以外，其他部分都可以用作主要内存。 后续内核载入之后会重新分配，以后再研究。 x86体系的PC启动之后由BIOS完成自检，然后开始搜索外存上的引导信息，查找设备进行系统启动。BIOS会把硬盘等设备的头512字节数据复制到内存，检查这512字节是不是以16进制的0x55和0xaa结尾，这是启动引导程序的标记。 用汇编写一个简易的引导程序 下面是一个简单的程序： 123456789.global start.code16.textstart: movw $0xb800,%ax movw %ax,%ds movb $0x41,(0) movb $0x1f,(1)hlt .global start 这一句把start标签做全局符号导出，后面链接的时候会用到。 把代码保存为boot.s文件。然后用gas的as.exe和ld.exe两个程序真正把这段汇编代码处理成机器码 123as -o boot.o boot.sld -Ttext 0x200 -s -o boot --entry start boot.o -Ttext 0x200 的作用是让输出的代码设置为从boot输出文件的0x200位置开头，否则我们就很难确定我们得到的boot文件从哪开始才是代码，一会写入的时候也从这里写就行。 ld 命令里的 --entry start 这段就是我们之前的汇编文件中需要写 .global start 这句声明的原因。指定入口。 在保护模式启动之前，微机处于实地址模式，这个时候需要先设定数据段段寄存器的地址，然后就可以用一个16位的地址来访问内存开头的1M空间了。 .code16 这一句声明是为了让gas了解到这段程序是为实模式编写的，且限定为16位的汇编编译。 首先把数据段寄存器设置成0xb800，这样，地址0就会指向PC的CPU中用来显示屏幕的一段内存的开头。在这个CPU的最初的阶段，一般屏幕有80行，25列，而当我们把数据段寄存器赋予值0xb800，地址0所指向的字节就代表第一行第一个字符，这里是ascii码中的大写字母A，即0x41。地址1现在指向的字节代表了第一行第一个字符的字体颜色和背景颜色，格式与windows命令行的color命令一样。 详细如下： 123456789101112131415161718颜色设置颜色属性由两个十六进制数字指定 -- 第一个为背景，第二个则为前景。每个数字可以为以下任何值之一:0 = 黑色1 = 蓝色2 = 绿色3 = 浅绿色4 = 红色5 = 紫色6 = 黄色7 = 白色8 = 灰色9 = 淡蓝色A = 淡绿色B = 淡浅绿色C = 淡红色D = 淡紫色E = 淡黄色F = 亮白色 如果把上面这段程序作为一个引导写进硬盘，则之后理论上将能够看到运行结果是屏幕上出现一个蓝底白字的A。 准备写入存储设备，了解一下BIOS启动的流程，主要是MBR 完成编译之后需要把这块写入硬盘/U盘等存储设备。 刚开始我是拿U盘来试的，结果差点把我的U盘给毁了……汗……后来想到在VMware虚拟机里面直接新建一块硬盘来做。 此时这块新建的磁盘里面什么都没有，但是用winhex打开磁盘1，看到的是这样的： winhex是一个16进制的编辑器，可以直接用来查看文件或者读取硬盘，免费版只能查看不能修改，不过这个对我们来说已经够了，因为之后的写硬盘操作我们要用C语言直接完成。 不管硬盘上设定的一扇区有多大，第一个扇区的引导都是固定512B，可以看到前512B的最后两个字节是 55 AA，即引导结束标志。然后开始部分是一些初始程序和找不到操作系统的提示信息。 这些东西都是在创建硬盘的时候里面默认写着的，如果对此块硬盘进行引导的话，将返回这些错误信息。 下面解释一下MBR的相关知识： MBR全称是Master Boot Record，硬盘主引导记录，它存在于硬盘的第一个物理扇区，也就是0面0磁道1扇区。 软盘上的引导扇区的作用是把软盘上存放的真正的操作系统内核读到内存中，然后跳到内核去执行。而MBR的作用却是找到硬盘上的活动分区，然后把活动分区上的引导记录读到内存中，然后再跳到此引导记录中去执行，后面的过程就与从软盘启动计算机一致了。 这里，可以把硬盘上的活动分区都看成一是个软盘。我们知道一个硬盘可以分成多个分区，也就是说一个硬盘可以视为由多个软盘组成，每一个分区都可以有自己的引导记录，这就如每个软盘上都有自己的引导记录一样。那么哪一个分区（软盘）用来真正引导计算机呢？这就由MBR来完成，MBR查看每个分区记录，当找到一个活动分区时，就把此活动分区的引导记录载入。 显然，必须有一个地方来存放硬盘的分区记录，这就是所谓的硬盘分区表。这个硬盘分区表也在MBR中，不过MBR只有一个扇区大小，既要有用来查找硬盘分区表的代码，还要包含硬盘分区表，所以这个硬盘分区表不可能很大。实际上，这个表只有四个表项，也就是说在一块硬盘里，最多有4个分区能用来引导电脑。 再来完整的看看MBR载入的流程，计算机启动的时候，MBR先把自己搬移到0x0600处，随后MBR开始查找内部的硬盘分区表表项，之后找到的引导程序会被BIOS载入到内存的0x7C00处，然后BIOS跳转到0x7C00处执行。 每个硬盘分区表表项的第一个字节只能是0x80或者0x00。如果是0×80表示这是一个活动分区，然后MBR再从这个表项中读出此分区在硬盘上的位置，将此分区的第一个扇区（含有此分区的引导记录）读入0x7C00处，然后跳到0x7C00处执行。如果表项的第一个字节是0x00，则查找下一个表项，直到四个表项都找完为止。如果此第一个字节既不是0x80又不是0x00，则打印一条错误信息。 MBR的整个结构，总长度上面说过了，一共512字节： |地址|说明|长度（Byte）|-||0x000（0）|MBR中的执行代码|440|0x1B8（440）|磁盘商标（可选）|6|0X1BE（446）|分区表（共4项，每项16字节）|4*16=64|0X1FE（510）|引导记录标志（55AA）|2 分区表表项结构如图所示，总长度一共16字节： |地址偏移量|说明|长度（Byte）|-||0x0|状态码，0x80表示为活动分区，0x00表示为非活动分区，其他值无效|1|0x1|分区的第一个扇区的CHS地址（磁头、柱面、扇区）|3|0x4|分区类型（FAT、NTFS、ext2、ext3等）|1|0x5|分区的最后一个扇区的CHS地址|3|0x8|分区的第一个扇区的LBA地址|4|0xC|分区的长度（多少个扇区）|4 所以写入前440个长度即可。后面的保持不变。 ……之前拿U盘试的时候直接改了前面512B的所有内容，导致后来电脑直接读不出来U盘的东西了。因为分区表整个被破坏了。 把引导写入硬盘 写硬盘用C语言来完成： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;windows.h&gt;int main()&#123; FILE *fp = NULL; DWORD length; HANDLE handle = NULL; byte buff[512],buf[512]; int i; //读取硬盘，并把原始的引导信息保存下来 handle = CreateFile(TEXT("\\\\.\\PHYSICALDRIVE1"),GENERIC_READ,0,NULL,OPEN_EXISTING,0,0); if (handle == INVALID_HANDLE_VALUE) &#123; printf("Cannot open the Device~!\n"); return -1; &#125; memset(buff,0,sizeof(buff)); ReadFile(handle,buff,sizeof(buff),&amp;length,NULL); CloseHandle(handle); for (i=0;i&lt;length;i++) &#123; if (i%16==0) printf("\n"); printf("%02X ",buff[i]); &#125; printf("\n"); //打开之前写好的引导文件 fp = fopen("boot","rb"); if (fp == NULL) &#123; printf("Cannot open the File~!\n"); return -1; &#125; memset(buf,0,sizeof(buf)); fseek(fp,0x200,SEEK_SET); //之前编译的时候指定的文件位置在这里用上 fread(buf,sizeof(buf),1,fp); fclose(fp); for (i=440;i&lt;512;i++) buf[i]=buff[i]; //保留原有的引导的440以后的内容 //把整理好的引导信息写进硬盘 handle = CreateFile(TEXT("\\\\.\\PHYSICALDRIVE1"),GENERIC_WRITE,0,NULL,OPEN_EXISTING,0,0); if (handle == INVALID_HANDLE_VALUE) &#123; printf("Cannot open the Device~!\n"); return -1; &#125; if (!WriteFile(handle,buf,sizeof(buf),&amp;length,NULL)) &#123; printf("Writing Error~!"); return -1; &#125; else printf("Writing Success~!"); CloseHandle(handle); return 0;&#125; 写入完成之后，重启虚拟机，把硬盘1的启动项调到硬盘0前面去，然后启动！ BIOS闪了一下之后，果然左上角出现了预先设计的A，至此引导程序实验成功。然后重新恢复启动项正常进去之后硬盘1也没有出什么问题。 最后…这篇文章给了我很大的帮助]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>内核</tag>
        <tag>BIOS</tag>
        <tag>引导</tag>
        <tag>mbr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理]]></title>
    <url>%2F2015%2F08%2F16%2F2015-08-16-compilers%2F</url>
    <content type="text"><![CDATA[缺的东西只能自己补上，开始补编译原理。 刚买了的龙书还在路上，先从 Coursera 上找个公开课看着。 介绍用来执行程序的工具一般分为两种：编译器（Compilers）和解释器（Interpreters） 编译器是将程序代码首先编译生成可执行文件，然后将数据输入可执行文件得到输出结果，也称为离线执行方式 解释器是将程序和数据输入解释器中，直接得到输出结果，过程是相当于程序代码代码指导解释器来完成对输入数据的处理，也称为在线执行方式 常把编译的过程分成5个阶段： Lexical Analysis 词法分析 Parsing 语法分析 Semantic Analysis 语义分析 Optimization 优化 Code Generation 代码生成 龙书终于到手咯~Hoho~ 在github上发现个整理得挺全的龙书答案，顺手fork下来做了个gitbook 当时写这篇的时候是挖了个坑，没想到过了快4个月才回来补坑，有空把Coursera公开课的内容在这里补上，现在先集中精力看龙书了。 龙书的笔记另开一篇好了：编译原理 龙书]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
        <tag>公开课</tag>
        <tag>StandFord</tag>
        <tag>Coursera</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MX-BOARD 3.0 红轴]]></title>
    <url>%2F2015%2F08%2F16%2F2015-08-16-cherry%2F</url>
    <content type="text"><![CDATA[人生中的第一块原厂红轴 0.0 默默地入坑了]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>机械键盘</tag>
        <tag>cherry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中山卡内基in广州]]></title>
    <url>%2F2015%2F07%2F27%2F2015-07-27-sysucmu%2F</url>
    <content type="text"><![CDATA[多图预警！！！ 千万不要用流量打开本帖！！！ 0.0 26号到的广州，27号报到。 其实想想科大已经非常让我满意了，找到了自己最喜欢的方向，也找到了国内这个方向上最强的导师。想说要不算了吧？直接买票回家吧？ 这边是一路顺，对方许诺的福利待遇都是最好的；另一边还要花心思去准备全英面试，拿不到全奖剩下的就是高额的学费。 李小莹童鞋说不管怎么选，最后总会后悔没有选择另一条。 我想我已经决定好了去中科大了。 接下来几天随便过吧~^.^ 初见广州这个城市，带给我的印象确实要比合肥好很多，这才是发达的大城市的样子 中山大学整体偏文，校园文化和环境也都非常不错，大概是我见过的这么多学校里面最好看的校园了。（厦大也不错，，，） 然后是学校里面的环境： 自行车棚。。。棚？ Day One 上午，开营仪式，参观JIE大楼以及广州超算中心 由于是这几年新建的楼，环境真心好到爆~ 开营仪式： 实验室环境： 之前在中科大已经见过超算中心了，来到这里主要能够让人觉得兴奋的是天河二号！好歹是目前为止世界上运算速度最快的东西了，对于一个热爱计算机的人来说，有机会看到本体还是很让人激动的 可惜一路参观都只是隔着一层玻璃，立面应该是重重防护的，据说有武警把守 更让人失望的是广州超算中心只是设备在这里而已，维护等工作大部分还是由其研发机构——国防科大来做的，而中山大学的学生基本是不可能有机会接触到超算的（想想在中科大参观的时候，演示的老师甚至把运算结点拆下来跟我们介绍里面的结构。。。-_-///差距啊） 下午，参观顺德JRI研究院 随手拍了张自己感觉很奇怪的照片~~&gt;_&lt; 作为JIE的附属研究机构，也招生一批中山大学单学位的硕士生，可以说是完全为了产业而培养人才 环境自然也是好的不得了，宿舍还是单人间啊啊啊 下面是个微波暗室： Day Two 上午，介绍CMU学习情况以及学业体系 整体跟我原来了解到的一致，JIE的学生完全等同于CMU的正式学生，学习方式、课程安排等都是纯美式的 下午，各位导师分别介绍各自实验室以及研究方向 。。。然而，并不感兴趣啊（话说我本来就准备好水掉明天的面试了的） Day Three 面试 JIE的面试相当吓人： 一个房间里坐着10多个老师，每人1分钟自我介绍，剩下9分钟由老师提问，每人定时只给10分钟，到时间会叫停，全英文面试。 英语方面我感觉没什么问题，关键9分钟提问阶段只问专业课，比赛、项目什么的经历直接被他们略过去了。然而一个外国教授随便从我成绩单里面随便挑了门课，，，挑中了高频，我去。。。 你要问我算法、数据结构，甚至信号系统、傅里叶变换啥的我都能扯啊，考我高频就悲剧了。 问我三极管在高频电路中的作用。 这玩意怎么答啊？基极调制，增益放大？然后下一个问题是为什么三极管在增益放大中是必不可缺的。 。。。好吧，我承认又被好久之前的吴老大说中了。 基础没学好！！ 主要是平时就记个放大电路，记个振荡电路，没有深入考虑过里面的原理。 哎哎，水完了就好。中大卡内基再见！ 早上好： 最后留几张吃的，午餐~~ 早餐： 放假了半个多月，终于要踏上回家的路了 也是在前几天惊闻浙大软硬件协同实验室的BOSS陈天洲教授去世，顿时觉得世界真是太残酷了。 曾经在去中科大之前我最想去的就是浙大的这个实验室，现在这样对我来说就不再需要思考其他的了，9月份也不用再给自己找麻烦了，中科大将是我唯一的选择！]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>Gallery</tag>
        <tag>保研</tag>
        <tag>SYSU</tag>
        <tag>CMU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中科大in合肥]]></title>
    <url>%2F2015%2F07%2F22%2F2015-07-22-ustc%2F</url>
    <content type="text"><![CDATA[多图预警！！！ 千万不要用流量打开本帖！！！ 0.0 7月19日到达USTC中科大，开始为期5天的夏令营。 拍了一堆照片记录下这几天的历程。。。 Hoho~~ 发的全部材料在这里啦： 营服、便利贴、笔什么的……右边那个神秘的绿色小袋子打开以后是个背包！！！ 校园卡，接下来几天吃饭就靠这个啦： 在学校里面随便逛了逛，下面是各种： 校外十字路口： 校内食堂： 吃的东西还不错哦： Day One 上午，开营仪式： 整个学校各个学院的夏令营成员都在这里。大约有一千多吧。。。中科大真是大手笔。。。 科大本科生与研究生的比例大约是 1:1.9 左右，算是个完完全全的研究性大学了。 研究内容包含了各种前言的科技：物理、化学、生物、计算机……各种 校史馆： 这位走来的是我们的志愿者廖东亮大大~~ 科大校训里面都有“又红又专”几个字，刚听到的时候真是满满的汗呐……不过想想也是，中科大头顶就是中科院，中央最核心的研究精神一脉相承，必然是得“红”啦…… 好处就是很多项目都是直接跟中科院那边搭上的，科研经费、设备这方面也都能够拿到很多资源。 校史馆里面的各种实物藏品： 看，卫星！ 下午，参观先研院以及科大讯飞 路上顺手留一张： 先研院大楼： 先进技术研究院，算是科大的一个校区吧，比较尖端的一些技术方向会在这边进行研究。也有很多是跟各个企业进行联系合作的实验室。 话说这一片是新建的，科研环境和生活环境那是没的说~~就是能不能来这里还得看导师的项目情况 先研院X楼： 整个楼据说从上方往下看就是一个大X哦 龙芯什么的，各种尖端研究平台。 科大讯飞： 科大讯飞就是讯飞输入法、讯飞语音啥的。。。 核心科技是语音识别起家。 而搞好语音识别之后，剩下再发展就是类似科幻片里面那种人工智能的系统了，然而他们目前做出来的东西，确实已经让我觉得有点科幻的感觉了。这也是他们正在研究的“超脑”系统。 科大负责新技术的研发，讯飞集团这边负责把技术转化为产品。 果然几天之后的导师面上，这个方向的导师真是被学生抢破门槛啊。。。0.0 Day Two 上午计算机学院开营仪式以及各实验室宣讲。 各个实验室的大牛导师纷纷使出浑身解数各种拉人，介绍各个实验室出过的论文、国家项目、资金什么的。 中科大计算机学院一共有10个左右的实验室，每个都各有特色，研究方向各有长处。 然而来的时候的目标就是安虹教授了，听了介绍下感觉还不错。目标更加坚定了。 下午西区实验室参观，主要就是科技楼这块。 在14楼又听了一次安教授的介绍。 先进系统结构研究所。 主要方向是计算机系统结构设计，超级计算机、并行运算优化方面的工作。 安虹教授本人也是曾经参与龙芯好几代设计的核心计算机科学家。 感觉能跟她的不少教学理念都能契合上。 Day Three 上午，东区实验室、超算中心参观 超算中心大楼： 下面就是各种超级计算机机群，整个机房里面都是各种风扇的轰鸣声，不过据说最大的一台不在这里放着 对计算机的兴趣就是觉得人类能够制造出瞬时计算速度如此庞大的设备来，特别是像超级计算机，代表了人类工业史上当前水平运算能力的极限！！ 一般的超算机组其实也就是一堆服务器连在一起，主要需要处理的是超多核心机群的并行协同方面的问题。 曙光I号： 由于我的目标已经基本上确定了，所以这边的实验室都没有具体再去逛了，也是听完各个导师的介绍就返回了宾馆。 下午，冷餐会： 主要还是提供一个学生与导师之间相互交流的环境，活动啊、游戏啊什么的 Besides……好多吃的啊，科大真心大手笔~赞一个！！！ Rio和蛋挞： 好多KFC蛋挞哇： 我是不会告诉你活动结束的时候我往书包里面塞了4盒蛋挞，然后抓了一大瓶没开过的可乐带回去了的。。。0~0 Day Four 面试啦啦啦啦~！！！！ 导师们都比较好，聊的挺开心。 刚开始背好了自我介绍，准备一进去啥都不说直接先背，结果人家一上来第一句话“这个姓怎么读？”……顿时就…… 面试也是挺顺利地就过了 下午也找到之前看好的安虹教授，坐聊了一下午，也是终于把接受意向谈妥了（有小伙伴觉得在她那边聊的时间太久，都没空去另外的实验室面试了，然而我的目标本来就这一个，聊的很开心） 至此，顺利通过学院面试，也顺利得到了导师的接收承诺，本次科大之旅终于可以说是圆满完成了！ 同房间的西交童鞋也顺利拿到了导师的offer！ Day Five 除了吃饭之外。。。今天算是在宾馆待了一整天吧，昨天谈妥了导师之后，剩下的就是邮件跟导师更多点交流了。 感觉上还是挺顺利，对这个结果也是挺满意的。科大的计算机体系结构在国内也是顶尖了，尤其还有超级计算机这样的硬件设备支持，另外也有很多机会接触龙芯、中科院计算所这些项目。 原本是考虑国内的学习比较心仪浙大的软硬件协同实验室，经过这次考察学习经历之后，中科大已经是我最满意的选择了！！ 然而又想只试这么几个就结束了，会不会太没追求了…… 要说总结的话，还是要自信吧。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>Gallery</tag>
        <tag>保研</tag>
        <tag>USTC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Developing Your Musicianship 学习笔记（未完成）]]></title>
    <url>%2F2015%2F07%2F21%2F2015-07-21-music%2F</url>
    <content type="text"><![CDATA[去年在Coursera上学过一段时间的基础乐理，可惜后来还是没完成…重新整理一下 MOOC课程地址 本门课的授课讲师是George Russell，来自伯克利音乐学院。 这门课旨在介绍简单的乐理知识，对于我这种音乐白痴来说简直是太适合了，而且看评论作业比较少，一周交一次，还是很方便滴。。。 这门课之前已经开过一次了，所以视频和字幕早都有人整理过了： 百度网盘 第一周 视频目录： •1-1 Berklee Faculty/Student Spotlight: Why Are Harmony and Ear Training Important? (4:11) •1-2 Welcome to the Course (4:18) •1-3 Important Definitions (4:01) •1-4 Structure of the Major Scale (4:23) •1-5 Sharps and Flats (1:33) •1-6 Major 2nd and 3rd Intervals (4:18) •1-7 Major 2nd and Major 3rd Interval Practice (1:47) •1-8 Tonal Center (7:51) •1-9 Lesson Review and Assignment Overview (2:50) •1-10 Berklee Student Performance: Boston Marathon, “I Don’t Have a Song for That” (5:41) 1-1 是对课程的整体介绍 1-2 差不多还是介绍，闲扯，然后展现一下老师的钢琴功力 1-3 乐理的基本定义。真正的课程从这里开始 和弦 chords；音阶 scales；旋律 melodies； 和声 Harmony: The study of chords, scales and melodies； 练耳 Ear Training: Indentifying what your ear hears； 音程 Interval: The distance between two notes；从基调开始，按照大调音阶数，大调音阶下节课有讲。 1-4 大调音阶 全音 whole steps；半音 half steps； 半音是键盘上相邻两个键的音，不分黑白键，两个半音组合成一个全音。所以一个大调音阶里面是全半音结合的，因为钢琴键盘上不是所有的白键中间都有黑键。 1-5 升降调 降半音 flat；升半音 sharp； 一个音升降半个音。 举例： D音，降半音为 D flat（或者其实就是 C sharp），升半音为 D sharp（E flat）。 F音，这个跟其他的比有点特殊的是降半音是E，因为E，F之间没有黑键，升半音为 F sharp（G flat）。 1-6 大二调和大三调音程 1-7 大二调和大三调音程练习 耳残哭晕在厕所…… 1-8 调性中心 调性中心 tonal center: The tonic or “Do” of the scale, or scale degree 1；听一首曲子中听到其中的主音就是调性中心 和弦建立在那个键的基础上…（还是觉得好抽象…） the key!!! 后面的学生练习演示部分根本听不出出来有什么区别好么…….. 作业怎么办……..各种汗…… 1-9 布置作业了 •Write in your own words the definitions of harmony, ear training, and interval. •Find three recordings of songs in the key of C and post links to performances of them that you find on YouTube, Vimeo, Dailymotion, or a similar public video site. •Write the C major scale on the staff by hand. due on Wed 30 Jul 5:00 am 然后就 see you next week 了，要滚去写作业了 1-10 伯克利音乐学院学生的表演 第二周 已经迫不及待要开始第二周的学习了~ 2-1 What’s Your Story 2-2 复习了一下上周所学的内容，其实也没多少 2-3 这节主要要讲纯四度和纯五度 Perfect 4th：纯四度。大调音阶中距离为四的音 可以用婚礼进行曲最熟悉的一段来记，就是“当当当当，当当当当”，前一段是纯四度，应该是，Do Fa Fa Fa Perfect 5th：纯五度。大调音阶中距离为五的音 记忆方法是《小星星》，Do Do So So La La So 2-4 大二度、大三度、纯四度、纯五度综合练习 视频里面有一些小练习 不过如果要更熟练掌握还需要再多加练习，Ear Traing还是很有必要的，能确定好音程的话，知道是哪一个大调那就可以听着曲子把谱扒下来了。 2-5 三和弦 Triads：三和弦，由大调音阶的三个级组成 Major Triads：大三和弦，由每个调的根音第三音和第五音组成 也就是一个大三度，加上，一个小三度 C大三和弦：C-E-G，C调中的I级和弦 F大三和弦：F-A-C，C调中的IV级和弦 G大三和弦：G-B-D，C调中的V级和弦 Minor Traids：小三和弦，把大三和弦中间的音降半音 也就是一个小三度，加上，一个大三度 C小三和弦：C-bE（#D）-G F大三和弦：F-bA（#G）-C Major 和弦听上去更明亮点，Minor要灰暗一些…… 耳残表示这个对我来说真心太难了，真心听不出来，后面的练习全错…… 2-6 三级和弦的进程 很多乐曲都由这些基本的和弦进程组成的 首先从第一个开始，I-&gt;IV-&gt;V-&gt;I 这里，I级和弦是主和弦，IV级和弦是下属和弦，V级和弦是属和弦 主和弦是基础，调性中心，下属和弦比属和弦低一个音级，这里的属和弦（IV级）由于导音的影响，又回到I级和弦的趋势 然后：I-&gt;V-&gt;IV-&gt;V-&gt;I 第三个：IV-&gt;V-&gt;IV-&gt;I 第四个：V-&gt;IV-&gt;V-&gt;I 2-7 课程总结+作业 作业也是比较简单的，而且主要以练习练耳为主。电脑上可以装everyone piano这个软件，配合一个比较好点的音源+一个完整的电脑键盘就能拿来练习钢琴了。 •Practice the major triads on the keyboard. •Practice the C major scale on the keyboard. •Write C, F, and G major triads on staff paper. 2-8 然后又是一个amazing的学生作品，，，棒极~ see you next week~ 第三周 Week Three Start! 3-1 一些音乐学习建议 说的很好 3-2 复习 开始新一周的学习之前，还是老样子，首先复习一下之前学过的内容 开场的一段弹奏是 Wade in the water，用以向我们介绍 Minor Pentatonic Scale 小调五音阶 大调音阶、三和弦、调性中心、I VI V级和弦 3-3 大六度和大七度音程 Major 6th 大六度、Major 7th 大七度 大七度可能是最难的一个音程 顺利完成后面的几个练习 各种音程还是需要多听，多练耳才能熟练掌握 3-4 小调五音阶 小调五音阶由五个音阶组成（大汗……什么鬼东西） 还是从C大调开始，五个分别是 C-Eb-F-G-Bb-C，最后回到C 或者 1-b3-4-5-b7-1 为了记忆，教了一首小歌（算歌吧……-_-////） 后面是一段 Big Performance~~！！！！！ 简直帅爆~！！！！！ 3-5 复习和作业 怎么感觉好少的样纸…第二周和第三周都是… anyway，复习完之后确实就结束了 除了练习之外，应该还是找曲子……不过为什么是附加作业，这个看样子不太好找呀 •Practice the minor pentatonic scale on the keyboard. Make sure to play up and down the scale. Download these play along tracks to practice the minor pentatonic scale with: Play Along 1 MP3 (To download Right-click (PC) or Control-click (MAC) and choose “Save Link As” from the menu) Play Along 2 MP3 •Find a song that features a Major 6th interval in the melody. •For bonus points, find a song that features a Major 7th interval in the melody. 3-6 amazing 的表演 See you next week~！ 话说8月2号的时候，第一周的作业结果出了，拿了45/50，有一个概念理解上出现了小问题。其他同学在评价的时候给出了解释，练耳跟绝对音感是不一样的 第四周 不知不觉居然已经到了第四周了….0.0 4-1 Tips，可惜咱以后并不打算走音乐路… 4-2 老样子，复习 小调五音阶 可能是美国音乐史上用的最多的音阶 4-3 七和弦 C Major 7th Chords：C大调的大七和弦 C大三和弦+第七个音，即：C - E - G - B 一种快速找到第七音的方法是，根音降半调，再升上去一个八度 F大七和弦：F - A - C - E 这俩和弦确实都挺好听，只用这两个就能弹出比较好听的曲子了 属七和弦：在大七和弦的基础上，七音降半调 C属七和弦：C - E - G - Bb 4-4 七和弦和蓝调音乐 G属七和弦：G - A - B - D - F 属七和弦中的： I级：C7：C - E - G - Bb IV级：F7：F - A - C - Eb V级：G7：G - A - C - E 蓝调音乐就是由这些和弦组成的 小调五音阶+属七和弦写蓝调即兴： 很炫酷的方法0.0： 用小调五声音阶在开始的两个小节写一段简单的旋律，然后两小节休止，到第五小节重复前面的旋律。七八休止，九十稍微变化旋律，十一再休止。 和弦部分只要用到三级属七和弦即可： 4-5 又到了我最怕的练习时间了 …… 老师说：大七和弦给人一种轻松愉悦的感觉，而属七和弦的效果带有一丝张力 在我看来，属七和弦听上去没有大七和弦那么纯粹，属七和弦末尾有种颤音之类的感觉（大概就是他说的紧张感？），所以还是可以区分出来滴 4-6 作业时间 •Practice the C and F Major 7th chords. •Practice the C, F, and G Dominant 7th Chords. •Practice the blues progression by playing the C, F, and G Dominant 7th chords. The blues progression is on page 4 of your study guide. •Find a song that features a blues progression. Have a nice day~! 第五周 5-1 音乐职业介绍 5-2 复习+新的大七和弦顺序介绍 演奏同一种和弦的另一种方式，但是效果会不同 5-3 曲式 Form: The overall structure of a piece of music. AABA顺序 拍子的问题： 上面的数字表示每小节有几拍 下面的数字表示以哪一个长度的音符作为一拍 之前的蓝调也算是一种曲式 ……..-_-////后面弹的这什么玩意…… 5-4 4/4拍子和4/3拍子 5-5 基本和弦谱 这是一个标准12小节的蓝调和弦谱： 最后一小节可以用C7或者G7 5-6 复习+作业 断了Oh…后来学到这里就断掉了]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>公开课</tag>
        <tag>Coursera</tag>
        <tag>乐理</tag>
        <tag>Berklee</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Next]]></title>
    <url>%2F2015%2F07%2F15%2F2015-07-15-next%2F</url>
    <content type="text"><![CDATA[Just as the 26 letters of the English alphabet do not limit how much can be written, the two letters of the computer alphabet do not limit how what computers can do. 几天前听说王小豪童鞋已经顺利拿到北大化学直博的offer，简直不能再赞啦~ 转眼今天已经是7月15了，18号晚踏上征程！！ 西安-合肥-广州-回家 希望走完这一路，8月初回到家时，所有的一切都能达成~！]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>保研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Makefile解析]]></title>
    <url>%2F2015%2F07%2F13%2F2015-07-13-makefile%2F</url>
    <content type="text"><![CDATA[之前啃内核的时候就见过这个东西，整理项目的时候正好用到，顺便好好学习一下。 详见：Makefile经典教程(掌握这些足够) Windows下的用法Windows下标准的Makefile应该是vs的nmake，当电脑上没有vs时，可以找一下gcc的目录。 一般情况下，Windows下gcc目录中会有个make.exe或者mingw32-make.exe，Dev-C和Codeblocks下的目录中都有。 如果是make.exe，按我的习惯gcc目录一般都是已经添加到系统环境里面去了的，就可以直接用了。 是mingw32-make.exe的话，我一般会把这个程序直接拷到需要运行的目录下，然后改名成make.exe，这样就能正常使用make命令了。 需要注意的是，这个make.exe虽然能在Windows下运行，不过里面似乎还是按照Linux的习惯写的，我在Makefile里面写cmd上的命令貌似有些是运行不了的，比如写个del *.exe会显示： 1/usr/bin/bash: del: command not found]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>内核</tag>
        <tag>Makefile</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Uzebox]]></title>
    <url>%2F2015%2F07%2F12%2F2015-07-12-uzebox%2F</url>
    <content type="text"><![CDATA[大二开始几个小伙伴一起跟杨俊做了个单片机的小东西 最后感觉也就是水过了 这几天准备重新把资料理出来看看，没准面试可以用一下。 Orz 基本原理: 从核心部分开始： UZEBOX主机 UZEBOX的Wiki页面 具体电路图如上所示，用到的芯片只有两个，单片机和后面的AD转换芯片，剩下的都是一些简单的电路。 硬件部分CPUCPU采用了ATmega644，8位的AVR单片机，主要功能是完成所有任务。 生成视频同步信号 画面的呈现。这包括滚动和确定精度的透明度来抑制背景 混合并且输出音乐和声音效果 读操纵杆按钮状态 读取和处理通过UART的MIDI数据流 处理对SD卡的读写操作 最后也是最重要的一步：运行游戏 这块芯片的标准主频是20MHz，但是使用时需要超频到28.6MHz。 因为AD725的工作频率是14.3MHz，为了防止视频信号采样发生混叠，MCU和AD725的信号需要同步发生。 视频信号如上图所示，UZEBOX的视频部分主要靠单片机的C口进行输出，以3/3/2的比例向外输出R/G/B视频信号 红和绿都是用3位表示，蓝色信号2位，经过一个R-2R权电阻网络进行简单的DA转换，可以把三种颜色的数字信号转成0~0.7V的模拟信号，然后继续输入AD725，转换成NTSC格式的视频信号 AD725的主要输入信号是前面来的RGB，然后再加上单片机的同步信号和时钟信号辅助，就能完成视频信号的转换了 声音信号声音输出只用到了PD7一个口，是单声道信号进行脉冲宽度调制（PWM）产生的。用电阻把输出信号幅值控制在1V的峰峰值即可 手柄这里用的是一个简单的SNES（Super Nintendo Entertainment System）的手柄。就是一个触发器和一个移位寄存器，单片机读取移位寄存器里面存储的状态即可。 软件部分内核内核就是烧录进单片机里面的主要内容。单片机上没有操作系统，因此内核里面的所有部分事实上只能顺序执行，但是因为执行速度足够快，可以表现得像多任务同时工作一样进行。单片机需要同时完成多项任务：产生视频信号、读取手柄的控制信息、播放音乐等等。 主要工作内容是： 初始化端口，计时器和其他硬件外围设备重置 生成复合视频AD725所需的同步脉冲 解码音乐数据，产生声音效果 混合四种声音样本 在规律的间隔时间从混合缓冲器中输出声音样本 对控制器按钮和鼠标移动的读取 读取UART中的入栈数据并将其存储到缓冲区中 中断内核使用定时器1，即16位的计数器来触发中断。 计时器开始为0，计数到1820，此时它会自动回转到0，并且生成一个中断。 1820的值是由主晶振频率（28.63636MHZ）除以NTSC扫描线率（15.73426KHZ）得到的。 内核中断负责暂停主程序，进行渲染视频和混合音乐。由于速度和时间要求，需要用汇编语言来写。 视频引擎内核提供12种不同的工作模式，每种都有不同的特点，用于实现视频绘制 声音引擎音乐处理涉及到音乐乐谱的读取和波封的处理，这里使用的是MIDI这种紧凑格式的音频，纯软件完成 游戏载入从github上下到完整的包是这样的 12345demos //各种运行工具以及游戏包gfx //颜色以及图像资源等kernel //内核，包括初始化、运行主程序、中断、视频/音频引擎等tools //编译工具Makefile //编译文件 执行make之后，会在Rom目录下生成一些.hex和.uze的文件 .uze文件是原本生成了.hex文件之后加上首部打包而成 载入的过程可以有两种： 直接将游戏包生成好的.hex文件烧录进Atmega644即可 往单片机中烧录gameloader的.hex文件，之后通过读取SD卡中的.uze文件来载入游戏。gameloader占内存4K，剩下还有60K的内存可供游戏调用 改进部分 游戏机制作完成之后，我们对其进行了两方面的改进 无线传输参考Tetsuo Kogawa的网站 将视频信号通过一个简易的无线电路发射出去，可以通过一般的CRT电视来接收 该电路非常简化，制作该电路的时候也是使用双面覆铜板完成 所有暴露在外面的导线都是尽可能地贴近覆铜板，使高频信号尽可能地衰减掉 选择一个电视频道，并调节可调电感来使得两个设备相互适配 当发射机与电视机的距离、频率都调整到一个合适的位置时候，电视机上可以清晰地收到显示的游戏画面 无线控制通过三轴加速度传感器来对感知空间三维姿态，并通过无线的方式将其传递到另一台主机上，主机进一步向外输出控制信号 这一部分可以代替原始设计中的手柄部分]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>单片机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的七月]]></title>
    <url>%2F2015%2F07%2F09%2F2015-07-09-qiyue%2F</url>
    <content type="text"><![CDATA[这是一年中最火热的日子， 我将完成这个很大程度上决定我下半生人生走向的选择 骨子里面还是个小人物，我一向是个不愿对自己倾注太多信心的人 一是自己确实拿不出什么值得骄傲的成绩 二是即使在自己擅长的领域内，走得越深，见到过的真正的青年科学家（！！）越多，便越发感到敬畏 拿什么跟人家比？ 前辈告诉我说要多投，海投，要勇敢地上！ 还是怪自己大概也是心气太高吧，一般的看不上，，，拿到太强的目标的邀请函了，却又觉得没有信心了 毕竟 CMU、USTC、PKU 哪一个都曾经是我仰望不已的存在。。。。。。 就像是向高空拉了几条钢丝锁，如果顺利，一步登天，如果摔下来，那就真是一点后路都没有留下了。 幸而投出去的夏令营已经3中2，而剩下的最后一个也有很大的希望能过。 尽管最想去的那一个告诉我的是高达80%的淘汰率，呵呵，， 趁着这段时间把以前学过的专业课好好复习下，然后给自己补补英语！ 无论最终结果会如何，已经无畏地出发在路上了，那是我的星辰大海！！]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>保研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 1402 A * B Problem Plus 快速高精度乘法（FFT）]]></title>
    <url>%2F2015%2F07%2F06%2F2015-07-06-HDU-1402%2F</url>
    <content type="text"><![CDATA[A * B Problem PlusTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/32768 K (Java/Others) Problem DescriptionCalculate A * B. InputEach line will contain two integers A and B. Process to end of file. Note: the length of each integer will not exceed 50000. OutputFor each case, output A * B in one line. Sample Input 1 210002 Sample Output 2 2000 题意做两个超大数的乘法，每个数极限是50000位，略大，一般普通的模拟高精会超时 分析普通的模拟高精是$O(n^2)$的复杂度，对于这种特殊的50000长的数据自然是会超了的 故本题的解法就是借用FFT加速的高精度乘法，复杂度在$O(nlogn)$ FFT的详细分析在上一篇中已经讲清楚了，本题即作为模板题 我的模板是在原来自己搞的高精的基础上改的，最初的高精用了4位押位，然而套上这个之后，发现FFT中间的结果大小跟位数也有关，当位数大到一定程度的时候，4位押位在int下会爆。。。然后50000位的极限数据在3位押位下都会爆int。。。好可怕 最后改到2位押位。 大概是我原本的输入输出部分模板写的不好，网上找的参考代码能在100ms以内过，我的这个需要花700ms左右。。。（捂脸） 下次用的时候考虑是不是需要套原本的高精模板，还有要注意押位的位数问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196/* ***********************************************MYID : Chen FanLANG : G++PROG : FFT_HDU_1402************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;#define nn 70000#define snn 60000using namespace std;struct gjtype&#123; int a[nn]; void clean() &#123; memset(a,0,sizeof(a)); &#125; void ntog(long long s) &#123; clean(); int i=0; while (s&gt;0) &#123; i++; a[i]=s%100; s=s/100; &#125; a[0]=i; &#125; void stog(char s1[]) &#123; clean(); char ss[snn],s[snn]; bool pos=true; if (s1[0]==&apos;-&apos;) &#123; strcpy(s,s1+1); pos=false; &#125; else strcpy(s,s1); int i=0; while (strlen(s)&gt;=2) &#123; i++; for (int j=strlen(s)-2;j&lt;strlen(s);j++) ss[j-strlen(s)+2]=s[j]; s[strlen(s)-2]=&apos;\0&apos;; a[i]=atoi(ss); &#125; if (strlen(s)&gt;0) &#123; i++; a[i]=atoi(s); &#125; a[0]=i; if (!pos) a[a[0]]=-a[a[0]]; &#125; void out() &#123; printf(&quot;%d&quot;,a[a[0]]); for (int i=a[0]-1;i&gt;=1;i--) printf(&quot;%02d&quot;,a[i]); putchar(&apos;\n&apos;); &#125;&#125;;gjtype a,b,c;const double PI = acos(-1.0);struct fstype&#123; double x,y; fstype(double real = 0.0,double imag = 0.0) &#123; x = real; y = imag; &#125; fstype operator -(const fstype &amp;b)const &#123; return fstype(x-b.x,y-b.y); &#125; fstype operator +(const fstype &amp;b)const &#123; return fstype(x+b.x,y+b.y); &#125; fstype operator *(const fstype &amp;b)const &#123; return fstype(x*b.x-y*b.y,x*b.y+y*b.x); &#125;&#125;;void fft(fstype y[],int len,int on)&#123; int i,j,k; for(i = 1, j = len/2;i &lt;len-1;i++) &#123; if (i &lt; j) swap(y[i],y[j]); k = len/2; while(j &gt;= k) &#123; j -= k; k /= 2; &#125; if(j &lt; k) j += k; &#125; for(int h = 2; h &lt;= len; h &lt;&lt;= 1) &#123; fstype wn(cos(-on*2*PI/h),sin(-on*2*PI/h)); for(int j = 0;j &lt; len;j+=h) &#123; fstype w(1,0); for(int k = j;k &lt; j+h/2;k++) &#123; fstype u = y[k]; fstype t = w*y[k+h/2]; y[k] = u+t; y[k+h/2] = u-t; w = w*wn; &#125; &#125; &#125; if(on == -1) for(int i = 0;i &lt; len;i++) y[i].x /= len;&#125;fstype x1[nn],x2[nn];gjtype cc;gjtype fftcheng(gjtype aa,gjtype bb)&#123; int len1=aa.a[0],len2=bb.a[0],len=1; while (len&lt;len1*2||len&lt;len2*2) len&lt;&lt;=1; for (int i=1;i&lt;=len1;i++) x1[i-1]=fstype(aa.a[i],0); for (int i=len1;i&lt;len;i++) x1[i]=fstype(0,0); for (int i=1;i&lt;=len2;i++) x2[i-1]=fstype(bb.a[i],0); for (int i=len2;i&lt;len;i++) x2[i]=fstype(0,0); fft(x1,len,1); fft(x2,len,1); for (int i=0;i&lt;len;i++) x1[i]=x1[i]*x2[i]; fft(x1,len,-1); cc.clean(); for (int i=0;i&lt;len;i++) cc.a[i+1]=(int)(x1[i].x+0.5); for (int i=1;i&lt;=len;i++) &#123; cc.a[i+1]+=cc.a[i]/100; cc.a[i]%=100; &#125; while(cc.a[len] == 0 &amp;&amp; len &gt; 1)len--; cc.a[0]=len; return cc;&#125;int main()&#123; //freopen(&quot;1.txt&quot;,&quot;r&quot;,stdin); //freopen(&quot;1.out&quot;,&quot;w&quot;,stdout); gjtype a,b,c; char s1[snn],s2[snn]; while (scanf(&quot;%s%s&quot;,s1,s2)!=EOF) &#123; a.stog(s1); b.stog(s2); c=fftcheng(a,b); c.out(); //c=cheng(a,b); //c.out(); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>FFT</tag>
        <tag>高精度算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速傅里叶变换FFT算法的用途]]></title>
    <url>%2F2015%2F07%2F05%2F2015-07-05-fft%2F</url>
    <content type="text"><![CDATA[正好复习信号到FFT了，想起来前段时间放了挺久的FFT大数乘法，顺便研究下。 普通的高精度乘法的复杂度需要$O(n^2)$，用了押位之后，一般押4位的话能够达到$O(n^2/16)$，看上去可能效果不错，但仍然是同一个数量级的。 对付更大一点的变态数据（。。。ACM赛场上很少有卡这货的吧。。。-_-////）就需要用的$O(nlogn)$的FFT大数乘法算法了。 数学基础 首先基础是DFT，FFT是DFT的一种快速算法。 从N点DFT出发： $$\begin{align}X[K]&amp;=\sum_{n=0}^{N-1}x[n]e^{-j\frac{2\pi K}{N}n}\&amp;=\sum_{n=0}^{\frac{N}{2}-1}x[2n]e^{-j\frac{2\pi K}{N}2n}+\sum_{n=0}^{\frac{N}{2}-1}x[2n+1]e^{-j\frac{2\pi K}{N}(2n+1)}\&amp;=\sum_{n=0}^{\frac{N}{2}-1}x_1[n]e^{-j\frac{2\pi K}{\frac{N}{2}}n}+\sum_{n=0}^{\frac{N}{2}-1}x_2[n]e^{-j\frac{2\pi K}{\frac{N}{2}}n}e^{-j\frac{2\pi K}{N}}\&amp;=X_1[K]+e^{-j\frac{2\pi K}{N}}X_2[K]\end{align}$$ 也就是说，通过上述变换，我们成功地将一个N点得DFT拆成了2个$\frac{N}{2}$点的DFT 分析一下复杂度： 首先N点DFT的结果$X[K]$有N点，每个点的计算都要对$x[n]$的n个序列值乘上负指数求和，复杂度是$O(n^2)$ 如果每一步都采用上面的那种变换方式，那么可以把多出来的一维降掉，变成$O(nlogn)$，这也就是快速傅里叶变换（FFT）了！ 更进一步然而上面这个式子简化到最后是有问题的。 我们把一个N点的DFT拆成$\frac{N}{2}$点的DFT之后，下面得到的两个序列$X_1[K]$和$X_2[K]$都只有$\frac{N}{2}$点。 也就是说直接这么写，当K大于$\frac{N}{2}-1$之后，放在代码里面，后面部分就会出现数组越界的问题！！ 继续分析后面部分： $$\begin{align}X_1[K]&amp;=\sum_{n=0}^{\frac{N}{2}-1}x_1[n]e^{-j\frac{2\pi K}{\frac{N}{2}}n}\X_1[\frac{N}{2}+K]&amp;=\sum_{n=0}^{\frac{N}{2}-1}x_1[n]e^{-j\frac{2\pi}{\frac{N}{2}}(\frac{N}{2}+K)n}\&amp;=\sum_{n=0}^{\frac{N}{2}-1}x_1[n]e^{-j\frac{2\pi K}{\frac{N}{2}}n}e^{-j2\pi n}\&amp;=-\sum_{n=0}^{\frac{N}{2}-1}x_1[n]e^{-j\frac{2\pi K}{\frac{N}{2}}n}\&amp;=-X_1[K]\end{align}$$ 好，这样问题就解决了。 令： $$W_N^K=e^{-j\frac{2\pi K}{N}}$$ 则重新整理上面的运算单元可以得到： 这就是著名的蝶形运算了。 然后整个图画出来是这样的： 8点FFT 或者： 16点FFT FFT的用途综上，FFT就是专门为计算机快速运算而设计的，1点变2点，2点变4点，4点变8点……这样就能在原始数据（原数组）的基础上不断计算下一层的结果，最终完成整个DFT的快速计算。 写代码的话，第一步是处理一下最左边一排原始数据的摆放顺序，是按照二进制逆序排过来的，然后一层一层求和迭代即可。 多项式 讲完FFT之后，下面重点来了。 多项式的系数表示法关于变量$x$的多项式可以表示成： $$A(x)=a_0+a_1x+a_2x^2+…+a_{n-1}x^{n-1}=\sum_{j=0}^{n-1}a_jx^j$$ 多项式是个广泛的概念，如果把大数对应到多项式上面去那就是类似这样： $$1234=4+3*10+2*10^2+1*10^3$$ 若有： $$B(x)=\sum_{j=0}^{n-1}b_jx^j$$ 则： $$A(x)+B(x)\Rightarrow C(x)=\sum_{j=0}^{n-1}c_jx^j$$ $$c_j=a_j+b_j$$ $$A(x)\cdot B(x)\Rightarrow C(x)=\sum_{j=0}^{2n-2}c_jx^j$$ $$c_j=\sum_{k=0}^{j}a_kb_{j-k}$$ 分别是多项式加法和多项式乘法 注意：多项式乘法的过程就是多项式系数卷积 多项式的点值表示法令： $$y_k=A(x_k)$$ 则一个次数界为n的多项式$A(x)$的点值表示就是n个这样的点值（任意n个）对所形成的集合： $$\{(x_0,y_0),(x_1,y_1),…,(x_{n-1},y_{n-1})\}$$ 从系数表示法写成点值表示法就是简单地说找n个不同的$x$代进去求出$y$，然后写在一起就行了。 然而用点值确定系数就是个线性求解的过程，称为插值，也是可以算的。 那么点值表示法有什么好处呢？ 设上面的点值表示是$A(x)$的，如果已知$B(x)$的是： $$\{(x_0,y’_0),(x_1,y’_1),…,(x_{n-1},y’_{n-1})\}$$ 那么： $$A(x)+B(x)\Rightarrow C(x)=\{(x_0,y_0+y’_0),(x_1,y_1+y’_1),…,(x_{n-1},y_{n-1}+y’_{n-1})\}$$ $$A(x)\cdot B(x)\Rightarrow C(x)=\{(x_0,y_0y’_0),(x_1,y_1y’_1),…,(x_{n-1},y_{n-1}y’_{n-1})\}$$ 分别是多项式加法和多项式乘法 关键在于，这里的多项式乘法的复杂度是$O(n)$！！！！！ 因此只要能够想办法快速地进行两种表示法之间的转换，那么就可以先从系数表示法转换成点值表示法，用$O(n)$相乘之后，再快速转换回去，即可做到快速的多项式相乘 所以！！！！ 转换的关键就是！！！！ FFT！！！！ 以下是多项式乘法的过程： 次数界增加一倍：通过加入n个值为0的高阶系数，将整个次数界扩充至$2n$ 求值，两次FFT分别求出$A(x)$和$B(x)$的长度为$2n$的点值表示$A(\omega)$和$B(\omega)$ 逐个相乘得到$C(\omega)$ 再做一次FFT，求逆傅里叶变换得到$C(x)$ 实现代码详见模板题：HDU 1402 A * B Problem Plus 快速高精度乘法（FFT）]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>复习笔记</tag>
        <tag>FFT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信号相关专业复习]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01-xinhaoxiangguan%2F</url>
    <content type="text"><![CDATA[信号系列是通信工程的重点内容，以复变函数与积分变换作为基础的数学课程，主要包含信号与系统、升级版数字信号处理、然后是以概率论与数理统计为数学基础课的信息论作为通信方面数学上的理论基础、最后得到通信原理、应用方面有移动通信。 复变函数与积分变换 欧拉公式： 二维平面中点的坐标：$(r\cos\theta,r\sin\theta)$ $$x=r\cos\theta$$ $$y=r\sin\theta$$ $$z=r(\cos\theta+i\sin\theta)=re^{i\theta}$$ 傅里叶变换 $$\mathscr{F}[f(t)]=F(w)=\int_{-\infty}^{+\infty}f(t)e^{-jwt}dt$$ $$\mathscr{F}^{-1}[F(w)]=f(t)=\frac{1}{2\pi}\int_{-\infty}^{+\infty}F(w)e^{jwt}dw$$ 冲激函数$\delta(t)$ 定义： $$\delta(t)=\begin{cases}\infty,&amp;t=0\\0,&amp;t\neq0\end{cases}$$ $$\int_{-\infty}^{+\infty}\delta(t)dt=1$$ 性质： $$\int_{-\infty}^{+\infty}\delta(t-t_0)f(t)dt=f(t_0)$$ $$\mathscr{F}[\delta(t)]=1$$ $$\mathscr{F}[1]=2\pi\delta(w)$$ 卷积积分 $$f_1(t)*f_2(t)=\int_{-\infty}^{+\infty}f_1(\tau)f_2(t-\tau)d\tau$$ $$\mathscr{F}[f_1(t)*f_2(t)]=F_1(w)F_2(w)$$ $$\mathscr{F}[f_1(t)f_2(t)]=\frac{1}{2\pi}F_1(w)*F_2(w)$$ 帕塞瓦尔等式 又作能量积分公式。 $$\int_{-\infty}^{+\infty}[f(t)]^2dt=\frac{1}{2\pi}|F(w)|^2dw$$ 拉普拉斯变换 $$\mathscr{L}[f(t)]=F(s)=\int_0^{+\infty}f(t)e^{-st}dt$$ $$\mathscr{L}^{-1}[F(s)]=f(t)$$ Z变换 $$\mathscr{Z}[f(n)]=F(z)=\sum_{i=0}^nf(n)z^{-n}$$ 信号与系统 阶跃函数$\epsilon(t)$ 定义： $$\epsilon(t)=\begin{cases}0,&amp;t&lt;0\\frac{1}{2},&amp;t=0\1,&amp;t&gt;0\end{cases}$$ $$\delta(t)=\frac{d\epsilon(t)}{dt}$$ $$\epsilon(t)=\int_{-\infty}^{t}\delta(x)dx$$ 系统的数学模型：描述连续系统的数学模型是微分方程，描述离散系统的数学模型是差分方程。 一个既具有分解特性、又具有零状态线性和零输入线性的系统称为线性系统。 如果系统的参数都是常数，不随时间变化，则称该系统为时不变系统。 线性时不变系统（LTI系统）用常系数微分方程和差分方程来描述。 微分： $$y^{(n)}(t)+a_{n-1}y^{(n-1)}(t)+…+a_1y^{(1)}(t)+a_0y(t)=b_mf^{(m)}(t)+b_{m-1}f^{(m-1)}(t)+…+b_1f^{(1)}(t)+b_0f(t)$$ 或写为： $$\sum_{j=0}^na_jy^{(j)}(t)=\sum_{i=0}^mb_if^{(i)}(t)$$ 差分： $$y(k)+a_{n-1}y(k-1)+…+a_0y(k-n)=b_mf(k)+b_{m-1}f(k-1)+…+b_0f(k-m)$$ 或写为： $$\sum_{j=0}^na_{n-j}y(k-j)=\sum_{i=0}^mb_{m-i}f(k-i)$$ 微分方程的齐次解和特解 |||函数形式|系数||-||齐次解|自由响应、瞬态响应|系统本身|激励|特解|强迫响应、稳态响应|激励|激励 零输入响应：激励为零时，仅有系统初始状态引起的响应 零输入条件下，微分方程右端为零： $$\sum_{j=0}^na_jy_{zi}^{(j)}(t)=0$$ 零状态响应：系统初始状态为零时，仅由输入信号引起的响应 全响应：初始状态不为零时LTI系统的响应 $$\overbrace{y(t)}^{全响应}=\rlap{\overbrace{\phantom{\sum_{j=1}^nC_{zij}e^{\lambda jt}}}^{零输入响应}}\underbrace{\sum_{j=1}^nC_{zij}e^{\lambda jt}+\rlap{\overbrace{\phantom{\sum_{j=1}^nC_{zsj}e^{\lambda jt}+y_p(t)}}^{零状态响应}}\sum_{j=1}^nC_{zsj}e^{\lambda jt}}_{自由响应}+\underbrace{y_p(t)}_{强迫响应}$$ 冲激响应：是激励为单位冲激函数$\delta(t)$时，系统的零状态响应。 $$h(t)=T[{0},\delta(t)]$$ 激励信号$\delta(t)$的作用是在$t=0$的瞬间给系统输入了若干能量，储存在系统中，而在$t&gt;0$时系统的激励为零，只有冲激引入的那些储能在起作用，因而系统的冲激响应应由上述储能唯一地确定，因此系统的冲激响应在$t&gt;0$时与该系统的零输入响应具有相同的函数形式。 阶跃响应：是激励为单位阶跃函数$\epsilon(t)$时，系统的零状态响应。 $$g(t)=T[{0},\epsilon(t)]$$ $$h(t)=\frac{dg(t)}{dt}$$ $$g(t)=\int_{-\infty}^th(x)dx$$ 卷积积分 由于LTI系统的线性性质，可将输入信号分解为一系列的冲激函数之和（或积分），利用冲激函数对LTI系统的冲激响应，求解LTI系统任意激励的零状态响应。 $$f(t-t_1)*\delta(t-t_2)=f(t-t_2)*\delta(t-t_1)=f(t-t1-t2)$$ $$f_1(t-t_1)*f_2(t-t_2)=f_1(t-t_2)*f_2(t-t_1)=f(t-t1-t2)$$ $$f^{(i)}(t)=f_1^{(j)}(t)*f_2^{(i-j)}(t)$$ 卷积的物理意义 从数学上来说卷积就是定义两个函数的一种乘法。 对离散序列来说就是两个多项式的乘法。 物理意义就是冲激响应的线性叠加，所谓冲激响应可以看作是两个函数，另一个函数按冲激信号正交展开。 详见知乎：卷积的物理意义是什么？ 信号的分解 在信号空间中可以找到若干个相互正交的信号作为基本信号，使得信号空间中任一信号均可表示成它们的线性组合。 周期信号的傅里叶级数 对周期信号进行分解： $$f(t)=\frac{1}{2}\sum_{n=-\infty}^{\infty}A_ne^{j\psi_n}e^{jn\Omega t}=\sum_{n=-\infty}^{\infty}F_ne^{jn\Omega t}$$ 傅里叶系数： $$F_n=\frac{1}{T}\int_{-\frac{2}{T}}^{\frac{2}{T}}f(t)e^{jn\Omega t}dt,n=0,\pm1,\pm2$$ 表明任意周期信号$f(t)$可分解为许多不同频率的虚指数信号$e^{jn\Omega t}$之和，其各分量的复数幅度为$F_n$ 帕塞瓦尔方程： $$\int_{t_1}^{t_2}f^2(t)dt=\sum_{j=1}^{\infty}C_j^2K_j$$ 表明在区间$t_1$和$t_2$之间信号所含有的能量恒等于此信号在完备正交函数集中各正交分量能量的总和 频谱密度与傅里叶变换 定义频谱密度函数为： $$F(j\omega)=\lim_{T\to\infty}F_nT=\int_{-\infty}^{\infty}f(t)e^{-j\omega t}dt$$ 上式称为傅里叶变换。 $$f(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}F(j\omega)e^{j\omega t}d\omega$$ $$F(j\omega)=|F(j\omega)|e^{j\psi(\omega)}=R(\omega)+jX(\omega)$$ 狄里赫利条件 傅里叶变换存在的充分条件：在无限区间内$f(t)$绝对可积，即： $$\int_{-\infty}^{\infty}|f(t)|dt&lt;\infty$$ 变换 |时域|频域||-||连续|非周期||离散|周期||周期|离散||非周期|连续| 频移特性/调制特性 若： $$f(t)\leftrightarrow F(j\omega)$$ 且$\omega_0$为常数，则： $$f(t)e^{\pm j\omega_0t}\leftrightarrow F[j(\omega\pm\omega_0)]$$ 信号的能量谱与其自相关函数是一对傅里叶变换 正余弦函数的傅里叶变换 已知常数1的傅里叶变换： $$\mathscr{F}[1]=2\pi\delta(w)$$ 根据频移特性： $$\mathscr{F}[e^{j\omega_0t}]=2\pi\delta(\omega-\omega_0)$$ $$\mathscr{F}[e^{-j\omega_0t}]=2\pi\delta(\omega+\omega_0)$$ 则： $$\mathscr{F}[cos(\omega_0t)]=\mathscr{F}[\frac{1}{2}(e^{j\omega_0t}+e^{-j\omega_0t})]=\pi[\delta(\omega-\omega_0)+\delta(\omega+\omega_0)]$$ $$\mathscr{F}[sin(\omega_0t)]=\mathscr{F}[\frac{1}{2j}(e^{j\omega_0t}-e^{-j\omega_0t})]=j\pi[\delta(\omega+\omega_0)-\delta(\omega-\omega_0)]$$ 冲激串序列的傅里叶变换 $$\mathscr{F}[\delta_T(t)]=\mathscr{F}[\sum_{m=-\infty}^{\infty}\delta(t-mT)]=\Omega\sum_{n=-\infty}^{\infty}\delta(\omega-n\Omega)=\Omega\delta_{\Omega}(\omega)$$ LTI系统的频率响应 $$\begin{matrix}f(t)&amp;*&amp;h(t)&amp;=&amp;y(t)\\ \updownarrow&amp;&amp;\updownarrow&amp;&amp;\updownarrow\\ F(j\omega)&amp;\cdot&amp;H(j\omega)&amp;=&amp;Y(j\omega)\end{matrix}$$ 奈奎斯特采样定理 采样： $$f(t)\times\delta_{T_s}(t)=f_s(t)$$ $$F(j\omega)*\omega_s\delta_{T_s}(\omega)=F_s(j\omega)$$ 频域上，相当于对原信号进行了无数次搬移，为了防止信号发生混叠，搬移之后的频谱不能重叠。故$\omega_s\geq2\omega_m$。 但这只是充分条件。 压缩采样：1.原始信号在某正交基底上是稀疏的；2.利用观测矩阵对该正交基底进行观测，得到观测值；（也可理解为一种采样，但是采样率可以不受奈奎斯特定理约束）3.传输观测之后的数据；4.恢复原始信号。 类似。。小波变换 知乎上对于压缩感知的问题 拉普拉斯变换与Z变换 都是傅里叶变换的几种特殊形式。 拉普拉斯变换-处理连续系统 Z变换-处理离散系统 数字信号处理 CTFT 假设有原始信号：$x(t)$ 频率：$f_0$ 周期：$T_0=\frac{1}{f_0}$ $\Omega_0=2\pi f_0=\frac{2\pi}{T_0}$ 定义连续傅里叶变换： $$X(j\Omega)=\int_{-\infty}^{\infty}x(t)e^{-j\Omega t}dt$$ DTFT 令：$x[n]=x(nT_s)$ 采样频率：$f_s$ 采样周期：$T_s=\frac{1}{f_s}$ $\omega_s=2\pi f_s=\frac{2\pi}{f_s}$ 定义离散时间傅里叶变换： $$X(j\omega)=\sum_{n=-\infty}^{\infty}x[n]e^{-j\omega n}$$ 不是CTFT乘上冲激串序列采样得到的！！！！DTFT直接是函数值，而$f(t)$乘上冲激串序列后变成了一个冲激串序列和函数 DFT 把有限长序列看成是周期序列的一个周期，则作N点DFT： $$X[K]=\sum_{n=0}^{N-1}x[n]e^{-j\frac{2\pi k}{N}n}$$ $$\omega=\frac{2\pi K}{N}$$ $$K=\frac{\omega N}{2\pi}$$ IDFT 离散傅里叶变换的逆变换： $$x[N]=\frac{1}{N}\sum_{k=0}^{N-1}X[k]e^{j\frac{2\pi n}{K}k}$$ 形式上其实跟正变换基本一致，就是差了个$\frac{1}{N}$的系数，和$\omega$指数中的正负号。 FFT 参见 快速傅里叶变换FFT算法的用途]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>复习笔记</tag>
        <tag>信号处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[长大OJ诞生记（三）用HEXO作为主页]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01-oj-hexo%2F</url>
    <content type="text"><![CDATA[之前找Lwy童鞋做的主页用的时间太久了，响应速度等方面现在都出现了一定的问题 正好前段时间搞自己博客的时候看到个带首页的Hexo主题，测试了一下，响应速度各方面都挺适合 顺手给换上了，顺便博文部分可以作为日常通知什么的 然后再顺手修复了一堆bug，重装了一下内网映射的虚拟机，目前稳定性更上一个台阶 最近都不崩溃了，花生壳稳定得我想哭。。。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微波技术与天线 复习笔记]]></title>
    <url>%2F2015%2F06%2F27%2F2015-06-27-weibojishuyutianxian%2F</url>
    <content type="text"><![CDATA[绪论 微波是电磁波谱中介于超短波与红外线之间的波段，频率范围从300MHz（波长1m）至3000GHz（波长0.1mm） 微波、天线与电波：三者的共同基础都是电磁场理论，是电磁场在不同边界值条件下的应用 微波：研究如何导引电磁波在微波传输系统中的有效传输 天线：1.有效地辐射或者接受电磁波；2.把无线电波能量转换为导行波能量 电波：分析和研究电波在空间的传播方式和特点 第一章 均匀传输线理论 微波传输线是用传输微波信息和能量的各种形式传输系统的总称，它的作用是引导电磁波沿一定方向传输，因此又称为导波系统，其所引导的电磁波被称为导行波 一般将截面尺寸、形状、媒质分布、材料及边界条件均不变的导波系统称为规则导波系统 微波无源器件、均匀传输线、有源元器件以及天线一起构成微波系统 将传输线上导行波的电压与电流之比定义为传输线的特性阻抗，用 $Z_0$ 来表示，其倒数称为特性导纳，用$Y_0$来表示 常用平行双导线传输线的特性阻抗有250$\Omega$、400$\Omega$和600$\Omega$三种 传输线上任意一点的电压与电流之比称为传输线在该点的阻抗 均匀无耗传输线上任意一点的输入阻抗与观察点的位置、传输线的特性阻抗、终端负载阻抗及工作频率有关 无耗传输线上任意相距${1 \over 2}$波长阻抗相同，${1 \over 4}$波长阻抗变换 定义传输线上任意一点的反射波电压（或电流）与入射波电压（或电流）之比为电压（或电流）的反射系数$\Gamma$ 终端反射系数： $$\Gamma_1=\frac {Z_1-Z_0}{Z_1+Z_0}=|\Gamma_1|e^{j\phi_1}$$ 反射系数： $$\Gamma(z)=|\Gamma_1|e^{-j2\beta z}=|\Gamma_1|e^{j(\phi_1-2\beta z)}$$ $$\Gamma(0)=\Gamma_1$$ 输入阻抗与反射系数的关系： $$Z_{in}(z)=Z_0\frac{1+\Gamma(z)}{1-\Gamma(z)}$$ $$\Gamma(z)=\frac{Z_{in}(z)-Z_0}{Z_{in}(z)+Z_0}$$ 相移常数 $$\beta=\frac{2\pi f}{c}=\frac{2\pi}{\lambda}$$ 反射系数也具有${1 \over 2}$波长重复性 当传输线特性阻抗一定时，输入阻抗与反射系数有一一对应的关系 当$Z_1=Z_0$时，反射系数为0，即终端无反射，称为负载匹配 定义传输线上波腹点电压振幅与波节点电压振幅之比为电压驻波比 $$\rho=\frac{|U|_{max}}{|U|_{min}}=\frac{1+|\Gamma_1|}{1-|\Gamma_1|}$$ $$\Gamma_1=\frac{\rho-1}{\rho+1}$$ |无耗传输线的状态|负载阻抗|任意一点的输入阻抗|反射系数|说明||-||行波状态|$Z_1=Z_0$|$Z_{in}(z)=Z_0$|$\Gamma_1=0$，终端无反射|负载匹配||纯驻波状态|短路、开路或纯电抗||$\Gamma_1=1$，终端全反射|||行驻波状态|终端接任意负数阻抗负载|||既有行波又有纯驻波| 行驻波状态时的波腹点和波节点 ||电压幅度|电流幅度|位置|阻抗||-||波腹点|最大|最小|$z_{max}=\frac{\lambda}{4\pi}\phi_1+n\frac{\lambda}{2}(n=0,1,2,…)$|$R_{max}=Z_0\rho$||波节点|最小|最大|$z_{min}=\frac{\lambda}{4\pi}\phi_1+(2n\pm 1)\frac{\lambda}{4}(n=0,1,2,…)$|$R_{min}=\frac{Z_0}{\rho}$| 可见： $$R_{max}R_{min}=Z_0^2$$ 匹配传输线的特性阻抗： $$Z_{01}=\sqrt{Z_0R}$$ 负载阻抗匹配：负载阻抗等于传输线的特性阻抗，传输线上只有从信源到负载的入射波，而无反射波。匹配负载完全吸收了由信源入射来的微波功率。 源阻抗匹配：电源内阻等于传输线的特性阻抗。入射功率不随负载变化。 共轭阻抗匹配：不匹配电源，当负载阻抗折合到电源参考面上的输入阻抗为电源内阻的共轭值时，负载能得到最大的功率值 第二章 规则金属波导 当工作波长小于截止波长时，此模可以在波导中传输，称为传导模 当工作波长大于截止波长时，此模不能在波导中传输，称为截止模 激励波导的三种方法：电激励、磁激励和电流激励 矩形波导 宽边尺寸a，窄边尺寸b 截止波数： $$k_{cmn}=\sqrt{(\frac{m\pi}{a})^2+(\frac{n\pi}{b})^2}$$ 截止波长： $$\lambda_{cTE_{mn}}=\lambda_{cTM_{mn}}=\frac{2\pi}{k_{cmn}}=\frac{2}{\sqrt{(\frac{m}{a})^2+(\frac{n}{b})^2}}$$ 相移常数 $$\beta=\frac{2\pi}{\lambda}\sqrt{1-(\frac{\lambda}{\lambda_c})^2}=\sqrt{k^2-k_c^2}$$ 相速与波导波长 $$\nu_p=\frac{\omega}{\beta}$$$$\lambda_g=\frac{2\pi}{\beta}$$ 例如： $$\lambda_{cTE_{10}}=2a$$$$\lambda_{cTE_{01}}=2b$$$$\lambda_{cTM_{11}}=\frac{2ab}{\sqrt{a^2+b^2}}$$ TE波 $TE_{01}$为最低次模（主模），其余称为高次模 TM波 $TM_{11}$为最低次模，其余均为高次模 圆形波导 外导体内径a 截止波数： $$k_{cTE_{mn}}=\frac{\mu_{mn}}{a}$$$$k_{cTM_{mn}}=\frac{\nu_{mn}}{a}$$ 截止波长： $$\lambda_{cTE_{mn}}=\frac{2\pi}{k_{cTE_{mn}}}=\frac{2\pi a}{\mu_{mn}}$$$$\lambda_{cTM_{mn}}=\frac{2\pi}{k_{cTM_{mn}}}=\frac{2\pi a}{\nu_{mn}}$$ 在所有的模式中，$TE_{11}$模截止波最长，其次为$TM_{01}$模，三种典型模式的截止波长为： $$\lambda_{cTE_{11}}=3.4126a$$$$\lambda_{cTM_{01}}=2.6127a$$$$\lambda_{cTE_{01}}=1.6398a$$ 第三章 微波集成传输线 各种集成微波传输系统： 准TEM波传输线，主要包括微带传输线和共面波导等 非TEM波传输线，主要包括槽线、鳍线等 开放式介质波导传输线，主要包括介质波导、镜像波导等 半开放式介质波导，主要包括H形波导、G形波导等 对于耦合微带线，可以将激励分为奇模激励和偶模激励 设两线的激励电压分别为$U_1、U_2$，则可表示为两个等幅同相电压激励$U_e（奇模激励）$和两个等幅反相电压激励$U_o$ 第四章 微波网络基础 阻抗矩阵： $$U_1=Z_{11}I_1+Z_{12}I_2$$ $$U_2=Z_{21}I_1+Z_{22}I_2$$ $$\begin{bmatrix}U_1\\U_2\end{bmatrix}=\begin{bmatrix}Z_{11}&amp;Z_{12}\\Z_{21}&amp;Z_{22}\end{bmatrix}\begin{bmatrix}I_1\\I_2\end{bmatrix}$$ $$[U]=[Z][I]$$ 其中$[U]$为电压矩阵，$[I]$为电流矩阵，$[Z]$为阻抗矩阵，其中$Z_{11}$、$Z_{22}$分别是端口1和端口2的自阻抗，$Z_{12}$、$Z_{21}$分别是端口1和端口2的互阻抗。 $\left.Z_{11}=\frac{U_1}{I_1}\right|_{I_2=0}$为$T_2$面开路时，端口1的输入阻抗 $\left.Z_{12}=\frac{U_1}{I_2}\right|_{I_1=0}$为$T_1$面开路时，端口2至端口1的转移阻抗 $\left.Z_{21}=\frac{U_2}{I_1}\right|_{I_2=0}$为$T_2$面开路时，端口1至端口2的转移阻抗 $\left.Z_{22}=\frac{U_2}{I_2}\right|_{I_1=0}$为$T_1$面开路时，端口2的输入阻抗 导纳矩阵 $$I_1=Y_{11}U_1+Y_{12}U_2$$ $$I_2=Y_{21}U_1+Y_{22}U_2$$ $$\begin{bmatrix}I_1\\I_2\end{bmatrix}=\begin{bmatrix}Y_{11}&amp;Y_{12}\\Y_{21}&amp;Y_{22}\end{bmatrix}\begin{bmatrix}U_1\\U_2\end{bmatrix}$$ $$[I]=[Y][U]$$ $\left.Y_{11}=\frac{I_1}{U_1}\right|_{U_2=0}$为$T_2$面短路时，端口1的输入导纳 $\left.Y_{12}=\frac{I_1}{U_2}\right|_{U_1=0}$为$T_1$面短路时，端口2至端口1的转移导纳 $\left.Y_{21}=\frac{I_2}{U_1}\right|_{U_2=0}$为$T_2$面短路时，端口1至端口2的转移导纳 $\left.Y_{22}=\frac{I_2}{U_2}\right|_{U_1=0}$为$T_1$面短路时，端口2的输入导纳 $$[Z][Y]=[E]$$ $$[Y]=[Z]^{-1}$$ 转移矩阵 转移矩阵也称为A矩阵 $$U_1=AU_2+B(-I_2)$$ $$I_1=CU_2+D(-I_2)$$ $$\begin{bmatrix}U_1\\I_1\end{bmatrix}=\begin{bmatrix}A&amp;B\\C&amp;D\end{bmatrix}\begin{bmatrix}U_2\\ -I_2\end{bmatrix}$$ $\left.A=\frac{U_1}{U_2}\right|_{I_2=0}$为$T_2$开路时电压的转移参数 $\left.B=\frac{U_1}{-I_2}\right|_{U_2=0}$为$T_2$短路时转移阻抗 $\left.C=\frac{I_1}{U_2}\right|_{I_2=0}$为$T_2$开路时转移导纳 $\left.D=\frac{I_1}{-I_2}\right|_{U_2=0}$为$T_2$短路时电流的转移参数 n个双端口网络级联，则有： $$[A]=[A_1][A_2]…[A_n]$$ 散射矩阵 对于线性网络，归一化入射波和归一化反射射波之间是线性关系，故有线性方程： $$b_1=S_{11}a_1+S_{12}a_2$$ $$b_2=S_{21}a_1+S_{22}a_2$$ $$\begin{bmatrix}b_1\\b_2\end{bmatrix}=\begin{bmatrix}S_{11}&amp;S_{12}\\S_{21}&amp;S_{22}\end{bmatrix}\begin{bmatrix}a_1\\a_2\end{bmatrix}$$ $$[b]=[S][a]$$ $\left.S_{11}=\frac{b_1}{a_1}\right|_{a_2=0}$表示端口2匹配时，端口1的反射系数 $\left.S_{12}=\frac{b_1}{a_2}\right|_{a_1=0}$表示端口1匹配时，端口2到端口1的反向传输系数 $\left.S_{21}=\frac{b_2}{a_1}\right|_{a_2=0}$表示端口2匹配时，端口1到端口2的正向传输系数 $\left.S_{22}=\frac{b_2}{a_2}\right|_{a_1=0}$表示端口1匹配时，端口2的反射系数 对于互易网络：$S_{12}=S_{21}$ 对于对称网络：$S_{11}=S_{22}$ 对于无耗网络：$[S]^+[S]=[E]$其中$[S]^+$是$[S]$的转置共轭矩阵，$[E]$为单位矩阵 第五章 微波元器件 理想衰减器 $$\begin{bmatrix}S_\alpha\end{bmatrix}=\begin{bmatrix}0&amp;e^{-\alpha l}\\e^{-\alpha l}&amp;0\end{bmatrix}$$ 理想相移器 $$\begin{bmatrix}S_\theta\end{bmatrix}=\begin{bmatrix}0&amp;e^{-j\theta}\\e^{-j\theta}&amp;0\end{bmatrix}$$ 理想隔离器 $$\begin{bmatrix}S\end{bmatrix}=\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}$$ Y形结环形器 $$\begin{bmatrix}S\end{bmatrix}=\begin{bmatrix}S_{11}&amp;S_{12}&amp;S_{13}\\S_{21}&amp;S_{22}&amp;S_{23}\\S_{31}&amp;S_{32}&amp;S_{33}\end{bmatrix}$$ 定向耦合器，是一种具有定向传输特性的四端口元件。 端口1：输入端 端口2：直通输出端 端口3：耦合输出端 端口4：隔离端 性能指标：耦合度、隔离度、定向度、输入驻波比和工作带宽 第六章 天线辐射与接收的基本理论 * 天线应具有以下功能： 天线应能将导波能量尽可能多转变为电磁波能量 天线应使电磁波尽可能集中于确定的方向上，即天线具有方向性 天线应能发射或接收规定极化的电磁波，即天线有适当的极化 天线应有足够的工作频带 电基本振子是一段长度$l$远小于波长、电流$I$振幅均匀分布、相位相同的直线电流元。它是线天线的基本组成部分，任意线天线均可看成是由一系列电基本振子组成的。 天线的电参数： 天线方向图：指在离天线一定距离处，辐射场的相对场强（归一化模值）随方向变化的曲线图，通常采用通过天线最大辐射方向上的两个相互垂直的平面方向图来表示 天线效率：定义为天线辐射功率与输入功率之比 增益系数：是综合衡量天线能量转换和方向特性的参数，它是方向系数与天线效率的乘积，记为G 极化和交叉极化电平：极化特性是指天线在最大辐射方向上电场矢量的方向随时间变化的规律。有线极化、圆极化、椭圆极化等。引入交叉极化电平来表征线极化的纯度。 频带宽度：当工作频率变化时，天线的有关电参数不超出规定范围的频率范围称为天线的频带宽度，简称天线的带宽 输入阻抗与驻波比 有效长度：在保持实际天线最大辐射方向上的场强值不变的条件下，假设天线上电流分布为均匀分布时天线的等效长度。有效长度越长，表明天线的辐射能力越强。 第七章 电波传播概论第八章 线天线 横向尺寸远小于纵向尺寸并小于波长的细长结构的天线称为线天线 为了加强天线的方向性，将若干辐射单元按某种方式排列所构成的系统称为天线阵列 相似元：各阵元的形状与尺寸相同，且以相同的姿态排列 元因子$|F(\theta,\varphi)|$表示组成天线阵的单个辐射元的方向图函数，其值仅取决于天线元本身的类型和尺寸，体现了天线元的方向性对天线阵方向性的影响 阵因子表示各向同性元所组成的天线阵的方向性，其值取决于天线阵的排列方式及其天线元上激励电流的相对振幅和相位 在各天线元为相似元的条件下，天线阵的方向图函数是单元因子与阵因子之积，这个特性称为方向图乘积定理 最大辐射方向在垂直于阵轴方向的天线阵称为边射式直线阵 最大辐射方向在阵轴方向的天线阵称为端射式直线阵 均匀直线阵是等间距、各阵元电流的幅度相等（等幅分布）而相位依次灯亮递增或递减的直线阵 第九章 面天线 面天线又称口径天线，它所载的电流沿天线体的金属表面分布，且面天线的口径尺寸远大于工作波长 惠更斯-菲涅尔原理：在空间任意一点的场，是包围天线的封闭曲面上各点的电磁扰动产生的次级辐射在该点叠加的结果 旋转抛物面天线由两部分组成：1.抛物线绕其焦轴旋转而成的抛物反射面；2.置于抛物面焦点处的馈源（照射器） 第十章 微波应用系统]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>复习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记]]></title>
    <url>%2F2015%2F06%2F25%2F2015-06-25-baoyan%2F</url>
    <content type="text"><![CDATA[直面现实的时候，才会觉得害怕 总有一天我们是要离开象牙塔的 今晚被拉去听了我们学院的考研解读会。 其实当初说要求每个班都有要去的人数指标的时候就已经想到了这可能会是个坑了，果然，我说我真没想到我听的第一个宣讲会会是我自己学校的。 整个解读会的主题就是留本校留本校留本校留本校留本校… 说真的，其实也是听了今晚院领导的各种“吹”，才赫然发现我们学校，真心不错！！也难怪分数线一年比一年高、、、 然而人各有志，也是真心想离开这里。。。 开始投夏令营的时候投了3个，中科大、北大、中山卡耐基，直到今天似乎终于大部分的学校都截止报名了，然后幸而我也收到了第一个确认。 之前想想还是挺忐忑的，虽然以前已经有了两年前PSW保去复旦的先例了，总还是对自己的学校不太有信心。 知名度太低，谁知道你是个211？没当成民办三本就已经很好啦（虽然我们平时自己也是这么自黑的） 而且纵观我们学院这几年保出去的那些，西交西电居多，如此大胆往这么高的地方投的，可能真的不多吧。 然而我不甘心。不甘心我的星辰大海，只有这么点大吧？ 也是怪自己要求太高，直博的不想去，挑计算机，又重点想找自己想学的系统结构比较强的。。。然后最后就只剩几个了。 一边考着试，一边忐忑地等着结果，今天考完了这学期的倒数第二门，距离告别大三只差最后一门仙姑的《微波技术与天线》啦~ 得到第一个接收回复算是终于稍微有点底了。 我现在最想拿的还是中山卡耐基的offer。 在中大待一年，赴美卡耐基待一年，然后拿到两个学校的学位，留美打拼几年之后再回国。 想想也是挺好的。 日子过得挺累，或许好多事情原本就不应该我去多担心吧。 就是自从去年家里掏空积蓄买了那套房开始，正好又碰上行业不景气，老爸的厂里生意巨差。 也是真的不敢给他们再多压力了，也是想自己赶紧能够出来赚到自己的钱，每次一想到爸妈，就会觉得他们很累。 之前花了好久考虑到底要不要读研？呵呵，直接毕业出去工作算了。。。 结果弄到最后我居然想的是出国这条要花更多钱的路？ 或许这就是一条专属于程序员的路吧。 我对自己尽最大的可能估测如果能拿到10k/月的薪资已经是很好了，然而这样给人打工能到什么时候？又能怎么还清背后的一屁股债啊。。。 程序员在美帝整体的待遇还是很不错的，EE/CS毕业生平均大概能找到6W/年的工资吧，努力一点，节省一点，折合成RMB就能多很多。 而且总的来说，这个行业在那边发展的也是更好一些，等到学成打拼几年之后带着积蓄归国，想想也是个不错的选择不是吗？ 也许我还是把事情想得太简单了吧，之前在知乎上搜了相关的问题也都是喜忧参半。 这篇文章给了我很大的鼓舞 家境一般如何出国留学？ 这个则是留给我更多的思考，当然我并不想留在美帝。。。情况应该能好一些 寒门学子能否通过赴美攻读研究生，最终留在美国？ T_T 夜深了，想不明白的只有回头再考虑了 明天中山卡耐基会在西交有个宣讲会，去听下再作打算吧。 然而我还不一定能过人家的初审。。。。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>保研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Size Balanced Tree]]></title>
    <url>%2F2015%2F06%2F19%2F2015-06-19-sbt%2F</url>
    <content type="text"><![CDATA[这将会是你踏入高级数据结构的第一步。 终于要开始给队里面讲这种比较高级的数据结构了，也趁此机会自己好好理理。 首先要讲的是这个： 二叉搜索树 二叉搜索树又叫二叉排序树，它的定义很简单： 这是一棵二叉树 令x为二叉树中某个结点上表示的值，那么其左子树上所有结点的值都要不大于x，其右子树上所有结点的值都要不小于x 存储结构一般用链表或者结构体数组模拟链表等等均可。 中序遍历二叉排序树可得到一个关键字的有序序列，一个无序序列可以通过构造一棵二叉排序树变成一个有序序列，构造树的过程也可以看成是对无序序列进行排序的过程。 基本操作有几种： 插入 每次插入的新的结点都是二叉排序树上新的叶子结点，在进行插入操作时，不必移动其它结点，只需改动某个结点的指针，由空变为非空即可。 查找 根据查找值的大小与当前子树的根节点相比，更小就找左边，更大就找右边，知道找到目标或者返回无结果。 子树中的最大/最小值 从根开始走到最左边是最小，走到最右边是最大。 前驱或后继 查找比当前点小的最大值（前驱）或比当前点更大的最小值（后继）。考虑左子树的最大值和右子树的最小值，若没有左右子树则考虑父节点。 删除 删除的第一步是要先找到该节点，然后在子树中找个前驱或者后继来替换掉该结点即可。 搜索，插入，删除的复杂度等于树高，因此一般的操作都是O(logn)的。 思想其实很简单，具体的实现就不贴代码了，因为确实也比较简单。 普通的二叉搜索树存在的问题 ！！！树高并不稳定！！！或者说，不平衡 考虑一组本来就有序的数列，将其插入二叉搜索树，结果就是二叉树会退化成一条链，所有结点只有右子树，左子树是空的。 因而预期的O(logn)的操作会退化成O(n)，数据稍大点这种结构就悲剧了。 为了解决这个问题，机智的人类想出了改进方案： 平衡树 平衡树是在二叉查找树的基础上，增加维护操作，使得二叉查找树保持左右子树平衡，以最大限度地保证整体的效率。这种结构就叫做平衡树。 当然，维护也是需要消耗时间的，一般来说维护消耗的时间越长，树越平衡。具体的还要看实际情况。 一般常见的平衡树有不少种：红黑树、AVL树、SBT、Treap、Splay等等。 它们的基本思想都是通过结点的左右旋来保持原本二叉搜索树的性质不变，然后高效完成。区别就是保持平衡的方式不同。 比如红黑树是把结点分成红黑两种，然后各种旋转稳定，貌似效率相当高，然而实现比较麻烦。 Treap是用优先级的思想，在树上加上堆($Treap=Tree+Heap$)。 相比起来Splay比较特殊，Splay其实并不是一棵严格意义上的平衡树，因为它的操作并不是主要为了保证左右平衡的，它的特点主要是结构比较灵活，可以用来处理一些正常平衡树完成不了的问题，缺点就是常数大，效率可能不高。嗯，这是后话。 今天的重点是这个： SBT 节点大小平衡树(Size Balanced Tree)是一种自平衡二叉查找树。 它是由中国广东中山纪念中学的陈启峰（也是个神人，本来那年拿到了北美地区的ACM冠军，结果封神之路上遇到了Watashi）发明的。陈启峰于2006年底完成论文《[Size Balanced Tree](http://jcf94.com/download/2015-06-19-sbt-Qifeng-Chen《Size Balanced Tree》.pdf)》，并在2007年的全国青少年信息学奥林匹克竞赛冬令营中发表。 相比红黑树、AVL树等自平衡二叉查找树，SBT更易于实现。据陈启峰在论文中称，SBT是“目前为止速度最快的高级二叉搜索树”。 旋转首先是所有平衡树中都会用到的旋转操作，平衡树需要不断改变树的结构，但是改变结构的同时又必须保证的是整棵树的二叉查找树性质不能被破坏掉（要是二叉查找树性质都没了，下面就不用玩了）。 调整方式就是逐点进行左旋或者右旋： 以上图作为例子，左右旋实现起来其实也很简单，就是交换一下x、y的父子关系，然后调整B子树的连接情况，且能够保证整体的二叉搜素性质不改变。 SBT的特殊性质我们给二叉搜索树的结点增加一个size域，用来保存以该节点为根的子树中一共有多少个结点。 上面说了，SBT是通过结点大小(Size)来调整整棵树的平衡性的，它相比一般的二叉搜索树多出来的性质有两条： 对于SBT中的每一个结点t，有： $size[right[t]]&gt;=size[left[left[t]]],size[right[left[t]]]$ $size[left[t]]&gt;=size[left[right[t]]],size[right[right[t]]]$ 以上图为例，性质是： $size[R]&gt;=size[A]$ $size[R]&gt;=size[B]$ $size[L]&gt;=size[C]$ $size[L]&gt;=size[D]$ 如何维护这种性质？Maintain(&amp; t)为了便于说明，以下部分左右旋与Maintain函数的参数传递均为实参 我们从上图开始，把整棵树的结构分成几种情况来看： Case 1：size[left[left[t]]]&gt;size[right[t]] 首先对t进行右旋，此时t更新成了原图中的L 得到如下的结果： 则对于图中的T，其子树不一定满足性质，需要Maintain(T) 当T调整完之后，T的子树与L可能也不一定满足性质，需要再次Maintain(L) 该过程的伪代码为： 1234If size[left[left[t]]]&gt;size[right[t]] then Right-Rotate(t) Maintain(right[t]) Maintain(t) Case 2: size[right[left[t]]]&gt;size[right[t]]这种情况要稍微复杂一些： 我们把原图中的B再往下画一层： 首先对L进行左旋 得到： 再右旋T 得到： 经过上面两步操作之后，整棵树的结构可以说是完全改变了，具体形态也可能变得难以预测。但是根据左右旋的性质，我们可以保证上图的结果中A、E、F、R都是性质完好的SBT，所以只要分别Maintain(L)和Maintain(T)即可。 经过上面那一步，我们能保证L和T以及其子树都是性质完好了，但是它们之间的任然不能确保，所以需要再Maintain(B)一次 该过程的伪代码： 123456If s[right[left[t]]&gt;s[right[t]] then Left-Rotate(left[t]) Right-Rotate(t) Maintain(left[t]) Maintain(right[t]) Maintain(t) Case 3: size[right[right[t]]]&gt;size[left[t]]情况与第一种类似，刚好相反 Case 4: size[left[right[t]]]&gt;size[left[t]]情况与第二种类似，刚好相反 总结按照上述说明，Maintain函数就是四个if语句分开即可，但是中间可能会有一些不必要的操作，故作者在[论文]((http://jcf94.com/download/2015-06-19-sbt-Qifeng-Chen《Size Balanced Tree》.pdf))中也对其进行了改进。 可以发现1、2与3、4的判断是可以分开的，于是可以添加一个标记参数，已确定接下来的Maintain中需要检查哪一边 改进之后的伪代码： 12345678910111213141516171819Maintain (t,flag) If flag=false then If size[left[left[t]]]&gt;size[right[t]] then Right-Rotate(t) Elseif size[right[left[t]]&gt;size[right[t]] then Left-Rotate(left[t]) Right-Rotate(t) Else exit Elseif size[right[right[t]]]&gt;size[left[t]] then Left-Rotate(t) Elseif size[left[right[t]]]&gt;size[left[t]] then Right-Rotate(right[t]) Left-Rotate(t) Else exit Maintain(left[t],false) Maintain(right[t],true) Maintain(t,false) Maintain(t,true) 愉快地使用SBTSBT的核心操作是Maintain，经过上面这么多内容，我想你已经掌握了。 SBT支持所有普通二叉查找树的操作（显而易见），而有了Maintain，之后所有的操作都是在普通的二叉查找树的基础上加以改进即可，可以衍生出更多有用的操作。 插入 正常的二叉查找树插入操作，插入完之后Maintain维护性质 删除 直接使用正常的二叉查找树删除操作即可。 你说SBT性质可能会被破坏？ 嗯，是的。确实可能会出现这种情况，然而平衡树的操作是为了是的树的平均深度的平衡性。对于删除来说，删除一个结点并不会增加树的深度，所以在这里不用Maintain对整体并不会有什么影响，下次其他操作的Maintain就可以修复这个问题了。 查找 与正常的二叉查找树相同 查找第k大 由于每个结点的Size域，我们可以快速找出整个数列中第k大的数。根结点是第size[左子树]+1小的数，所以从根结点出发，按照大小关系不断遍历左右子树即可。 复杂度也是O(logn)的。 最终代码示例这份模板是我自己改过好多遍的，当初学这个的时候特别痛苦。网站找的别人的模板要不就有问题，要不就写得让人觉得很难受。然后找到的几个人的模板还都不一样，参考都累。 最后自己在纸上推左右旋推了很久才定下来，后来做题的时候又改进过好多遍。 推荐大家多画图。。。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153/* ***********************************************MYID : Chen FanLANG : G++PROG : Size Balanced Tree************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 100010int sons[MAXN][2];int size[MAXN],data[MAXN];int sbt=0,sbttail=0;void rotate(int &amp;t,int w) //rotate(&amp;node,0/1)&#123; int k=sons[t][1-w]; if (!k) return ; sons[t][1-w]=sons[k][w]; sons[k][w]=t; size[k]=size[t]; size[t]=size[sons[t][0]]+size[sons[t][1]]+1; t=k;&#125;void maintain(int&amp; t,bool flag) //maintain(&amp;node,flag)&#123; if (!t) return ; if (!flag) if (size[sons[sons[t][0]][0]]&gt;size[sons[t][1]]) rotate(t,1); else if (size[sons[sons[t][0]][1]]&gt;size[sons[t][1]]) &#123; rotate(sons[t][0],0); rotate(t,1); &#125; else return ; else if (size[sons[sons[t][1]][1]]&gt;size[sons[t][0]]) rotate(t,0); else if (size[sons[sons[t][1]][0]]&gt;size[sons[t][0]]) &#123; rotate(sons[t][1],1); rotate(t,0); &#125; else return ; maintain(sons[t][0],false); maintain(sons[t][1],true); maintain(t,false); maintain(t,true);&#125;void insert(int&amp; t,int v,int pos) //insert(&amp;root,value,pos/0)&#123; if (!size[t]) &#123; if (!pos) &#123; sbttail++; pos=sbttail; &#125; data[pos]=v; size[pos]=1; sons[pos][0]=0; sons[pos][1]=0; t=pos; &#125; else &#123; size[t]++; if (v&lt;data[t]) insert(sons[t][0],v,pos); else insert(sons[t][1],v,pos); maintain(t,v&gt;=data[t]); &#125;&#125;int last; //last nodeint del(int&amp; t,int v) //value=del(&amp;root,key)&#123; size[t]--; if (v==data[t]||(v&lt;data[t]&amp;&amp;sons[t][0]==0)||(v&gt;data[t]&amp;&amp;sons[t][1]==0)) &#123; int ret=data[t]; if (sons[t][0]==0||sons[t][1]==0) &#123; last=t; t=sons[t][1]+sons[t][0]; &#125; else data[t]=del(sons[t][0],data[t]+1); return ret; &#125; else if (v&lt;data[t]) return del(sons[t][0],v); else return del(sons[t][1],v);&#125;int delk(int&amp; t,int k) //value=del(&amp;root,k)&#123; size[t]--; if (size[sons[t][0]]+1==k) &#123; int ret=data[t]; if (sons[t][0]==0||sons[t][1]==0) &#123; last=t; t=sons[t][1]+sons[t][0]; &#125; else data[t]=delk(sons[t][0],data[t]+1); return ret; &#125; else if (k&lt;=size[sons[t][0]]) return delk(sons[t][0],k); else return delk(sons[t][1],k-1-size[sons[t][0]]);&#125;int select(int t,int k) //node=select(root,k)&#123; if (k==size[sons[t][0]]+1) return t; if (k&lt;=size[sons[t][0]]) return select(sons[t][0],k); else return select(sons[t][1],k-1-size[sons[t][0]]);&#125;int search(int t,int x) //node=search(root,x)&#123; if (t==0||x==data[t]) return t; if (x&lt;data[t]) return search(sons[t][0],x); else return search(sons[t][1],x);&#125;void deb_out()&#123; printf("-------\n"); printf("sbttail=%d sbt=%d\n",sbttail,sbt); for (int i=1;i&lt;=sbttail;i++) printf("%2d: key=%2d size=%2d left=%2d right=%2d\n",i,data[i],size[i],sons[i][0],sons[i][1]); printf("-------\n");&#125;int main()&#123; sbttail=0; sbt=0; for (int i=1;i&lt;=15;i++) insert(sbt,i,0); deb_out(); //printf("%d\n",del(sbt,8)); insert(sbt,8,0); deb_out(); del(sbt,8); del(sbt,8); //printf("%d\n",search(sbt,8)); deb_out();&#125; 后话SBT可以用来快速维护一组数的插入、查找、删除、找第k大，然而你会发现只是想要单纯地实现这个功能，C++ STL 中的set就可以做到。哦对了，set不能找第k大…… 事实上set里面就是用红黑树来进行维护的。。。然而我并不会红黑树。。。然而你们也并不需要会红黑树。。。因为直接用set就好了啊。。。-_-///不用set那么用SBT嘛，这种东西学一个就差不多了 所以虽然SBT效率高，但是扩展性似乎并没有特别强，一般需要用到的也就是只有动态多次求第k的时候。 真正想要见识一下神器的，请在未来好好研究学习Treap和Splay吧，这两个在应用的时候比SBT的用途广很多。 如需要模板题练手，请点击页面下方的标签查看详情。 当然模板可以参考，不过还是不要复制粘贴代码了，最好能自己写一遍。]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>SBT</tag>
        <tag>平衡树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专业知识整理]]></title>
    <url>%2F2015%2F06%2F19%2F2015-06-19-zhuanye%2F</url>
    <content type="text"><![CDATA[昨天刚开了大三大四的专业班会，本来是几个大四的讲解保研、考研、工作经验的，最后系主任吴老大站起来总结陈词。。。 不愧是吴老大啊！一下就抓住了问题的关键，连着又把我们教育了一通。。。 起因是：保研的学姐说她面试的时候被人老师问到个整流电路，答不上来，问到个采样定理，答不上来。找工作的学长说他面试的时候被人总监让写个C语言程序，写不出来。 然后吴老大果断抓住重点，开始批评教育！！！ 隐忧啊什么的要重基础！要重基础！要重基础！重要的事情要说好多遍！ 好吧，好像确实很有道理的样子。正好接下来也要准备夏令营面试。打算慢慢开始重新整理下大一到现在学过的专业课。 先占个坑。 嗯！首先理下之前学过的课, 前三年学过的课程 2012-2013学年秋 [24050010]计算机应用基础[12070060]机械制图[14030010]体育（1）[64050010]军事理论[11070010]思想道德修养与法律基础[13030011]大学英语（1）[12031011]高等数学(1)[24040010]通信技术导论[2404001S]入学教育与毕业教育[6405001S]军训春 [12040011]大学物理（1）[14030020]体育（2）[11160010]中国近现代史纲要[13030012]大学英语（2）[12031012]高等数学(2)[24050030]C语言程序设计[33**]公共艺术类课[24040020]数据库原理及应用[24040030]计算机软件技术基础 2013-2014学年秋 [11050010]马克思主义基本原理概论[13030013]大学英语（3）[12031030]线性代数[12040012]大学物理（2）[14030030]体育（3）[12031110]复变函数与积分变换[12040110]物理实验[11080440]文献检索[32051010]电路分析基础[24040040]电磁场与电磁波[24040050]信号与系统[2404031S]公益劳动春 [11060010]毛泽东思想和中国特色社会主义理论体系概论[13030014]大学英语（4）[12031040]概率论与数理统计[14030040]体育（4）[32053010]模拟电子技术基础[24040060]数字信号处理[24040070]自动控制概论[24040080]传感器基础[11**]思想政治理论课实践[2404006S]数字信号处理课程设计 2014-2015学年秋 [24040090]微机原理与接口技术[24040100]高频电子线路[24040110]通信系统原理[32054010]数字电子技术基础[24040130]信息论与编码技术[24040120]专业英语[24040150]数字图像处理[2404009S]微机原理与接口技术课程设计[3205702S]电子技术基础课程设计[2404010S]高频电子线路课程设计春 [24040160]数字光纤通信[24040170]微波技术与天线[24040180]现代交换技术[24040190]移动通信[24040230]数据通信与计算机网络[24040210]单片机原理[24040220]VHDL语言与数字EDA设计[24040200]数字视频技术[2404032S]电装实习[2404022S]EDA课程设计 数学相关高等数学线性代数复变函数与积分变换作为信号系列的数学基础课，向上支撑信号系统、数字信号处理之类的课。 详见 信号相关专业复习 概率论与数理统计 专业课 电学/无线电电路分析基础模拟电子技术基础数字电子技术基础高频电子线路电磁场与电磁波微波技术与天线主要内容是研究由电磁场与电磁波衍生出来的无线电技术，微波、天线与电波传播 详见 微波技术与天线 复习笔记 信号/通信信号与系统数字信号处理信息论与编码技术通信系统原理详见 信号相关专业复习 数字光纤通信现代交换技术介绍一些交换网中使用的技术，程控交换机等。 详见 现代交换原理 复习笔记 部分 移动通信 计算机应用/硬件C语言程序设计数据库原理及应用微机原理与接口技术数据通信与计算机网络单片机原理VHDL语言与数字EDA设计 自动化自动控制概论传感器基础 图像处理数字图像处理数字视频技术介绍数字视频处理技术。 详见 数字视频 复习笔记]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>复习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[退役帖]]></title>
    <url>%2F2015%2F06%2F13%2F2015-06-13-tuiyi%2F</url>
    <content type="text"><![CDATA[曾经为之付出过这么多的努力，在这个圈子中也认识了这么多优秀的人 要我怎么说放手？ 之前最后一次外出参赛是今年的省赛，也就是上周的这个时候吧，还正坐在赛场上苦思冥想中。。。 丧气的话也不想说了，毕竟还是自己实力不够，心态不行，真正的高手就算被水题坑了，也还是能够坚持做出其他的题来。 这段时间忙的事很多，学校里面是考试、实验、课设，另一边还要自己准备材料投夏令营。原本想着这个自己做的博客也要好好写，好好弄，结果干完正事之后也就几乎不想再碰电脑了。 然后翻了下前面写的几篇，越看越觉得画风不太对。。。都是日常和随笔。。。莫名其妙地就从原本想搞的技术博变成个情感交流博了（捂脸） 也是想了好多事。 回忆篇：说说我回忆中的历程吧。 OI是个起点要说我最早接触coding是在什么时候？ 其实早到初中。 我已经忘了那年是怎么被老师看中的了。唯一记得的是那时还是初一，还是在三中北校区的时候，有天，漂亮的女电脑老师找到我和脑袋，交给我们一份Pascal的资料说问我们想不想学？于是历史的车轮开始转动。 现在想想小时候还真是全能搞。。。-_-///初中参加过作文比赛、数学竞赛、英语竞赛、物理竞赛、化学竞赛，然而当时也并不看重计算机的。只是记得就上过一学期的课，然后NOIP普及组的初赛并没有过。。。 后来我们之后三中就再也没有听说过老师有再带过其他人了。想来也是命运和偶然。 后来考到了温中，跟脑袋一起考进了前四班，又遇到了在我人生中领路的又一个重要老师。军辉顶着当时学校其他老师都不怎么支持的压力，毅然拉出了一个WZOI团队！ 机房里每人一台电脑，一本黄皮书，一个暑假。刷完书的留下来，这台电脑贴上标签以后归你用，刷不完的继续或者离开。然后军辉自己开车带我们去椒江打NOIP的初赛，自己开车带我们去绍兴打复赛。 三年的暑假都给了OI，其实我也从来没有后悔过。开始的初衷是感兴趣，也许拿了奖高考能加分。后来我们能出成绩那年，国家出政策说加分取消。我问自己还要不要学下去？ 要！这么好玩的事情，真的不想放手！ 然后高三即使到了最后，我也依然继续留在了机房。 前几天翻那年今日还翻出以前自己高二时候写的东西来： 后来那年比的最后一次NOIP也就这样了，成绩并没有好到能进省队啊，NOI啊，保送啊。我仍然是个弱渣，小透明而已。 再后来高考没有考好。出成绩的时候就已经蒙了。 我爸妈说后面的事情你自己选择吧，如果想复读一年，大不了再接送你上学一年 最终我还是没勇气打电话给之前联系好的复读班，第二天爸妈找了个路桥那边专业帮人填志愿的老师，我翘掉了这天本来要去参加的初中同学会。 那时候我提的唯一两个要求 211通信工程 最后出录取结果的那天，前三个志愿都差几分，第四个——长安大学，上了。 什么鬼？从来没听过这个学校的名字好么。。。 然而我来了。 ACM不是终点大学是个新开始，我极尽锋芒地展现着我的一切。 顺利地选上军训负责人，当上班长，加入我想去的学生组织，带领班级开始在学院的各种我能做到的比赛中争冠，选上下一级的班导员，作为成功学长给下几级新生讲解经验，建立社团…… 我敢说我们专业是我们学院中我们这级，乃至我们往后这么多级中表现最为出色的！ 话说我越想越觉得我在这里和在家里就是两个完全不同的人。。。-_-///大概在家习惯了小透明，习惯了在所有人眼中我都是可以忽略的那一个。可是在这里，有太多我需要承担的事情了，上面是老师，下面是我的班，我的社团，都是我的人。然后自己下意识地就把两个世界分开了。我从来没想过要介绍大学同学给高中同学认识，也从来不想跟大学的同学讲太多我以前的事情。就像是踏上火车的那一瞬间，一个我沉睡，另一个我就切换过来了。只有在电话里和网络的另一头，才维持着另外一种联系。说双子座双重人格神马的都可以哪凉快哪呆着去啦。。。哥才没有精神分裂！！ 很难描述当我第一次听说长大也有一群人在搞ACM时的心情。那就像是，竟然都会有他乡遇故知的感觉。 后来我见到了在我这条路上给予我们最大支持的老师，也知道了长大的ACM现状其实并不好。然而对我来说，这样一个平台让我觉得我的未来还能够继续坚持我所喜欢的，还能继续为我曾经的梦想奋斗！ 荆老师也很高兴能够有一个有着很多基础的新生加入，那时候也是对我寄予厚望吧。。。只可惜最后在我手上还是没能够实现长大ACM的辉煌。 我的第一队大一一来马上就被荆老师加入了当时长大ACM的一队。我的队友是高我两级的PSW，和高我一级的LQ。 PSW是长大之前实力最强的ACM选手了，我们一起去过南京邀请赛、西安省赛、成都现场赛。可惜最后他要毕业离开的时候，还是只能遗憾地希望我能够撑起未来振兴长大的重担。 还能说什么呢？只能怪那时候的自己没有觉悟，要忙班里的事，要忙学生组织的事，总想着未来还有时间，没有能再多抓紧时间训练了。 我后来想了下，我的一整个大一乃至大二，编程实力都没有过太大的进步，除了Pascal转C之外，几乎都只是在吃高中留下来的那点老本。 现实总是这么讽刺，没有经历的时候很难自己觉悟，等到自己觉得晚了的时候，那就真的已经晚了。 第二队PSW毕业去了复旦，然后是LW转到我们队开始一起努力。 LW也是从大一开始就跟我关系最好的几个之一了。 我们一起加的科协，一起在组织里学习奋斗，一起在学院的比赛中为自己的班级相互拼。 这年我们第一次开始尝试运营我们自己的OJ站。遗憾最后IP还是被学校回收了。 这年情况也没有怎么好转，打过西安邀请赛、北京邀请赛。还是一次一次让荆老师失望。 Xorzip!!真正让我们觉得长大ACM事业出现转机的还是在大二最后的时候。 我和LW一起遇到了SYH。（代码也可以这样有趣） 他对于算法，对于ACM的狂热之于我们俩有过之而无不及，也是比我们两个还要努力。 只能说相见恨晚吧，如果再早一年遇到他，可能长大历史会更早一些被改写，因为这一年中，我们开创了长大史上太多个第一次。 虽然那年的网络赛我们还是一样菜，我们第一次开始能够分析清楚比赛中出现的各种题目类型了！ 长大的外出参赛队第一次有了正式的队名！从来以前派出去的队伍都只叫长大队，Xorzip也是我们历史上第一个独立队名 以前从来参赛都是比完就走，我们第一次开始留下来详细听赛后题目解析，体会颁奖典礼中其他人的感受 第一次开始真正融入ACM的圈子，第一次开始跟外校的其他高手们有了交流 第一次建立了我们的ACM协会，长大ACM集训队第一次有了正式的组织 第一次有了一套初步的训练体系，第一次从大一开始就带出了许多新人入门 第一次举办规模盛大的赛事，邀请到这么多学校的高手们过来参加 …… …… 这一年我们见证了温岭中学WZOI团队中4位当时跟我一起努力奋斗的同学为他们的学校打进了Final。 见证了俄罗斯男神tourist在Final中AC掉所有题目，破纪录封神！ 见证了陕西省整体的ACM环境逐渐活跃起来，各校都开始奋起直追。 然而… 然而… 我还是没有做到。 我从不后悔当初离开家乡当初来到这里，从不后悔走上这样一条路，只是恨自己没有实力。 赛场上最后几分钟时看着其他队伍纷纷出题，看着自己的排名一步一步往下滑，那种感觉刻骨铭心。 要说现在觉得遗憾的事情，很多： 遗憾自己到最后都还是没有足够的实力把长大带向辉煌 遗憾自己这么多年还是辜负了荆老师、前辈的期望 遗憾当初不能醒悟过来，抓紧时间 不过也很高兴这一路上认识了这么多优秀的人，认识了一起从零开始把A协拉起来的这么多小伙伴。 要感谢的人也太多太多。 然而… 然而… 我们马上就要大四了 上周省赛回来的时候，同队的队友都比较沮丧。本来是抱着争金保银的目标去的，却因为出题方判题和数据的问题弄了个什么都没有。 这种心情真是难受至极。 LW说这应该是他最后一次比赛了。 未来篇：本来也想着，如果保研能够顺利，下学期还能再拼一把区域赛。 现实是，虽然总是被认识的人称为大神，然而我知道我并不是啊！ 我剩余的大学时光都奉献给了ACM，却没有能够创造出太大的成就。而其他方面对于我来说却基本都是零。 之前已经有过被拒和落选的经历了，我也不能够确定剩下的三所学校的夏令营能不能顺利通过申请。 保研或是考研，区别只是先后而已，最终面对的竞争仍然还是那么激烈。 也是很感谢爸妈从高中以来的支持，无论是我高三毅然坚持OI竞赛的选择，大学选专业，还是到了大学之后做的各种事情，他们都是说只要我觉得那些事情是正确的，他们就会支持我。 今年暑假的大部分时间我应该还会待在学校里，可能会到8月初再回家一趟。 也是希望能将我所学的所有算法知识都教给现在长大ACM的队员们，今年暑假抓紧时间的话，应该还是能有很大进步的。 至少过了这个暑假之后，他们的实力要赶上、超越我们现在的水平！！ 我相信，长大ACM的未来是光明的！]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生日 记]]></title>
    <url>%2F2015%2F06%2F02%2F2015-06-02-birthday%2F</url>
    <content type="text"><![CDATA[不知不觉已经到了第21个生日，真心不想承认，又老了一岁…… 然后就想着，去年的今天我在干什么？ 明年的今天我会在干什么？明年大概就是快到了大家都要离开的日子了吧，嗯，我想明年一定不会放过这个日子！ 最常用的两个应用的提醒倒是跟的很快~~ 空间总是最早就跟上的。记得大学刚开始那段日子，每天刷空间总是会记得先看看最近好友的生日，后来看得越来越少了，再后来空间看得也越来越少了。 也是越长大，似乎想说的话就越少了/^.^/，也都是下意识地只想说给自己那一个小圈子里的人听。于是空间状态越发越少，朋友圈越发越少… 话说从用网易云音乐开始就觉得它推荐的歌单很准，每次打开都能找到自己想听的歌。 换了不少音乐站，最后一直在用的也就是网易云和QQ啦。 然后今年的生日没有告诉过别人，想着本来也就不是太看重这个日子啦。然后这几天事情各种多，考试、报告的deadline、然后是各种不算太好的消息、包括对未来的迷茫。原本想好的好好读个研究生的想法也因为突然的各种现实而动摇了，唉…不想说了 只是觉得心里挺空的，之前在忙着考试忙着赶报告的时候可以什么都不想。一旦考完了，放松下来之后，好多问题就自己冒出来到脑子里啦。总是想得都太远，也不知道这样是好事还是坏事。 嗯！生日嘛，本该就该是个高兴的日子啦，还有几个人能够记着，那就够了。 最后是刚到的这两本书。 虽说还要纠结下半年究竟该做何选择，然而我总认为我所要坚持的大方向还是这块自己感兴趣的东西，那就继续学咯~不管以后的日子会怎么样，不辜负自己的心意就好了。 愿你们的每一天都过得愉快。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数字视频 复习笔记]]></title>
    <url>%2F2015%2F05%2F30%2F2015-05-30-shuzishipin%2F</url>
    <content type="text"><![CDATA[第一章 绪论视频图像处理的概念视频是一组图像在时间轴上的有序排列，是二维图像在一维时间轴上构成的图像序列，又称动态图像、活动图像、运动图像。不止包含了静止图像的内容，还包含了目标的运动信息和客观世界随时间变化的信息。 序列&gt;帧&gt;像素 视频图像处理系统的组成 图像采集 图像显示 图像存储 图像通信 图像处理和分析 图像处理和分析： 图像变换 图像编码压缩 图像增强复原 图像分割 图像理解和识别 第二章 视频图象的表示图像的基本概念 光通量：光源辐射出来的光功率，单位流明（lm） 照度：单位面积上的光通量，单位勒克斯（lx，$1ls=1lm/m^2$） 亮度：观察者感觉光的量度，有主观性，又称辉度 视敏度：人眼对不同波长可见光的敏感程度 亮度适应级：一定条件下，人眼的亮度适应区间 分辨率 图像分辨率图像中存储的信息，单位像素/英寸（pixel per inch, ppi） 显示分辨率构成画面像素点的多少，用宽高像素*像素表示 打印分辨率又叫输出分辨率，决定打印机打印的惊喜程度，单位点/英寸（dot per inch, dpi） 人眼的视觉特性||锥状细胞|柱状细胞||-|结构|一个细胞连一个神经末梢|几个细胞连一个神经末梢|功能|感光/感色，区分细节|感光，提供整体形象||作用时间|强光下、白天|弱光下、黑夜||视觉|亮视觉|暗视觉| 锥状细胞分为红敏、绿敏、蓝敏。 三基色原理。 影响人眼分辨力的因素：环境照度、景物相对对比度、被观察物体的距离和运动状态 人眼对亮度响应有非线性特性，对灰度误差不敏感 人眼对亮度信号的空间分辨率大于对色度信号的空间分辨率 人眼容易感觉到边缘位置的变化，而对于边缘部分的灰度误差不敏感 时间域的掩蔽效应，当视频图像序列中相邻画面的变化剧烈（如场景切换）时，人眼的分辨率会突然剧烈的下降 视觉惰性，人眼的亮度感觉总是滞后于实际亮度，视觉暂留 闪烁感觉，周期性光脉冲频率高到一定程度后人眼无法区分 颜色模型||RGB|HSI|YUV||-|主要用于|计算机显示器|描述人眼的视觉特性|电视信号，彩色兼容黑白||说明|分别表示红绿蓝三色的分解采样量|色调H，饱和度S，密度I（对应亮度和灰度）|亮度Y，色度U、V 真彩色：每个像素值都用三基色分量描述 伪彩色：每个像素颜色采用颜色查找表表现 直接色：每个像素点的RGB值分别用查找表表现 图像的数字化视频图象数字化两种方法： 复合编码：先数字化再分离先用高速模/数转换器对模拟视频（彩色全电视信号）进行数字化，再在数字域中分离 分量编码：先分离再数字化从模拟视频（彩色电视信号）中分离出彩色分量的亮度和色度，得到YUV或YIQ分量，再用3个模/数转换器对3个分量分别数字化 采样方式：隔行采样 图像量化 有记忆量化和无记忆量化：取样点是否独立 均匀量化和非均匀量化：量化步长是否一致 标量量化和矢量量化：标量量化：一维量化，所有取样使用同一个量化器进行量化，每个取样的量化都和其他所有取样无关(无记忆量化)，常用均方误差量化器矢量量化：多维量化，先将K个取样值序列形成K维空间中的一个矢量,然后将此矢量进行量化 非均匀量化： 基于人的视觉特性特点，对于亮度值急剧变化的部分，粗量化；对亮度值变化比较平缓的部分，细量化。 计算所有可能的亮度值出现的概率分布，对于出现概率大的那些亮度值，细量化；对于出现概率小的那些亮度值，粗量化 非均匀采样：在细节多区域采样密，在平滑区域采样少 空间分辨率：图像的大小，空间分辨率=最大行数*每行的最大像素数。 灰度分辨率：一个像素值单位幅度上包含的灰度级，用一个字节存储一个像素值，则灰度级数为256 图像格式||矢量图形|位图图像||-|定义|图形，指用一组绘图指令描述和记录的各种图形，包括直线、弧线、圆、矩形的大小形状等|图像，由一组计算机内存位组成，这些位定义了图像中每个像素点的亮度和颜色||文件内容|图形指令|图像点阵数据||生成|相应软件生成|用绘图软件生成；用彩色扫描仪扫描二维图片；用摄像机以及帧捕获设备获得数字化画面||所需磁盘空间|小|大||常用格式|WHF、DRW、CDR、DXF、EPS、FLI、FLC、CGM等|BMP、PCX、GIF、TIFF| 第三章 图像变换傅里叶变换 FT离散傅里叶变换 DFT快速傅里叶变换 FFT (掌握)离散余弦变换 DCTK-L变换均方误差（MSE）意义下的最佳变换。 连续小波变换 CWT小波变换：把一个信号分解为将基本小波经过缩放和平移之后的一系列小波 小波变换第六章 视频图像分割视频图像分割的相关概念从语义信息分析，视频由大量场景组成的，每个场景由一个或多个镜头组成，镜头由一系列的帧组成的。 视频对象分割：把视频序列中（一个镜头）人们感兴趣的或具有某种重要特性的一个或多个视频对象从视频场景中提取出来。 同一视频对象具有相似的属性，如亮度、色彩、纹理及运动特征等。 一个视频对象是指视频图像序列中的同一个物理对象。位于一个图像中的视频对象称为一个视频对象区。 为了表示用户感兴趣的区域 ，MPEG-4标准提出了视频对象的概念，其编码是基于对象的，由此，对比特率控制可以基于对象。为了实现高效压缩，每个视频对象用三类信息来描述：运动信息、形状信息、纹理信息，再根据这些信息完成视频的编码和解码。 视频分割基本理论按照是否需要人工参与分：自动方式、半自动方式 按照视频分割过程中利用的信息分： 时域分割算法用来检测和分割物体的运动边缘； 空间域分割算法利用特定的判决原则将图像分割成区域集合； 时空联合分割算法结合了时域分割和空间域分割的优点。 按照是否提供压缩形式分：压缩域分割、非压缩域分割 按照用途分：用于视频压缩编码、用于内容交互多媒体应用 视频分割技术空间域分割 视频帧内的分割，即图像分割，将图像分割为有意义的区域集合，这些区域之间通常具有明显的边界。 基于空间域的分割技术主要有： 基于灰度的空域分割； 基于区域的空域分割； 基于边缘的空域分割； 基于纹理的空域分割等 分水岭算法：基于数学形态学的图像分割算法 区域生长（Region Growing）（掌握） 将具有相似性质的像素集合起来构成区域 区域生长法的关键： 选择或确定一组能正确代表所需区域的种子像素； 确定在生长过程中能将相邻像素包括进来的准则； 制定让生长过程停止的条件或规则 常见区域生长算法种类： 以单像素为单位的区域生长法 以区域为单位的区域生长法（子图合并） 时间域分割技术帧间差分的变化检测 时域中检测运动对象的基本方法，采用帧间差分法可消除视频数据的帧间冗余信息，提取变化检测掩模（Change Detection Mask）。 基本原理：通过检测前、后帧之间的帧差，从而把当前视频分割成相对于参考帧“变化的”和“未变化的”区域。 前后帧的偏移帧差：$$d_{k,k+1}(x,y)=|f_{k+1}(x,y)-f_k(x,y)|$$ 局限性： 阈值选取至关重要的，无通用阈值设定方法； 对某些视频序列无能为力的，如纹理不够充分、某些帧对象存在运动而另外一些帧没有运动； 对于对象间的相互遮挡区、显露区的判断是直接用亮度信号估算参数； 差分检测受噪声影响不敏感，还受运动估算精度的影响 块匹配的运动估计 基于对时间图像序列变化的检测。 运动物体特征：检测图像变化，可以在不同的层次上进行，如像素、边缘或区域 二维运动估计：估计运动前后相邻时刻两幅图像上对应点的坐标,以获取二维运动矢量。假设物体点的亮度在此运动和时间间隔保持不变。 几种匹配准则： 最大互相关函数 最小均方误差函数（MSE） 最小平均绝对差值函数（MAD） 最大匹配像素统计（MPC） 光流法运动场：给图像中每一像素点赋予一个速度向量，就形成了图像运动场(motion field)，对应于物体的三维运动 光流定义：视频序列空间坐标关于时间的变化率，对应于像素的瞬时速度矢量，也是运动物体在一帧图像到下一帧图像相对应像素点间的位移量 光流：图像亮度模式的表观(视在)运动，“可察觉”的二维运动，依赖于光照条件和物体表面的纹理。假设光流就是真实的二维运动，估算二维运动矢量。 第九章 视频跟踪第十章 视频压缩编码]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>复习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代交换原理 复习笔记 部分]]></title>
    <url>%2F2015%2F05%2F28%2F2015-05-28-xiandaijiaohuan%2F</url>
    <content type="text"><![CDATA[感觉我也是醉醉的，人比较懒，愿意打字都不愿意动笔了…… 第1章 绪论交换所谓交换，就是在通信网上，负责在通信的源和目的终端之间建立通信信道，传送通信信息的机制，也就是根据目的地，在源和目的终端之间传送通信信息。网内的任一个用户可以按着自己的要求与网内的其他用户进行信息交换 交换式通信网通信过程 呼叫建立阶段 消息传输阶段 释放阶段 交换方式 电路交换 CS （Circuit Switch）报文交换 MS （Message Switch）分组交换 PS （Package Switch） 数据报 Datagram虚电路 VC (Virtual Circuit) 交换虚电路 SVC （Switching Virtual Circuit）永久虚电路 PVC （Permanent Virtual Circuit） |分类|电路交换|报文交换|分组交换||-|方式|通信之前先建立通路，通信时通路独占，通信结束，通路释放|无需建立通路（无连接）|将报文分成若干个报文组（packet)，每个报文组要加上地址、编号、校验码，然后以报文组为存储转发单位逐节点转发，到达目的地再按编号组装成原报文||接续时间|较长|较短|较短||传输时延|短|长，偏差很大|较短||数据可靠性|一般|较高|高|对业务过载的反应|拒绝接收呼叫|信息存储在交换机中，传输时延加大|减小用户输入信息流量（流量控制），时延加大||异种终端之间的相互通信|不可|可|可||电路利用率|低|高|高||交换机成本|较低|较高|较高||实时会话业务|适用|不适用|轻负载下适用| 分组交换 基于统计时分复用，实时性较好，线路利用率高。 数据报方式:以分组为基本单位逐节点转发，且不同节点可沿不同的路径传输，类似报文交换。 虚电路:首先网络在通信两端建立逻辑连接，然后用户数据以分组为单位沿该路径顺序传送到达终点（逐节点转发）。类似于电路交换，但通路不是一直占用。 程控交换机 交换机网络接续方式 空分 时分 控制方式 集中控制 分级控制 全分散控制 五级长途电话网 省间中心 C1 省中心 C2 地区中心 C3 县中心 C4 本地端局 C5 三级国际电话网 国际中心局 CT1：各CT1之间均有直达链路 CT2 接口局 CT3 路由分类 基干路由：一部分是C1级交换中心之间的低呼损电路群；另一部分是同一交换区内相邻级之间的低呼损电路群。基干路由上的话务量不允许溢出 直达路由：先选择高效直达路由。当高效直达路由忙时，选择迂回路由。选择的顺序是“由远而近”，即先在被叫端“自下而上”选择。然后在主叫端“自上而下”选择。最后选择最终路由。 本地电话网 去话汇接 来话汇接 来去话汇接 集中汇接 主辅汇接 第2章 交换机理论基础 话务量考察时间内各次服务时间总和Y表示话务量，T表示时间总和，$$A= {Y\over T}=n{S\over T}=λS$$$$λ={n\over T}$$ 第3章 信令系统概述 信令信令是各交换局在完成呼叫接续中使用的一种通信语言，它是控制交换机产生动作的命令。 信令的分类 按工作区域分1) 用户线信令2) 局间信令3) 交换机内部信令 按传送方向分1) 前向信令2) 后向信令 按功能分1) 监视信令（线路信令）：用来监视或改变线路上的呼叫状态或条件2) 选择信令（记发器信令）：由主叫用户发出的数字信号（电话号码），即被叫用户的地址信息3) 操作信令（管理信令） 按传播途径分1) 随路信令（带内信令）：通过话路传送的信令2) 公共信道信令（带外信令）：信令传送和话路分开的信令 信令的传送方式 端到端：对电路质量要求较高 逐段转发：对线路要求较低 混合方式：中国1号记发器信令在劣质电路中使用逐段转发方式，在优质电路中使用端到端方式，No.7信令通常使用逐段转发方式但也可提供端到端信令。 控制方式 非互控（脉冲方式）：设备简单，但可靠性差 半互控方式：发端每发送一组信令之后必须受到确认后才能再发 全互控方式：连续发送，不中断，直到受到证实之后才停止。抗干扰能力强，可靠性好，但发码速度慢中国1号记发器信令使用全互控方式，保证可靠性，但影响了速度；No.7信令使用非互控方式，速度快，同时采用一些措施来保证可靠性。 中国No.1信令中国1号信令是一种随路信令。 方式分为线路信令和记发器信令。数据类型分为模拟型线路信令（直流型线路信令、带内单频信令）和数字型线路信令。 带内单频线路信令 单频2600Hz*，基本脉冲为长脉冲600ms，短脉冲150ms，两信令最小标称间隔300ms。 局间数字型线路信令一个复帧由16个子帧组成，记为F0~F15每一个子帧有32个时隙，256bit，T=125us，记为TS0~TS31每一个时隙包含8bit二进制码字 TS0*用于收发端同步，称为帧同步时隙，也称为帧定位码组 TS1TS15以及TS17TS31*是话音时隙 TS16*用来传送复帧同步及数字型线路信令，称为信令时隙。一路话音信号的线路信令只需要4bit，即一个TS16时隙可以传送两路。 No.1记发器信令 RS（Resister Signal） 多频互控方式信令，MFC（Multiple Frequency Control）*，传送方式为端到端，劣质电路上也可采用逐段转发方式，控制方式为全互控。 |组别|名称|基本含义|容量||-|I|KA|主叫用户类别|15||I|KC|长途接续类别|5||I|KE|市内接续类别|5||I|数字信令|0~9|10|II|KD|发端呼叫类别|6|A|A|收号状态和接续状态|6|B|KB|被叫用户状态|6 前向I组和后向A组互控，前向II组和后向B组互控 No.7 信令局间的NO.7信令链路是由两端的信令终端设备和它们之间的数据链路组成。最适合采用64kbit/s的数字信道，也适合模拟信道和较低速率下的工作，适合由数字程控交换机和数字传输设备所组成的综合数字网。高可靠性。公共信道信令。 用于支持蜂窝移动通信、PCN、ATM等，用于支持国际网和国内网。 No.7信令网的基本组成部件有信令点SP、信令转接点STP和信令链路SL SP是处理控制消息的节点，产生消息的信令点为该消息的起源点，消息到达的信令点为该消息的目的地节点。信令点编码有两种： 14位（国际）*：3位大区、8位区域网、3位信号点 24位（国内）*：8位主信令区、8位分信令区、8位信令点源信令点编码OPC(Origin Point Code)，目的信令点编码记为DPC(Destination Point Code)。 信令转接点STP分为综合型和独立型两种。独立型STP是只具有消息传递部分MTP和信令连接控制部分SCCP功能的信令转接点设备；综合型STP是除了具有消息传递部分MTP和信令连接控制部分SCCP的功能外，还具有用户部分功能(例如TUP/ISUP、TCAP、INAP)的信令转接点设备。 信令链路SL连接各个信令点或信令转接点在两个相邻信令点之间传送信令消息的链路称为信令链路。相同属性的信令链路组成一组链路集。到同一局向的所有链路可属一个链路集，也可属多个链路集；但两个相邻的信令点之间的信令链路只能属于一个链路集。对于相邻两个信令点之间的所有链路，都有一个统一编号，称为信令链路编码SLC(Signalling Link Code)。 信令链路的连接方式根据通话电路和信令链路的关系，可采用三种工作方式： 直连工作方式：信令链路直接连接两个SP，信令消息直达。 准直联工作方式：信令消息可能会通过STP转接，路径是预先设定好的。 完全分离工作方式：可能会有多条转接路径，而路由是动态分配的。 协议集No.7信令的基本功能结构由消息传递部分(MTP)（底层）和用户部分(UP)（高层）组成。 消息传递部分 MTP主要是在信令网中提供可靠的信令消息传递，并在系统和信令网故障情况下，具有为保证可靠的信息传送而作出响应并采取必要措施的能力。 它由三个功能级组成： 信令数据链路功能(MTP1)：相当于物理层，定义了数字信令链路的物理、电气及功能特性 信令链路控制功能(MTP2)：相当于数据链路层，提供流量控制、消息顺序确认及检错重传功能 信令网络功能(MTP3)：相当于网络层，提供路由功能 用户部分 UP定义了通信网的各类用户(业务)所需要的信令及其编码，规定用户部分(UP)与消息传递部分（MTP）之间的信号传输关系。控制各种基本呼叫的建立和释放。 ISDN用户部分ISUP、电话用户部分TUP、数据用户部分DUP：一般都用ISUP替代全部。 信令连接控制部分SCCP：本身相当于传输层，但是与MTP-3共同组成网络层，与MTP共同组成网络业务部分NSP。加强消息传递功能。 事务处理能力应用部分TCAP：提供节点之间的传递信息的手段以及对相互独立的各种应用提供通用的业务 TC用户：指各种应用，目前有智能网应用部分(INAP)、移动应用部分(MAP)、运行维护管理应用部分(OMAP) CCS7信令单元格式信令消息是以信号单元的方式传送，而且采用不等长信号单元。以8bit为长度单位它有三种信号单元： 消息信令单元MSU：用来传送第三级以上的各层发送的信息。 链路状态信令单元LSSU：用来传送信令链路状态 填充信令单元FISU：是在信令链路上没有消息要传送时，向对端发送的空信号，用来维持信令链路的通信状态，同时可证实对端发来的信令单元。 信令信息字段 SIF(Signalling Information Field)是MSU特有的，长度为2~272个8位组，包含路由标记和信令数据，就是实际发送的消息。 LSSU和FISU都由信令链路功能级生成及处理。 电话用户部分TUP电话用户消息的内容是在消息信令单元MSU中的信令信息字段SIF中传送的。由标记、标题码和信令信息三部分组成。 初始地址消息IAM（无附加）或IAI（附加信息）：含有下一个交换局为建立呼叫、确定路由所需的有关信息。初始地址消息蕴含了占用电路的功能 后序地址消息SAM（一次传多位）和SAO（一次传一位）：传送剩余的被叫号码。 地址全消息ACM 地址不全消息ADI 被叫用户状态：用户市忙SLB、用户长忙STB、线路不工作LOS、空号UNN和发送专用信息音SST 应答信号ANC（计费）、ANN（免费） 后向拆线信号CBK、前向拆线信号CLF 首先发送： |内容|空|CIC|OPC|DPC|-|长度|4|12|24|24 话音电路标识CIC ISDN用户部分ISUPISUP可以全面支持ISDN用户的基本承载业务和补充业务，而且可以完全实现TUP（电话用户部分）和DUP（数据用户部分）的功能 首先发送： |内容|空|SLS|OPC|DPC|-|长度|4|12|24|24 信令链路选择SLS 初始地址消息IAM（无附加）或IAI（附加信息）：含有下一个交换局为建立呼叫、确定路由所需的有关信息。初始地址消息蕴含了占用电路的功能 后序地址消息SAM（一次传多位）和SAO（一次传一位）：传送剩余的被叫号码。 地址全消息ACM 地址不全消息ADI 被叫用户状态：用户市忙SLB、用户长忙STB、线路不工作LOS、空号UNN和发送专用信息音SST 应答信号ANC（计费）、ANN（免费） 后向拆线信号CBK、前向拆线信号CLF |—|ISUP|TUP|-|初始地址消息|IAM|IAM/IAI|后序地址消息（重叠发送时）||SAM/SAO|地址全消息|ACM|ACM|地址不全消息|ADI|ADI|应答信号|ANM|ANC/ANN|故障信号|包含在REL中|SLB/STB/LOS/UNN|挂机信号||CBK（后向）|拆线信号|REL（链路清除）|CLF（前向）|拆线证实|RLC|RLG 第4章 数字程控交换原理与技术 交换机的基本组成 话路部分用户电路中继器交换网络信令设备 控制系统 控制方式 集中控制 分散控制：资源分散，控制功能集中1) 分级控制：上下级结构、每个层次的处理机处理一部分内容2) 全分散控制：平行结构、将系统划分为几个模块，每个模块相互通信，相互配合3) 基于容量分担的全分散控制：综合上两个、在平行结构中分层容量分担：资源分散、功能集中功能分担：资源集中、功能分散话务分担：容量分担： |—|集中控制|分散控制||-|优点|处理机能够及时掌握、了解整个系统的运行状态，使用和管理系统的全部资源，不会出现资源争夺的冲突|任何一个模块故障，整体仍然能够运行，扩容方便，灵活|缺点|灵活性差，经济性差（处理级复杂、昂贵），软件庞大，维护困难|各部分独立，可能会出现资源冲突，公共资源难以共享，单独模块的容量过小|适合|小容量交换机| 处理机的冗余配置 重要的进行1+1配置 不重要的进行n+1配置 1+1的三种方式 |—|同步双工|双机互助（话务分担）|主/备用|-|特点|两机同步工作，比较执行结果|各自独立工作，一旦一台出错，另一台承担全部工作|一台联机一台备用，一旦出故障则主/备用切换|优点|对硬件故障反应快|过负荷能力强，对软件故障有容错能力，可一台服务一台调试升级|实现简单，热备用时备用机保存主机的相关数据，可以随时接替工作|缺点|对软件无容错能力，需要不停复核，降低了效率|为避免双机争抢资源，双机通信频繁，软件复杂，对硬件故障反应速度不如同步双工|冷备用切换时会产生延误或者连接中断等 4.3.1 程控交换机的硬件结构程控交换机的硬件包括话路系统、中央处理系统（控制系统），维护与操作系统三部分 用户电路的七大功能，被称为：BORSCHT |B|Battery Feed|馈电|-|O|Overvoltage|过压保护|R|Ring|振铃|S|Supervision|监视|C|CODEC&amp;filter|编解码和滤波|H|Hybrid|混合|T|Test|测试 交换网络结构交换网络是由若干个交换单元按照一定的拓扑结构和控制方式构成的网络 交换网络的三个基本要素交换单元、不同交换单元间的拓扑连接和控制方式 多级交换网络的拓扑结构可用三个参数来说明： 每个交换单元的容量。 交换单元的级数。 交换单元间的连接通路(链路) 内部阻塞从交换网络不同输入端来的信息在交换网络中交换时发生的对同一公共资源发生争抢的情况。在竞争资源时失败的信息会被阻塞，直到公共资源被释放。 内部阻塞概率 $$a={A \over n*m}$$ 无阻塞网络 Clos网络 输入m*n 输出 j*k |—|第一级|第二级|第三级||-|个数|m|n+j-1|k||入线数|n|m|n+j-1||出线数|n+j-1|k|j| 中间一级可以继续拆分 空分交换机/空间接线器用来实现多个输入复用线与多个输出复用线之间的空间交换，而不改变其时隙位置 基本结构 交叉点矩阵：开关阵列 控制存储器 时分交换机共享存储器型的交换单元 话音存储器SM：用于存储话音信号 控制存储器CM：用于存储控制命令字 TST网络TST网络是在电路交换系统中经常使用的一种交换网络，它是三级交换网络，两侧为T接线器，中间一级为S接线器，S级的出入线数决定于两侧T接线器的数量。 第1级，T接线器：负责输入母线的时隙交换。 S接线器：负责母线之间的空间交换。 第2级，T接线器：负责输出母线的时隙交换 常用的有$TS^nT$、$ST^nS$、T/结合、TTT等 交换软件中使用的语言 规范描述语言SDL：用于系统设计阶段，用来说明对程控交换机的各种功能要求和技术规范，并描述功能和状态的变化情况； 高级语言和汇编语言：用来编写软件程序； 人机对话语言MML：主要用于人机对话，在软件测试和运行维护阶段使用 局数据 配置数据 路由和中继规则 No.7信令数据 计费数据 新业务提供情况 用户数据 用户电话号码、用户设备号码 用户线类别 话机类别 用户的服务等级 用户对新业务的使用权及已经登记的新业务 用户计费数据 程控交换机中操作系统主要功能是： 任务调度 存储器管理 进程之间的通信、处理机之间通信 定时管理 系统监督和恢复 I／O设备管理、文件管理等 优先级按照对实时性要求的不同，程序的优先级大致可分为中断级、时钟级和基本级 比特型时间表一个时间表所能调度的程序数等于该时间表的列数，时间表能够支持的不同周期数等于时间表行数n的不同因子数 呼叫处理的过程 输入处理：负责采集话路设备的状态信息的变化和有关信息，只负责检测事件而不进行处理。本质上是软件和硬件之间的接口程序，和硬件设备直接联系，通过周期性的扫描程序实现状态信息的采集。 内部处理：主要任务是分析收集的信息和各类发生的事件，分配资源，并根据所发生的时间及与该事件有关进程的当前状态决定下一步的动作。由于对时间没有严格要求，一般情况下，采用队列方式来完成。 输出处理：依据内部分析的结果完成对话路设备的驱动。如交换网络的接续、向用户发生各种信令音等 第5章 分组交换原理与技术第6章 ATM原理与技术]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>复习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通宵那些事]]></title>
    <url>%2F2015%2F05%2F28%2F2015-05-28-diary%2F</url>
    <content type="text"><![CDATA[已经快要忘了上一次通宵是在什么时候了，曾经做数学建模的时候是通过的，最后应该也是在期末的某次，一边要弄课设，一边又有考试，然后想着通宵赶紧把课设做做完，留出白天的时间来好好复习。 说起来还是自己平时的执行效率太低，总是没有办法全心全意地复习，拖啊拖啊拖啊的，就拖成了现在这个样子。]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开始使用Github]]></title>
    <url>%2F2015%2F05%2F26%2F2015-05-26-start%2F</url>
    <content type="text"><![CDATA[之前的博客都是写在博客园上的，现在转移至github。 第一次用Markdown这种语法，真心简洁明了，短小精悍！ 算是从5月26号正式开始跑这个博客吧，27号最后换了个比较喜欢的主题，然后开始慢慢改自己的东西，估计以后一段时间用的都是这个样式了。 然后有空会开始把博客园的东西一篇一篇移回到这里来。 hexo官方文档镜像站：http://hexo.jcf94.com/]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2015%2F05%2F26%2F2015-05-26-test%2F</url>
    <content type="text"><![CDATA[#一级标题 ##二级标题 这是一段不加格式的代码 #include&lt;iostream&gt; #include&lt;cstdio&gt; using namespace std; int main() { printf(&quot;Helloworld\n&quot;); return 0; }###三级标题 ####四级标题 1这是一段正常的代码 1234567891011#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;int main()&#123; printf("Helloworld\n"); return 0;&#125; $$这是一串数学公式$$$$e=mc^2$$ 这是一堆引用 这是一堆引用 这是一堆引用 这是一堆引用 又名块注释 这是一句斜体 这也是一句斜体 这是一句加粗 这也是一句加粗 这是无序列表 这也是无序列表 这还是无序列表 这是有序列表 这也是有序列表 这是一个链接 这是一堆链接Google，baidu 下面来测试一下带图 删除线 这是一个 表格 OK 左对齐 居中 右对齐]]></content>
      <tags>
        <tag>测试</tag>
        <tag>Markdown</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2015%2F05%2F26%2F2015-05-26-hello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 5044 Tree 树链剖分]]></title>
    <url>%2F2015%2F05%2F08%2F2015-05-08-HDU-5044%2F</url>
    <content type="text"><![CDATA[TreeTime Limit: 10000/5000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionYou are given a tree (an acyclic undirected connected graph) with N nodes. The tree nodes are numbered from 1 to N There are N - 1 edges numbered from 1 to N - 1. Each node has a value and each edge has a value. The initial value is 0. There are two kind of operation as follows: ADD1 u v k: for nodes on the path from u to v, the value of these nodes increase by k. ADD2 u v k: for edges on the path from u to v, the value of these edges increase by k. After finished M operation on the tree, please output the value of each node and edge. InputThe first line of the input is T (1 ≤ T ≤ 20), which stands for the number of test cases you need to solve. The first line of each case contains two integers N ,M (1 ≤ N, M ≤105),denoting the number of nodes and operations, respectively. The next N - 1 lines, each lines contains two integers u, v(1 ≤ u, v ≤ N ), denote there is an edge between u,v and its initial value is 0. For the next M line, contain instructions “ADD1 u v k” or “ADD2 u v k”. (1 ≤ u, v ≤ N, -105 ≤ k ≤ 105) OutputFor each test case, print a line “Case #t:”(without quotes, t means the index of the test case) at the beginning. The second line contains N integer which means the value of each node. The third line contains N - 1 integer which means the value of each edge according to the input order. Sample Input 2 4 21 22 32 4ADD1 1 4 1ADD2 3 4 24 21 22 31 4ADD1 1 4 5ADD2 3 2 4 Sample Output Case #1:1 1 0 10 2 2Case #2:5 0 0 50 4 0 题意维护一棵树，操作是对某两点路径上的所有点的权值加上某个值，或者对某两点路径上的所有边的权值加上某个值。 分析Kuang神出题的时候特别为这题卡了数据，LCT目测是不够过的，用树链剖分转成线性加输入挂也是勉强过，4700ms左右，那种1000~2000ms左右的算法不太明白是怎么写的。 1.维护两点路径上的点权值是树链剖分的基本操作 2.维护两点路径上的边权值需要在剖分的时候直接按照边的情况，直接用子结点的序号来代表一条边，然后做好边原本序号的记录 一般的树链剖分是直接用树状数组，但是由于本题的最终结果要求的是输出所有点和所有边的权值，因此采用树状数组每次求一次sum来得到每个点的值就有点浪费效率了。 可以参考原本树状数组的方式，直接用一个普通的数组来记录情况，在起点+value，在终点后的一个点-value，表示对整段线段完成加权值；最终求和的时候只需要从头向后扫一遍，求出每个点的sum(i)即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 5044************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN (int)1E5+10#define MAXM (int)2E5+10int son[MAXN],ans[MAXN];int father[MAXN],size[MAXN],level[MAXN],data[MAXN],top[MAXN];int start[MAXN];typedef struct nod&#123; int to,next,no;&#125; node;node edge[MAXM];int last,n;void addedge(int from,int to,int k)&#123; last++; edge[last].to=to; edge[last].next=start[from]; edge[last].no=k; start[from]=last;&#125;int c[MAXN],pos[MAXN],fp[MAXN];int ec[MAXN],epos[MAXN],fep[MAXN];int tot;int q[MAXN];void bfs()&#123; int head=1,tail=1; q[1]=1; level[1]=0; father[1]=0; while(head&lt;=tail) &#123; int now=q[head]; size[now]=1; for (int i=start[now];i;i=edge[i].next) &#123; int temp=edge[i].to; if (temp!=father[now]) &#123; father[temp]=now; fep[temp]=edge[i].no; level[temp]=level[now]+1; tail++; q[tail]=temp; &#125; &#125; head++; &#125; for (int i=n;i&gt;=1;i--) &#123; int now=q[i]; if (father[now]) &#123; size[father[now]]+=size[now]; if (son[father[now]]==0||size[now]&gt;size[son[father[now]]]) son[father[now]]=now; &#125; &#125; for (int i=1;i&lt;=n;i++) &#123; int now=q[i]; if (son[father[now]]==now) top[now]=top[father[now]]; else &#123; top[now]=now; while(now) &#123; tot++; pos[now]=tot; fp[tot]=now; now=son[now]; &#125; &#125; &#125;&#125;void change(int x,int y,int value)&#123; while(top[x]!=top[y]) &#123; if (level[top[x]]&lt;level[top[y]]) swap(x,y); c[pos[top[x]]]+=value; c[pos[x]+1]-=value; x=father[top[x]]; &#125; if (level[x]&gt;level[y]) swap(x,y); c[pos[x]]+=value; c[pos[y]+1]-=value;&#125;void change_edge(int x,int y,int value)&#123; while(top[x]!=top[y]) &#123; if (level[top[x]]&lt;level[top[y]]) swap(x,y); ec[pos[top[x]]]+=value; ec[pos[x]+1]-=value; x=father[top[x]]; &#125; if (level[x]&gt;level[y]) swap(x,y); ec[pos[son[x]]]+=value; ec[pos[y]+1]-=value;&#125;int INT() &#123; char ch; int res; bool neg; while (ch = getchar(), !isdigit(ch) &amp;&amp; ch != '-') ; if (ch == '-') &#123; neg = true; res = 0; &#125; else &#123; neg = false; res = ch - '0'; &#125; while (ch = getchar(), isdigit(ch)) res = res * 10 + ch - '0'; return neg ? -res : res;&#125;int main()&#123; freopen("5044.txt","r",stdin); int t; t=INT(); for (int tt=1;tt&lt;=t;tt++) &#123; printf("Case #%d:\n",tt); int m; n=INT();m=INT(); memset(start,0,sizeof(start)); last=0; for (int i=1;i&lt;n;i++) &#123; int u,v; u=INT(); v=INT(); addedge(u,v,i); addedge(v,u,i); &#125; memset(son,0,sizeof(son)); tot=0; bfs(); memset(c,0,sizeof(c)); memset(ec,0,sizeof(ec)); for (int i=1;i&lt;=m;i++) &#123; char s; int c1,c2,k; s=getchar(); while (!isdigit(s)) s=getchar(); c1=INT(); c2=INT(); k=INT(); if (s=='1') change(c1,c2,k); else change_edge(c1,c2,k); &#125; int ss=0; for (int i=1;i&lt;=n;i++) &#123; ss+=c[i]; ans[fp[i]]=ss; &#125; printf("%d",ans[1]); for (int i=2;i&lt;=n;i++) &#123; putchar(' '); printf("%d",ans[i]); &#125; putchar('\n'); ss=0; for (int i=1;i&lt;=n;i++) &#123; ss+=ec[i]; ans[fep[fp[i]]]=ss; &#125; if (n&gt;1) printf("%d",ans[1]); for (int i=2;i&lt;n;i++) &#123; putchar(' '); printf("%d",ans[i]); &#125; putchar('\n'); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>树链剖分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 3966 Aragorn's Story 动态树 树链剖分]]></title>
    <url>%2F2015%2F04%2F27%2F2015-04-27-HDU-3966%2F</url>
    <content type="text"><![CDATA[Aragorn’s StoryTime Limit: 10000/3000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionOur protagonist is the handsome human prince Aragorn comes from The Lord of the Rings. One day Aragorn finds a lot of enemies who want to invade his kingdom. As Aragorn knows, the enemy has N camps out of his kingdom and M edges connect them. It is guaranteed that for any two camps, there is one and only one path connect them. At first Aragorn know the number of enemies in every camp. But the enemy is cunning , they will increase or decrease the number of soldiers in camps. Every time the enemy change the number of soldiers, they will set two camps C1 and C2. Then, for C1, C2 and all camps on the path from C1 to C2, they will increase or decrease K soldiers to these camps. Now Aragorn wants to know the number of soldiers in some particular camps real-time. InputMultiple test cases, process to the end of input. For each case, The first line contains three integers N, M, P which means there will be N(1 ≤ N ≤ 50000) camps, M(M = N-1) edges and P(1 ≤ P ≤ 100000) operations. The number of camps starts from 1. The next line contains N integers A1, A2, …AN(0 ≤ Ai ≤ 1000), means at first in camp-i has Ai enemies. The next M lines contains two integers u and v for each, denotes that there is an edge connects camp-u and camp-v. The next P lines will start with a capital letter ‘I’, ‘D’ or ‘Q’ for each line. ‘I’, followed by three integers C1, C2 and K( 0≤K≤1000), which means for camp C1, C2 and all camps on the path from C1 to C2, increase K soldiers to these camps. ‘D’, followed by three integers C1, C2 and K( 0≤K≤1000), which means for camp C1, C2 and all camps on the path from C1 to C2, decrease K soldiers to these camps. ‘Q’, followed by one integer C, which is a query and means Aragorn wants to know the number of enemies in camp C at that time. OutputFor each query, you need to output the actually number of enemies in the specified camp. Sample Input 3 2 51 2 32 12 3I 1 3 5Q 2D 1 2 2Q 1Q 3 Sample Output 7 48 Hint1.The number of enemies may be negative. 2.Huge input, be careful. 分析解1：动态树动态维护树中路径上点的边权值。 两个点之间的路径只要找到最近公共祖先即可 主要还是找LCA这块，动态树中的这个操作是改造一下access，然后注意各标志的下放。 教训跟上一次一样，也是TLE了很久，关键在于上组数据的状态量没有清理干净，重点是father和sons没有清空，结果就被坑了很久。 下次尤其注意！！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 3966************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;#define MAXN (int)5E4+10#define MAXM (int)1E5+10typedef struct nod&#123; int a,b;&#125; node;node edge[MAXM];bool op(node a,node b)&#123; if (a.a==b.a) return a.b&lt;b.b; else return a.a&lt;b.a;&#125;int sons[MAXN][2];int father[MAXN],size[MAXN],data[MAXN],change[MAXN];int start[MAXN],num[MAXN];bool root[MAXN];void bfs(int s)&#123; queue&lt;int&gt;q; q.push(s); root[s]=true; change[s]=0; while(!q.empty()) &#123; int now=q.front(); for (int i=0;i&lt;num[now];i++) if (!root[edge[start[now]+i].b]) &#123; father[edge[start[now]+i].b]=now; root[edge[start[now]+i].b]=true; change[edge[start[now]+i].b]=0; q.push(edge[start[now]+i].b); &#125; q.pop(); &#125;&#125;void down(int x)&#123; if (change[x]) &#123; data[x]+=change[x]; change[sons[x][0]]+=change[x]; change[sons[x][1]]+=change[x]; change[x]=0; &#125;&#125;void rotate(int x,int w) //rotate(node,0/1)&#123; int y=father[x]; down(y); down(x); sons[y][!w]=sons[x][w]; if (sons[x][w]) father[sons[x][w]]=y; father[x]=father[y]; if (father[y]&amp;&amp;(!root[y])) sons[father[y]][y==sons[father[y]][1]]=x; sons[x][w]=y; father[y]=x; if (root[y]) &#123; root[x]=true; root[y]=false; &#125;&#125;void splay(int x) //splay(node)&#123; down(x); while(!root[x]) &#123; if (root[father[x]]) rotate(x,x==sons[father[x]][0]); else &#123; int t=father[x]; int w=(sons[father[t]][0]==t); if (sons[t][w]==x) &#123; rotate(x,!w); rotate(x,w); &#125; else &#123; rotate(t,w); rotate(x,w); &#125; &#125; &#125;&#125;void splay____(int x) //splay(node)&#123; down(x); while(!root[x]) &#123; if (sons[father[x]][0]==x) rotate(x,1); else rotate(x,0); &#125;&#125;void access(int v)&#123; int u=v; v=0; while(u) &#123; splay(u); down(u); root[sons[u][1]]=true; sons[u][1]=v; root[v]=false; v=u; u=father[u]; &#125;&#125;void update(int v,int u,int k)&#123; access(v); v=0; while(u) &#123; splay(u); if (!father[u]) &#123; data[u]+=k; change[v]+=k; change[sons[u][1]]+=k; return; &#125; down(u); root[sons[u][1]]=true; sons[u][1]=v; root[v]=false; v=u; u=father[u]; &#125;&#125;int INT() &#123; char ch; int res; bool neg; while (ch = getchar(), !isdigit(ch) &amp;&amp; ch != '-') ; if (ch == '-') &#123; neg = true; res = 0; &#125; else &#123; neg = false; res = ch - '0'; &#125; while (ch = getchar(), isdigit(ch)) res = res * 10 + ch - '0'; return neg ? -res : res;&#125;char CHAR() &#123; char res; while (res = getchar(), !isalpha(res)) ; return res;&#125;int main()&#123;// freopen("3966.txt","r",stdin); int n,m,p; while(scanf("%d%d%d",&amp;n,&amp;m,&amp;p)!=EOF) &#123; memset(father,0,sizeof(father)); memset(sons,0,sizeof(sons)); for (int i=1;i&lt;=n;i++) data[i]=INT(); for (int i=1;i&lt;=m;i++) &#123; int a,b; a=INT(); b=INT(); edge[i*2].a=a; edge[i*2].b=b; edge[i*2-1].a=b; edge[i*2-1].b=a; &#125; m*=2; sort(&amp;edge[1],&amp;edge[m+1],op); memset(num,0,sizeof(num)); int o=-1; for (int i=1;i&lt;=m;i++) &#123; if (o!=edge[i].a) &#123; o=edge[i].a; start[o]=i; &#125; num[o]++; &#125; memset(root,0,sizeof(root)); bfs(1); for (int i=1;i&lt;=p;i++) &#123; char s; s=CHAR(); switch(s) &#123; case 'I': int c1,c2,k; c1=INT();c2=INT();k=INT(); update(c1,c2,k); break; case 'D': c1=INT();c2=INT();k=INT(); update(c1,c2,-k); break; case 'Q': int c; c=INT(); splay(c); printf("%d\n",data[c]); &#125; &#125; &#125; return 0;&#125; 解2：树链剖分由于这里的树结构是固定不变的，因此也可以使用树链剖分来做。 教训……DFS剖分爆栈的问题还是比较严重啊……T_T……也是第一次写，没经验，差错查了好久，用BFS代替之 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 3966_TreeCut************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN (int)5E4+10#define MAXM (int)1E5+10int n;int son[MAXN];int father[MAXN],size[MAXN],level[MAXN],data[MAXN],top[MAXN];int start[MAXN],num[MAXN];typedef struct nod&#123; int a,b;&#125; node;node edge[MAXM];bool op(node a,node b)&#123; if (a.a==b.a) return a.b&lt;b.b; else return a.a&lt;b.a;&#125;int lowbit(int s)&#123; return s&amp;(-s);&#125;int c[MAXN],pos[MAXN];int tot;void update(int x,int s) &#123; while (x&lt;=n) &#123; c[x]+=s; x+=lowbit(x); &#125;&#125;int sum(int x)&#123; int t=0; while (x&gt;0) &#123; t+=c[x]; x-=lowbit(x); &#125; return t;&#125;void dfs(int now,int front,int d)&#123; level[now]=d; father[now]=front; size[now]=1; for (int i=0;i&lt;num[now];i++) &#123; int temp=edge[start[now]+i].b; if (temp!=front) &#123; dfs(temp,now,d+1); size[now]+=size[temp]; if (son[now]==0||size[temp]&gt;size[son[now]]) son[now]=temp; &#125; &#125;&#125;int q[MAXN];void bfs()&#123; int head=1,tail=1; q[1]=1; level[1]=0; father[1]=0; while(head&lt;=tail) &#123; int now=q[head]; size[now]=1; for (int i=0;i&lt;num[now];i++) &#123; int temp=edge[start[now]+i].b; if (temp!=father[now]) &#123; father[temp]=now; level[temp]=level[now]+1; tail++; q[tail]=temp; &#125; &#125; head++; &#125; for (int i=n;i&gt;=1;i--) &#123; int now=q[i]; if (father[now]) &#123; size[father[now]]+=size[now]; if (son[father[now]]==0||size[now]&gt;size[son[father[now]]]) son[father[now]]=now; &#125; &#125; for (int i=1;i&lt;=n;i++) &#123; int now=q[i]; if (son[father[now]]==now) top[now]=top[father[now]]; else &#123; top[now]=now; while(now) &#123; tot++; pos[now]=tot; now=son[now]; &#125; &#125; &#125;&#125;void treecut(int now,int root)&#123; top[now]=root; tot++; pos[now]=tot; if (!son[now]) return ; treecut(son[now],root); for (int i=0;i&lt;num[now];i++) &#123; int temp=edge[start[now]+i].b; if (temp!=father[now]&amp;&amp;temp!=son[now]) treecut(temp,temp); &#125;&#125;void change(int x,int y,int value)&#123; while(top[x]!=top[y]) &#123; if (level[top[x]]&lt;level[top[y]]) swap(x,y); update(pos[top[x]],value); update(pos[x]+1,-value); x=father[top[x]]; &#125; if (level[x]&gt;level[y]) swap(x,y); update(pos[x],value); update(pos[y]+1,-value);&#125;int main()&#123; freopen("3966.txt","r",stdin); int m,p; while(scanf("%d%d%d",&amp;n,&amp;m,&amp;p)!=EOF) &#123; for (int i=1;i&lt;=n;i++) scanf("%d",&amp;data[i]); for (int i=1;i&lt;=m;i++) &#123; int a,b; scanf("%d%d",&amp;a,&amp;b); edge[i*2].a=a; edge[i*2].b=b; edge[i*2-1].a=b; edge[i*2-1].b=a; &#125; m*=2; sort(&amp;edge[1],&amp;edge[m+1],op); memset(num,0,sizeof(num)); int o=-1; for (int i=1;i&lt;=m;i++) &#123; if (o!=edge[i].a) &#123; o=edge[i].a; start[o]=i; &#125; num[o]++; &#125; memset(son,0,sizeof(son)); tot=0; //dfs(1,0,0); //treecut(1,1); bfs(); memset(c,0,sizeof(c)); for (int i=1;i&lt;=n;i++) &#123; update(pos[i],data[i]); update(pos[i]+1,-data[i]); &#125; for (int i=1;i&lt;=p;i++) &#123; char s; s=getchar(); while (s!='Q'&amp;&amp;s!='I'&amp;&amp;s!='D') s=getchar(); if (s=='Q') &#123; int cc; scanf("%d",&amp;cc); printf("%d\n",sum(pos[cc])); &#125; else &#123; int c1,c2,k; scanf("%d%d%d",&amp;c1,&amp;c2,&amp;k); if (s=='D') k=-k; change(c1,c2,k); &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>树链剖分</tag>
        <tag>动态树</tag>
        <tag>LCT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 2475 BOX 动态树 Link-Cut Tree]]></title>
    <url>%2F2015%2F04%2F25%2F2015-04-25-HDU-2475%2F</url>
    <content type="text"><![CDATA[BoxTime Limit: 10000/5000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionThere are N boxes on the ground, which are labeled by numbers from 1 to N. The boxes are magical, the size of each one can be enlarged or reduced arbitrarily. Jack can perform the “MOVE x y” operation to the boxes: take out box x; if y = 0, put it on the ground; Otherwise, put it inside box y. All the boxes inside box x remain the same. It is possible that an operation is illegal, that is, if box y is contained (directly or indirectly) by box x, or if y is equal to x. In the following picture, box 2 and 4 are directly inside box 6, box 3 is directly inside box 4, box 5 is directly inside box 1, box 1 and 6 are on the ground. The picture below shows the state after Jack performs “MOVE 4 1”: Then he performs “MOVE 3 0”, the state becomes: During a sequence of MOVE operations, Jack wants to know the root box of a specified box. The root box of box x is defined as the most outside box which contains box x. In the last picture, the root box of box 5 is box 1, and box 3’s root box is itself. InputInput contains several test cases. For each test case, the first line has an integer N (1 &lt;= N &lt;= 50000), representing the number of boxes. Next line has N integers: a1, a2, a3, … , aN (0 &lt;= ai &lt;= N), describing the initial state of the boxes. If ai is 0, box i is on the ground, it is not contained by any box; Otherwise, box i is directly inside box ai. It is guaranteed that the input state is always correct (No loop exists). Next line has an integer M (1 &lt;= M &lt;= 100000), representing the number of MOVE operations and queries. On the next M lines, each line contains a MOVE operation or a query: MOVE x y, 1 &lt;= x &lt;= N, 0 &lt;= y &lt;= N, which is described above. If an operation is illegal, just ignore it. QUERY x, 1 &lt;= x &lt;= N, output the root box of box x. OutputFor each query, output the result on a single line. Use a blank line to separate each test case. Sample Input 2 0 15 QUERY 1QUERY 2MOVE 2 0MOVE 1 2QUERY 16 0 6 4 6 1 04 MOVE 4 1QUERY 3MOVE 1 4QUERY 1 Sample Output 1 12 1 1 题意动态地维护一些盒子套盒子的操作，询问根。 分析盒子与盒子的关系可以直观地用树的结构来表示，一个结点下的子结点可以表示大盒子里面直接套着的小盒子。 所以本题就是一个裸的Link-Cut Tree模型了。 关于LCT树，还是推荐Yang Zhe的QTREE论文吧。 动态树是用访问操作来划分树链，对于每一条树链，使用Splay来维护，用深度作为splay的左右关系。 看了很多代码，觉得还是写不好，总觉得别人的用起来不顺，最后是在自己原来Splay的基础上改的。 原本的整棵树是个splay，但是在LCT中，整棵树是由很多棵分散的Splay组合起来的，于是在其中的一些点上加上root标记，表示以这一点为根下面可以形成一棵splay树。多个这样的splay组合完成之后就是一棵LCT了。 后面的代码中加入了输入输出挂。。。。。。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 2475************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 50010int sons[MAXN][2];int father[MAXN],pathfather[MAXN],data[MAXN];bool root[MAXN];int spttail=0;void rotate(int x,int w) //rotate(node,0/1)&#123; int y=father[x]; sons[y][!w]=sons[x][w]; if (sons[x][w]) father[sons[x][w]]=y; father[x]=father[y]; if (father[y]&amp;&amp;(!root[y])) sons[father[y]][y==sons[father[y]][1]]=x; sons[x][w]=y; father[y]=x; if (root[y]) &#123; root[x]=true; root[y]=false; &#125;&#125;void splay(int x) //splay(node)&#123; while(!root[x]) &#123; if (root[father[x]]) rotate(x,x==sons[father[x]][0]); else &#123; int t=father[x]; int w=(sons[father[t]][0]==t); if (sons[t][w]==x) &#123; rotate(x,!w); rotate(x,w); &#125; else &#123; rotate(t,w); rotate(x,w); &#125; &#125; &#125;&#125;void access(int v)&#123; int u=v; v=0; while(u) &#123; splay(u); root[sons[u][1]]=true; sons[u][1]=v; root[v]=false; v=u; u=father[u]; &#125;&#125;int findroot(int v)&#123; access(v); splay(v); while (sons[v][0]) v=sons[v][0]; //splay(v,0); return v;&#125;void cut(int v)&#123; access(v); splay(v); father[sons[v][0]]=0; root[sons[v][0]]=true; sons[v][0]=0;&#125;void join(int v,int w)&#123; if (!w) cut(v); else &#123; access(w); splay(w); int temp=v; while(!root[temp]) temp=father[temp]; if (temp!=w) &#123; cut(v); father[v]=w; &#125; &#125;&#125;int INT() &#123; char ch; int res; while (ch=getchar(),!isdigit(ch)); for (res = ch - '0';ch = getchar(),isdigit(ch);) res = res * 10 + ch - '0'; return res;&#125;char CHAR() &#123; char ch, res; while (res = getchar(), !isalpha(res)); while (ch = getchar(), isalpha(ch)); return res;&#125;int main()&#123; //freopen("2475.txt","r",stdin); int n; double flag=false; while(scanf("%d",&amp;n)!=EOF) &#123; if (flag) printf("\n"); flag=true; memset(father,0,sizeof(father)); memset(sons,0,sizeof(sons)); for (int i=1;i&lt;=n;i++) &#123; //scanf("%d",&amp;father[i]); father[i]=INT(); root[i]=true; &#125; int m; m=INT(); for (int i=1;i&lt;=m;i++) &#123; char s=CHAR(); if (s=='M') &#123; int x,y; x=INT(); y=INT(); join(x,y); &#125; else &#123; int q; q=INT(); printf("%d\n",findroot(q)); &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>动态树</tag>
        <tag>LCT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 3487 Play with Chain | Splay]]></title>
    <url>%2F2015%2F03%2F28%2F2015-03-28-HDU-3487%2F</url>
    <content type="text"><![CDATA[Play with ChainTime Limit: 6000/2000 MS (Java/Others) Memory Limit: 65536/32768 K (Java/Others) Problem DescriptionYaoYao is fond of playing his chains. He has a chain containing n diamonds on it. Diamonds are numbered from 1 to n. At first, the diamonds on the chain is a sequence: 1, 2, 3, …, n. He will perform two types of operations: CUT a b c: He will first cut down the chain from the ath diamond to the bth diamond. And then insert it after the cth diamond on the remaining chain. For example, if n=8, the chain is: 1 2 3 4 5 6 7 8; We perform “CUT 3 5 4”, Then we first cut down 3 4 5, and the remaining chain would be: 1 2 6 7 8. Then we insert “3 4 5” into the chain before 5th diamond, the chain turns out to be: 1 2 6 7 3 4 5 8. FLIP a b: We first cut down the chain from the ath diamond to the bth diamond. Then reverse the chain and put them back to the original position. For example, if we perform “FLIP 2 6” on the chain: 1 2 6 7 3 4 5 8. The chain will turn out to be: 1 4 3 7 6 2 5 8 He wants to know what the chain looks like after perform m operations. Could you help him? InputThere will be multiple test cases in a test data. For each test case, the first line contains two numbers: n and m (1≤n, m≤3*100000), indicating the total number of diamonds on the chain and the number of operations respectively. Then m lines follow, each line contains one operation. The command is like this: CUT a b c // Means a CUT operation, 1 ≤ a ≤ b ≤ n, 0≤ c ≤ n-(b-a+1). FLIP a b // Means a FLIP operation, 1 ≤ a &lt; b ≤ n. The input ends up with two negative numbers, which should not be processed as a case. OutputFor each test case, you should print a line with n numbers. The ith number is the number of the ith diamond on the chain. Sample Input 8 2CUT 3 5 4FLIP 2 6-1 -1 Sample Output 1 4 3 7 6 2 5 8 题意给出一列数，然后对整个数列执行两种操作：切下一段插入到另外的位置，或者把其中的一整段整个翻转一下。 求经过一系列操作之后，数列最后的样子。 分析数据范围最高能够到达3e5那么大，因此算法至少要是O(nlogn)复杂度以下才可能达到要求。 考虑采用Splay解决（这样的题目只能用这种动态维护的树结构不是么？） 初始先建树，把1~n加入Splay树。由于数列在后面是要被打乱顺序的，Splay二叉平衡树的性质只有在初始的时候是被保持的，之后是靠size，即每个点在中序遍历中的位置来维护。最后输出数列则只需要中序遍历一遍即可。 切割操作：若要切下ab段，则把第a-1个结点移到根，把第b+1个结点移到根以下（即跟的右子树），则整个ab段就落在b+1的左子树上，切出来。插入到c的时候，将c移到根，c+1移到根的右子树，则切出来的插入到c+1的左子树即可 翻转操作：用上面相同的方法把a~b整合到一棵子树上，然后可以参考线段树标记的方法，通过标记来完成访问结点的翻转等操作。 具体可以在纸上模拟一下…… 教训教训还是比较惨痛的…卡在这道题上好久了。 首先是输入输出以后要特别注意结尾方式，两个负数结尾还是两个-1结尾 把各种可能出现的不同情况考虑完整 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU3487************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 300010int sons[MAXN][2];int father[MAXN],size[MAXN],data[MAXN],list[MAXN];bool flag[MAXN];int spt=0,spttail=0;void down(int x)&#123; if (flag[x]) &#123; flag[x]=0; swap(sons[x][0],sons[x][1]); flag[sons[x][0]]^=1; flag[sons[x][1]]^=1; &#125;&#125;void rotate(int x,int w) //rotate(node,0/1)&#123; int y=father[x]; down(y);down(x); sons[y][!w]=sons[x][w]; if (sons[x][w]) father[sons[x][w]]=y; father[x]=father[y]; if (father[y]) sons[father[y]][y==sons[father[y]][1]]=x; sons[x][w]=y; father[y]=x; size[x]=size[y]; size[y]=size[sons[y][0]]+size[sons[y][1]]+1;&#125;void splay(int x,int y) //splay(node,position)&#123; down(x); while(father[x]!=y) &#123; if (father[father[x]]==y) rotate(x,x==sons[father[x]][0]); else &#123; int t=father[x]; int w=(sons[father[t]][0]==t); if (sons[t][w]==x) &#123; rotate(x,!w); rotate(x,w); &#125; else &#123; rotate(t,w); rotate(x,w); &#125; &#125; &#125; if (!y) spt=x;&#125;void select(int x,int v,int p) //select(root,k,position)&#123; down(x); while(v!=size[sons[x][0]]+1) &#123; if (v&lt;=size[sons[x][0]]) &#123; x=sons[x][0]; down(x); &#125; else &#123; v-=size[sons[x][0]]+1; x=sons[x][1]; down(x); &#125; &#125; splay(x,p);&#125;bool done=false;void outp(int x)&#123; down(x); if (sons[x][0]) outp(sons[x][0]); if (done) printf(" "); done=true; printf("%d",data[x]); if (sons[x][1]) outp(sons[x][1]);&#125;void maketree(int l,int r)&#123; spttail++; int now=spttail,w=(l+r)/2,ls=0,rs=0; data[now]=w; flag[now]=false; sons[now][0]=0; sons[now][1]=0; if (l&lt;=w-1) &#123; ls=spttail+1; sons[now][0]=ls; father[ls]=now; maketree(l,w-1); &#125; if (w+1&lt;=r) &#123; rs=spttail+1; sons[now][1]=rs; father[rs]=now; maketree(w+1,r); &#125; size[now]=size[ls]+size[rs]+1;&#125;int main()&#123; freopen("3487.txt","r",stdin); int n,m; scanf("%d%d",&amp;n,&amp;m); while(!(n&lt;0&amp;&amp;m&lt;0)) &#123; spt=1; spttail=0; father[1]=0; maketree(1,n); for (int i=1;i&lt;=m;i++) &#123; char s[10]; scanf("%s",&amp;s); if (s[0]=='C') &#123; int a,b,c,temp; scanf("%d%d%d",&amp;a,&amp;b,&amp;c); if (a&gt;1) &#123; select(spt,a-1,0); if (b&lt;n) &#123; select(spt,b+1,spt); temp=sons[sons[spt][1]][0]; sons[sons[spt][1]][0]=0; size[spt]-=size[temp]; size[sons[spt][1]]-=size[temp]; &#125; else &#123; temp=sons[spt][1]; sons[spt][1]=0; size[spt]-=size[temp]; &#125; &#125; else &#123; if (b&lt;n) &#123; select(spt,b+1,0); temp=sons[spt][0]; sons[spt][0]=0; size[spt]-=size[temp]; &#125; else temp=spt; &#125; if (c&gt;0) &#123; select(spt,c,0); if (c==size[spt]) &#123; sons[spt][1]=temp; father[temp]=spt; size[spt]+=size[temp]; &#125; else &#123; select(spt,c+1,spt); sons[sons[spt][1]][0]=temp; father[temp]=sons[spt][1]; size[spt]+=size[temp]; size[sons[spt][1]]+=size[temp]; &#125; &#125; else &#123; if (spt!=temp) &#123; select(spt,1,0); sons[spt][0]=temp; father[temp]=spt; size[spt]+=size[temp]; &#125; &#125; &#125; else &#123; int a,b,temp; scanf("%d%d",&amp;a,&amp;b); if (a&gt;1) &#123; select(spt,a-1,0); if (b&lt;n) &#123; select(spt,b+1,spt); temp=sons[sons[spt][1]][0]; &#125; else &#123; temp=sons[spt][1]; &#125; &#125; else &#123; if (b&lt;n) &#123; select(spt,b+1,0); temp=sons[spt][0]; &#125; else temp=spt; &#125; flag[temp]^=1; &#125; &#125; done=false; outp(spt); printf("\n"); scanf("%d%d",&amp;n,&amp;m); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>平衡树</tag>
        <tag>Splay</tag>
        <tag>伸展树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 3726 Graph and Queries 平衡树+前向星+并查集+离线操作+逆向思维 数据结构大综合题]]></title>
    <url>%2F2015%2F03%2F23%2F2015-03-23-HDU-3726-Graph-and-Queries%2F</url>
    <content type="text"><![CDATA[Graph and QueriesTime Limit: 10000/5000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionYou are given an undirected graph with N vertexes and M edges. Every vertex in this graph has an integer value assigned to it at the beginning. You’re also given a sequence of operations and you need to process them as requested. Here’s a list of the possible operations that you might encounter: 1) Deletes an edge from the graph. The format is [D X], where X is an integer from 1 to M, indicating the ID of the edge that you should delete. It is guaranteed that no edge will be deleted more than once. 2) Queries the weight of the vertex with K-th maximum value among all vertexes currently connected with vertex X (including X itself). The format is [Q X K], where X is an integer from 1 to N, indicating the id of the vertex, and you may assume that K will always fit into a 32-bit signed integer. In case K is illegal, the value for that query will be considered as undefined, and you should return 0 as the answer to that query. 3) Changes the weight of a vertex. The format is [C X V], where X is an integer from 1 to N, and V is an integer within the range [-106, 106]. The operations end with one single character, E, which indicates that the current case has ended. For simplicity, you only need to output one real number - the average answer of all queries. InputThere are multiple test cases in the input file. Each case starts with two integers N and M (1 &lt;= N &lt;= 2 * 104, 0 &lt;= M &lt;= 6 * 104), the number of vertexes in the graph. The next N lines describes the initial weight of each vertex (-106 &lt;= weight[i] &lt;= 106). The next part of each test case describes the edges in the graph at the beginning. Vertexes are numbered from 1 to N. The last part of each test case describes the operations to be performed on the graph. It is guaranteed that the number of query operations [Q X K] in each case will be in the range [1, 2 * 105], and there will be no more than 2 * 105 operations that change the values of the vertexes [C X V].There will be a blank line between two successive cases. A case with N = 0, M = 0 indicates the end of the input file and this case should not be processed by your program. OutputFor each test case, output one real number – the average answer of all queries, in the format as indicated in the sample output. Please note that the result is rounded to six decimal places. Sample Input 3 201 23 3D 3Q 1 2Q 2 1D 2Q 3 2C 1 50Q 1 1E 3201 23 3Q 1 1Q 1 2Q 1 3E 0 Sample Output Case 1: 25.000000Case 2: 16.666667 【Hint】 For the first sample:D 3 – deletes the 3rd edge in the graph (the remaining edges are (1, 2) and (2, 3))Q 1 2 – finds the vertex with the second largest value among all vertexes connected with 1. The answer is 20.Q 2 1 – finds the vertex with the largest value among all vertexes connected with 2. The answer is 30.D 2 – deletes the 2nd edge in the graph (the only edge left after this operation is (1, 2))Q 3 2 – finds the vertex with the second largest value among all vertexes connected with 3. The answer is 0 (Undefined).C 1 50 – changes the value of vertex 1 to 50.Q 1 1 – finds the vertex with the largest value among all vertex connected with 1. The answer is 50.E – This is the end of the current test case. Four queries have been evaluated, and the answer to this case is (20 + 30 + 0 + 50) / 4 = 25.000. For the second sample, caution about the vertex with same weight:Q 1 1 – the answer is 20Q 1 2 – the answer is 20Q 1 3 – the answer is 10 题意给出一张无向图，并在图中进行多种操作： 1.删除一条边；2.改变一个点的权值；3.询问x能够到达的所有点中，第k大的是多少 分析花费了好多时间在这道题上，算是这段时间中做到的最综合的数据结构题了。 首先本题的无向图一开始就是个陷阱，如果单纯地从图的角度来考虑，每次询问都需要遍历全图来找第k大的值，这显然是不可取的，而中间又存在删边操作，图的连通性不是稳定的，结点的权值会变，而且可能多次改变，所以整个图是完全不稳定的，没办法用图论的方法来解决。 考虑倒过来操作，如果我们从做完所有操作之后最后的结果图出发，逆序回去，则原本的删边可以看成是将两个连通块连接到一起，询问第k值是在x点当前所属的连通块中进行，对点权的修改也是，而对于每一个独立的连通块，最后这两步可以用平衡树来实现。 所以算法的雏形就有了，询问连通块k值、修改连通块中点的点权操作——平衡树，维护点的连通性——并查集，保存点权的修改信息——前向星 完整过程： 1.首先从完整图出发，读入所有操作，记录删掉的边，按顺序记录点权的变化情况，记录其他信息； 2.用删边信息建立终图的连通性，并查集维护，对于每一个独立的连通块，建立一个独立的平衡树（这里我用的是SBT，网上题解搜出来好多人用的Splay，我其实有点不太理解，感觉这里没有需要用到Splay特殊结构的地方，单纯的维护平衡树的话Splay的稳定性和效率应该是不如SBT的。有大神路过看到这个的话，希望能交流下~）； 3.从最后一条操作开始逆序回来： i. 询问，则在x所属的平衡树中找第k值； ii. 修改，则在x所属的平衡树中删掉原始的值，插入新值，这里对点权的顺序维护我用了前向星，要保证点权的操作也是要有序的； iii.删边，在这里就是如果两个点属于两个不同的连通块，则将两个连通块连起来，并查集合并，同时平衡树合并。平衡树合并的时候只能把小的那棵树一个一个加到大的树中去，貌似Splay有个启发式合并，用了finger search神马的东西，可以把合并在O（nlogn）内完成，不会写，ORZ。 后记写这道题的时候，SBT模板改了两次，-_-///，然后中间有SBT结合并查集结合前向星的，代码里面就是数组套数组套数组套数组……好多地方写着写着就写乱了，教训是如果能简单，一定不要往复杂了写。 然后并查集的教训：father[]绝对不能直接引用，必须调用getfather（） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU3726************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 20010typedef struct enod&#123; int p,q; bool enable;&#125; enode;enode e[60010];typedef struct qnod&#123; int x,k;&#125; qnode;qnode lisq[200010];typedef struct nod&#123; int no,value,time;&#125; node;node lis[300010];int sons[MAXN][2],size[MAXN],data[MAXN],sbt[MAXN],sbttail;int lisd[60010],taild,tailq,tail,tailtot,lisc[200010],tailc=0;int start[MAXN],num[MAXN],father[MAXN];char listot[500010];void rotate(int &amp;t,int w) //rotate(&amp;node,0/1)&#123; int k=sons[t][1-w]; if (!k) return ; sons[t][1-w]=sons[k][w]; sons[k][w]=t; size[k]=size[t]; size[t]=size[sons[t][0]]+size[sons[t][1]]+1; t=k;&#125;void maintain(int&amp; t,bool flag) //maintain(&amp;node,flag)&#123; if (!t) return ; if (!flag) if (size[sons[sons[t][0]][0]]&gt;size[sons[t][1]]) rotate(t,1); else if (size[sons[sons[t][0]][1]]&gt;size[sons[t][1]]) &#123; rotate(sons[t][0],0); rotate(t,1); &#125; else return ; else if (size[sons[sons[t][1]][1]]&gt;size[sons[t][0]]) rotate(t,0); else if (size[sons[sons[t][1]][0]]&gt;size[sons[t][0]]) &#123; rotate(sons[t][1],1); rotate(t,0); &#125; else return ; maintain(sons[t][0],false); maintain(sons[t][1],true); maintain(t,false); maintain(t,true);&#125;void insert(int&amp; t,int v,int pos) //insert(&amp;root,value)&#123; if (!size[t]) &#123; if (!pos) &#123; sbttail++; pos=sbttail; &#125; data[pos]=v; size[pos]=1; sons[pos][0]=0; sons[pos][1]=0; t=pos; &#125; else &#123; size[t]++; if (v&lt;data[t]) insert(sons[t][0],v,pos); else insert(sons[t][1],v,pos); maintain(t,v&gt;=data[t]); &#125;&#125;int last;int del(int&amp; t,int v) //node=del(&amp;root,key)&#123; size[t]--; if (v==data[t]||(v&lt;data[t]&amp;&amp;sons[t][0]==0)||(v&gt;data[t]&amp;&amp;sons[t][1]==0)) &#123; int ret=data[t]; if (sons[t][0]==0||sons[t][1]==0) &#123; last=t; t=sons[t][1]+sons[t][0]; &#125; else data[t]=del(sons[t][0],data[t]+1); return ret; &#125; else if (v&lt;data[t]) return del(sons[t][0],v); else return del(sons[t][1],v);&#125;int select(int t,int k)&#123; if (k==size[sons[t][0]]+1) return t; if (k&lt;=size[sons[t][0]]) return select(sons[t][0],k); else return select(sons[t][1],k-1-size[sons[t][0]]);&#125;void clean_father(int n)&#123; for (int i=1;i&lt;=n;i++) &#123; father[i]=i; sbt[i]=i; &#125;&#125; int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link(int x,int y)&#123; int xx=getfather(x),yy=getfather(y); if (size[sbt[xx]]&gt;size[sbt[yy]]) &#123; father[yy]=xx; while(size[sbt[yy]]&gt;0) &#123; int temp=del(sbt[yy],data[sbt[yy]]); insert(sbt[xx],temp,last); &#125; &#125; else &#123; father[xx]=yy; while(size[sbt[xx]]&gt;0) &#123; int temp=del(sbt[xx],data[sbt[xx]]); insert(sbt[yy],temp,last); &#125; &#125;&#125;bool op(node a,node b)&#123; if (a.no==b.no) return a.time&lt;b.time; else return a.no&lt;b.no;&#125;int main()&#123; freopen("3726.txt","r",stdin); int n,m,tt=0; while(scanf("%d%d",&amp;n,&amp;m)==2&amp;&amp;(n+m&gt;0)) &#123; for (int i=1;i&lt;=n;i++) &#123; lis[i].no=i; lis[i].time=i; scanf("%d",&amp;lis[i].value); &#125; for (int i=1;i&lt;=m;i++) &#123; scanf("%d%d",&amp;e[i].p,&amp;e[i].q); e[i].enable=true; &#125; taild=0;tailq=0;tailc=0;tail=n;tailtot=0; bool doit=true; while(doit) &#123; char c=getchar(); while(c!='D'&amp;&amp;c!='Q'&amp;&amp;c!='C'&amp;&amp;c!='E') c=getchar(); tailtot++; listot[tailtot]=c; switch(c) &#123; case 'D': taild++; scanf("%d",&amp;lisd[taild]); e[lisd[taild]].enable=false; break; case 'Q': tailq++; scanf("%d%d",&amp;lisq[tailq].x,&amp;lisq[tailq].k); break; case 'C': tail++; lis[tail].time=tail; scanf("%d%d",&amp;lis[tail].no,&amp;lis[tail].value); tailc++; lisc[tailc]=lis[tail].no; break; default: doit=false; &#125; &#125; sort(&amp;lis[1],&amp;lis[tail+1],op); int o=0; memset(num,0,sizeof(num)); for (int i=1;i&lt;=tail;i++) &#123; if (o!=lis[i].no) &#123; o=lis[i].no; start[o]=i; &#125; num[o]++; &#125; clean_father(n); sbttail=0; memset(size,0,sizeof(size)); for (int i=1;i&lt;=n;i++) insert(sbt[i],lis[start[i]+num[i]-1].value,0); for (int i=1;i&lt;=m;i++) if (e[i].enable) if (getfather(e[i].p)!=getfather(e[i].q)) link(e[i].p,e[i].q); int ansq=tailq; double ans=0; for (int i=tailtot-1;i&gt;=1;i--) switch(listot[i]) &#123; case 'Q': if (lisq[tailq].k&gt;0&amp;&amp;size[sbt[getfather(lisq[tailq].x)]]&gt;=lisq[tailq].k) ans+=data[select(sbt[getfather(lisq[tailq].x)],size[sbt[getfather(lisq[tailq].x)]]-lisq[tailq].k+1)]; tailq--; break; case 'D': if (getfather(e[lisd[taild]].p)!=getfather(e[lisd[taild]].q)) link(e[lisd[taild]].p,e[lisd[taild]].q); taild--; break; case 'C': num[lisc[tailc]]--; del(sbt[getfather(lisc[tailc])],lis[start[lisc[tailc]]+num[lisc[tailc]]].value); insert(sbt[getfather(lisc[tailc])],lis[start[lisc[tailc]]+num[lisc[tailc]]-1].value,last); tailc--; &#125; tt++; printf("Case %d: %.6f\n",tt,ans/ansq); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>SBT</tag>
        <tag>平衡树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VIJOS P1081 野生动物园 SBT、划分树模板]]></title>
    <url>%2F2015%2F03%2F16%2F2015-03-16-vijos-p1081%2F</url>
    <content type="text"><![CDATA[野生动物园描述cjBBteam拥有一个很大的野生动物园。这个动物园坐落在一个狭长的山谷内，这个区域从南到北被划分成N个区域，每个区域都饲养着一头狮子。这些狮子从北到南编号为1,2,3,…,N。每头狮子都有一个觅食能力值Ai，Ai越小觅食能力越强。饲养员cmdButtons决定对狮子进行M次投喂，每次投喂都选择一个区间[I,J]，从中选取觅食能力值第K强的狮子进行投喂。值得注意的是，cmdButtons不愿意对某些区域进行过多的投喂，他认为这样有悖公平。因此cmdButtons的投喂区间是互不包含的。你的任务就是算出每次投喂后，食物被哪头狮子吃掉了。 输入格式输入第一行有两个数N和M。此后一行有N个数，从南到北描述狮子的觅食能力值。此后M行，每行描述一次投喂。第t+2的三个数I,J,K表示在第t次投喂中，cmdButtons选择了区间[I,J]内觅食能力值第K强的狮子进行投喂。 输出格式输出有M行，每行一个整数。第i行的整数表示在第i次投喂中吃到食物的狮子的觅食能力值。 样例输入 7 21 5 2 6 3 7 41 5 32 7 1 样例输出 3 2 分析解法一、平衡树由题目给出的区间互相不包含可以得出，若将每次询问的区间按照起始区域进行排序，那一定是一段接一段，只有可能是两种情况： 下一段的左端与上一段的右端不相交或者相交。 这两种情况都是前面的数据与后面的数据互不影响，因此将区间排序之后，对于每一个区间，删除掉前面多余的，插入后面不够的，使平衡树中仅留下该区间中的数据，然后直接找第k小即可。 SBT可解。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160/* ***********************************************MYID : Chen FanLANG : G++PROG : VIJOS1081************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 100010int sons[MAXN][2];int size[MAXN],data[MAXN];int sbt=0,sbttail=0;void rotate(int &amp;t,int w) //rotate(&amp;node,0/1)&#123; int k=sons[t][1-w]; if (!k) return ; sons[t][1-w]=sons[k][w]; sons[k][w]=t; size[k]=size[t]; size[t]=size[sons[t][0]]+size[sons[t][1]]+1; t=k;&#125;void maintain(int&amp; t,bool flag) //maintain(&amp;node,flag)&#123; if (!t) return ; if (!flag) if (size[sons[sons[t][0]][0]]&gt;size[sons[t][1]]) rotate(t,1); else if (size[sons[sons[t][0]][1]]&gt;size[sons[t][1]]) &#123; rotate(sons[t][0],0); rotate(t,1); &#125; else return ; else if (size[sons[sons[t][1]][1]]&gt;size[sons[t][0]]) rotate(t,0); else if (size[sons[sons[t][1]][0]]&gt;size[sons[t][0]]) &#123; rotate(sons[t][1],1); rotate(t,0); &#125; else return ; maintain(sons[t][0],false); maintain(sons[t][1],true); maintain(t,false); maintain(t,true);&#125;void insert(int&amp; t,int v) //insert(&amp;root,0,value)&#123; if (!t) &#123; sbttail++; data[sbttail]=v; size[sbttail]=1; sons[sbttail][0]=0; sons[sbttail][1]=0; t=sbttail; &#125; else &#123; size[t]++; if (v&lt;data[t]) insert(sons[t][0],v); else insert(sons[t][1],v); maintain(t,v&gt;=data[t]); &#125;&#125;int del(int&amp; t,int v) //del(&amp;root,key)&#123; size[t]--; if (v==data[t]||(v&lt;data[t]&amp;&amp;sons[t][0]==0)||(v&gt;data[t]&amp;&amp;sons[t][1]==0)) &#123; int ret=data[t]; if (sons[t][0]==0||sons[t][1]==0) t=sons[t][1]+sons[t][0]; else data[t]=del(sons[t][0],data[t]+1); return ret; &#125; else if (v&lt;data[t]) return del(sons[t][0],v); else return del(sons[t][1],v);&#125;int select(int t,int k)&#123; if (k==size[sons[t][0]]+1) return t; if (k&lt;=size[sons[t][0]]) return select(sons[t][0],k); else return select(sons[t][1],k-1-size[sons[t][0]]);&#125;typedef struct nod&#123; int i,l,r,k;&#125; node;node d[50010];bool op(node a,node b)&#123; if (a.l==b.l) return a.r&lt;b.r; else return a.l&lt;b.l;&#125;int a[MAXN];typedef struct nod1&#123; int i,ans;&#125; node1;node1 out[50010];bool op1(node1 a,node1 b)&#123; return a.i&lt;b.i;&#125;int main()&#123; freopen("1.txt","r",stdin); sbt=0,sbttail=0; int n,m; scanf("%d%d",&amp;n,&amp;m); for (int i=1;i&lt;=n;i++) scanf("%d",&amp;a[i]); for (int i=1;i&lt;=m;i++) &#123; scanf("%d%d%d",&amp;d[i].l,&amp;d[i].r,&amp;d[i].k); d[i].i=i; &#125; sort(&amp;d[1],&amp;d[m+1],op); int l=0,r=0; for (int i=1;i&lt;=m;i++) &#123; if (r&lt;d[i].l) &#123; sbt=0; sbttail=0; for (int j=d[i].l;j&lt;=d[i].r;j++) insert(sbt,a[j]); &#125; else &#123; for (int j=l;j&lt;d[i].l;j++) del(sbt,a[j]); for (int j=r+1;j&lt;=d[i].r;j++) insert(sbt,a[j]); &#125; l=d[i].l; r=d[i].r; int temp=select(sbt,d[i].k); out[i].i=d[i].i; out[i].ans=data[temp]; &#125; sort(&amp;out[1],&amp;out[m+1],op1); for (int i=1;i&lt;=m;i++) printf("%d\n",out[i].ans); return 0;&#125; 解法二、划分树划分树是一种类似快排的数据结构，可以快速在O（logn）的时间内直接求出某个区间内的k值。 然后本题就是……一棵裸的划分树，直接套即可 。。。。。。最后的结果是，不知道为什么比SBT要慢很多，直观的感觉上划分树没有多余的删除操作，应该会快很多的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/* ***********************************************MYID : Chen FanLANG : G++PROG : VIJOS1081_SortTree************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 100010int a[MAXN],dp[20][MAXN],tree[20][MAXN];void maketree(int c,int l,int r)&#123; int mid=(l+r)/2,ls=l,rs=mid+1,num=0; for (int i=mid;i&gt;=l&amp;&amp;a[i]==a[mid];i--) num++; for (int i=l;i&lt;=r;i++) &#123; if (i==l) dp[c][i]=0; else dp[c][i]=dp[c][i-1]; if (tree[c][i]&lt;a[mid]) &#123; dp[c][i]++; tree[c+1][ls]=tree[c][i]; ls++; &#125; else if (tree[c][i]&gt;a[mid]) &#123; tree[c+1][rs]=tree[c][i]; rs++; &#125; else &#123; if (num) &#123; num--; dp[c][i]++; tree[c+1][ls]=tree[c][i]; ls++; &#125; else &#123; tree[c+1][rs]=tree[c][i]; rs++; &#125; &#125; &#125; if (l==r) return ; maketree(c+1,l,mid); maketree(c+1,mid+1,r);&#125;int query(int c,int l,int r,int ql,int qr,int k)&#123; if (l==r) return tree[c][l]; int s,ss,mid=(l+r)/2; if (l==ql) &#123; s=0; ss=dp[c][qr]; &#125; else &#123; s=dp[c][ql-1]; ss=dp[c][qr]-s; &#125; if (k&lt;=ss) return query(c+1,l,mid,l+s,l+s+ss-1,k); else return query(c+1,mid+1,r,mid-l+1+ql-s,mid-l+1+qr-s-ss,k-ss);&#125;int main()&#123; //freopen("1.in","r",stdin); //freopen("zoo8.in","r",stdin); //freopen("1.out","w",stdout); int n,m; scanf("%d%d",&amp;n,&amp;m); for (int i=1;i&lt;=n;i++) &#123; scanf("%d",&amp;a[i]); tree[0][i]=a[i]; &#125; sort(&amp;a[1],&amp;a[n+1]); maketree(0,1,n); for (int i=1;i&lt;=m;i++) &#123; int l,r,k; scanf("%d%d%d",&amp;l,&amp;r,&amp;k); printf("%d\n",query(0,1,n,l,r,k)); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>SBT</tag>
        <tag>平衡树</tag>
        <tag>划分树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VIJOS P1647 不差钱 SBT]]></title>
    <url>%2F2015%2F03%2F15%2F2015-03-15-vijos-p1647%2F</url>
    <content type="text"><![CDATA[不差钱描述同学们一起看了小品《不差钱》，LX神突发奇想，想刁难一下十八居士，他让十八居士模拟一下点菜的过程。 输入格式输入第一行为一个数price，表示价钱大于price的菜赵本山都不要。 以下几行表示点菜的过程，每行两个整数p，n p=1 表示在菜谱中添加一个价格为n的菜,这是第i个1号命令，这个菜的编号就是i， p=2 表示菜谱中第n号菜已卖完（但不代表菜谱中没有了这种菜）， p=3 表示赵本山点第n贵的菜。 输入文件以0结束。 菜的价格0&lt;n&lt;=10^6。 3种命令， 30%数据命令最多300次， 60%数据命令最多3000次， 100%数据命令最多100000次。 输出格式对于每个p=3， 如果第n贵的菜价格高于price，则输出“Dui bu qi,Mei you.”。 如果第n贵的菜价格不高于price，且没有卖完，则输出“You.”然后输出价格” m Yuan.”； 如果已卖完，则输出“Mei you. Zhe ge ke yi you. Zhe ge zhen mei you!” 输入样例 401 411 391 1001 2041 11 271 181 793 13 23 52 53 82 73 71 103 80 输出样例 Dui bu qi,Mei you.Dui bu qi,Mei you.You. 39 Yuan.Mei you. Zhe ge ke yi you. Zhe ge zhen mei you! Mei you. Zhe ge ke yi you. Zhe ge zhen mei you! You. 10 Yuan. 分析题目意思表达得比较明确，本题也很适合作为SBT的模板题。 菜的编号就是存在SBT静态数组中的下标，只要加一个是否empty的标记即可。 唯一稍微可能有点问题的是出现多个相同菜价的菜时的选择问题，一开始按照普通二叉树的写法写SBT的前趋后继，后来发现这样做其实是不严谨的，因为左右旋操作的存在，虽然一开始相等的数是插入到右边去，但是不能保证不会因为左旋而把父节点旋到左子树去了，所以最后只能保证左子树的值不大于根，右子树的值不小于根，相等值是完全没办法判断的。 可能还是我的SBT写法不够严谨 ……最后只好继续查找第n-1大、n-2大…第n+1大、第n+2大…直到找到不相等为止，O（k*logn）遍历一遍相等的数。 好在测试时间上好像还不错，不知道又没有更好的办法？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161/* ***********************************************MYID : Chen FanLANG : G++PROG : VIJOS1647************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 100010typedef struct sbtnod&#123; int key,left,right,size; bool empty;&#125; sbtnode;int sbttail,sbt;sbtnode tree[MAXN];void rrotate(int&amp; t)&#123; int k=tree[t].left; if (!k) return ; tree[t].left=tree[k].right; tree[k].right=t; tree[k].size=tree[t].size; tree[t].size=tree[tree[t].left].size+tree[tree[t].right].size+1; t=k;&#125;void lrotate(int&amp; t)&#123; int k=tree[t].right; if (!k) return ; tree[t].right=tree[k].left; tree[k].left=t; tree[k].size=tree[t].size; tree[t].size=tree[tree[t].left].size+tree[tree[t].right].size+1; t=k;&#125;void maintain(int&amp; t,bool flag)&#123; if (!t) return ; if (!flag) if (tree[tree[tree[t].left].left].size&gt;tree[tree[t].right].size) rrotate(t); else if (tree[tree[tree[t].left].right].size&gt;tree[tree[t].right].size) &#123; lrotate(tree[t].left); rrotate(t); &#125; else return ; else if (tree[tree[tree[t].right].right].size&gt;tree[tree[t].left].size) lrotate(t); else if (tree[tree[tree[t].right].left].size&gt;tree[tree[t].left].size) &#123; rrotate(tree[t].right); lrotate(t); &#125; else return ; maintain(tree[t].left,false); maintain(tree[t].right,true); maintain(t,false); maintain(t,true);&#125;void insert(int&amp; t,int v)&#123; if (!t) &#123; sbttail++; tree[sbttail].key=v; tree[sbttail].size=1; tree[sbttail].empty=false; t=sbttail; &#125; else &#123; tree[t].size++; if (v&lt;tree[t].key) insert(tree[t].left,v); else insert(tree[t].right,v); maintain(t,v&gt;=tree[t].key); &#125;&#125;int select(int t,int k)&#123; if (k==tree[tree[t].left].size+1) return t; if (k&lt;=tree[tree[t].left].size) return select(tree[t].left,k); else return select(tree[t].right,k-1-tree[tree[t].left].size);&#125;int main()&#123; freopen("1.txt","r",stdin); int pri; scanf("%d",&amp;pri); sbt=0; sbttail=0; int p,n; while(scanf("%d%d",&amp;p,&amp;n)==2) &#123; switch(p) &#123; case 1: insert(sbt,n); break; case 2: tree[n].empty=true; break; case 3: n=sbttail-n+1; int temp=select(sbt,n); if (tree[temp].key&gt;pri) printf("Dui bu qi,Mei you.\n"); else &#123; bool done=false; if (!tree[temp].empty) &#123; done=true; printf("You. %d Yuan.\n",tree[temp].key); &#125; else &#123; int pre,p=n; while(p&gt;1&amp;&amp;(!done)) &#123; p--; pre=select(sbt,p); if (tree[pre].key&lt;tree[temp].key) break; if (!tree[pre].empty) &#123; done=true; printf("You. %d Yuan.\n",tree[temp].key); &#125; &#125; int suc,s=n; while(s&lt;n&amp;&amp;(!done)) &#123; s++; suc=select(sbt,s); if (tree[suc].key&gt;tree[temp].key) break; if (!tree[suc].empty) &#123; done=true; printf("You. %d Yuan.\n",tree[temp].key); &#125; &#125; &#125; if (!done) printf("Mei you. Zhe ge ke yi you. Zhe ge zhen mei you!\n"); &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>SBT</tag>
        <tag>平衡树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 1890 Robotic Sort | Splay]]></title>
    <url>%2F2015%2F03%2F13%2F2015-03-13-HDU-1890%2F</url>
    <content type="text"><![CDATA[Robotic SortTime Limit: 6000/2000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionSomewhere deep in the Czech Technical University buildings, there are laboratories for examining mechanical and electrical properties of various materials. In one of yesterday’s presentations, you have seen how was one of the laboratories changed into a new multimedia lab. But there are still others, serving to their original purposes. In this task, you are to write software for a robot that handles samples in such a laboratory. Imagine there are material samples lined up on a running belt. The samples have different heights, which may cause troubles to the next processing unit. To eliminate such troubles, we need to sort the samples by their height into the ascending order. Reordering is done by a mechanical robot arm, which is able to pick up any number of consecutive samples and turn them round, such that their mutual order is reversed. In other words, one robot operation can reverse the order of samples on positions between A and B. A possible way to sort the samples is to find the position of the smallest one (P1) and reverse the order between positions 1 and P1, which causes the smallest sample to become first. Then we find the second one on position P and reverse the order between 2 and P2. Then the third sample is located etc. The picture shows a simple example of 6 samples. The smallest one is on the 4th position, therefore, the robot arm reverses the first 4 samples. The second smallest sample is the last one, so the next robot operation will reverse the order of five samples on positions 2–6. The third step will be to reverse the samples 3–4, etc. Your task is to find the correct sequence of reversal operations that will sort the samples using the above algorithm. If there are more samples with the same height, their mutual order must be preserved: the one that was given first in the initial order must be placed before the others in the final order too. InputThe input consists of several scenarios. Each scenario is described by two lines. The first line contains one integer number N , the number of samples, 1 ≤ N ≤ 100 000. The second line lists exactly N space-separated positive integers, they specify the heights of individual samples and their initial order. The last scenario is followed by a line containing zero. OutputFor each scenario, output one line with exactly N integers P1 , P1 , . . . PN ,separated by a space. Each Pi must be an integer (1 ≤ Pi ≤ N ) giving the position of the i-th sample just before the i-th reversal operation. Note that if a sample is already on its correct position Pi , you should output the number Pi anyway, indicating that the “interval between Pi and Pi ” (a single sample) should be reversed. Sample Input 6 3 4 5 1 6 24 3 3 2 10 Sample Output 4 6 4 5 6 64 2 4 4 题意机械臂可以翻转整个数列中连续的某一串序列，要求用这种翻转的方式完成整个数列的排序。 分析排序的方式题目已经给出了，只需要按照要求去模拟即可，而模拟这个过程的方法很重要，时间上还是要求比较严格的。 获取数据之后首先排序一遍，记录下排完序之后每个点原本的位置，这个就是等一下在树中进行操作时需要用到的了。 题目要求的翻转方式是每次刚好将第i个翻转到位，每次翻转之后固定一个数，然后翻转下一个数，直到翻转完全部的数。 只要用splay将每次的目标结点移到根，那么其左子树的大小就是数列中排在它左边的点的个数，也就是题目要求输出的部分。然后翻转整个左子树（将子树的左右儿子交换，递归到底），删除根，就完成了一次操作。 思路很清晰，但是这里遇到的问题是如何翻转左子树，如果每次都手动递归向下翻转所有树的话，时间上必然是要超时的。从线段树中启发，可以用标记的方式标记翻转情况，减少直接操作的次数，用传递标记的方式传下去： 输出结果后，将左子树打上翻转标记，然后删除根。 一个结点翻转标记的传递必须要发生在对其子树进行操作之前，本题中，后续直接操作子树的过程是接下来的删根操作（我的代码中要从左右子树中找前后继）和下一轮将下一个目标移到根的过程。所以要在这两个过程前加入维护的过程。 删根中找前后继因为是从上往下的，刚好可以顺便查看标记情况对子树完成翻转。问题是下一次提升结点到根的时候不能确定上方父节点的标记情况，因此只能从当前结点开始向上查到根，然后从根开始向下回到目标结点，一路查看标记完成翻转维护……这一块我只能想出这个方法了，不知道其他大神有没有什么更好的方法。这里用递归反查结果栈溢出了，只好开了个大数组记录，，，最后提交AC的通过时间也比较长，应该确实是还有更好的方法吧 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU1890************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct nod&#123; int key,num;&#125; node;#define MAXN 100010node a[MAXN];int list[MAXN];bool op(node a,node b)&#123; if (a.key==b.key) return a.num&lt;b.num; else return a.key&lt;b.key;&#125;int sons[MAXN][2];int father[MAXN],size[MAXN],data[MAXN];bool flag[MAXN];int spt=0,spttail=0;void rotate(int x,int w) //rotate(node,0/1)&#123; int y=father[x]; sons[y][1-w]=sons[x][w]; if (sons[x][w]) father[sons[x][w]]=y; father[x]=father[y]; if (father[y]) if (y==sons[father[y]][0]) sons[father[y]][0]=x; else sons[father[y]][1]=x; sons[x][w]=y; father[y]=x; size[x]=size[y]; size[y]=size[sons[y][0]]+size[sons[y][1]]+1;&#125;void splay(int x,int y) //splay(node,position)&#123; if (!x) return ; //check(x); while(father[x]!=y) &#123; if (father[father[x]]==y) if (x==sons[father[x]][0]) rotate(x,1); else rotate(x,0); else if (father[x]==sons[father[father[x]]][0]) if (x==sons[father[x]][0]) &#123; rotate(father[x],1); rotate(x,1); &#125; else &#123; rotate(x,0); rotate(x,1); &#125; else if (x==sons[father[x]][1]) &#123; rotate(father[x],0); rotate(x,0); &#125; else &#123; rotate(x,1); rotate(x,0); &#125; &#125; if (!y) spt=x;&#125;void insert(int w) //insert(value)&#123; spttail++; data[spttail]=w; size[spttail]=1; sons[spttail][0]=0; sons[spttail][1]=0; flag[spttail]=0; if (!spt) &#123; father[spttail]=0; spt=spttail; &#125; else &#123; int x=spt; while(1) &#123; size[x]++; if (w&lt;data[x]) if (sons[x][0]) x=sons[x][0]; else break; else if (sons[x][1]) x=sons[x][1]; else break; &#125; father[spttail]=x; if (w&lt;data[x]) sons[x][0]=spttail; else sons[x][1]=spttail; splay(spttail,0); &#125;&#125;void down(int x)&#123; flag[x]=0; int t=sons[x][1]; sons[x][1]=sons[x][0]; sons[x][0]=t; flag[sons[x][0]]=1-flag[sons[x][0]]; flag[sons[x][1]]=1-flag[sons[x][1]];&#125;void del(int x) //del(number)&#123; splay(x,0); int y=sons[x][0]; if (flag[y]) down(y); while(sons[y][1]) &#123; y=sons[y][1]; if (flag[y]) down(y); &#125; int z=sons[x][1]; if (flag[z]) down(z); while(sons[z][0]) &#123; z=sons[z][0]; if (flag[z]) down(z); &#125; if (y) &#123; splay(y,0); size[y]--; if (z) &#123; splay(z,y); sons[z][0]=0; size[z]--; &#125; else sons[y][1]=0; &#125; else &#123; if (z) &#123; splay(z,0); sons[z][0]=0; size[z]--; &#125; else &#123; spt=0; spttail=0; &#125; &#125;&#125;void check(int x)&#123; //if (father[x]) check(father[x]); //if (flag[x]) down(x); int i=0; while (father[x]) &#123; i++; list[i]=x; x=father[x]; &#125; while (i&gt;0) &#123; if (flag[list[i]]) down(list[i]); i--; &#125;&#125;int main()&#123; //freopen("1.txt","r",stdin); int n; scanf("%d",&amp;n); while(n) &#123; spt=0; spttail=0; for (int i=1;i&lt;=n;i++) &#123; scanf("%d",&amp;a[i].key); a[i].num=i; insert(i); &#125; sort(&amp;a[1],&amp;a[n+1],op); for (int i=1;i&lt;=n;i++) &#123; check(a[i].num); splay(a[i].num,0); if (i&gt;1) printf(" "); printf("%d",size[sons[spt][0]]+i); flag[sons[spt][0]]=1-flag[sons[spt][0]]; del(spt); &#125; printf("\n"); scanf("%d",&amp;n); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>平衡树</tag>
        <tag>Splay</tag>
        <tag>伸展树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NOI2004 郁闷的出纳员 Splay]]></title>
    <url>%2F2015%2F03%2F12%2F2015-03-12-NOI-2004-yumendechunayuan%2F</url>
    <content type="text"><![CDATA[郁闷的出纳员问题描述OIER公司是一家大型专业化软件公司，有着数以万计的员工。作为一名出纳员，我的任务之一便是统计每位员工的工资。这本来是一份不错的工作，但是令人郁闷的是，我们的老板反复无常，经常调整员工的工资。如果他心情好，就可能把每位员工的工资加上一个相同的量。反之，如果心情不好，就可能把他们的工资扣除一个相同的量。我真不知道除了调工资他还做什么其它事情。 工资的频繁调整很让员工反感，尤其是集体扣除工资的时候，一旦某位员工发现自己的工资已经低于了合同规定的工资下界，他就会立刻气愤地离开公司，并且再也不会回来了。每位员工的工资下界都是统一规定的。每当一个人离开公司，我就要从电脑中把他的工资档案删去，同样，每当公司招聘了一位新员工，我就得为他新建一个工资档案。 老板经常到我这边来询问工资情况，他并不问具体某位员工的工资情况，而是问现在工资第k多的员工拿多少工资。每当这时，我就不得不对数万个员工进行一次漫长的排序，然后告诉他答案。 好了，现在你已经对我的工作了解不少了。正如你猜的那样，我想请你编一个工资统计程序。怎么样，不是很困难吧？ 输入文件第一行有两个非负整数n和min。n表示下面有多少条命令，min表示工资下界。 接下来的n行，每行表示一条命令。命令可以是以下四种之一： |名称|格式|作用||-||I命令|I_k|新建一个工资档案，初始工资为k。如果某员工的初始工资低于工资下界，他将立刻离开公司。|A命令|A_k|把每位员工的工资加上k|S命令|S_k|把每位员工的工资扣除k|F命令|F_k|查询第k多的工资 _（下划线）表示一个空格，I命令、A命令、S命令中的k是一个非负整数，F命令中的k是一个正整数。 在初始时，可以认为公司里一个员工也没有。 输出文件输出文件的行数为F命令的条数加一。 对于每条F命令，你的程序要输出一行，仅包含一个整数，为当前工资第k多的员工所拿的工资数，如果k大于目前员工的数目，则输出-1。 输出文件的最后一行包含一个整数，为离开公司的员工的总数。 样例输入 9 10I 60I 70S 50F 2I 30S 15A 5F 1F 2 样例输出 1020-12 约定 I命令的条数不超过100000 A命令和S命令的总条数不超过100 F命令的条数不超过100000 每次工资调整的调整量不超过1000 新员工的工资不超过100000 题意要求设计一种数据结构，能够快速进行以上4种操作，完成对整个工资单的动态维护。 分析这种动态问题，很明显的要用到动态的数据结构来维护，可以使用一般的线段树或者平衡树进行解决，而本题的特点非常适合Splay的发挥。 首先是看到A和S命令，都是针对整个工资单中的所有员工进行操作的，因此可以考虑不改变每个员工单独的值（n个员工就要改n次，开玩笑……），而是用另外一个独立的变量把所有的加减操作都记录下来，判断员工出局的时候再结合题目给定的最低值计算出下限。这里要注意的是，但是当一个员工新加入时，之前的调工资操作应该对他是不产生影响的，因为那时候这个人还不在，但是用来记录工资加减的独立变量只有一个，所以在新员工加入的时候要把之前的工资加减情况减掉，这样最后计算时才可以把前面的部分抵消掉。 另，若一个人的初始工资小于底线，则这个人的离开不算到最后的答案中。 插入和找第k值都是基本的二叉树很容易解决，删除操作是本题的重点： 测试模板和修改删除部分花了大把的时间..T_T 根据上面的思路，用mi表示给定的底线，tot记录工资加减情况，则最后mi-tot就是初始工资的相对底线，每次出现S，也就是减了工资之后，就需要把树中低于mi-tot的所有值都删掉。但是splay在实际使用过程中，若树中存在多个mi-tot的值，则由于中间有各种旋转、splay操作，直接查找mi-tot得到的位置不能够确定剩下的是在左子树还是右子树还是两个都有。于是采取的方案： 1.搜索mi-tot-1 2.若不存在，则插入一个mi-tot-1，将其旋转到根，然后把根和左子树都删掉！！！树中剩下的就是大于mi-tot-1，也就是大于等于mi-tot的值了，注意计数时不要忘记这个根是自己加进去的，不要算进去。 3.若存在mi-tot-1，则同样将根和左子树都删掉，然后在右子树中搜索mi-tot-1，旋转到根删掉，不断重复直到整棵树中不存在mi-tot-1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204/* ***********************************************MYID : Chen FanLANG : G++PROG : cashier************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 100010int sons[MAXN][2];int father[MAXN],size[MAXN],data[MAXN];int spt=0,spttail=0,tot=0,men=0;void rotate(int x,int w) //rotate(node,0/1)&#123; int y=father[x]; sons[y][1-w]=sons[x][w]; if (sons[x][w]) father[sons[x][w]]=y; father[x]=father[y]; if (father[y]) if (y==sons[father[y]][0]) sons[father[y]][0]=x; else sons[father[y]][1]=x; sons[x][w]=y; father[y]=x; size[x]=size[y]; size[y]=size[sons[y][0]]+size[sons[y][1]]+1;&#125;void splay(int x,int y) //splay(node,position)&#123; if (!x) return ; while(father[x]!=y) &#123; if (father[father[x]]==y) if (x==sons[father[x]][0]) rotate(x,1); else rotate(x,0); else if (father[x]==sons[father[father[x]]][0]) if (x==sons[father[x]][0]) &#123; rotate(father[x],1); rotate(x,1); &#125; else &#123; rotate(x,0); rotate(x,1); &#125; else if (x==sons[father[x]][1]) &#123; rotate(father[x],0); rotate(x,0); &#125; else &#123; rotate(x,1); rotate(x,0); &#125; &#125; if (!y) spt=x;&#125;void search(int x,int w)&#123; while(data[x]!=w) &#123; if (w&lt;data[x]) &#123; if (sons[x][0]) x=sons[x][0]; else break; &#125; else if (w&gt;data[x]) &#123; if (sons[x][1]) x=sons[x][1]; else break; &#125; &#125; splay(x,0);&#125;void insert(int w) //insert(value)&#123; spttail++; data[spttail]=w; size[spttail]=1; sons[spttail][0]=0; sons[spttail][1]=0; if (!spt) &#123; father[spttail]=0; spt=spttail; &#125; else &#123; int x=spt; while(1) &#123; size[x]++; if (w&lt;data[x]) if (sons[x][0]) x=sons[x][0]; else break; else if (sons[x][1]) x=sons[x][1]; else break; &#125; father[spttail]=x; if (w&lt;data[x]) sons[x][0]=spttail; else sons[x][1]=spttail; splay(spttail,0); &#125;&#125;void select(int x,int v) //select(root,k)&#123; while(v!=size[sons[x][0]]+1) &#123; if (v&lt;=size[sons[x][0]]) x=sons[x][0]; else &#123; v-=size[sons[x][0]]+1; x=sons[x][1]; &#125; &#125; splay(x,0);&#125;int main()&#123; freopen("cashier.in","r",stdin); freopen("cashier.out","w",stdout); int n,mi; scanf("%d%d",&amp;n,&amp;mi); spt=0; spttail=0; tot=0; men=0; for (int i=1;i&lt;=n;i++) &#123; char c; c=getchar(); while(c!='I'&amp;&amp;c!='A'&amp;&amp;c!='S'&amp;&amp;c!='F') c=getchar(); int k; scanf("%d",&amp;k); if (c=='I') &#123; if (k&gt;=mi) insert(k-tot); &#125; else if (c=='A') &#123; tot+=k; &#125; else if (c=='S') &#123; tot-=k; search(spt,mi-tot-1); if (data[spt]!=mi-tot-1) &#123; insert(mi-tot-1); men+=size[sons[spt][0]]; spt=sons[spt][1]; father[spt]=0; &#125; else &#123; men+=size[sons[spt][0]]+1; spt=sons[spt][1]; father[spt]=0; search(spt,mi-tot-1); while(data[spt]==mi-tot-1) &#123; men++; spt=sons[spt][1]; father[spt]=0; search(spt,mi-tot-1); &#125; &#125; &#125; else &#123; if (k&gt;size[spt]) printf("-1\n"); else &#123; select(spt,size[spt]-k+1); printf("%d\n",data[spt]+tot); &#125; &#125; //printf("Size:%d mi-tot+1:%d\n",size[spt],mi-tot); //debug &#125; //printf("%d\n",men-size[spt]); printf("%d\n",men); return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>平衡树</tag>
        <tag>Splay</tag>
        <tag>伸展树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 4006 The kth great number 优先队列、平衡树模板题（SBT）]]></title>
    <url>%2F2015%2F03%2F08%2F2015-03-08-HDU-4006-The-kth-great-number%2F</url>
    <content type="text"><![CDATA[The kth great numberTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65768/65768 K (Java/Others) Problem DescriptionXiao Ming and Xiao Bao are playing a simple Numbers game. In a round Xiao Ming can choose to write down a number, or ask Xiao Bao what the kth great number is. Because the number written by Xiao Ming is too much, Xiao Bao is feeling giddy. Now, try to help Xiao Bao. InputThere are several test cases. For each test case, the first line of input contains two positive integer n, k. Then n lines follow. If Xiao Ming choose to write down a number, there will be an “ I” followed by a number that Xiao Ming will write down. If Xiao Ming choose to ask Xiao Bao, there will be a “Q”, then you need to output the kth great number. OutputThe output consists of one integer representing the largest number of islands that all lie on one line. Sample Input 3 I 1I 2I 3Q I 5Q I 4Q Sample Output 1 23 HintXiao Ming won’t ask Xiao Bao the kth great number when the number of the written number is smaller than k. (1=&lt;k&lt;=n&lt;=1000000). 题意给出一系列操作： 1.记录一个数；2.求第k小的数 分析解法一、优先队列题目每次询问的只是其中的一个数，这种情况下用一个堆来维护所有数的集合即可。 而且本题的k是一个固定值，因此只需要一个小根堆即可；若k不是一个固定值，则需要一个小根堆配合大根堆共同完成。 堆可以用STL中的优先队列来代替。 创建一个小根堆并向其加入数据，若堆中的数量大于k则弹出堆顶元素，始终保持整个堆中只有k个元素。遇到询问时，读取堆顶元素。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU4006************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;struct cmp&#123; bool operator()(int x,int y) &#123; return x&gt;y; &#125;&#125;;int main()&#123; //freopen("1.txt","r",stdin); //freopen("std.txt","w",stdout); int n,k; while(scanf("%d%d",&amp;n,&amp;k)==2) &#123; priority_queue&lt;int,vector&lt;int&gt;,cmp&gt;q; char c; for (int i=1;i&lt;=n;i++) &#123; c=getchar(); while(c!='I'&amp;&amp;c!='Q') &#123; c=getchar(); &#125; if (c=='I') &#123; int now; scanf("%d",&amp;now); q.push(now); if (q.size()&gt;k) q.pop(); &#125; else &#123; printf("%d\n",q.top()); &#125; &#125; &#125; return 0;&#125; 解法二、平衡树本题可作为平衡树模板题，虽然因为有点大材小用内存占用比较大，而且时间上并没有太大优势。 动态维护一棵平衡树，求k大值。 以下采用Size Balanced Tree完成： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU4006_SBT************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;#define MAXN 1000010typedef struct sbtnod&#123; int key,left,right,size;&#125; sbtnode;int sbttail,sbt;sbtnode tree[MAXN];void rrotate(int&amp; t)&#123; int k=tree[t].left; if (!k) return ; tree[t].left=tree[k].right; tree[k].right=t; tree[k].size=tree[t].size; tree[t].size=tree[tree[t].left].size+tree[tree[t].right].size+1; t=k;&#125;void lrotate(int&amp; t)&#123; int k=tree[t].right; if (!k) return ; tree[t].right=tree[k].left; tree[k].left=t; tree[k].size=tree[t].size; tree[t].size=tree[tree[t].left].size+tree[tree[t].right].size+1; t=k;&#125;void maintain(int&amp; t,bool flag)&#123; if (!t) return ; if (!flag) if (tree[tree[tree[t].left].left].size&gt;tree[tree[t].right].size) rrotate(t); else if (tree[tree[tree[t].left].right].size&gt;tree[tree[t].right].size) &#123; lrotate(tree[t].left); rrotate(t); &#125; else return ; else if (tree[tree[tree[t].right].right].size&gt;tree[tree[t].left].size) lrotate(t); else if (tree[tree[tree[t].right].left].size&gt;tree[tree[t].left].size) &#123; rrotate(tree[t].right); lrotate(t); &#125; else return ; maintain(tree[t].left,false); maintain(tree[t].right,true); maintain(t,false); maintain(t,true);&#125;void insert(int&amp; t,int v)&#123; if (!t) &#123; sbttail++; tree[sbttail].key=v; tree[sbttail].size=1; t=sbttail; &#125; else &#123; tree[t].size++; if (v&lt;tree[t].key) insert(tree[t].left,v); else insert(tree[t].right,v); maintain(t,v&gt;=tree[t].key); &#125;&#125;int del(int&amp; t,int v)&#123; int ret; tree[t].size--; if (v==tree[t].key||(v&lt;tree[t].key&amp;&amp;tree[t].left==0)||(v&gt;tree[t].key&amp;&amp;tree[t].right==0)) &#123; ret=tree[t].key; if (tree[t].left==0||tree[t].right==0) t=tree[t].left+tree[t].right;// else tree[t].key=del(tree[t].left,tree[t].key+1); &#125; else &#123; if (v&lt;tree[t].key) ret=del(tree[t].left,v); else ret=del(tree[t].right,v); &#125; return ret;&#125;int select(int t,int k)&#123; if (k==tree[tree[t].left].size+1) return t; if (k&lt;=tree[tree[t].left].size) return select(tree[t].left,k); else return select(tree[t].right,k-1-tree[tree[t].left].size);&#125;int main()&#123; //freopen("1.txt","r",stdin); //freopen("st.txt","w",stdout); int n,k; while(scanf("%d%d",&amp;n,&amp;k)==2) &#123; memset(tree,0,sizeof(tree)); sbttail=0; sbt=0; char c; for (int i=1;i&lt;=n;i++) &#123; c=getchar(); while(c!='I'&amp;&amp;c!='Q') &#123; c=getchar(); &#125; int now; if (c=='I') &#123; scanf("%d",&amp;now); insert(sbt,now); &#125; else &#123; now=select(sbt,sbttail-k+1); printf("%d\n",tree[now].key); &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>优先队列</tag>
        <tag>SBT</tag>
        <tag>平衡树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 5176 The Experience of Love 带权并查集]]></title>
    <url>%2F2015%2F02%2F27%2F2015-02-27-HDU-5176%2F</url>
    <content type="text"><![CDATA[The Experience of LoveTime Limit: 4000/2000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionA girl named Gorwin and a boy named Vivin is a couple. They arrived at a country named LOVE. The country consisting of N cities and only N−1 edges (just like a tree), every edge has a value means the distance of two cities. They select two cities to live，Gorwin living in a city and Vivin living in another. First date, Gorwin go to visit Vivin, she would write down the longest edge on this path(maxValue).Second date, Vivin go to Gorwin, he would write down the shortest edge on this path(minValue),then calculate the result of maxValue subtracts minValue as the experience of love, and then reselect two cities to live and calculate new experience of love, repeat again and again. Please help them to calculate the sum of all experience of love after they have selected all cases. InputThere will be about 5 cases in the input file. For each test case the first line is a integer N, Then follows n−1 lines, each line contains three integers a, b, and c, indicating there is a edge connects city a and city b with distance c. [Technical Specification] 1&lt;N&lt;=150000,1&lt;=a,b&lt;=n,1&lt;=c&lt;=10^9 OutputFor each case，the output should occupies exactly one line. The output format is Case #x: answer, here x is the data number, answer is the sum of experience of love. Sample Input 3 1 2 12 3 25 1 2 22 3 52 4 73 5 4 Sample Output Case #1: 1Case #2: 17 Hinthuge input,fast IO method is recommended. In the first sample: The maxValue is 1 and minValue is 1 when they select city 1 and city 2, the experience of love is 0. The maxValue is 2 and minValue is 2 when they select city 2 and city 3, the experience of love is 0. The maxValue is 2 and minValue is 1 when they select city 1 and city 3, the experience of love is 1. so the sum of all experience is 1; 题意给出一张图（其实是一棵树），要求计算图上任意两点间路径上最长边的总和以及任意两点间路径上最短边的总和，求其差。 分析直白地考虑，枚举任意两点是O（n^2）的复杂度，然后还要找到两点的路径，然后还要记下路径上最长/最短边，然后求和，这样的复杂度是完全不可能承受的。 考虑一个子图被一条长度为c的边分成两部分，c的两端点为a和b，且这两部分中的所有边长都小于c。 则a所连接的点的数量*b所连接的点的数量就是这个子图中，任意两点直接路径上最长边为c的点对的总数，同理只要递增/递减地枚举每一条边，可以根据这个方法算出以每一条边为路径最长边的点对的总数，乘上边长求和即可。 需要解决的就是如何快速得到每条边两端的点数，带权并查集可以解决这个问题。 根据这个想法，可以首先对所有边进行排序，用并查集维护点对集关系，用权保存下每个点集中点的总数，用于每次得到一条边两端连接的点的个数。 最长边总和与最短边总和的思路完全一样，改一下代码方向即可。 注意这里还有个坑是注意数据范围，要开到unsigned long long 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU5176************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct nod&#123; int a,b,c;&#125; node;node a[150010];bool op(node a,node b)&#123; if (a.c==b.c) return a.a&lt;b.a; else return a.c&lt;b.c;&#125;int father[150010],rank[150010]; void clean_father(int n)&#123; for (int i=1;i&lt;=n;i++) &#123; father[i]=i; rank[i]=1; &#125;&#125; int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link(int x,int y)&#123; int xx=getfather(x),yy=getfather(y); father[xx]=yy; rank[yy]+=rank[xx];&#125;int main()&#123; int n,t=0; while(scanf("%d",&amp;n)==1) &#123; for (int i=1;i&lt;n;i++) scanf("%d%d%d",&amp;a[i].a,&amp;a[i].b,&amp;a[i].c); sort(&amp;a[1],&amp;a[n],op); clean_father(n); unsigned long long ans1=0; for (int i=1;i&lt;n;i++) &#123; int x=getfather(a[i].a),y=getfather(a[i].b); ans1+=(unsigned long long)rank[x]*rank[y]*a[i].c; link(x,y); &#125; clean_father(n); unsigned long long ans2=0; for (int i=n-1;i&gt;=1;i--) &#123; int x=getfather(a[i].a),y=getfather(a[i].b); ans2+=(unsigned long long)rank[x]*rank[y]*a[i].c; link(x,y); &#125; t++; printf("Case #%d: %llu\n",t,ans1-ans2); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 5172 GTY's gay friends 线段树]]></title>
    <url>%2F2015%2F02%2F16%2F2015-02-16-HDU-5172%2F</url>
    <content type="text"><![CDATA[GTY’s gay friendsTime Limit: 6000/3000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionGTY has n gay friends. To manage them conveniently, every morning he ordered all his gay friends to stand in a line. Every gay friend has a characteristic value ai , to express how manly or how girlish he is. You, as GTY’s assistant, have to answer GTY’s queries. In each of GTY’s queries, GTY will give you a range [l,r] . Because of GTY’s strange hobbies, he wants there is a permutation [1..r−l+1] in [l,r]. You need to let him know if there is such a permutation or not. InputMulti test cases (about 3) . The first line contains two integers n and m ( 1≤n,m≤1000000 ), indicating the number of GTY’s gay friends and the number of GTY’s queries. the second line contains n numbers seperated by spaces. The ith number ai ( 1≤ai≤n ) indicates GTY’s ith gay friend’s characteristic value. The next m lines describe GTY’s queries. In each line there are two numbers l and r seperated by spaces ( 1≤l≤r≤n ), indicating the query range. OutputFor each query, if there is a permutation [1..r−l+1] in [l,r], print ‘YES’, else print ‘NO’. Sample Input 8 52 1 3 4 5 2 3 11 31 12 24 81 53 21 1 11 11 2 Sample Output YESNOYESYESYESYESNO 题意给出一个数列，询问连续的从l开始到r为止的数是否刚好能够组成从1开始到r-l+1的数列。 分析每一次询问都是一个区间询问。 对于每一个区间询问，需要判断区间内的数是否刚好可以组成1到k的连续数列，主要的判断标准有两个： 1.区间数字的总和与（1+k）*k/2相等； 2.保证区间内所有数都只出现一次。 第一个可以在读入数据时用前缀和解决。 第二个就要用到线段树了，读入时预处理记录下与当前数相同的数最近一次出现的位置。询问l~r的区间时，检索每个数的最近出现位置位置，若得到的所有结果都在区间左端的左边，那就说明区间中所有的数都是不重复出现的，则满足条件。这里就是用线段树判断区间最大值小于区间左端的过程了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU5172************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N=1e6+10;int last[N],a,sum[N]; typedef struct treetyp&#123; int a,b,l,r,data;&#125; treetype;treetype tree[2*N];int treetail;void maketree(int l,int r)&#123; treetail++; int now=treetail; tree[now].a=l; tree[now].b=r; if (l&lt;r) &#123; tree[now].l=treetail+1; maketree(l,(l+r)/2); tree[now].r=treetail+1; maketree((l+r)/2+1,r); &#125;&#125;void add(int n,int i,int data)&#123; if (tree[n].data&lt;data) tree[n].data=data; if (i==tree[n].a&amp;&amp;i==tree[n].b) return ; else if (i&lt;=(tree[n].a+tree[n].b)/2) add(tree[n].l,i,data); else add(tree[n].r,i,data);&#125;int res;void search(int n,int a,int b)&#123; if (tree[n].a&gt;=a&amp;&amp;tree[n].b&lt;=b) &#123; if (res&lt;tree[n].data) res=tree[n].data; return ; &#125; if (tree[n].a==tree[n].b) return ; if (a&lt;=(tree[n].a+tree[n].b)/2) search(tree[n].l,a,b); if (b&gt;=(tree[n].a+tree[n].b)/2+1) search(tree[n].r,a,b);&#125;int main()&#123; int n,m; while(scanf("%d%d",&amp;n,&amp;m)==2) &#123; memset(sum,0,sizeof(sum)); memset(last,0,sizeof(last)); treetail=0; maketree(1,n); for (int i=1;i&lt;=n;i++) &#123; scanf("%d",&amp;a); sum[i]=sum[i-1]+a; add(1,i,last[a]); last[a]=i; &#125; for (int i=1;i&lt;=m;i++) &#123; int l,r; scanf("%d%d",&amp;l,&amp;r); if ((r-l+1)*(r-l+2)/2==sum[r]-sum[l-1]) &#123; res=0; search(1,l,r); if (res&lt;l) printf("YES\n"); else printf("NO\n"); &#125; else printf("NO\n"); &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>线段树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 5171 GTY's birthday gift 矩阵快速幂]]></title>
    <url>%2F2015%2F02%2F09%2F2015-02-09-HDU-5171%2F</url>
    <content type="text"><![CDATA[GTY’s birthday giftTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionFFZ’s birthday is coming. GTY wants to give a gift to ZZF. He asked his gay friends what he should give to ZZF. One of them said, ‘Nothing is more interesting than a number multiset.’ So GTY decided to make a multiset for ZZF. Multiset can contain elements with same values. Because GTY wants to finish the gift as soon as possible, he will use JURUO magic. It allows him to choose two numbers a and b(a,b∈S), and add a+b to the multiset. GTY can use the magic for k times, and he wants the sum of the multiset is maximum, because the larger the sum is, the happier FFZ will be. You need to help him calculate the maximum sum of the multiset. InputMulti test cases (about 3) . The first line contains two integers n and k (2≤n≤100000,1≤k≤1000000000). The second line contains n elements ai (1≤ai≤100000)separated by spaces , indicating the multiset S . OutputFor each case , print the maximum sum of the multiset (mod 10000007). Sample Input 3 23 6 2 Sample Output 35 题意按照规则扩展一个集合k次，然后求其总和。 分析扩展规则很简单，就是一个斐波那契数列，但是如果按照模拟的方法手动推算，复杂度对于本题的数据范围来说是不太合适的。 可以利用矩阵快速幂来迅速完成。 $$\begin{bmatrix}S_{n-1}&amp;F_n&amp;F_{n-1}\end{bmatrix}*\begin{bmatrix}1&amp;0&amp;0\\1&amp;1&amp;1\\1&amp;1&amp;0\end{bmatrix}=\begin{bmatrix}S_n&amp;F_{n+1}&amp;F_n\end{bmatrix}$$ 剩下要注意的就是数据范围要开到long long了，因为可能涉及到$10^9 * 10^9$这样的数量级。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU5171************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#define MOD 10000007using namespace std;typedef struct matrixnod&#123; long long m[3][3];&#125; matrix;matrix ex=&#123; 1,0,0, 1,1,1, 1,1,0&#125;;matrix mat(matrix a,matrix b)&#123; matrix c; for (int i=0;i&lt;3;i++) for (int j=0;j&lt;3;j++) &#123; c.m[i][j]=0; for (int k=0;k&lt;3;k++) c.m[i][j]+=(a.m[i][k]*b.m[k][j])%MOD; c.m[i][j]%=MOD; &#125; return c;&#125;matrix mat2(matrix a,matrix b)&#123; matrix c; for (int j=0;j&lt;3;j++) &#123; c.m[0][j]=0; for (int k=0;k&lt;3;k++) c.m[0][j]+=(a.m[0][k]*b.m[k][j])%MOD; c.m[0][j]%=MOD; &#125; return c;&#125;matrix doexpmat(matrix b,int n)&#123; matrix a= &#123; 1,0,0, 0,1,0, 0,0,1 &#125;; while(n) &#123; if (n&amp;1) a=mat(a,b); n=n&gt;&gt;1; b=mat(b,b); &#125; return a;&#125;int main()&#123; int n,k; int a[100010]; while(scanf("%d%d",&amp;n,&amp;k)==2) &#123; long long sum=0; for (int i=1;i&lt;=n;i++) &#123; scanf("%d",&amp;a[i]); sum=(sum+a[i])%MOD; &#125; sort(&amp;a[1],&amp;a[n+1]); matrix start; start.m[0][0]=0; start.m[0][1]=a[n]; start.m[0][2]=a[n-1]; start=mat2(start,doexpmat(ex,k)); sum=(sum+start.m[0][0])%MOD; printf("%lld\n",sum); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>矩阵快速幂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵快速幂模板]]></title>
    <url>%2F2015%2F02%2F09%2F2015-02-09-juzhengkuaisumi%2F</url>
    <content type="text"><![CDATA[二分快速幂12345678910111213long long doexp(int x,int y)&#123; long long i=1,j=x; long long k; if (x==1||y==1) return x; while (y) &#123; if (y&amp;1) i=(i*j)%k; j=(j*j)%k; y=y&gt;&gt;1; &#125; return i;&#125; 利用了二分log(n)级的快速迭代平方运算，可以很高效地完成幂运算。 与数的乘幂相类似，矩阵乘法同样存在结合律，所以可以利用相同的方法加速运算，得到矩阵快速幂。 矩阵快速幂1234567891011121314151617181920212223242526272829303132333435363738//首先定义一下矩阵类型typedef struct matrixnod&#123; int m[3][3];&#125; matrix;//3*3的矩阵乘法matrix mat(matrix a,matrix b)&#123; matrix c; int mod; for (int i=0;i&lt;3;i++) for (int j=0;j&lt;3;j++) &#123; c.m[i][j]=0; for (int k=0;k&lt;3;k++) c.m[i][j]+=(a.m[i][k]*b.m[k][j])%mod; c.m[i][j]%=mod; &#125; return c;&#125;//矩阵快速幂 b^nmatrix doexpmat(matrix b,int n)&#123; matrix a= //单位矩阵 &#123; 1,0,0, 0,1,0, 0,0,1 &#125;; while(n) &#123; if (n&amp;1) a=mat(a,b); n=n&gt;&gt;1; b=mat(b,b); &#125; return a;&#125; 形式上来说跟普通的二分快速幂相同 对于这一类题目，解题的核心就在于如何给出一个可行的矩阵来将初始状态推到最终需要的结果上。 这里可以采用DP等思路来把（状态转移）矩阵给推出来，话说DP优化里面本来也就有一项是矩阵优化……殊途同归啊]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>矩阵快速幂</tag>
        <tag>快速幂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 5166 Missing number 简单数论]]></title>
    <url>%2F2015%2F02%2F06%2F2015-02-06-HDU-5166%2F</url>
    <content type="text"><![CDATA[Missing numberTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionThere is a permutation without two numbers in it, and now you know what numbers the permutation has. Please find the two numbers it lose. InputThere is a number T shows there are T test cases below. (T≤10)For each test case , the first line contains a integers n , which means the number of numbers the permutation has. In following a line , there are n distinct postive integers.(1≤n≤1,000) OutputFor each case output two numbers , small number first. Sample Input 2 33 4 51 1 Sample Output 1 22 3 题意找出一个数列中缺的两个数。 分析本来没啥好说的，记录完了之后O(n)扫描一遍即可，启发就是一种简单的O(1)的算法也要多多注意下。 12Sum(n) =n*(n+1)/2Sum(n^2) =n*(n+1)*(2n+1)/6]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>数论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[长大OJ诞生记（二）VJUDGE搭建]]></title>
    <url>%2F2014%2F12%2F23%2F2014-12-23-oj-vjudge%2F</url>
    <content type="text"><![CDATA[占坑，回头补。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>Virtual_Judge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[长大OJ诞生记（一）HUSTOJ搭建]]></title>
    <url>%2F2014%2F12%2F22%2F2014-12-22-oj-hustoj%2F</url>
    <content type="text"><![CDATA[OJ是跑在LAMP的环境中，Linux+Apache+Mysql+Php 搭建运行环境 Apache2Apache用来解释html的请求。 1$ apt-get install apache2 装好之后再浏览器中访问 http://localhost 看到如下页面就是正常工作了： Apache2的配置信息在： /etc/apache2 默认目录在： /var/www/html 浏览器访问本台Ubuntu的时候就是读取这个目录下面的index.html网页 Mysql1$ apt-get install mysql-server mysql-client 安装过程中需要设置root账号的密码： 装好之后，配置信息在： /etc/mysql 可以测试一下mysql是否正常运行。 我上面设的密码是root，所以下面的指令就是： 1$ mysql -uroot -proot 然后Mysql的默认字符集是latin1，中文可能会有乱码的情况，因此需要修改下字符集： 找到： /etc/mysql/my.cnf 在最下面添加： 12345678910[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]collation-server = utf8_unicode_ciinit-connect=&apos;SET NAMES utf8&apos;character-set-server = utf8 然后重启mysql： 1$ /etc/init.d/mysql restart 重新打开mysql，输入 1&gt; show variables like &apos;char%&apos;; 出现： 看到编码都是utf8就正常了 php5安装php5以及Apache的php5模块 1$ apt-get install php5 libapache2-mod-php5 装好之后重启apache，因为我们需要的是apache与php的连通 1$ /etc/init.d/apache2 restart 然后测试一下php： 在 /var/www/html/ 下新建 info.php 文件，内容只有一句： 1&lt;?php phpinfo() ?&gt; 然后在浏览器中访问 http://localhost/info.php ，看到php版本信息即正常了 Apache2已经能够正常与php5进行通信了，但是mysql与php5之间连通还需要另外的模块： 1$ apt-get install php5-mysql php5-curl php5-gd php5-intl php-pear php5-imagick php5-imap php5-mcrypt php5-memcache php5-ming php5-ps php5-pspell php5-recode php5-snmp php5-sqlite php5-tidy php5-xmlrpc php5-xsl 重启apache2，然后继续在 /var/www/html/ 下新建 test.php 文件，内容是： 123456789&lt;?php $link = mysql_connect("localhost", "root", "root"); if (!$link) &#123; die('Could not connect: '.mysql_error()); &#125; else echo "MySQL连接成功"; mysql_close($link);?&gt; 访问 http://localhost/test.php 返回连接成功即可 另外也可以安装一下 phpmyadmin 来管理 svn、make、以及其他编译软件1$ apt-get install subversion make 然后根据需要安装好java、fpc等等编译器的包 Hustoj本体的安装直接从github上把整个项目包checkout到本地 1svn checkout https://github.com/zhblue/hustoj/trunk/trunk/install hustoj 修改 install.sh 以及 judge.conf 中数据库的账号和密码，前面设置的是root/root，刚好这里默认的就是root/root 然后运行 install.sh ，缺少的包会在安装时自己补上]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>HUSTOJ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ 2728 Desert King 最优比率生成树]]></title>
    <url>%2F2014%2F11%2F06%2F2014-11-06-POJ-2728%2F</url>
    <content type="text"><![CDATA[Desert KingTime Limit: 3000MS Memory Limit: 65536K DescriptionDavid the Great has just become the king of a desert country. To win the respect of his people, he decided to build channels all over his country to bring water to every village. Villages which are connected to his capital village will be watered. As the dominate ruler and the symbol of wisdom in the country, he needs to build the channels in a most elegant way. After days of study, he finally figured his plan out. He wanted the average cost of each mile of the channels to be minimized. In other words, the ratio of the overall cost of the channels to the total length must be minimized. He just needs to build the necessary channels to bring water to all the villages, which means there will be only one way to connect each village to the capital. His engineers surveyed the country and recorded the position and altitude of each village. All the channels must go straight between two villages and be built horizontally. Since every two villages are at different altitudes, they concluded that each channel between two villages needed a vertical water lifter, which can lift water up or let water flow down. The length of the channel is the horizontal distance between the two villages. The cost of the channel is the height of the lifter. You should notice that each village is at a different altitude, and different channels can’t share a lifter. Channels can intersect safely and no three villages are on the same line. As King David’s prime scientist and programmer, you are asked to find out the best solution to build the channels. InputThere are several test cases. Each test case starts with a line containing a number N (2 &lt;= N &lt;= 1000), which is the number of villages. Each of the following N lines contains three integers, x, y and z (0 &lt;= x, y &lt; 10000, 0 &lt;= z &lt; 10000000). (x, y) is the position of the village and z is the altitude. The first village is the capital. A test case with N = 0 ends the input, and should not be processed. OutputFor each test case, output one line containing a decimal number, which is the minimum ratio of overall cost of the channels to the total length. This number should be rounded three digits after the decimal point. Sample Input 4 0 0 00 1 11 1 21 0 30 Sample Output 1.000 题意给出一张完全图，每条边都有长度和花费，要求在图中找到一棵生成树，使得Sum(Cost)/Sum(dist)达到最小。 分析据说05年ACM的某场比赛上，教主怒切一题最优比率生成树，坑死了无数跟榜着…-_-//// 最优比率生成树的前导知识是01分数规划。 基本思路是Dinkelbach逼近法： 整体思路跟原本的01分数规划基本相同，方程F(L)=Sum(cost[i])/Sum(dist[i])，只要把L’的生成过程改成Prim即可。 Prim堆加边的时候，用cost-l*dist作为边权。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/* ***********************************************MYID : Chen FanLANG : G++PROG : PKU_2728************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;#include &lt;queue&gt;#include &lt;bitset&gt;using namespace std;typedef struct nod&#123; int x,y,z;&#125; node;node p[1010];double getdist(int i,int j)&#123; return sqrt((p[i].x-p[j].x)*(p[i].x-p[j].x)+(p[i].y-p[j].y)*(p[i].y-p[j].y));&#125;typedef struct enod&#123; int x,y; double dist,cost,r; friend bool operator &lt; (enod a,enod b) &#123; return a.r&gt;b.r; &#125;&#125; enode;enode gete(int x,int y,double dist,double cost,double l)&#123; enode a; a.x=x;a.y=y;a.dist=dist;a.cost=cost; a.r=cost-l*dist; return a;&#125;double prim(int n,double l)&#123; priority_queue&lt;enode&gt; q; while (!q.empty()) q.pop(); bitset&lt;1010&gt; flag; flag.reset(); flag[1]=1; double cost=0,dist=0; for (int i=2;i&lt;=n;i++) q.push(gete(1,i,getdist(1,i),abs(p[1].z-p[i].z),l)); for (int i=1;i&lt;n;i++) &#123; enode now=q.top(); q.pop(); while (flag[now.y]) &#123; now=q.top(); q.pop(); &#125; flag[now.y]=1; cost+=now.cost;dist+=now.dist; for (int j=1;j&lt;=n;j++) if (j!=now.y&amp;&amp;!flag[j]) q.push(gete(now.y,j,getdist(now.y,j),abs(p[now.y].z-p[j].z),l)); &#125; return cost/dist;&#125;int main()&#123; freopen("2728.txt","r",stdin); int n; while (scanf("%d",&amp;n)) &#123; if (n==0) break; for (int i=1;i&lt;=n;i++) scanf("%d%d%d",&amp;p[i].x,&amp;p[i].y,&amp;p[i].z); double l=1,ans; while (1) &#123; ans=prim(n,l); if (fabs(ans-l)&lt;1e-6) break; else l=ans; &#125; printf("%.3f\n",ans); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
        <tag>01分数规划</tag>
        <tag>最优比率生成树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ 2976 Dropping tests 01分数规划 模板]]></title>
    <url>%2F2014%2F11%2F04%2F2014-11-04-POJ-2976%2F</url>
    <content type="text"><![CDATA[Dropping testsTime Limit: 1000MS Memory Limit: 65536K DescriptionIn a certain course, you take n tests. If you get ai out of bi questions correct on test i, your cumulative average is defined to be $$100*\frac{\sum_{i=1}^na_i}{\sum_{j=1}^nb_j}$$ Given your test scores and a positive integer k, determine how high you can make your cumulative average if you are allowed to drop any k of your test scores. Suppose you take 3 tests with scores of 5/5, 0/1, and 2/6. Without dropping any tests, your cumulative average is $100*\frac{5+0+2}{5+1+6}=50$. However, if you drop the third test, your cumulative average becomes $100*\frac{5+0}{5+1}\approx 83.33$. InputThe input test file will contain multiple test cases, each containing exactly three lines. The first line contains two integers, 1 ≤ n ≤ 1000 and 0 ≤ k &lt; n. The second line contains n integers indicating ai for all i. The third line contains n positive integers indicating bi for all i. It is guaranteed that 0 ≤ ai ≤ bi ≤ 1, 000, 000, 000. The end-of-file is marked by a test case with n = k = 0 and should not be processed. OutputFor each test case, write a single line with the highest cumulative average possible after dropping k of the given test scores. The average should be rounded to the nearest integer. Sample Input 3 15 0 25 1 64 21 2 7 95 6 7 90 0 Sample Output 83100 HintTo avoid ambiguities due to rounding errors, the judge tests have been constructed so that all answers are at least 0.001 away from a decision boundary (i.e., you can assume that the average is never 83.4997). 算法分析所谓的01分数规划问题就是指这样的一类问题，给定两个数组，a[i]表示选取i的收益，b[i]表示选取i的代价。如果选取i，定义x[i]=1，否则x[i]=0。 每一个物品只有选或者不选两种方案，求一个选择方案即从其中选取k组a[i]/b[i]，使得$R=\frac{\sum(a[i]*x[i])}{\sum(b[i]*x[i])}$取得最值，即所有选择物品的总收益/总代价的值最大或是最小。 01分数规划问题主要包含一般的01分数规划、最优比率生成树问题、最优比率环问题等。 来看目标式： $$R=\frac{\sum(a[i]*x[i])}{\sum(b[i]*x[i])}$$ 我们的目标是使得R达到最大或者最小，首先定义一个函数 $$F(L)=\sum(a[i]*x[i])-L*\sum(b[i]*x[i])$$ 显然这只是对目标式的一个简单的变形。 分离参数，得到： $$F(L)=\sum((a[i]-L*b[i])*x[i])$$ 记$d[i]=a[i]-L*b[i]$，那么： $$F(L)=\sum(d[i]*x[i])$$ 接下来重点注意一下d[i]和F(L)的单调性。 如果我们已知了L，则所有的的d[i]是已知的，那么这里的贪心策略是为了得到一个最优的F(L)，我们只需要将d[i]排序之后取其中的前k个或者后k个。也就是说，给定L，我们可以直接求出对应的F(L)。 接下来是F(L)，因为b[i]是正的，显然F(L)对于L是单调递减的，这就启发我们可以通过某种方法把F(L)逼近至0，当F(L)=0时，即$\sum(a[i]*x[i])-L*\sum(b[i]*x[i])=0$，那么此时的L就是最优值。 然后还要注意到的问题是，我们每次贪心地取d[i]中最小的k个，则最终使得F(L)趋于0的L会是最小的比值；如果每次贪心地取d[i]中最大的k个，则最终使得F(L)趋于0的L会是最大的比值。 考虑$F(L)=\sum((a[i]-L*b[i])*x[i])$式中，我们取了最后k个d[i]使得F(L)=0，则如果用此时的L去取全部的数，F(L)_tot将是小于零的，也即使得整个F(L)_tot趋于0的L_tot是小于L的。故L是取K组数的情况下，最大的比值。（这段说的有点绕口） 另外再注意到的一点是，如果将a[i]与b[i]上下颠倒，求解的方法相同，结果跟颠倒前也是刚好相对应的。 最后又想到了一个关键点，k的值对最后的结果会是什么影响： 贪心地说，为了使结果尽可能的大，k也要尽可能的大，即尽可能多的舍弃一些，剩下选取的数越少，均值越大 总结F(L)的两个重要性质如下就是： 1. F(L)单调递减 2. F(max(L)/min(L)) = 0 之后的逼近方法可以有两种选择 1.二分法二分区间，不断把F(L)逼近至0，若F(L)&lt;0说明L过大，若F(L)&gt;0说明L过小，直到逼近得到一个最优的L； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/* ***********************************************MYID : Chen FanLANG : G++PROG : PKU_2976_Erfen************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#define eps 1e-6using namespace std;typedef struct nod&#123; int a,b; double r;&#125; node;bool op(node a,node b)&#123; return a.r&gt;b.r;&#125;node p[1010];int main()&#123; freopen("2976.txt","r",stdin); int n,k; while(scanf("%d%d",&amp;n,&amp;k)) &#123; if (n==0&amp;&amp;k==0) break; for (int i=1;i&lt;=n;i++) scanf("%d",&amp;p[i].a); for (int i=1;i&lt;=n;i++) scanf("%d",&amp;p[i].b); int m=n-k; double left=0,right=100; while (left+eps&lt;right) &#123; double mid=(left+right)/2; for (int i=1;i&lt;=n;i++) p[i].r=p[i].a-mid*p[i].b; sort(&amp;p[1],&amp;p[n+1],op); double temp=0; for (int i=1;i&lt;=m;i++) temp+=p[i].r; if (temp&gt;0) left=mid; else right=mid; &#125; printf("%.0f\n",left*100); &#125; return 0;&#125; 2.Dinkelbach从另外一个角度考虑，每次给定一个L，除了我们用来判定的F(L)之外，我们可以通过重新计算得到一个L’，而且能够证明L’一定是更接近最优解的，那么我们可以考虑直接把L移动到L’上去。当L=L’时，说明已经没有更接近最优的解了，则此时的L就是最优解。 注意在Dinkelbach算法中，F(L)仍然是判定是否更接近最优解的工具，也即此时d[i]的选择仍然与之前相同，只是最后移动解的时候是把L直接移往L’ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/* ***********************************************MYID : Chen FanLANG : G++PROG : PKU_2976_Dinkelbach************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;#define eps 1e-6using namespace std;typedef struct nod&#123; int a,b,index; double r;&#125; node;bool op(node a,node b)&#123; return a.r&gt;b.r;&#125;node p[1010];int main()&#123; freopen("2976.txt","r",stdin); int n,k; while(scanf("%d%d",&amp;n,&amp;k)) &#123; if (n==0&amp;&amp;k==0) break; for (int i=1;i&lt;=n;i++) scanf("%d",&amp;p[i].a); for (int i=1;i&lt;=n;i++) scanf("%d",&amp;p[i].b); int m=n-k; double l=0,temp=1; while (fabs(l-temp)&gt;=eps) &#123; l=temp; for (int i=1;i&lt;=n;i++) p[i].r=p[i].a-l*p[i].b; sort(&amp;p[1],&amp;p[n+1],op); int x=0,y=0; for (int i=1;i&lt;=m;i++) &#123; x+=p[i].a; y+=p[i].b; &#125; temp=(double)x/y; &#125; printf("%.0f\n",temp*100); &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>01分数规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 4081 Qin Shi Huang's National Road System 次小生成树变种]]></title>
    <url>%2F2014%2F11%2F04%2F2014-11-04-HDU-4081%2F</url>
    <content type="text"><![CDATA[Qin Shi Huang’s National Road SystemTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionDuring the Warring States Period of ancient China(476 BC to 221 BC), there were seven kingdoms in China —- they were Qi, Chu, Yan, Han, Zhao, Wei and Qin. Ying Zheng was the king of the kingdom Qin. Through 9 years of wars, he finally conquered all six other kingdoms and became the first emperor of a unified China in 221 BC. That was Qin dynasty —- the first imperial dynasty of China(not to be confused with the Qing Dynasty, the last dynasty of China). So Ying Zheng named himself “Qin Shi Huang” because “Shi Huang” means “the first emperor” in Chinese. Qin Shi Huang undertook gigantic projects, including the first version of the Great Wall of China, the now famous city-sized mausoleum guarded by a life-sized Terracotta Army, and a massive national road system. There is a story about the road system: There were n cities in China and Qin Shi Huang wanted them all be connected by n-1 roads, in order that he could go to every city from the capital city Xianyang. Although Qin Shi Huang was a tyrant, he wanted the total length of all roads to be minimum,so that the road system may not cost too many people’s life. A daoshi (some kind of monk) named Xu Fu told Qin Shi Huang that he could build a road by magic and that magic road would cost no money and no labor. But Xu Fu could only build ONE magic road for Qin Shi Huang. So Qin Shi Huang had to decide where to build the magic road. Qin Shi Huang wanted the total length of all none magic roads to be as small as possible, but Xu Fu wanted the magic road to benefit as many people as possible —- So Qin Shi Huang decided that the value of A/B (the ratio of A to B) must be the maximum, which A is the total population of the two cites connected by the magic road, and B is the total length of none magic roads. Would you help Qin Shi Huang? A city can be considered as a point, and a road can be considered as a line segment connecting two points. InputThe first line contains an integer t meaning that there are t test cases(t &lt;= 10). For each test case: The first line is an integer n meaning that there are n cities(2 &lt; n &lt;= 1000). Then n lines follow. Each line contains three integers X, Y and P ( 0 &lt;= X, Y &lt;= 1000, 0 &lt; P &lt; 100000). (X, Y) is the coordinate of a city and P is the population of that city. It is guaranteed that each city has a distinct location. OutputFor each test case, print a line indicating the above mentioned maximum ratio A/B. The result should be rounded to 2 digits after decimal point. Sample Input 2 41 1 201 2 30200 2 80200 1 1003 1 1 201 2 302 2 40 Sample Output 65.0070.00 题意秦国有n个城市构成，每个城市都有一定的人口。现在要修路，要求最终修成的路花费最少代价使得所有的城市都连通。然后修路的时候可以使用一个魔法，免去一条路的费用，最终结果要使使用了魔法的那条路两端的城市总人口数除以剩下所有路的长度最大。 分析最少代价使得所有点都连通，很容易能够想到最小生成树。 从最终答案是A/B入手，要使这个结果最大，但是明显A与B的大小会互相影响，故不符合贪心的要求。所以采用的只能是枚举每一条边，在指定A的前提下，使B最小。 考虑一下删边是两种情况： i,j边恰好在最小生成树上，那么直接删掉； i,j边不在最小生成树上，那么要在最小生成树中找到i,j路径上最长的边删去； 由于本题完全图的特殊性，如果i,j边在最小生成树上，那么i,j边直接就是路径上的最长边了，也即问题转化为给定两个点，要求在最小生成树上找到两个点路径上最长的边。 而这恰好是求解次小生成树的方法。 思路： 按照次小生成树的求法，在Prim的过程中就顺便把路径上的最长边记录下来。Kruskal也可以完成，但是在记录最长边的过程中，Prim是有序扩展，故复杂度会更低。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141/* ***********************************************MYID : Chen FanLANG : G++PROG : 4081************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;#include &lt;cmath&gt;#include &lt;bitset&gt;using namespace std;typedef struct pnod&#123; int x,y,p;&#125; pnode;pnode p[1010];typedef struct nod&#123; int a,b; double c; friend bool operator &lt; (nod a,nod b) &#123; return a.c&gt;b.c; &#125;&#125; node;node edge[1000010];int start[1010],num[1010];bool op(node a,node b)&#123; if (a.a==b.a) return a.c&lt;b.c; else return a.a&lt;b.a;&#125;node ntoh(int a,int b,double c)&#123; node x; x.a=a; x.b=b; x.c=c; return x;&#125;double maxx[1010][1010];bitset&lt;1010&gt; inway[1010];double prim(int s,int n)&#123; /**/ int list[1010],listail=1; list[1]=s; /**/ priority_queue&lt;node&gt; heap; while (!heap.empty()) heap.pop(); bitset&lt;1010&gt; flag; flag.reset(); flag[s]=1; double ans=0; memset(maxx,0,sizeof(maxx)); for (int i=0;i&lt;num[s];i++) heap.push(edge[start[s]+i]); for (int i=1;i&lt;n;i++) &#123; node now=heap.top(); heap.pop(); while (flag[now.b]) &#123; now=heap.top(); heap.pop(); &#125; /**/ for (int j=1;j&lt;=listail;j++) &#123; maxx[list[j]][now.b]=max(maxx[list[j]][now.a],now.c); maxx[now.b][list[j]]=maxx[list[j]][now.b]; &#125; listail++; list[listail]=now.b; /**/ flag[now.b]=true; ans+=now.c; for (int j=0;j&lt;num[now.b];j++) if (!flag[edge[start[now.b]+j].b]) heap.push(edge[start[now.b]+j]); &#125; return ans;&#125;double getdis(int x,int y)&#123; return sqrt((p[x].x-p[y].x)*(p[x].x-p[y].x)+(p[x].y-p[y].y)*(p[x].y-p[y].y));&#125;int main()&#123; freopen("4081.txt","r",stdin); int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n; scanf("%d",&amp;n); for (int i=1;i&lt;=n;i++) scanf("%d%d%d",&amp;p[i].x,&amp;p[i].y,&amp;p[i].p); int m=0; for (int i=1;i&lt;=n;i++) &#123; start[i]=m+1; num[i]=n-1; for (int j=1;j&lt;=n;j++) if (i!=j) &#123; m++; edge[m].a=i; edge[m].b=j; edge[m].c=getdis(i,j); &#125; &#125; double sum=prim(1,n); double ma=0; for (int i=1;i&lt;=m;i++) &#123; double temp=(p[edge[i].a].p+p[edge[i].b].p)/(sum-maxx[edge[i].a][edge[i].b]); if (ma&lt;temp) ma=temp; &#125; printf("%.2f\n",ma); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
        <tag>次小生成树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 4408 Minimum Spanning Tree 最小生成树计数]]></title>
    <url>%2F2014%2F11%2F03%2F2014-11-03-HDU-4408%2F</url>
    <content type="text"><![CDATA[Minimum Spanning TreeTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionXXX is very interested in algorithm. After learning the Prim algorithm and Kruskal algorithm of minimum spanning tree, XXX finds that there might be multiple solutions. Given an undirected weighted graph with n (1&lt;=n&lt;=100) vertexes and m (0&lt;=m&lt;=1000) edges, he wants to know the number of minimum spanning trees in the graph. InputThere are no more than 15 cases. The input ends by 0 0 0. For each case, the first line begins with three integers — the above mentioned n, m, and p. The meaning of p will be explained later. Each the following m lines contains three integers u, v, w (1&lt;=w&lt;=10), which describes that there is an edge weighted w between vertex u and vertex v( all vertex are numbered for 1 to n) . It is guaranteed that there are no multiple edges and no loops in the graph. OutputFor each test case, output a single integer in one line representing the number of different minimum spanning trees in the graph. The answer may be quite large. You just need to calculate the remainder of the answer when divided by p (1&lt;=p&lt;=1000000000). p is above mentioned, appears in the first line of each test case. ## Sample Input 5 10 122 5 32 4 23 1 33 4 21 2 35 4 35 1 34 1 15 3 33 2 30 0 0 Sample Output 4 题意一张无向图，要求求出其中最小生成树的棵树。 分析生成树计数可以使用Matrix-Tree定理解决，本题最主要的区别是有了一个最小生成树的额外条件。 首先考虑一下如何得到最小生成树。 Kruskal算法的基本思想是，按照边长排序，然后不断将短边加入集合，最终一步如果能成功把n-1条边都加入同一个集合，则找到了最小生成树。在维护集合时，可以使用并查集来快速处理。 如果把Kruskal的过程按照边长划分成多个阶段，实际上是处理了所有短边的连通性之后继续处理下一个长度的边的连通性，并依次继续处理剩下边的连通性。然后我们可以发现，不同长度的边之间的连通性互不影响！！！ 假设存在n1条长度为c1的边，n2条长度为c2的边…则Kruskal首先处理c1边的连通性，然后处理c2边的连通性，对于c1边的连通性的处理可能有多种方案，即从n1条边中取出一定数量的边构成最大连通图，但是最终处理完之后的结果对于c2来说是完全一样的。因此算法就出来了，在Kruskal的基础上，使用Matrix-Tree定理处理每个阶段生成树的种数，最后将所有阶段的结果相乘即可。 具体实现为： 在Kruskal的基础上，每完成一个阶段（检查完一个长度），就将所有遍历过的点缩成一个点，然后用Matrix-Tree定理计算该点与下一组点组成的连通图中生成树的个数。最终把每一个阶段的结果相乘即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/* ***********************************************MYID : Chen FanLANG : G++PROG : Counting_MST_HDU4408************************************************ */#include &lt;vector&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;bitset&gt;#define N 405#define M 4005using namespace std;typedef struct nod&#123; int a,b,c;&#125; node;node edge[M];bool op(node a,node b)&#123; return a.c&lt;b.c;&#125;int n,m,o,fa[N],ka[N];long long ans,mod,gk[N][N],kir[N][N];bitset&lt;N&gt; flag;vector&lt;int&gt; gra[N];int getfather(int x,int father[])&#123; if (father[x]!=x) father[x]=getfather(father[x],father); return father[x];&#125;long long det(long long a[][N],int n) //Matrix-Tree定理求Kirchhoff矩阵&#123; for (int i=0;i&lt;n;i++) for (int j=0;j&lt;n;j++) a[i][j]%=mod; long long ret=1; for (int i=1;i&lt;n;i++) &#123; for (int j=i+1;j&lt;n;j++) while (a[j][i]) &#123; long long t=a[i][i]/a[j][i]; for (int k=i;k&lt;n;k++) a[i][k]=(a[i][k]-a[j][k]*t)%mod; for (int k=i;k&lt;n;k++) swap(a[i][k],a[j][k]); ret=-ret; &#125; if (a[i][i]==0) return 0; ret=ret*a[i][i]%mod; //ret%=mod; &#125; return (ret+mod)%mod;&#125;void matrix_tree()&#123; for (int i=1;i&lt;=n;i++) //根据访问标记找出连通分量 if (flag[i]) &#123; gra[getfather(i,ka)].push_back(i); flag[i]=0; &#125; for (int i=1;i&lt;=n;i++) if (gra[i].size()&gt;1) //枚举连通分量 &#123; memset(kir,0,sizeof(kir)); int len=gra[i].size(); for (int a=0;a&lt;len;a++) for (int b=a+1;b&lt;len;b++) &#123; int la=gra[i][a],lb=gra[i][b]; kir[b][a]-=gk[la][lb]; kir[a][b]=kir[b][a]; kir[a][a]+=gk[la][lb]; kir[b][b]+=gk[la][lb]; &#125; //构造矩阵 long long ret=det(kir,len); ret%=mod; ans=(ans*ret%mod)%mod; for (int a=0;a&lt;len;a++) fa[gra[i][a]]=i; &#125; for (int i=1;i&lt;=n;i++) //连通图缩点+初始化 &#123; fa[i]=getfather(i,fa); ka[i]=fa[i]; gra[i].clear(); &#125;&#125;int main()&#123; freopen("4408.txt","r",stdin); while (scanf("%d%d%lld",&amp;n,&amp;m,&amp;mod)==3) &#123; if (n==0&amp;&amp;m==0&amp;&amp;mod==0) break; for (int i=1;i&lt;=m;i++) scanf("%d%d%d",&amp;edge[i].a,&amp;edge[i].b,&amp;edge[i].c); sort(&amp;edge[1],&amp;edge[m+1],op); for (int i=1;i&lt;=n;i++) gra[i].clear(); for (int i=1;i&lt;=n;i++) &#123; fa[i]=i; ka[i]=i; &#125; flag.reset(); memset(gk,0,sizeof(gk)); ans=1; o=edge[1].c; for (int i=1;i&lt;=m;i++) &#123; int pa=getfather(edge[i].a,fa),pb=getfather(edge[i].b,fa); if (pa!=pb) &#123; flag[pa]=1; flag[pb]=1; //访问标记 ka[getfather(pa,ka)]=getfather(pb,ka); gk[pa][pb]++; gk[pb][pa]++; //邻接矩阵 &#125; if (i==m||edge[i+1].c!=o) //所有相同的边并成一组 &#123; matrix_tree(); o=edge[i+1].c; &#125; &#125; bool done=true; for (int i=2;i&lt;=n;i++) if(ka[i]!=ka[i-1]) &#123; done=false; break; &#125; if (!done) printf("0\n"); else &#123; ans%=mod; printf("%lld\n",ans); &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
        <tag>生成树计数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成树计数]]></title>
    <url>%2F2014%2F11%2F01%2F2014-11-01-shengchengshujishu%2F</url>
    <content type="text"><![CDATA[(本帖排版有问题，回头需要重新整理) 生成树计数就是统计一张图中一共有多少种构造生成树的方案。 大概要用到组合数学等等的数学知识。 问题的引入以下内容均来自NOI2007国家集训队论文 周冬 《生成树的计数及其应用》： Matrix-Tree定理(Kirchhoff矩阵-树定理)。Matrix-Tree定理是解决生成树计数问题最有力的武器之一。它首先于1847年被Kirchhoff证明。在介绍定理之前，我们首先明确几个概念： 1、G的度数矩阵D[G]是一个n*n的矩阵，并且满足：当i≠j时,dij=0；当i=j时，dij等于vi的度数。 2、G的邻接矩阵A[G]也是一个n*n的矩阵， 并且满足：如果vi、vj之间有边直接相连，则aij=1，否则为0。 我们定义G的Kirchhoff矩阵(也称为拉普拉斯算子)C[G]为C[G]=D[G]-A[G]，则Matrix-Tree定理可以描述为： G的所有不同的生成树的个数等于其Kirchhoff矩阵C[G]任何一个n-1阶主子式的行列式的绝对值。 所谓n-1阶主子式，就是对于r(1≤r≤n)，将C[G]的第r行、第r列同时去掉后得到的新矩阵，用Cr[G]表示。 在证明Matrix-Tree定理的过程中，我们使用了无向图G的关联矩阵B。B是一个n行m列的矩阵，行对应点而列对应边。B满足，如果存在一条边e={vi,vj}，那在e所对应的列中，vi和vj所对应的那两行，一个为1、另一个为-1，其他的均为0。至于谁是1谁是-1并不重要。 接下来，我们考察BBT。容易证明，(BBT)ij等于B的第i行与第j列的点积。所以，当i=j时，(BBT)ij等于与vi相连的边的个数，即vi的度数；而当i≠j时，如果有一条连接vi、vj的边，则(BBT)ij等于-1，否则等于0。这与Kirchhoff矩阵的定义完全相同！因此，我们得出：C=BBT！也就是说，我们可以将C的问题转化到BBT上来。 这里没办法帖公式，只好省去详细的证明过程了，详见论文原文吧：http://files.cnblogs.com/jcf94/Counting_Spanning_Tree.rar 例题有： SPOJ P104 Highways 题目就是裸的生成树计数，用Matrix_Tree定理可以直接解出。 然后贴一个Kuang神的模板： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* ***********************************************MYID : Chen FanLANG : G++PROG : Count_Spaning_Tree_From_Kuangbin************************************************ */#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;algorithm&gt;#include &lt;iostream&gt;#include &lt;math.h&gt;using namespace std;const double eps = 1e-8;const int MAXN = 110;int sgn(double x)&#123; if(fabs(x) &lt; eps)return 0; if(x &lt; 0)return -1; else return 1;&#125;double b[MAXN][MAXN];double det(double a[][MAXN],int n)&#123; int i, j, k, sign = 0; double ret = 1; for(i = 0;i &lt; n;i++) for(j = 0;j &lt; n;j++) b[i][j] = a[i][j]; for(i = 0;i &lt; n;i++) &#123; if(sgn(b[i][i]) == 0) &#123; for(j = i + 1; j &lt; n;j++) if(sgn(b[j][i]) != 0) break; if(j == n)return 0; for(k = i;k &lt; n;k++) swap(b[i][k],b[j][k]); sign++; &#125; ret *= b[i][i]; for(k = i + 1;k &lt; n;k++) b[i][k]/=b[i][i]; for(j = i+1;j &lt; n;j++) for(k = i+1;k &lt; n;k++) b[j][k] -= b[j][i]*b[i][k]; &#125; if(sign &amp; 1)ret = -ret; return ret;&#125;double a[MAXN][MAXN];int g[MAXN][MAXN];int main()&#123; int T; int n,m; int u,v; scanf("%d",&amp;T); while(T--) &#123; scanf("%d%d",&amp;n,&amp;m); memset(g,0,sizeof(g)); while(m--) &#123; scanf("%d%d",&amp;u,&amp;v); u--;v--; g[u][v] = g[v][u] = 1; &#125; memset(a,0,sizeof(a)); for(int i = 0;i &lt; n;i++) for(int j = 0;j &lt; n;j++) if(i != j &amp;&amp; g[i][j]) &#123; a[i][i]++; a[i][j] = -1; &#125; double ans = det(a,n-1); printf("%.0lf\n",ans); &#125; return 0;&#125; 最小生成树计数：在ACM2012金华赛区的网赛中曾经出现过这么一道题，现在是HDU 4408，统计最小生成树的个数。 方法是Kruskal+Matrix_Tree定理，也就是把上面的算法加上Kruskal： 来自：http://blog.csdn.net/jarily/article/details/8902402 的算法解释： 算法引入： 给定一个含有N个结点M条边的无向图,求它最小生成树的个数t(G); 算法思想： 抛开“最小”的限制不看，如果只要求求出所有生成树的个数，是可以利用Matrix-Tree定理解决的; Matrix-Tree定理此定理利用图的Kirchhoff矩阵,可以在O(N3)时间内求出生成树的个数; kruskal算法： 将图G={V,E}中的所有边按照长度由小到大进行排序,等长的边可以按照任意顺序; 初始化图G’为{V,Ø},从前向后扫描排序后的边,如果扫描到的边e在G’中连接了两个相异的连通块,则将它插入G’中; 最后得到的图G’就是图G的最小生成树; 由于kruskal按照任意顺序对等长的边进行排序,则应该将所有长度为L0的边的处理当作一个阶段来整体看待; 令kruskal处理完这一个阶段后得到的图为G0,如果按照不同的顺序对等长的边进行排序,得到的G0也是不同; 虽然G0可以随排序方式的不同而不同,但它们的连通性都是一样的,都和F0的连通性相同(F0表示插入所有长度为L0的边后形成的图); 在kruskal算法中的任意时刻,并不需要关注G’的具体形态,而只要关注各个点的连通性如何(一般是用并查集表示); 所以只要在扫描进行完第一阶段后点的连通性和F0相同,且是通过最小代价到达这一状态的,接下去都能找到最小生成树; 经过上面的分析,可以看出第一个阶段和后面的工作是完全独立的; 第一阶段需要完成的任务是使G0的连通性和F0一样,且只能使用最小的代价; 计算出这一阶段的方案数,再乘上完成后续事情的方案数,就是最终答案; 由于在第一个阶段中,选出的边数是一定的,所有边的长又都为L0; 所以无论第一个阶段如何进行代价都是一样的,那么只需要计算方案数就行了; 此时Matrix-Tree定理就可以派上用场了,只需对F0中的每一个连通块求生成树个数再相乘即可; 同样，贴一个bin神模板： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139/* ***********************************************MYID : Chen FanLANG : G++PROG : Counting_MST_From_Kuangbin_HDU4408************************************************ */#include &lt;map&gt;#include &lt;stack&gt;#include &lt;queue&gt;#include &lt;math.h&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#define N 405#define M 4005#define E#define inf 0x3f3f3f3f#define dinf 1e10#define linf (LL)1&lt;&lt;60#define LL long long#define clr(a,b) memset(a,b,sizeof(a))using namespace std;LL mod;struct Edge&#123; int a,b,c; bool operator&lt;(const Edge &amp; t)const &#123; return c&lt;t.c; &#125;&#125;edge[M];int n,m;LL ans;int fa[N],ka[N],vis[N];LL gk[N][N],tmp[N][N];vector&lt;int&gt;gra[N];int findfa(int a,int b[])&#123;return a==b[a]?a:b[a]=findfa(b[a],b);&#125;LL det(LL a[][N],int n)&#123; for(int i=0;i&lt;n;i++)for(int j=0;j&lt;n;j++)a[i][j]%=mod; long long ret=1; for(int i=1;i&lt;n;i++) &#123; for(int j=i+1;j&lt;n;j++) while(a[j][i]) &#123; LL t=a[i][i]/a[j][i]; for(int k=i;k&lt;n;k++) a[i][k]=(a[i][k]-a[j][k]*t)%mod; for(int k=i;k&lt;n;k++) swap(a[i][k],a[j][k]); ret=-ret; &#125; if(a[i][i]==0)return 0; ret=ret*a[i][i]%mod; //ret%=mod; &#125; return (ret+mod)%mod;&#125;int main()&#123; while(scanf("%d%d%I64d",&amp;n,&amp;m,&amp;mod)==3) &#123; if(n==0 &amp;&amp; m==0 &amp;&amp; mod==0)break; memset(gk,0,sizeof(gk)); memset(tmp,0,sizeof(tmp)); memset(fa,0,sizeof(fa)); memset(ka,0,sizeof(ka)); memset(tmp,0,sizeof(tmp)); for(int i=0;i&lt;N;i++)gra[i].clear(); for(int i=0;i&lt;m;i++) scanf("%d%d%d",&amp;edge[i].a,&amp;edge[i].b,&amp;edge[i].c); sort(edge,edge+m); for(int i=1;i&lt;=n;i++)fa[i]=i,vis[i]=0; int pre=-1; ans=1; for(int h=0;h&lt;=m;h++) &#123; if(edge[h].c!=pre||h==m) &#123; for(int i=1;i&lt;=n;i++) if(vis[i]) &#123; int u=findfa(i,ka); gra[u].push_back(i); vis[i]=0; &#125; for(int i=1;i&lt;=n;i++) if(gra[i].size()&gt;1) &#123; for(int a=1;a&lt;=n;a++) for(int b=1;b&lt;=n;b++) tmp[a][b]=0; int len=gra[i].size(); for(int a=0;a&lt;len;a++) for(int b=a+1;b&lt;len;b++) &#123; int la=gra[i][a],lb=gra[i][b]; tmp[a][b]=(tmp[b][a]-=gk[la][lb]); tmp[a][a]+=gk[la][lb];tmp[b][b]+=gk[la][lb]; &#125; long long ret=(long long)det(tmp,len); ret%=mod; ans=(ans*ret%mod)%mod; for(int a=0;a&lt;len;a++)fa[gra[i][a]]=i; &#125; for(int i=1;i&lt;=n;i++) &#123; ka[i]=fa[i]=findfa(i,fa); gra[i].clear(); &#125; if(h==m)break; pre=edge[h].c; &#125; int a=edge[h].a,b=edge[h].b; int pa=findfa(a,fa),pb=findfa(b,fa); if(pa==pb)continue; vis[pa]=vis[pb]=1; ka[findfa(pa,ka)]=findfa(pb,ka); gk[pa][pb]++;gk[pb][pa]++; &#125; int flag=0; for(int i=2;i&lt;=n&amp;&amp;!flag;i++)if(ka[i]!=ka[i-1])flag=1; ans%=mod; printf("%I64d\n",flag?0:ans); &#125; return 0;&#125; 然后，最后这一题： HDU4305 综合性相当强的一题： 首先需要用计算几何的知识对平面上的点构图，形成图之后用Matrix-Tree定理求解生成树的个数。 但是题目数据比较大，对行列式求值的时候，需要用到高斯消元求上三角阵，行列式的值等于对角线元素的积。 由于是整数然后再mod。消元时需要求最小公倍数。还需要拓展欧几里得算法求逆元。 日后等有人能集我们队三者之大成之后，应该就可以挑战这道题了。]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
        <tag>生成树计数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 4009 Transfer water 最小树形图]]></title>
    <url>%2F2014%2F11%2F01%2F2014-11-01-HDU-4009%2F</url>
    <content type="text"><![CDATA[Transfer waterTime Limit: 5000/3000 MS (Java/Others) Memory Limit: 65768/65768 K (Java/Others) Problem DescriptionXiaoA lives in a village. Last year flood rained the village. So they decide to move the whole village to the mountain nearby this year. There is no spring in the mountain, so each household could only dig a well or build a water line from other household. If the household decide to dig a well, the money for the well is the height of their house multiplies X dollar per meter. If the household decide to build a water line from other household, and if the height of which supply water is not lower than the one which get water, the money of one water line is the Manhattan distance of the two households multiplies Y dollar per meter. Or if the height of which supply water is lower than the one which get water, a water pump is needed except the water line. Z dollar should be paid for one water pump. In addition,therelation of the households must be considered. Some households may do not allow some other households build a water line from there house. Now given the 3‐dimensional position (a, b, c) of every household the c of which means height, can you calculate the minimal money the whole village need so that every household has water, or tell the leader if it can’t be done. InputMultiple cases. First line of each case contains 4 integers n (1&lt;=n&lt;=1000), the number of the households, X (1&lt;=X&lt;=1000), Y (1&lt;=Y&lt;=1000), Z (1&lt;=Z&lt;=1000). Each of the next n lines contains 3 integers a, b, c means the position of the i‐th households, none of them will exceeded 1000. Then next n lines describe the relation between the households. The n+i+1‐th line describes the relation of the i‐th household. The line will begin with an integer k, and the next k integers are the household numbers that can build a water line from the i‐th household. If n=X=Y=Z=0, the input ends, and no output for that. OutputOne integer in one line for each case, the minimal money the whole village need so that every household has water. If the plan does not exist, print “poor XiaoA” in one line. Sample Input 2 10 20 301 3 22 4 11 22 1 20 0 0 0 Sample Output 30 题意有一个村庄需要修建供水系统。每户居民的房子都有一个三维坐标，每户居民可以选择自己挖井或者从其他居民家里引水。挖水井和引水分别需要花费不同的钱。每户居民有一个意愿表，只愿意对表内的居民家供水。最后要求计算出能为所有居民供水的最小花费。 分析本题具有明显的方向性，如果把供水用一条有向边表示的话，很明显能够发现本题抽象到最后是一个最小树形图。 因此思路就明确了，求解最小树形图可以直接套模板，需要做的就是根据题目条件来建图。 这里没有指定水源从哪来，但是至少是需要有1户人家挖水井才能有水。因此首先用0表示虚节点，从虚节点向所有人家连有向边，边长即为每户人家挖水井需要的花费，然后根据每户人家的意愿表加入有向边，处理一下边权。 最后套一个模板求解即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 4009************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;const int M = N*N;typedef struct nod&#123; int a,b,c;&#125; node;node edge[M];int predge[N],id[N],visit[N],in[N];int dmst(int root,int n,int m)//n表示点数，m表示边数，root表示根&#123; int u,v; int ret=0; while(true) &#123; for(int i=0;i&lt;n;i++) in[i]=INT_MAX; for(int i=0;i&lt;m;i++) &#123; if(edge[i].c&lt;in[edge[i].b]&amp;&amp;edge[i].a!=edge[i].b) &#123; predge[edge[i].b]=edge[i].a;//找出每个点的最小入弧 in[edge[i].b]=edge[i].c; &#125; &#125; for(int i=0;i&lt;n;i++) &#123; if(i==root) continue; if(in[i]==INT_MAX) return -1; &#125; in[root]=0; int cnt=0; memset(id,-1,sizeof(id)); memset(visit,-1,sizeof(visit)); for(int i=0;i&lt;n;i++) &#123; ret+=in[i];//进行缩圈 v=i; while(visit[v]!=i&amp;&amp;id[v]==-1&amp;&amp;v!=root) &#123; visit[v]=i; v=predge[v]; &#125; if(v!=root&amp;&amp;id[v]==-1) &#123; for(u=predge[v];u!=v;u=predge[u]) id[u]=cnt; id[v]=cnt++; &#125; &#125; if (cnt==0) break; for (int i=0;i&lt;n;i++) if (id[i]==-1) id[i]=cnt++; for (int i=0;i&lt;m;i++) &#123; v=edge[i].b;//进行缩点，重新标记。 edge[i].a=id[edge[i].a]; edge[i].b=id[edge[i].b]; if (edge[i].a!=edge[i].b) edge[i].c-=in[v]; &#125; n=cnt; root=id[root]; &#125; return ret;&#125;typedef struct hnod&#123; int x,y,z;&#125; hnode;hnode house[N];int dis(int x,int y)&#123; return abs(house[x].x-house[y].x)+abs(house[x].y-house[y].y)+abs(house[x].z-house[y].z);&#125;int main()&#123; int n,x,y,z; scanf("%d%d%d%d",&amp;n,&amp;x,&amp;y,&amp;z); while (n||x||y||z) &#123; for (int i=1;i&lt;=n;i++) scanf("%d%d%d",&amp;house[i].x,&amp;house[i].y,&amp;house[i].z); int tot=0; for (int i=1;i&lt;=n;i++) &#123; int p; scanf("%d",&amp;p); for (int j=1;j&lt;=p;j++) &#123; int d; scanf("%d",&amp;d); edge[tot].a=i; edge[tot].b=d; edge[tot].c=dis(i,d)*y; if (house[i].z&lt;house[d].z) edge[tot].c+=z; tot++; &#125; &#125; for (int i=1;i&lt;=n;i++) &#123; edge[tot].a=0; edge[tot].b=i; edge[tot].c=house[i].z*x; tot++; &#125; printf("%d\n",dmst(0,n+1,tot)); scanf("%d%d%d%d",&amp;n,&amp;x,&amp;y,&amp;z); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小树形图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 2121 Ice_cream’s world II 最小树形图 模板]]></title>
    <url>%2F2014%2F11%2F01%2F2014-11-01-HDU-2121%2F</url>
    <content type="text"><![CDATA[开始学习最小树形图，模板题。 Ice_cream’s world IITime Limit: 3000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionAfter awarded lands to ACMers, the queen want to choose a city be her capital. This is an important event in ice_cream world, and it also a very difficult problem, because the world have N cities and M roads, every road was directed. Wiskey is a chief engineer in ice_cream world. The queen asked Wiskey must find a suitable location to establish the capital, beautify the roads which let capital can visit each city and the project’s cost as less as better. If Wiskey can’t fulfill the queen’s require, he will be punishing. InputEvery case have two integers N and M (N&lt;=1000, M&lt;=10000), the cities numbered 0…N-1, following M lines, each line contain three integers S, T and C, meaning from S to T have a road will cost C. OutputIf no location satisfy the queen’s require, you must be output “impossible”, otherwise, print the minimum cost in this project and suitable city’s number. May be exist many suitable cities, choose the minimum number city. After every case print one blank. Sample Input 3 10 1 1 4 40 1 100 2 101 3 202 3 30 Sample Output impossible 40 0 分析一道裸的最小树形图模板题，算法思路比较简单，但是写起来感觉很不优美…仅作为模板记录吧。 本题不确定起点，故在读入完数据之后建立一个虚根，把所有的点都与这个虚根相连，边权为所有边边权和+1（保证虚边足够长），最后减掉即可。 确定起点的题可以从模板中删掉这部分。 然后就是这个模板是从0开始存储…强迫症好不爽啊… 详细参见：http://www.cnblogs.com/nanke/archive/2012/04/11/2441725.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 2121************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;const int M = N*N;typedef struct nod&#123; int a,b,c;&#125; node;node edge[M];int predge[N],id[N],visit[N];//id用来标记圈的int in[N];//入弧最小的int ansroot;int dmst(int root,int n,int m)//n表示点数，m表示边数，root表示根&#123; int u,v; int ret=0; while(true) &#123; for(int i=0;i&lt;n;i++) in[i]=INT_MAX; for(int i=0;i&lt;m;i++) &#123; if(edge[i].c&lt;in[edge[i].b]&amp;&amp;edge[i].a!=edge[i].b) &#123; predge[edge[i].b]=edge[i].a;//找出每个点的最小入弧 if(edge[i].a==root) ansroot=i; in[edge[i].b]=edge[i].c; &#125; &#125; for(int i=0;i&lt;n;i++) &#123; if(i==root) continue; if(in[i]==INT_MAX) return -1; &#125; in[root]=0; int cnt=0; memset(id,-1,sizeof(id)); memset(visit,-1,sizeof(visit)); for(int i=0;i&lt;n;i++) &#123; ret+=in[i];//进行缩圈 v=i; while(visit[v]!=i&amp;&amp;id[v]==-1&amp;&amp;v!=root) &#123; visit[v]=i; v=predge[v]; &#125; if(v!=root&amp;&amp;id[v]==-1) &#123; for(u=predge[v];u!=v;u=predge[u]) id[u]=cnt; id[v]=cnt++; &#125; &#125; if (cnt==0) break; for (int i=0;i&lt;n;i++) if (id[i]==-1) id[i]=cnt++; for (int i=0;i&lt;m;i++) &#123; v=edge[i].b;//进行缩点，重新标记。 edge[i].a=id[edge[i].a]; edge[i].b=id[edge[i].b]; if (edge[i].a!=edge[i].b) edge[i].c-=in[v]; &#125; n=cnt; root=id[root]; &#125; return ret;&#125;int main()&#123; freopen("2121.txt","r",stdin); int n,m,m1; while(scanf("%d%d",&amp;n,&amp;m)!=EOF) &#123; int a,b; int r=0; m1=m; for(int i=0;i&lt;m;i++) &#123; scanf("%d%d%d",&amp;edge[i].a,&amp;edge[i].b,&amp;edge[i].c); r+=edge[i].c; &#125; r++; for(int i=0;i&lt;n;i++) &#123; edge[m].a=n; edge[m].b=i; edge[m].c=r; m++; &#125; int ans=dmst(n,n+1,m); ansroot-=m1;//最小根对应的标号为i-m1 if(ans==-1||ans&gt;=2*r) printf("impossible\n"); else printf("%d %d\n",ans-r,ansroot); printf("\n"); &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>最小树形图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACM-ICPC Asia Regional Anshan]]></title>
    <url>%2F2014%2F10%2F31%2F2014-10-31-The-2014-ACM-ICPC-Asia-Regional-Anshan%2F</url>
    <content type="text"><![CDATA[继续复盘下一场Regional！ 【A】-_-/// 【B】模拟（之前每次遇到模拟、暴搜都直接跳了，题目太长也是一个原因…下次是在不行可以尝试一下） 【C】数论 互质、容斥？ 【D】数学推导（方差）+贪心 【E】签到DP 【F】-_-/// 【G】最小割的灵活运用 【H】搜索+打表 【I】签到题 【J】-_-/// 【K】计算几何、置换群（Polya计数） 【L】AC自动机+树链剖分 D、HDU 5073 GalaxyTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Special Judge Problem DescriptionGood news for us: to release the financial pressure, the government started selling galaxies and we can buy them from now on! The first one who bought a galaxy was Tianming Yun and he gave it to Xin Cheng as a present. To be fashionable, DRD also bought himself a galaxy. He named it Rho Galaxy. There are n stars in Rho Galaxy, and they have the same weight, namely one unit weight, and a negligible volume. They initially lie in a line rotating around their center of mass. Everything runs well except one thing. DRD thinks that the galaxy rotates too slow. As we know, to increase the angular speed with the same angular momentum, we have to decrease the moment of inertia. The moment of inertia I of a set of n stars can be calculated with the formula where wi is the weight of star i, di is the distance form star i to the mass of center. As DRD’s friend, ATM, who bought M78 Galaxy, wants to help him. ATM creates some black holes and white holes so that he can transport stars in a negligible time. After transportation, the n stars will also rotate around their new center of mass. Due to financial pressure, ATM can only transport at most k stars. Since volumes of the stars are negligible, two or more stars can be transported to the same position. Now, you are supposed to calculate the minimum moment of inertia after transportation. InputThe first line contains an integer T (T ≤ 10), denoting the number of the test cases. For each test case, the first line contains two integers, n(1 ≤ n ≤ 50000) and k(0 ≤ k ≤ n), as mentioned above. The next line contains n integers representing the positions of the stars. The absolute values of positions will be no more than 50000. OutputFor each test case, output one real number in one line representing the minimum moment of inertia. Your answer will be considered correct if and only if its absolute or relative error is less than 1e-9. Sample Input 2 3 2-1 0 14 2-2 -1 1 2 Sample Output 0 0.5 题意平面中有n个点，最多可以移动k个点，使原题条件中的I达到最小值。物理题，计算转动惯量。 分析开始的时候真是死活看不懂题…… 原题中的wi始终为1，即可忽略，然后di指的是x距离所有点重心的距离，这里的重心即是所有点坐标的平均值了。所以原式可以写成这样： 分析一下这个东西跟方差很像，所以可以写成： 然后： 最后： 基本的I的计算方法就推导出来了，接下来是k的问题。 为了让I尽可能的小，也就是让方差尽可能的小，那么最佳的方案就是把距离重心最远的k个点都移到重心去。则首先根据坐标排序，从两端开始枚举取数，前面取i个那最后就取k-i个，找到一种最佳的取数方案把最外围的点都移掉即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 5073************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;long long a[50010],sum[50010],sum2[50010];int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,k; scanf("%d%d",&amp;n,&amp;k); for (int i=1;i&lt;=n;i++) scanf("%lld",&amp;a[i]); if (n==k) &#123; printf("0\n"); continue; &#125; sort(&amp;a[1],&amp;a[n+1]); memset(sum,0,sizeof(sum)); memset(sum2,0,sizeof(sum2)); for (int i=1;i&lt;=n;i++) &#123; sum[i]=sum[i-1]+a[i]; sum2[i]=sum2[i-1]+a[i]*a[i]; &#125; double mi=sum2[n-k]-(double)sum[n-k]*sum[n-k]/(n-k); for (int i=1;i&lt;=k;i++) &#123; double temp=sum2[n-k+i]-sum2[i]-(double)(sum[n-k+i]-sum[i])*(sum[n-k+i]-sum[i])/(n-k); if (mi&gt;temp) mi=temp; &#125; printf("%.10f\n",mi); &#125; return 0;&#125; 杭电上%lld和%I64d老是出问题…%lld提交通不过的话改成%I64d应该就可以了 E、HDU 5074 Hatsune MikuTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Problem DescriptionHatsune Miku is a popular virtual singer. It is very popular in both Japan and China. Basically it is a computer software that allows you to compose a song on your own using the vocal package. Today you want to compose a song, which is just a sequence of notes. There are only m different notes provided in the package. And you want to make a song with n notes. Also, you know that there is a system to evaluate the beautifulness of a song. For each two consecutive notes a and b, if b comes after a, then the beautifulness for these two notes is evaluated as score(a, b). So the total beautifulness for a song consisting of notes a1, a2, . . . , an, is simply the sum of score(ai, ai+1) for 1 ≤ i ≤ n - 1. Now, you find that at some positions, the notes have to be some specific ones, but at other positions you can decide what notes to use. You want to maximize your song’s beautifulness. What is the maximum beautifulness you can achieve? InputThe first line contains an integer T (T ≤ 10), denoting the number of the test cases. For each test case, the first line contains two integers n(1 ≤ n ≤ 100) and m(1 ≤ m ≤ 50) as mentioned above. Then m lines follow, each of them consisting of m space-separated integers, the j-th integer in the i-th line for score(i, j)( 0 ≤ score(i, j) ≤ 100). The next line contains n integers, a1, a2, . . . , an (-1 ≤ ai ≤ m, ai ≠ 0), where positive integers stand for the notes you cannot change, while negative integers are what you can replace with arbitrary notes. The notes are named from 1 to m. OutputFor each test case, output the answer in one line. Sample Input 2 5 383 86 7715 93 3586 92 493 3 3 1 210 536 11 68 67 2982 30 62 23 6735 29 2 22 5869 67 93 56 1142 29 73 21 19-1 -1 5 -1 4 -1 -1 -1 4 -1 Sample Output 270625 分析简单的签到DP，f(i,j)表示摆到第i个符号，且最后一个取j的最大分数，则f(i,j)=max(f(i-1,k)+score(k,j)) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 5074************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int a[110];int f[110][60];int score[60][60];int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,m; scanf("%d%d",&amp;n,&amp;m); memset(score,0,sizeof(score)); for (int i=1;i&lt;=m;i++) for (int j=1;j&lt;=m;j++) scanf("%d",&amp;score[i][j]); for (int i=1;i&lt;=n;i++) scanf("%d",&amp;a[i]); memset(f,0,sizeof(f)); for (int i=2;i&lt;=n;i++) &#123; if (a[i]&gt;0) if (a[i-1]&gt;0) f[i][a[i]]=f[i-1][a[i-1]]+score[a[i-1]][a[i]]; else for (int j=1;j&lt;=m;j++) f[i][a[i]]=max(f[i][a[i]],f[i-1][j]+score[j][a[i]]); else if (a[i-1]&gt;0) for (int j=1;j&lt;=m;j++) f[i][j]=f[i-1][a[i-1]]+score[a[i-1]][j]; else for (int j=1;j&lt;=m;j++) for (int k=1;k&lt;=m;k++) f[i][j]=max(f[i][j],f[i-1][k]+score[k][j]); &#125; if (a[n]&gt;0) printf("%d\n",f[n][a[n]]); else &#123; int ma=0; for (int i=1;i&lt;=m;i++) if (ma&lt;f[n][i]) ma=f[n][i]; printf("%d\n",ma); &#125; &#125; return 0;&#125; I、HDU 5078 Osu!Time Limit: 2000/1000 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Special Judge Problem DescriptionOsu! is a very popular music game. Basically, it is a game about clicking. Some points will appear on the screen at some time, and you have to click them at a correct time. Now, you want to write an algorithm to estimate how diffecult a game is. To simplify the things, in a game consisting of N points, point i will occur at time ti at place (xi, yi), and you should click it exactly at ti at (xi, yi). That means you should move your cursor from point i to point i+1. This movement is called a jump, and the difficulty of a jump is just the distance between point i and point i+1 divided by the time between ti and ti+1. And the difficulty of a game is simply the difficulty of the most difficult jump in the game. Now, given a description of a game, please calculate its difficulty. InputThe first line contains an integer T (T ≤ 10), denoting the number of the test cases. For each test case, the first line contains an integer N (2 ≤ N ≤ 1000) denoting the number of the points in the game. Then N lines follow, the i-th line consisting of 3 space-separated integers, ti(0 ≤ ti &lt; ti+1 ≤ 106), xi, and yi (0 ≤ xi, yi ≤ 106) as mentioned above. OutputFor each test case, output the answer in one line. Your answer will be considered correct if and only if its absolute or relative error is less than 1e-9. Sample Input 2 52 1 93 7 25 9 06 6 37 6 01011 35 6723 2 2929 58 2230 67 6936 56 9362 42 1167 73 2968 19 2172 37 8482 24 98 Sample Output 9.219544457354.5893762558 分析签到题，唯一要注意的是数据范围，两个1e6数量级的数相乘有超int的可能。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACM-ICPC Asia Mudanjiang Regional]]></title>
    <url>%2F2014%2F10%2F30%2F2014-10-30-The-2014-ACM-ICPC-Asia-Mudanjiang-Regional%2F</url>
    <content type="text"><![CDATA[继续复盘之前的Regional……出题者说这一套题太简单，对当时没有AK很不满……真是醉了，弱校没法活了 【A】签到题 【B】树结构，树的中心 【C】-_-/// 【D】概率DP 【E】-_-/// 【F】树结构填数 【G】-_-/// 【H】模拟 【I】签到题 【J】-_-/// 【K】贪心，构造后缀表达式 A、ZOJ 3819 Average ScoreTime Limit: 2 SecondsMemory Limit: 65536 KB DescriptionBob is a freshman in Marjar University. He is clever and diligent. However, he is not good at math, especially in Mathematical Analysis. After a mid-term exam, Bob was anxious about his grade. He went to the professor asking about the result of the exam. The professor said: “Too bad! You made me so disappointed.”“Hummm… I am giving lessons to two classes. If you were in the other class, the average scores of both classes will increase.” Now, you are given the scores of all students in the two classes, except for the Bob’s. Please calculate the possible range of Bob’s score. All scores shall be integers within [0, 100]. InputThere are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: The first line contains two integers N (2 &lt;= N &lt;= 50) and M (1 &lt;= M &lt;= 50) indicating the number of students in Bob’s class and the number of students in the other class respectively. The next line contains N - 1 integers A1, A2, .., AN-1 representing the scores of other students in Bob’s class. The last line contains M integers B1, B2, .., BM representing the scores of students in the other class. OutputFor each test case, output two integers representing the minimal possible score and the maximal possible score of Bob. It is guaranteed that the solution always exists. Sample Input 2 4 35 5 54 4 36 55 5 4 5 31 3 2 2 1 Sample Output 4 42 4 分析签到，写个方程稍微推一下即可得到区间 B、ZOJ 3820 Building Fire StationsTime Limit: 5 SecondsMemory Limit: 131072 KBSpecial Judge DescriptionMarjar University is a beautiful and peaceful place. There are N buildings and N - 1 bidirectional roads in the campus. These buildings are connected by roads in such a way that there is exactly one path between any two buildings. By coincidence, the length of each road is 1 unit. To ensure the campus security, Edward, the headmaster of Marjar University, plans to setup two fire stations in two different buildings so that firefighters are able to arrive at the scene of the fire as soon as possible whenever fires occur. That means the longest distance between a building and its nearest fire station should be as short as possible. As a clever and diligent student in Marjar University, you are asked to write a program to complete the plan. Please find out two proper buildings to setup the fire stations. InputThere are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: The first line contains an integer N (2 &lt;= N &lt;= 200000). For the next N - 1 lines, each line contains two integers Xi and Yi. That means there is a road connecting building Xi and building Yi (indexes are 1-based). OutputFor each test case, output three integers. The first one is the minimal longest distance between a building and its nearest fire station. The next two integers are the indexes of the two buildings selected to build the fire stations. If there are multiple solutions, any one will be acceptable. Sample Input 2 41 21 31 45 1 22 33 44 5 Sample Output 1 1 21 2 4 题意题目给出一棵树作为一个城镇的地图，现在要在地图中设置两个救火点，使得最后任意一点发生火灾，救火都能及时到达，即使得救火点到达每个城镇的最大距离最小。 分析首先比较明显的能想到基于树的直径做一些文章。 浙大的出题者在他的人人日志中给出了本题结论的证明： http://blog.renren.com/blog/240107793/937020122?bfrom=01020100200 最终得出的结论是，两个点a、b必然都在树的直径上。（结论1） 之后的思路有两种： 作者的正解是扫出直径之后对每一个点向外扩展一次，得到每个点外延的最大距离，然后将其转化为一个一维的RMQ问题，二分一下区间即可。 第一次得到树的直径之后，从直径的中点切开，对两侧的两棵子树分别再做一次树的直径，找到的两个直径的中点就是最终的a和b。 关于思路2的证明： 1）子树的直径的中点距离这棵子树剩余的点一定是最远的，则这个点满足作为a、b点的条件。 2）然后简单推测一下，子树的直径中点一定在原树的直径上。这一点与结论1相吻合。 这个简单地举几个例子试一下就好，跟上面那个结论一样，能简单地想到大概是这样，但是原谅我数学不好……写不出证明。 3）上面两个结论组合一下，最后的交点就是a、b的最终位置了。 然后需要注意的是根据树的直径的结点个数，要分奇数和偶数分别讨论。偶数个结点只要从中间一条边切开即可，但是奇数个结点的情况就要继续分成两种情况，考虑中点分别切给左边那个子树和切给右边那棵子树的结果，取距离最小的那一个。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231/* ***********************************************MYID : Chen FanLANG : G++PROG : ZOJ 3820************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;#include &lt;bitset&gt;using namespace std;typedef struct nod&#123; int a,b;&#125; node;node edge[400010];int start[200010],num[200010],front[200010],list[200010];int listn;bitset&lt;200010&gt; flag;bool op(node a,node b)&#123; if (a.a==b.a) return a.b&lt;b.b; else return a.a&lt;b.a;&#125;int bfs(int s)&#123; queue&lt;int&gt; q; while (!q.empty()) q.pop(); int dist[200010]; memset(dist,0,sizeof(dist)); flag[s]=1; q.push(s); int ma=0,out=s; while (!q.empty()) &#123; int now=q.front(); q.pop(); for (int i=0;i&lt;num[now];i++) &#123; int next=edge[start[now]+i].b; if (!flag[next]) &#123; flag[next]=1; front[next]=now; q.push(next); dist[next]=dist[now]+1; if (ma&lt;dist[next]) &#123; ma=dist[next]; out=next; &#125; &#125; &#125; &#125; return out;&#125;void getlist(int s)&#123; if (!front[s]) &#123; listn=1; list[1]=s; return ; &#125; while (front[s]!=0) &#123; listn++; list[listn]=s; s=front[s]; &#125; listn++; list[listn]=s;&#125;int main()&#123; freopen("3820.txt","r",stdin); int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n; scanf("%d",&amp;n); for (int i=1;i&lt;n;i++) &#123; scanf("%d%d",&amp;edge[i*2].a,&amp;edge[i*2].b); edge[i*2-1].a=edge[i*2].b; edge[i*2-1].b=edge[i*2].a; &#125; int m=n*2-2; sort(&amp;edge[1],&amp;edge[m+1],op); int o=0; memset(num,0,sizeof(num)); for (int i=1;i&lt;=m;i++) &#123; if (o!=edge[i].a) &#123; o=edge[i].a; start[o]=i; &#125; num[o]++; &#125; flag.reset(); int p1=bfs(1); flag.reset(); memset(front,0,sizeof(front)); int p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); if (listn&amp;1) &#123; int a=list[listn/2],b=list[listn/2+1],c=list[listn/2+2]; flag.reset(); flag[b]=1; p1=bfs(a); flag.reset(); flag[b]=1; memset(front,0,sizeof(front)); p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); int ans1=listn/2; int ansa1=list[listn/2+1]; flag.reset(); flag[a]=1; p1=bfs(b); flag.reset(); flag[a]=1; memset(front,0,sizeof(front)); p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); ans1=max(ans1,listn/2); int ansb1=list[listn/2+1]; flag.reset(); flag[b]=1; p1=bfs(c); flag.reset(); flag[b]=1; memset(front,0,sizeof(front)); p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); int ans2=listn/2; int ansa2=list[listn/2+1]; flag.reset(); flag[c]=1; p1=bfs(b); flag.reset(); flag[c]=1; memset(front,0,sizeof(front)); p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); ans2=max(ans2,listn/2); int ansb2=list[listn/2+1]; if (ans1&lt;ans2) printf("%d %d %d\n",ans1,ansa1,ansb1); else printf("%d %d %d\n",ans2,ansa2,ansb2); &#125; else &#123; int a=list[listn/2],b=list[listn/2+1]; flag.reset(); flag[b]=1; p1=bfs(a); flag.reset(); flag[b]=1; memset(front,0,sizeof(front)); p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); int ans1=listn/2; int ansa1=list[listn/2+1]; flag.reset(); flag[a]=1; p1=bfs(b); flag.reset(); flag[a]=1; memset(front,0,sizeof(front)); p2=bfs(p1); listn=0; memset(list,0,sizeof(list)); getlist(p2); ans1=max(ans1,listn/2); int ansb1=list[listn/2+1]; printf("%d %d %d\n",ans1,ansa1,ansb1); &#125; &#125; return 0;&#125; D、ZOJ 3822 DominationTime Limit: 8 SecondsMemory Limit: 131072 KBSpecial Judge DescriptionEdward is the headmaster of Marjar University. He is enthusiastic about chess and often plays chess with his friends. What’s more, he bought a large decorative chessboard with N rows and M columns. Every day after work, Edward will place a chess piece on a random empty cell. A few days later, he found the chessboard was dominated by the chess pieces. That means there is at least one chess piece in every row. Also, there is at least one chess piece in every column. “That’s interesting!” Edward said. He wants to know the expectation number of days to make an empty chessboard of N × M dominated. Please write a program to help him. InputThere are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: There are only two integers N and M (1 &lt;= N, M &lt;= 50). OutputFor each test case, output the expectation number of days. Any solution with a relative or absolute error of at most 10-8 will be accepted. Sample Input 2 1 32 2 Sample Output 3.0000000000002.666666666667 题意有一个棋盘，目标是在棋盘中摆放棋子，使得最终每一个格子都能够被控制，这里控制的意思是满足这个格子的同行和同列上都有棋子存在。最终需要求出的是达成这种条件的期望。 分析比较裸的概率DP…原谅我看到的第一眼居然没想到…还是做题太少啊 f(i,j,k)表示使用k个棋子，控制完n行中的任意i行，m列中的任意j列所需要的概率。则最后的期望就是求一个f(n,m,k)*k的总和即可。 简单的方程如下： 1234567f(i,j,k) = Sum&#123; f(i,j,k+1)*(i*j-k)/(n*m-k), f(i+1,j,k+1)*(n-i)*j/(n*m-k), f(i,j+1,k+1)*i*(m-j)/(n*m-k), f(i+1,j+1,k+1)*(n-i)*(m-j)/(n*m-k)&#125; 方程中需要尤其注意的是当达成n行m列全部被控制之后，就可以结束了，即转移中f(n,m,k)-&gt;f(n,m,k+1)是不存在的！！！！因为这个原因悲剧了好久，后来对比其他人的代码才发现这个重大的问题。。。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* ***********************************************MYID : Chen FanLANG : G++PROG : ZOJ 3822************************************************ *//* ***********************************************f(i,j,k) = Sum&#123; f(i,j,k+1)*(i*j-k)/(n*m-k), f(i+1,j,k+1)*(n-i)*j/(n*m-k), f(i,j+1,k+1)*i*(m-j)/(n*m-k), f(i+1,j+1,k+1)*(n-i)*(m-j)/(n*m-k)&#125;************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;double f[60][60][4000];int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,m; scanf("%d%d",&amp;n,&amp;m); memset(f,0,sizeof(f)); f[1][1][1]=1; for (int k=1;k&lt;=m*n;k++) for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=m;j++) &#123; if (k&lt;max(i,j)) continue; if (k+1&lt;=i*j&amp;&amp;i!=n||j!=m) f[i][j][k+1]+=f[i][j][k]*(i*j-k)/(n*m-k); if (k+1&lt;=(i+1)*j) f[i+1][j][k+1]+=f[i][j][k]*(n-i)*j/(n*m-k); if (k+1&lt;=i*(j+1)) f[i][j+1][k+1]+=f[i][j][k]*i*(m-j)/(n*m-k); if (k+1&lt;=(i+1)*(j+1)) f[i+1][j+1][k+1]+=f[i][j][k]*(n-i)*(m-j)/(n*m-k); &#125; double ans=0; for (int i=max(n,m);i&lt;=m*n;i++) ans+=f[n][m][i]*i; printf("%.10f\n",ans); &#125; return 0;&#125; I、ZOJ 3827 Information EntropyTime Limit: 2 SecondsMemory Limit: 65536 KBSpecial Judge DescriptionInformation Theory is one of the most popular courses in Marjar University. In this course, there is an important chapter about information entropy. Entropy is the average amount of information contained in each message received. Here, a message stands for an event, or a sample or a character drawn from a distribution or a data stream. Entropy thus characterizes our uncertainty about our source of information. The source is also characterized by the probability distribution of the samples drawn from it. The idea here is that the less likely an event is, the more information it provides when it occurs. Generally, “entropy” stands for “disorder” or uncertainty. The entropy we talk about here was introduced by Claude E. Shannon in his 1948 paper “A Mathematical Theory of Communication”. We also call it Shannon entropy or information entropy to distinguish from other occurrences of the term, which appears in various parts of physics in different forms. Named after Boltzmann’s H-theorem, Shannon defined the entropy Η (Greek letter Η, η) of a discrete random variable X with possible values {x1, x2, …, xn} and probability mass function P(X) as: H(X)=E(−ln(P(x))) Here E is the expected value operator. When taken from a finite sample, the entropy can explicitly be written as H(X)=−∑i=1nP(xi)log b(P(xi)) Where b is the base of the logarithm used. Common values of b are 2, Euler’s number e, and 10. The unit of entropy is bit for b = 2, nat for b = e, and dit (or digit) for b = 10 respectively. In the case of P(xi) = 0 for some i, the value of the corresponding summand 0 logb(0) is taken to be a well-known limit: 0log b(0)=limp→0+plog b(p) Your task is to calculate the entropy of a finite sample with N values. Input There are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: The first line contains an integer N (1 &lt;= N &lt;= 100) and a string S. The string S is one of “bit”, “nat” or “dit”, indicating the unit of entropy. In the next line, there are N non-negative integers P1, P2, .., PN. Pi means the probability of the i-th value in percentage and the sum of Pi will be 100. OutputFor each test case, output the entropy in the corresponding unit. Any solution with a relative or absolute error of at most 10-8 will be accepted. Sample Input 3 3 bit25 25 507 nat1 2 4 8 16 32 3710 dit10 10 10 10 10 10 10 10 10 10 Sample Output 1.5000000000001.4808108324651.000000000000 分析签到题，难点可能是在题面上，学过信息论的话看到样例就能直接打代码了。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2013 ACMICPC Asia Regional Chengdu]]></title>
    <url>%2F2014%2F10%2F27%2F2014-10-27-The-2013-ACMICPC-Asia-Regional-Chengdu%2F</url>
    <content type="text"><![CDATA[还有19天出发北京站，今年北京站的出题方是上交，去年他们出的成都现场的赛题，首先复盘一下。 去年的成都是我经历的第一次现场赛，也是近距离第一次见到了CLJ的真人，最后也是被虐惨了，那时候是声闻大神带着我们去的，也是在那次现场之后，深深地感受到了差距。现在我们进步了，只可惜选手都在发展，比赛也在发展，别人大概是进步得更多吧，上场西安赛站也只能遗憾。 没想到最后一场居然又能碰到开场第一次能够遇上的出题方，也是个奇妙的巧合吧。 【A】构造图 【B】模拟 【C】-_-/// 【D】BFS（写的时候遇到一个大坑） 【E】计算几何 【F】构造生成树 【G】在线AC自动机 【H】签到题 【I】模拟（用STL中的set） 【J】数论 A、HDU 4781 Assignment For PrincessTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Special Judge Problem DescriptionLong long ago, in the Kingdom Far Far Away, there lived many little animals. And you are the beloved princess who is marrying the prince of a rich neighboring kingdom. The prince, who turns out to be a handsome guy, offered you a golden engagement ring that can run computer programs! The wedding will be held next summer because your father, the king, wants you to finish your university first. But you did’t even have a clue on your graduation project. Your terrible project was to construct a map for your kingdom. Your mother, the queen, wanted to make sure that you could graduate in time. Or your wedding would have to be delayed to the next winter. So she told you how your ancestors built the kingdom which is called the Roads Principle: Your kingdom consists of N castles and M directed roads. There is at most one road between a pair of castles. There won’t be any roads that start at one castle and lead to the same one. She hoped those may be helpful to your project. Then you asked your cousin Coach Pang (Yes, he is your troubling cousin, he always asks you to solve all kinds of problems even you are a princess.), the Minister of Traffic, about the castles and roads. Your cousin, sadly, doesn’t have a map of the kingdom. Though he said the technology isn’t well developed and it depends on your generation to contribute to the map, he told you the Travelers Guide, the way travelers describe the amazing road system: No matter which castle you start with, you can arrive at any other castles. Traveling on theM roads will take 1, 2, 3, … ,M days respectively, no two roads need the same number of days. You can take a round trip starting at any castle, visiting a sequence of castles, perhaps visiting some castles or traveling on some roads more than once, and finish your journey where you started. The total amount of days spent on any round trip will be a multiple of three. But after a month, you still couldn’t make any progress. So your brother, the future king, asked your university to assign you a simpler project. And here comes the new requirements. Construct a map that satisfies both the Roads Principle and the Travelers Guide when N and M is given. There would probably be several solutions, but your project would be accepted as long as it meets the two requirements. Now the task is much easier, furthermore your fiance sent two assistants to help you. Perhaps they could finish it within 5 hours and you can think of your sweet wedding now. InputThe first line contains only one integer T, which indicates the number of test cases. For each test case, there is one line containing two integers N, M described above.(10 &lt;= N &lt;= 80, N+3 &lt;= M &lt;= N2/7 ) OutputFor each test case, first output a line “Case #x:”, where x is the case number (starting from 1). Then output M lines for each test case. Each line contains three integers A, B, C separated by single space, which denotes a road from castle A to castle B and the road takes C days traveling. Oh, one more thing about your project, remember to tell your mighty assistants that if they are certain that no map meets the requirements, print one line containing one integer -1 instead. Note that you should not print any trailing spaces. Sample Input 1 6 8 Sample Output Case #1:1 2 12 3 22 4 33 4 44 5 55 6 75 1 66 1 8 HintThe restrictions like N &gt;= 10 will be too big for a sample. So the sample is just a simple case for the detailed formats of input and output,and it may be helpful for a better understanding. Anyway it won’t appear in actual test cases. 题意要求构造一张图，同时满足以下7个条件： 1.有n个点和m条有向边； 2.两点点之间无重边； 3.无自环； 4.整张图都是连通的； 5.m条边的长度分别为1、2、…、m； 6.从任意一点出发都能够回到出发点，点和边可多次经过。 7.每一次Travel走过的总长度都一定是3的倍数。 分析前面几个条件都是对图的基本性质的保证。从一次Traval可重复经过点和边的性质+Travel总长度是3的倍数这两个条件入手，分析一下即所有有向环的总长都是3的倍数。 首先构造一个简单的回路，从1至n每两点依次连边，这样就用掉了1~n-1，第n条边放上n、n+1或者n+2使得第一个环总长为三的倍数。 接下来就是把剩下那些边放到图中去了，然后需要的是在放边的同时保持原有的图的性质不变。设下一条要放上去的边长为x，dist[i][j]表示点i与点j之间的路径长度，则(dist[i][j]+dist[j][i])%3==0，也即dist[i][j]%3+dist[j][i]%3==3。则对于x，找到两个点i、j使得dist[i][j]%3==x%3，则在i、j之间加上边x之后x就能够替代原来的长边，构成一个新的总长为3的倍数的有向环。依次解决完所有的剩余边即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU 4781************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;using namespace std;int mapl[81][81],dist[81][81];bool use[81];int main()&#123; int t,n,m; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; printf("Case #%d:\n",tt); scanf("%d%d",&amp;n,&amp;m); int tot=0; bool bq=false; memset(mapl,0,sizeof(mapl)); memset(dist,-1,sizeof(dist)); for (int i=1;i&lt;n;i++) &#123; tot+=i; mapl[i][i+1]=i; dist[i][i+1]=i; &#125; for (int i=n;i&lt;n+3;i++) if ((tot+i)%3==0) &#123; mapl[n][1]=i; dist[n][1]=i; break; &#125; memset(use,0,sizeof(use)); use[mapl[n][1]]=true; for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=n;j++) if (i!=j) for (int k=1;k&lt;=n;k++) if (i!=k&amp;&amp;j!=k) if (dist[j][i]&gt;0&amp;&amp;dist[i][k]&gt;0) if (dist[j][k]==-1) dist[j][k]=dist[j][i]+dist[i][k]; for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=n;j++) if (dist[i][j]&gt;-1) dist[i][j]%=3; //for (int i=1;i&lt;=n;i++) //for (int j=1;j&lt;=n;j++) printf("%d %d %d\n",i,j,dist[i][j]); for (int i=n;i&lt;=m;i++) &#123; if (!use[i]) &#123; for (int j=1;j&lt;=n;j++) for (int k=1;k&lt;=n;k++) if (!use[i]) if (mapl[j][k]==0&amp;&amp;mapl[k][j]==0&amp;&amp;dist[j][k]&gt;-1&amp;&amp;dist[j][k]%3==i%3) &#123; mapl[j][k]=i; use[i]=true; &#125; &#125; if (!use[i]) &#123; printf("-1\n"); bq=true; break; &#125; &#125; if (!bq) for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=n;j++) if (mapl[i][j]&gt;0) printf("%d %d %d\n",i,j,mapl[i][j]); &#125; return 0;&#125; D、HDU 4784 Dinner Coming SoonTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 102400/102400 K (Java/Others) Problem DescriptionCoach Pang loves his boyfriend Uncle Yang very much. Today is Uncle Yang’s birthday, Coach Pang wants to have a romantic candlelit dinner at Uncle Yang’s house and he has to arrive there in T minutes. There are N houses in their city numbered from 1 to N. Coach Pang lives in house 1 while Uncle Yang lives in house N. The houses are connected byM directed roads. It takes some time and usually a fee to pass one road. Coach Pang wants to reach Uncle Yang’s house before the dinner starts with as much money as possible. But the matter is not so simple. Coach Pang decides to do some salt trade on the way to Uncle Yang’s house. The host of each house offers a price of a bag of salt, so Coach Pang can make a profit from the price differences. Each time when Coach Pang arrives at a house (except the house 1 and the house N). He can buy one bag of salt, sell one bag of salt or do nothing. Coach Pang can carry at most B bags of salt with him, and he carries no salt when he leaves his house. The trading is so efficient that the time cost of trading can be ignored. However, the problem is more complicated than imagine. Coach Pang has a handheld device that can perform a journey around K parallel universes numbered from 0 to K-1. Coach Pang lives in the universe 0. When Coach Pang uses the device in universe i, he will be transported to the same place and the same time of universe (i+1) modK. The host of the house at the same place in different universe may offer a different price of salt. Luckily, the time cost and fee of the city roads are uniform among the K universes. The journey between universes costs no time but Coach Pang has to stand still watching the ads on the device for one minute every time before the device works. Remember, Coach Pang should never visit house 1 or house N in a universe other than universe 0, because the situation might become uncontrollable if he bumps into himself or his boyfriend in another universe. The time is running out. Coach Pang asks you to tell him whether he can arrive at Uncle Yang’s house in time, and how much money Coach Pang can have at most when the dinner starts. Coach Pang has R yuan at the start, and will end his journey immediately once he arrives at Uncle Yang’s house. He must arrive at Uncle Yang’s house in T minutes, and he can’t have negative amount of money anywhere anytime. Please help him! InputThe first line of the input is an integer C representing the number of test cases. For each test case, the first line will contain 6 integers N, M, B, K, R, T, as described above. (2 &lt;= N &lt;= 100, 0 &lt;= M &lt;= 200, 1 &lt;= B &lt;= 4, 2 &lt;= K &lt;= 5, 0 &lt;= R &lt;= 105, 0 &lt;= T &lt;= 200) The following K lines contain N integers each, indicating the price pij (0 &lt;= i &lt; K, 1 &lt;= j &lt;= N) for a bag of salt offered by the host of house j in the universe i. The price of house 1 and house N will be marked as -1.(1 &lt;= pij &lt;= 100) Then M lines follow, each contains 4 integers a, b, t and m, indicating that there is a road from house a to house b that costs t minutes of time and m yuan of money. (1 &lt;= a,b &lt;= N, a&lt;&gt; b, 1 &lt;= t &lt;=15, 0 &lt;= m &lt;= 100) OutputFor each test case, output one line containing “Case #x: y”, where x is the case number (starting from 1) and y is the most money Coach Pang can have if he can have dinner with Uncle Yang on time. Print “Forever Alone” otherwise. Sample Input 2 3 2 1 2 10 6-1 1 -1-1 5 -11 2 1 02 3 1 12 2 1 2 5 5-1 -1-1 -11 2 10 21 2 2 10 Sample Output Case #1: 17Case #2: Forever Alone 题意给出一张有向图，主人公需要在时间T之内从编号为1的点走到编号为n的点。图中存在一个货币系统，主人公在初始状态的时候身上有R单位的货币，在图中除了1和n之外的所有点都可以买进或者卖出一包盐，每一个点的盐价不同，因此主人公可以通过这种方式在不同的位置进行盐的买卖赚取收益。图中的每一条有向边都有两个代价，时间花费和货币花费，而且除此之外这张图中还有K个“平行世界”，分别标记为0~K-1，在每个不同的平行世界中每个点的盐价也不同，主人公可以花费1单位时间从第i个平行世界的某个点中跳跃到第(i+1)%K个平行世界的同一个点上，唯一的限制是1点和n点只能从第0个平行世界中进入，且一旦到达第n点，这次遍历就结束了。 现在，最终的问题就是主人公从1点出发走到n点，身上最多可能持有多少单位的货币。 分析写这道题的时候代码遭遇神坑啊！！！！ 首先构图非常简单，相对于一般的图，本题只是把普通的单一代价有向边再加上一个代价，平行世界的跳转只是从i跳到(i+1)%k，且花费只有1单位时间。对于图中每一个点的情况，很容易能够想到状态转移方程式，用f(i,j,k,l)表示当前在第i点，花费了j时间，手上持有k包盐，目前在平行世界k的最大持有货币数。出现的位置状态转移只有两种：沿着有向边走向下一个点或者进行平行世界跳转。位置转移完成之后，接下来发生的价值转移有三种，不作任何操作、买入一包盐或者卖出一包盐。大致的方程是这样的，这里省略了一些转移条件： 123456f(i,j,k,l)=max&#123; f(p,j+edge[i,p].time,k,l), f(p,j+edge[i,p].time,k+1,l)-price[l][p], f(p,j+edge[i,p].time,k-1,l)+price[l][p], f(i,j+1,k,(l+1)%K), f(i,j+1,k+1,(l+1)%K)-price[(l+1)%K][i], f(i,j+1,k-1,(l+1)%K)+price[(l+1)%K][i] &#125; 最直接的想法是使用SPFA作为整体的DP框架，虽然整体数据范围不太大，但是这里反复进出队列的话，超时的可能性非常大。这里DP遇到的最大问题是后效性，关于这一点，我们可以看到状态递推的方向是花费时间增加的方向。普通的SPFA不能保证后面加入队列的状态时间一定是递增的，因此需要反复多次进出队列，所以在这里使用一个花费时间递增的优先队列来替换掉SPFA中的普通队列，保证状态按照花费时间递增的顺序来扩展，就能解决DP的后效性问题，记录一下加入过队列的点不用再次加入即可节省下来大量的时间。 写代码的时候因为一点失误被坑了好久……吸取的教训是：DP中后续的六种状态的值必须从原始状态出发！其实这个原本是非常直观的，写到图论的结构里面之后因为代码太多了，居然简单地就直接从1往2、3写，从4往5、6写了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU4784************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;typedef struct nod&#123; int a,b,c,d;&#125; node;int start[110],num[110];int p[10][110];int n,m,b,k,r,tim;node edge[210];typedef struct qnod&#123; int s,t,b,k; friend bool operator &lt; (qnod a,qnod b) &#123; return a.t&gt;b.t; &#125;&#125; qnode;int f[110][210][10][10];bool op(node a,node b)&#123; return a.a&lt;b.a;&#125;int main()&#123; freopen("test.txt","r",stdin); int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; scanf("%d%d%d%d%d%d",&amp;n,&amp;m,&amp;b,&amp;k,&amp;r,&amp;tim); for (int i=0;i&lt;k;i++) for (int j=1;j&lt;=n;j++) scanf("%d",&amp;p[i][j]); for (int i=1;i&lt;=m;i++) scanf("%d%d%d%d",&amp;edge[i].a,&amp;edge[i].b,&amp;edge[i].c,&amp;edge[i].d); sort(&amp;edge[1],&amp;edge[m+1],op); int o=0; memset(num,0,sizeof(num)); for (int i=1;i&lt;=m;i++) &#123; if (o!=edge[i].a) &#123; o=edge[i].a; start[o]=i; &#125; num[o]++; &#125; printf("Case #%d: ",tt); priority_queue&lt;qnode&gt; q; while (!q.empty()) q.pop(); memset(f,-1,sizeof(f)); qnode st; st.s=1;st.t=0;st.b=0;st.k=0; f[1][0][0][0]=r; q.push(st); int ans=-1; while (!q.empty()) &#123; qnode now=q.top(); q.pop(); for (int i=0;i&lt;num[now.s];i++) &#123; qnode next; int mm=f[now.s][now.t][now.b][now.k]-edge[start[now.s]+i].d; next.t=now.t+edge[start[now.s]+i].c; if (mm&lt;0||next.t&gt;tim) continue; next.s=edge[start[now.s]+i].b; next.b=now.b; next.k=now.k; if (next.s==1&amp;&amp;next.k!=0) continue; if (next.s==n) &#123; if (next.k!=0) continue; if (ans&lt;mm) ans=mm; &#125; else &#123; if (f[next.s][next.t][next.b][next.k]&lt;0) q.push(next); if (f[next.s][next.t][next.b][next.k]&lt;mm) f[next.s][next.t][next.b][next.k]=mm; if (next.s==1) continue; if (next.b&lt;b&amp;&amp;f[next.s][next.t][next.b][next.k]&gt;=p[next.k][next.s]) &#123; qnode temp=next; temp.b++; mm=f[now.s][now.t][now.b][now.k]-edge[start[now.s]+i].d-p[next.k][next.s]; //!!!!! if (f[temp.s][temp.t][temp.b][temp.k]&lt;0) q.push(temp); if (f[temp.s][temp.t][temp.b][temp.k]&lt;mm) f[temp.s][temp.t][temp.b][temp.k]=mm; &#125; if (next.b&gt;0) &#123; qnode temp=next; temp.b--; mm=f[now.s][now.t][now.b][now.k]-edge[start[now.s]+i].d+p[next.k][next.s]; if (f[temp.s][temp.t][temp.b][temp.k]&lt;0) q.push(temp); if (f[temp.s][temp.t][temp.b][temp.k]&lt;mm) f[temp.s][temp.t][temp.b][temp.k]=mm; &#125; &#125; &#125; if (now.s==1) continue; qnode next=now; int mm=f[now.s][now.t][now.b][now.k]; next.k=(next.k+1)%k; next.t++; if (next.t&gt;tim) continue; if (f[next.s][next.t][next.b][next.k]&lt;0) q.push(next); if (f[next.s][next.t][next.b][next.k]&lt;mm) f[next.s][next.t][next.b][next.k]=mm; if (next.b&lt;b&amp;&amp;f[next.s][next.t][next.b][next.k]&gt;=p[next.k][next.s]) &#123; qnode temp=next; temp.b++; mm=f[now.s][now.t][now.b][now.k]-p[next.k][next.s]; if (f[temp.s][temp.t][temp.b][temp.k]&lt;0) q.push(temp); if (f[temp.s][temp.t][temp.b][temp.k]&lt;mm) f[temp.s][temp.t][temp.b][temp.k]=mm; &#125; if (next.b&gt;0) &#123; qnode temp=next; temp.b--; mm=f[now.s][now.t][now.b][now.k]+p[next.k][next.s]; if (f[temp.s][temp.t][temp.b][temp.k]&lt;0) q.push(temp); if (f[temp.s][temp.t][temp.b][temp.k]&lt;mm) f[temp.s][temp.t][temp.b][temp.k]=mm; &#125; &#125; if (ans&lt;0) printf("Forever Alone\n"); else printf("%d\n",ans); &#125; return 0;&#125; F、HDU 4786 Fibonacci TreeTime Limit: 4000/2000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionCoach Pang is interested in Fibonacci numbers while Uncle Yang wants him to do some research on Spanning Tree. So Coach Pang decides to solve the following problem: Consider a bidirectional graph G with N vertices and M edges. All edges are painted into either white or black. Can we find a Spanning Tree with some positive Fibonacci number of white edges? (Fibonacci number is defined as 1, 2, 3, 5, 8, … ) InputThe first line of the input contains an integer T, the number of test cases. For each test case, the first line contains two integers N(1 &lt;= N &lt;= 105) and M(0 &lt;= M &lt;= 105). Then M lines follow, each contains three integers u, v (1 &lt;= u,v &lt;= N, u&lt;&gt; v) and c (0 &lt;= c &lt;= 1), indicating an edge between u and v with a color c (1 for white and 0 for black). OutputFor each test case, output a line “Case #x: s”. x is the case number and s is either “Yes” or “No” (without quotes) representing the answer to the problem. Sample Input 2 4 41 2 12 3 13 4 11 4 05 61 2 11 3 11 4 11 5 13 5 14 2 1 Sample Output Case #1: YesCase #2: No 题意给出一张无向图，图中每一条边都有黑或者白两种颜色。询问是否可以从这张图中找到这样一个生成树，使得白边的条数为一个斐波那契数。 分析考虑使用Kruskal构造最小生成树的方法，如果把颜色作为边权值，则可以构造出白边优先或者黑边优先的生成树。 白边优先树是尽可能多的取白边，能得到最多的使生成树成立的白边数，黑边优先树则是尽可能多的取黑边，得到最少的使生成树成立的白边数。从白边优先树出发的话，如果我们取掉一条白边，添加一条黑边，则构成的新图一定还是一棵树。即根据这个原理，可以构造出白边数介于最少白边数和最大白边数之间的所有生成树。接下来只要判断一下介于这两个数之间是不是存在斐波那契数即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU4786************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;bitset&gt;typedef struct nod&#123; int a,b,color;&#125; node;node edge[200010];using namespace std;bool op(node a,node b)&#123; return a.color&lt;b.color;&#125;int n,m;int father[100010];int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link(int x,int y)&#123; father[getfather(x)]=getfather(y);&#125; int kruskal1()&#123; for (int i=1;i&lt;=n;i++) father[i]=i; int num=0,tot=0; for (int i=1;i&lt;=m;i++) if (getfather(edge[i].a)!=getfather(edge[i].b)) &#123; link(edge[i].a,edge[i].b); if (edge[i].color) num++; tot++; &#125; if (tot!=n-1) return -1; else return num;&#125;int kruskal2()&#123; for (int i=1;i&lt;=n;i++) father[i]=i; int num=0,tot=0; for (int i=m;i&gt;=1;i--) if (getfather(edge[i].a)!=getfather(edge[i].b)) &#123; link(edge[i].a,edge[i].b); if (edge[i].color) num++; tot++; &#125; if (tot!=n-1) return -1; else return num;&#125;int main()&#123; int tot=0; bitset&lt;100010&gt;fbn; fbn.reset(); int a=1,b=1,c; fbn[1]=1; while (1) &#123; c=a+b; if (c&gt;=100010) break; fbn[c]=1; a=b; b=c; &#125; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; scanf("%d%d",&amp;n,&amp;m); for (int i=1;i&lt;=m;i++) scanf("%d%d%d",&amp;edge[i].a,&amp;edge[i].b,&amp;edge[i].color); printf("Case #%d: ",tt); sort(&amp;edge[1],&amp;edge[m+1],op); int b1=kruskal1(); if (b1&lt;0) &#123; printf("No\n"); continue; &#125; int b2=kruskal2(); bool done=false; for (int i=b1;i&lt;=b2;i++) if (fbn[i]) &#123; done=true; break; &#125; if (done) printf("Yes\n"); else printf("No\n"); &#125; return 0;&#125; H、HDU 4788 Hard Disk DriveTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionYesterday your dear cousin Coach Pang gave you a new 100MB hard disk drive (HDD) as a gift because you will get married next year. But you turned on your computer and the operating system (OS) told you the HDD is about 95MB. The 5MB of space is missing. It is known that the HDD manufacturers have a different capacity measurement. The manufacturers think 1 “kilo” is 1000 but the OS thinks that is 1024. There are several descriptions of the size of an HDD. They are byte, kilobyte, megabyte, gigabyte, terabyte, petabyte, exabyte, zetabyte and yottabyte. Each one equals a “kilo” of the previous one. For example 1 gigabyte is 1 “kilo” megabytes. Now you know the size of a hard disk represented by manufacturers and you want to calculate the percentage of the “missing part”. InputThe first line contains an integer T, which indicates the number of test cases. For each test case, there is one line contains a string in format “number[unit]” where number is a positive integer within [1, 1000] and unit is the description of size which could be “B”, “KB”, “MB”, “GB”, “TB”, “PB”, “EB”, “ZB”, “YB” in short respectively. OutputFor each test case, output one line “Case #x: y”, where x is the case number (starting from 1) and y is the percentage of the “missing part”. The answer should be rounded to two digits after the decimal point. Sample Input 2 100[MB]1[B] Sample Output Case #1: 4.63%Case #2: 0.00% 分析签到题 J、HDU 4790 Just RandomTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionCoach Pang and Uncle Yang both love numbers. Every morning they play a game with number together. In each game the following will be done: Coach Pang randomly choose a integer x in [a, b] with equal probability. Uncle Yang randomly choose a integer y in [c, d] with equal probability. If (x + y) mod p = m, they will go out and have a nice day together. Otherwise, they will do homework that day. For given a, b, c, d, p and m, Coach Pang wants to know the probability that they will go out. InputThe first line of the input contains an integer T denoting the number of test cases. For each test case, there is one line containing six integers a, b, c, d, p and m(0 &lt;= a &lt;= b &lt;= 109, 0 &lt;=c &lt;= d &lt;= 109, 0 &lt;= m &lt; p &lt;= 109). OutputFor each test case output a single line “Case #x: y”. x is the case number and y is a fraction with numerator and denominator separated by a slash (‘/‘) as the probability that they will go out. The fraction should be presented in the simplest form (with the smallest denominator), but always with a denominator (even if it is the unit). Sample Input 4 0 5 0 5 3 00 999999 0 999999 1000000 00 3 0 3 8 73 3 4 4 7 0 Sample Output Case #1: 1/3Case #2: 1/1000000Case #3: 0/1Case #4: 1/1 题意给定两个区间，每次分别从这两个区间中取出一个数求和，问有多少种组合的和是能够对p取模之后等于m的概率。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACMICPC Asia Regional Xian]]></title>
    <url>%2F2014%2F10%2F27%2F2014-10-27-The-2014-ACMICPC-Asia-Regional-Xian%2F</url>
    <content type="text"><![CDATA[2题继续遗憾收场，每次都是只差最后一步。这一场却是之前那么多场中感觉距离奖牌最近的时候。好好总结一下经验教训，复盘之后好好准备下一场北京的最后一战吧。 一开始的状态非常不错，10分钟跟榜完成1A，第二个题是K，虽说开始的时候卡了不少时间，后来还是努力在1小时左右的时候出了，算是跟榜跟的还行。最终的问题应该是出在后面的3小时的策略上。 教训： 1.把3小时全部赌在一题上，现在想想确实是有点冒险，下次应该最多留一人或者两人继续磕题就好，剩下的可以看下别的。从这一点上来说，从一开始一个队友一人做F，然后一个还在看计算几何，我是把各道题差不多都翻了一遍，但是没有看到明显可出的题。这个时候状态还没算偏了太多。下面就是第二个教训； 2.不要轻易下决心把所有的胜算都赌在一道题上，决定要赌上的话也要特别确认此题可出才可。我们最终的选择是因为发现其他的题可出性不太大了，然后毅然决定放弃其他的题目，死磕出这一题就是最后的胜利了。最最遗憾的是没有让队友敲一下计算几何的，其实这题也是可出。等到最后结束时发现组合数不对而且公式也不对的时候已经来不及了； 3.今天在出题者讲解出题思路的时候特别需要重视的是逆向思维，几道题里面都需要逆向思维考虑一下。 4.牢牢跟榜确实没错，但是有时候也是要注意榜单存在一定误导性，不能盲目追一题死磕。前两题的榜单跟的很好，F题则真的是个坑了，早点发现此题确实知识不够不可出的话应该会磕一下其他题。 【A】签到题 【B】暴搜（一道裸搜题，遗憾题目太长，当时也没人想着出这个，太可惜） 【C】构造最大密度子图（一开始想的是DP是否可解，发现后效性是解决不了的，遗憾收场。妈蛋！谁能想到把它转化为图论题做啊！） 【D】树结构+遍历顺序 【E】计算几何（遗憾没有让队友试一下） 【F】组合数+容斥原理（坑死在这里） 【G】回文自动机（毛子的正解）或者字符串hash可出（这个不会） 【H】博弈论 【I】IP地址，路由表（一直没看懂题） 【J】图 【K】类似GCD的处理题 暂时还没有被收录到OJ，只能出了之后再复盘了。 A分析签到题 B分析一道裸的搜索题，稍微加一点剪枝即可。主要是读题（T_T……） 启发坑死的时候不妨看看题目很长的题……说不定就水了呢……T_T C分析原题求的是逆序对/序列长度最大的子序列，一种神奇的想法是在逆序对的两个数之间连边，然后做最大密度子图（边数/点数最大的子图）。 启发还是做题量太少，思维跟不上。想不到转化为图做这是第一点，第二点是就算当时想到了这个方案，估计我也写不出来…… K分析类似GCD的辗转相除相减，开始时思考方向有点不对，本来还打算从两个数找规律的角度去考虑。好在后来还是努力分析出来了，一开始想的东西也恰好有一些能够用上。 启发把数据抽象成A、B去考虑会好一点，能比较快地发现问题的本质。除非能明显确认是要从找规律的方面出发，要不最好不要直接从数据本身开始凑。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACMICPC Asia Invitational Xian]]></title>
    <url>%2F2014%2F10%2F24%2F2014-10-24-The-2014-ACMICPC-Asia-Invitational-Xian%2F</url>
    <content type="text"><![CDATA[上半年邀请赛的时候真是险而又险地2题拿了个铜，确实其实跟没拿一样……现场前复盘一下，长长记性 【A】签到题 【B】最短路+DFS 【C】最短路 【D】构造+欧拉回路 【E】数论，最佳平方逼近 【F】望而却步… 【G】望而却步… 【H】望而却步… 【I】 【J】最短路+TSP A看到的第一眼考虑KMP，最后才知道原来签到题直接模拟即可。 B第一步完成点对点之间的最短路这点肯定没有问题，后悔当时没有尝试直接暴搜…… 启发还是能做的一定努力出一下，暴搜也得试！！！！ C主要是读懂题意，读完之后就是一道简单的最短路题了 D分析当时没有想过详细计算一下最大可能的长度，最终的构造方案也都是非常直观的。 相同的长度不能超过4，则4位的最大可能性是26^4种，即以串中的每一个字母都可以作为起点，一共可以有26^4个起点，加上最后的3位长度一共是26^4+3，超过这个长度的就不可能构造出来了。 一个简单的构造方案：aaaabbbbcccc……zzzz 排完，然后用一个flag[26][26][26][26]就足够存储标记了，不断枚举下一位，只要判断它跟前面末三位加起来是不是已经被标记过就可以决定是不是可以摆放。 或者使用欧拉回路的构造想法。 启发构造题预计肯定会出！！！！构造的时候千万不能盲目啊！！！！先用数学的方式尽可能地把能够计算的东西都算一下，比如说理论上的最大长度什么的，算完以后基本上很大程度上就能够看出来到底应该怎么构造了！！！！]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime 2 配置]]></title>
    <url>%2F2014%2F10%2F01%2F2014-10-01-sublime2%2F</url>
    <content type="text"><![CDATA[在队友的推荐下，爱上了这款神一样的文本编辑器，熟练之后编辑效率真心是大幅提升啊。 一、Package ControlSublime拥有很强大的插件功能，而自带的缺少个管理工具，这个包可以用来很方便地管理各种插件， 详细说明如下： 功能：安装包管理 简介：sublime插件控制台，提供添加、删除、禁用、查找插件等功能。一个好用的插件安装器。 使用：https://sublime.wbond.net/installation 安装方法： 1.CTRL+` ，调出控制台 2.粘贴以下代码至控制台 1import urllib2,os; pf=&apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler( ))); open( os.path.join( ipp, pf), &apos;wb&apos; ).write( urllib2.urlopen( &apos;http://sublime.wbond.net/&apos; +pf.replace( &apos; &apos;,&apos;%20&apos; )).read()); print( &apos;Please restart Sublime Text to finish installation&apos;) 回车安装。 完成以后输入CTRL+SHIFT+P 调出控制台，输入：pci （即Package Control Install的简写）即可调出控制菜单，之后就可以方便地通过这个工具来安装其他功能强大的插件包了。 1.ConvertToUTF8 支持中文显示 2.其他的……以后再加吧 二、Sublime 绑定编译器之后就能变成一个简易的IDE（1）G++篇在把编译器的地址加入系统环境变量之后，从CMD中就可以直接调用G++了。Sublime的强大功能就是可以预设在编辑器内调用命令行，Build和Run命令都是预先设置好的，只要CMD能用，不太需要增加其他设定即可调用。 Build 的快捷键是：CTRL+B Run 的快捷键是：CTRL+SHIFT+B 测试了一下，默认状态下，编译可以直接用，但是调用Run的时候出了点问题，所以还需要再改下配置文件： Preference -&gt; Bowser Packages -&gt; /C++/C++.sublime-build 默认的配置文件是这样的： 1234567891011121314&#123; "cmd": ["g++", "$&#123;file&#125;", "-o", "$&#123;file_path&#125;/$&#123;file_base_name&#125;"], "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$", "working_dir": "$&#123;file_path&#125;", "selector": "source.c, source.c++", "variants": [ &#123; "name": "Run", "cmd": ["bash", "-c", "g++ '$&#123;file&#125;' -o '$&#123;file_path&#125;/$&#123;file_base_name&#125;' &amp;&amp; '$&#123;file_path&#125;/$&#123;file_base_name&#125;'"] &#125; ]&#125; 问题出在最后一段，这里是通过bash进行运行的，而Windows下没有bash，运行当然会出错了，改成： 123456789101112131415&#123; "cmd": ["g++", "$&#123;file&#125;", "-o", "$&#123;file_path&#125;/$&#123;file_base_name&#125;"], "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$", "working_dir": "$&#123;file_path&#125;", "selector": "source.c, source.c++", "variants": [ &#123; "name": "Run", //"cmd": ["bash", "-c", "g++ '$&#123;file&#125;' -o '$&#123;file_path&#125;/$&#123;file_base_name&#125;' &amp;&amp; '$&#123;file_path&#125;/$&#123;file_base_name&#125;'"] "cmd": ["$&#123;file_path&#125;/$&#123;file_base_name&#125;"] &#125; ]&#125; 即可 可以运行了之后，然后又碰到个让人恼火的事情……这货只能在里面运行，不会像其他正常的IDE那样弹出一个CMD的窗口出来，也就是说，没有办法输入数据，这是个严重的问题。 几经周折，终于让我找到了解决的方案。 参考了：http://blog.csdn.net/lhshu2008/article/details/17582949 的方案： 改build文件为： 1234567891011121314151617181920212223&#123; "cmd": ["g++", "$&#123;file&#125;", "-o", "$&#123;file_path&#125;/$&#123;file_base_name&#125;"], "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$", "working_dir": "$&#123;file_path&#125;", "selector": "source.c, source.c++", "variants": [ &#123; "name": "Run", "cmd": ["$&#123;file_path&#125;/$&#123;file_base_name&#125;"] //"cmd": ["cmd", "/c", "g++", "$&#123;file&#125;", "-o", "$&#123;file_path&#125;/$&#123;file_base_name&#125;", "&amp;&amp;", "cmd", "/c", "$&#123;file_path&#125;/$&#123;file_base_name&#125;"] &#125;, &#123; "name": "RunInCommand", "cmd": ["cmd", "/c", "start", "cmd", "/c", "$&#123;file_path&#125;/$&#123;file_base_name&#125; &amp; pause"] &#125;, &#123; "name": "BuildAndRun", "cmd": ["cmd", "/c", "g++", "$&#123;file&#125;", "-o", "$&#123;file_path&#125;/$&#123;file_base_name&#125;", "&amp;&amp;", "start", "cmd", "/c", "$&#123;file_path&#125;/$&#123;file_base_name&#125; &amp; pause"] &#125; ]&#125; 然后修改快捷键配置： Preference -&gt; Key Bindings – User 123[ &#123; "keys": ["f10"], "command": "build", "args": &#123;"variant": "RunInCommand"&#125; &#125;] 这样，按F10调用cmd窗口来运行程序，终于解决了 翻了下默认的配置，F7和CTRL+B的功能居然是一样的，浪费我一个快捷键啊….0.0 于是把默认的F7改为： 1keys": ["f7"], "command": "build", "args": &#123;"variant": "BuildAndRun"&#125; 即上面定义的编译后调出CMD窗口来运行 这样够了，，除了不能用debug，这个简易的IDE拥有超强的文本编辑功能，还是相当好用的。 刚刚发现，默认的Sublime2里面C和C++是同一个配置文件，用的都是g++。好像一般情况下没啥问题，因为g++本身就是向下兼容gcc的，但是遇到在c里面的某些结构体的用法到了g++就不对了。所以还是改过来比较好。另外新建一个c的build文件，然后把”selector”这一项改掉即可。 （2）JAVA篇由上面的G++配置启发，我想到了JAVA也可以这么改。JAVA本身就没有太好的IDE，往常可能大多数童鞋都是直接拿CMD调的比较多，既然sublime如此强大，改一下正好可以弥补JAVA的问题。 但是对sublime的调用语法了解太少，cmd指令部分差几个空格都会出问题。。。汗。。。另外一个是java运行的时候需要调用java.exe，而且后面不能输入绝对路径，试了好久才终于解决所有问题。 调了一晚上，终于把JAVA的配置部分也完成了。 首先还是系统环境变量配置，这里就不记了，配好之后要保证能从CMD里面调用javac和java就行 之后还是一样，在JAVA对应的文件夹里面找到JAVA的sublime编译配置文件，改成下面这样： Preference -&gt; Bowser Packages -&gt; /JAVA/JavaC.sublime-build 123456789101112131415161718192021&#123; "cmd": ["javac", "$&#123;file&#125;"], "file_regex": "^(...*?):([0-9]*):?([0-9]*)", "selector": "source.java", "encoding": "gbk", "variants": [ &#123; "name": "Run", "cmd": ["java", "$&#123;file_base_name&#125;"] &#125;, &#123; "name": "RunInCommand", "cmd": ["cmd", "/c", "start", "cmd", "/c", "java $&#123;file_base_name&#125; &amp; pause"] &#125;, &#123; "name": "BuildAndRun", "cmd": ["cmd", "/c", "javac", "$&#123;file&#125;", "&amp;&amp;", "start", "cmd", "/c", "java $&#123;file_base_name&#125; &amp; pause"] &#125; ]&#125; 因为JAVA编译错误的时候会产生中文提示（CMD设为中文的情况下），所以加上encoding gbk这一句就不会出现因为字符码引起的错误了 键位设置等等其他的都跟G++一样，只是换了其中的指令部分（妈蛋，这里同一个语句放在前一个引号和后一个引号结果都会不一样，，，明明在最后的调试台上看到的指令一模一样，真不知道这是闹哪样，也就是在这里卡了很久，一直跟我报错） CTRL+B 编译 CTRL+SHIFT+B sublime内运行 F10 调用CMD窗口运行 F7 编译并且调用CMD窗口运行 运行缓慢的eclipse之流可以去死了哈哈哈，大胜利！！！ 三、用Snippet保存代码段Sublime中有个很方便的功能是根据标签自动补全提前保存好的代码段。 Tools -&gt; New Snippet 新建一个Snippet文件，能看到默认的模板内容是这样： 123456789&lt;snippet&gt; &lt;content&gt;&lt;![CDATA[Hello, $&#123;1:this&#125; is a $&#123;2:snippet&#125;.]]&gt;&lt;/content&gt; &lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt; &lt;!-- &lt;tabTrigger&gt;hello&lt;/tabTrigger&gt; --&gt; &lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt; &lt;!-- &lt;scope&gt;source.python&lt;/scope&gt; --&gt;&lt;/snippet&gt; Sublime中的Snippet用XML语言描述，而且是一个文件对应一个Snippet。 CDSTA[] 中写上需要保存的代码段； &lt;tabTrigger&gt;&lt;/tabTrigger&gt;中间是触发条件 &lt;scope&gt;&lt;/scope&gt;中间是指定这一个Snippet在什么语言中可以使用，不指定就是全局通用 下面这段： 123456789101112131415161718192021222324252627&lt;snippet&gt; &lt;content&gt;&lt;![CDATA[/* ***********************************************ID : Chen FanLANG : G++PROG : $&#123;1:&#125;************************************************ */#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;using namespace std;int main()&#123; $&#123;0:/* code */&#125; return 0;&#125;]]&gt;&lt;/content&gt; &lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt; &lt;tabTrigger&gt;jcf&lt;/tabTrigger&gt; &lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt; &lt;!-- &lt;scope&gt;source.python&lt;/scope&gt; --&gt;&lt;/snippet&gt; 写好之后，当我编辑C++文件时，输入jcf+TAB，即可迅速调出上面这段起始代码了，中间的 ${0:} 指定复制完成之后光标最后的位置 重点来了！ ${0:}这种标号可以有多个！ 试验了一下，默认是从1开始按照升序的顺序跳转（如果有多个标号），改完一个地方之后按下TAB即可跳到下一个标号，最后在0处结束。即上面的这段snippet，贴出来之后光标首先会停在PROG：后面，改好后按下TAB即可跳转至下面的代码部分，很多常用的代码段都可以用这种方式自定义添加，大大加快了代码编辑的速度。 然后还有个问题，Sublime默认给出的main()函数的Snippet是带argc和argv的，这就让有强迫症的我很不爽，然后又找不到默认的设置在哪，郁闷了好久。 好在后来终于找到了： Preference -&gt; Bowser Packages -&gt; /C++ 这个目录下存放了所有默认指定了C++格式的snippet，改之即可。 其他格式专属的snippet应该也是在其对应的文件夹中 四、Vim模式据说Sublime的作者原本打算做一个类Vim的编辑器的，在Sublime中也隐藏了一个Vim模式，可以兼容大部分的Vim操作。 开启的方法很简单： Preference -&gt; Settings – user 调出设置文件，初始的时候应该里面只有一句话： 1"ignored_packages": [Vintage] 这是是一个包的禁用列表，其中的Vintage应该就是VIM的启动组件，将其从列表中删掉即可 编辑模式下按ESC进入VIM模式]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Sublime_Text_2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACMICPC Asia Regional Shanghai Online]]></title>
    <url>%2F2014%2F09%2F28%2F2014-09-28-The-2014-ACMICPC-Asia-Regional-Shanghai-Online%2F</url>
    <content type="text"><![CDATA[XorZip小队第一次合作，虽然结果还是有些可惜，但是状态和感觉都还不错。 【A】数论+二分（-_-///） 【B】Lucas定理+数位DP（-_-///） 【C】LCA、LCT+树链剖分 【D】题目分解成DP+状态压缩 【E】DLX（-_-///） 【F】推算公式+大数 【G】几何题，线段与椭球的交点 【H】Kuangbin说这是个简单DP（简单…0.0…）矩阵优化 【I】GCD+大数 【J】本福特定律（妈蛋，这是个啥？……） 【K】LCT 【L】签到题 D、ContestTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionIn the ACM International Collegiate Programming Contest, each team consist of three students. And the teams are given 5 hours to solve between 8 and 12 programming problems. On Mars, there is programming contest, too. Each team consist of N students. The teams are given M hours to solve M programming problems. Each team can use only one computer, but they can’t cooperate to solve a problem. At the beginning of the ith hour, they will get the ith programming problem. They must choose a student to solve this problem and others go out to have a rest. The chosen student will spend an hour time to program this problem. At the end of this hour, he must submit his program. This program is then run on test data and can’t modify any more. Now, you have to help a team to find a strategy to maximize the expected number of correctly solved problems. For each problem, each student has a certain probability that correct solve. If the ith student solve the jth problem, the probability of correct solve is Pij . At any time, the different between any two students’ programming time is not more than 1 hour. For example, if there are 3 students and there are 5 problems. The strategy {1,2,3,1,2}, {1,3,2,2,3} or {2,1,3,3,1} are all legal. But {1,1,3,2,3},{3,1,3,1,2} and {1,2,3,1,1} are all illegal. You should find a strategy to maximize the expected number of correctly solved problems, if you have know all probability InputThe first line of the input is T (1 ≤ T ≤ 20), which stands for the number of test cases you need to solve. The first line of each case contains two integers N ,M (1 ≤ N ≤ 10,1 ≤ M ≤ 1000),denoting the number of students and programming problem, respectively. The next N lines, each lines contains M real numbers between 0 and 1 , the jth number in the ith line is Pij . OutputFor each test case, print a line “Case #t: ”(without quotes, t means the index of the test case) at the beginning. Then a single real number means the maximal expected number of correctly solved problems if this team follow the best strategy, to five digits after the decimal point. Look at the output for sample input for details. Sample Input 1 2 30.6 0.3 0.40.3 0.7 0.9 Sample Output Case #1: 2.20000 题意这是另一个次元的ACM比赛么…0.0…每个队伍N个人，一共M道题，每小时一题，每题限定只能有1人做题，其他人出去玩。给出每个队员对每一道题解出的概率，最后要求计算出一种方案，使得最终的出题数数学期望最大化。 分析初看感觉是个贪心啊，但是文中有一句当时困扰了比赛N多队伍的描述： At any time, the different between any two students’ programming time is not more than 1 hour. 后面还有个让人摸不着头脑的sample，顿时好多童鞋表示看不懂。 额，这句话的意思是队伍中所有成员的做题数必须平均，不能出现一名队员比另外一名队员多做1题以上的情况，简单地说就是必须要等所有成员都做过1题之后，才能有人继续做第2个题。 这样思路就不难想出来了： N的上限是10，则把M按照N个一组分开，每一组都要求每个人选择一题，使得总和最大。相当于一个N*N的矩阵，每行每列取一个数使得总和最大。 想明白之后顺手直接分组，DFS搜了一遍交上去，果然TLE。于是明白了，这个要用状态压缩的DP来解。 对于每一组： f(i,j)表示第i题，j状态时达到的最大和，j用一个最大为2^10的数来表示每个人在当前组是否有做过题目的状态则：f(i,j)=max{f(i-1,j-1&lt;&lt;k)+a[k][i]} j&amp;(1&lt;&lt;k)==(1&lt;&lt;k) 想明白方程之后就是分组细节上的处理了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* ***********************************************MYID : Chen FanLANG : G++PROG : D1004************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int po[]=&#123;1,2,4,8,16,32,64,128,256,512,1024,2048&#125;;double a[20][1010];bool flag[20];double f[20][2000];int n;double doit(int l,int r,double sum)&#123; memset(f,0,sizeof(f)); for (int i=1;i&lt;=r-l+1;i++) for (int j=0;j&lt;=po[n]-1;j++) &#123; double temp=0; for (int k=0;k&lt;n;k++) if (((1&lt;&lt;k)&amp;j-(1&lt;&lt;k))==0) if (temp&lt;f[i-1][j-(1&lt;&lt;k)]+a[k+1][l+i-1]) temp=f[i-1][j-(1&lt;&lt;k)]+a[k+1][l+i-1]; f[i][j]=temp; &#125; return f[r-l+1][po[n]-1];&#125;int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int m; scanf("%d%d",&amp;n,&amp;m); for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=m;j++) scanf("%lf",&amp;a[i][j]); double ans=0; int t1=m/n; for (int i=1;i&lt;=t1;i++) &#123; double temp=doit((i-1)*n+1,i*n,0); ans+=temp; &#125; double temp=doit(t1*n+1,m,0); ans+=temp; printf("Case #%d: %.5f\n",tt,ans); &#125; return 0;&#125; 启发有些题目可能局部是满足DP性质的，可以分解一下再做DP。 F、SawtoothTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionThink about a plane: One straight line can divide a plane into two regions. Two lines can divide a plane into at most four regions. Three lines can divide a plane into at most seven regions. And so on… Now we have some figure constructed with two parallel rays in the same direction, joined by two straight segments. It looks like a character “M”. You are given N such “M”s. What is the maximum number of regions that these “M”s can divide a plane ? InputThe first line of the input is T (1 ≤ T ≤ 100000), which stands for the number of test cases you need to solve. Each case contains one single non-negative integer, indicating number of “M”s. (0 ≤ N ≤ 1012) OutputFor each test case, print a line “Case #t: ”(without quotes, t means the index of the test case) at the beginning. Then an integer that is the maximum number of regions N the “M” figures can divide. Sample Input 2 12 Sample Output Case #1: 2Case #2: 19 题意在平面上摆上M，摆上一个的时候能把平面分成2个部分，摆上2个的时候能分成19个部分，求摆上n个的时候能分成多少个部分。 分析这样的题目首先能想到的肯定是通过公式推算完成，但是由两组数据还很难推出公式，于是我们艰难地画出了第三个M，最后数出来是52。 经过一番艰难地推算，算出来了公式是ans=8n^2-7n+1。 开始感觉估算出来long long是足够大的，交了一次没过，以为是公式问题，，，后来才发现10^12平方之后早就爆了64位了，于是转用大数。 比赛时用了JAVA，TLE，然后之前又没有自己准备C的大数模板，悲剧收场… JAVA队友整理了个好用的JAVA模板，这玩意以前还不怎么会，经过这次之后应该是没问题了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/* ***********************************************MYID : Chen FanLANG : JAVAPROG : HDU5047************************************************ */import java.io.*;import java.math.BigInteger;import java.util.*;public class Main &#123; static PrintWriter out=new PrintWriter(new BufferedWriter( new OutputStreamWriter(System.out))); public static void main(String[] args) throws IOException &#123; Scan scan=new Scan(); int t=scan.nextInt(); for (int tt=1;tt&lt;=t;tt++) &#123; String s=scan.next(); BigInteger a=new BigInteger(s); a=a.multiply(BigInteger.valueOf(8)).multiply(a).subtract(a.multiply(BigInteger.valueOf(7))).add(BigInteger.valueOf(1)); out.printf("Case #%d: ",tt); out.println(a); &#125; out.flush(); &#125;&#125;class Scan &#123; BufferedReader buffer; StringTokenizer tok; Scan() &#123; buffer=new BufferedReader(new InputStreamReader(System.in)); &#125; boolean hasNext() &#123; while (tok==null||!tok.hasMoreElements()) &#123; try &#123; tok=new StringTokenizer(buffer.readLine()); &#125; catch (Exception e) &#123; return false; &#125; &#125; return true; &#125; String next() &#123; if (hasNext()) return tok.nextToken(); return null; &#125; int nextInt() &#123; return Integer.parseInt(next()); &#125;&#125; 不用JAVA其实最终的结果没有超出long long太多，当时比赛的时候脑子没转过来，用两个long long的变量把结果分成两部分完全就能存下来了-_-////哭死，这个思路就是简化版的大数操作。 I、Divided LandTime Limit: 8000/4000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionIt’s time to fight the local despots and redistribute the land. There is a rectangular piece of land granted from the government, whose length and width are both in binary form. As the mayor, you must segment the land into multiple squares of equal size for the villagers. What are required is there must be no any waste and each single segmented square land has as large area as possible. The width of the segmented square land is also binary. InputThe first line of the input is T (1 ≤ T ≤ 100), which stands for the number of test cases you need to solve. Each case contains two binary number represents the length L and the width W of given land. (0 &lt; L, W ≤ 21000) OutputFor each test case, print a line “Case #t: ”(without quotes, t means the index of the test case) at the beginning. Then one number means the largest width of land that can be divided from input data. And it will be show in binary. Do not have any useless number or space. Sample Input 3 10 100100 11010010 1100 Sample Output Case #1: 10Case #2: 10Case #3: 110 题意给出一块平面，要求使用一些正方形去摆满，求最大正方形的边长。 分析首先要想到正方形分矩形的最大边长是其长和宽的最大公约数。 然后就是计算二进制（大数）的最大公约数的问题了。 很不好意思的是，JAVA中也是直接有模板可以套用的，而且借由快速的输入输出模板，题目变得so easy了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/* ***********************************************MYID : Chen FanLANG : JAVAPROG : HDU5050************************************************ */import java.io.*;import java.math.BigInteger;import java.util.*;public class Main &#123; static PrintWriter out=new PrintWriter(new BufferedWriter( new OutputStreamWriter(System.out))); public static void main(String[] args) throws IOException &#123; Scan scan=new Scan(); int t=scan.nextInt(); for (int tt=1;tt&lt;=t;tt++) &#123; String s=scan.next(); BigInteger a=new BigInteger(s,2); s=scan.next(); BigInteger b=new BigInteger(s,2); out.printf("Case #%d: ",tt); out.println(a.gcd(b).toString(2)); &#125; out.flush(); &#125;&#125;class Scan &#123; BufferedReader buffer; StringTokenizer tok; Scan() &#123; buffer=new BufferedReader(new InputStreamReader(System.in)); &#125; boolean hasNext() &#123; while (tok==null||!tok.hasMoreElements()) &#123; try &#123; tok=new StringTokenizer(buffer.readLine()); &#125; catch (Exception e) &#123; return false; &#125; &#125; return true; &#125; String next() &#123; if (hasNext()) return tok.nextToken(); return null; &#125; int nextInt() &#123; return Integer.parseInt(next()); &#125;&#125; L、the Sum of CubeTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionA range is given, the begin and the end are both integers. You should sum the cube of all the integers in the range. InputThe first line of the input is T(1 &lt;= T &lt;= 1000), which stands for the number of test cases you need to solve. Each case of input is a pair of integer A,B(0 &lt; A &lt;= B &lt;= 10000),representing the range[A,B]. OutputFor each test case, print a line “Case #t: ”(without quotes, t means the index of the test case) at the beginning. Then output the answer – sum the cube of all the integers in the range. Sample Input 2 1 32 5 Sample Output Case #1: 36Case #2: 224 分析求a~b的立方和，防止爆零。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑科技--位集--bitset]]></title>
    <url>%2F2014%2F09%2F28%2F2014-09-28-bitset%2F</url>
    <content type="text"><![CDATA[自从上次网赛发现这么个东西之后，深深地感受到了bitset的强大，0.0。 正常的bool占用1字节空间，bitset可以把这个缩到1bit，空间上8倍优化。正常用起来可能会跟位运算状态压缩类似，但是其中的每个位又能进行单独操作，所以确实相当方便。 下面是原版的文档： class template std::bitset template class bitset; BitsetAbtsetstores bits (elements with only two possible values: 0 or 1, true or false, …). //位集用于存储0、1元素。 The class emulates an array of bool elements, but optimized for space allocation: generally, each element occupies only one bit (which, on most systems, is eight times less than the smallest elemental type: char). //这种类模拟了bool数组，但是单个元素占空间只有1bit。（！！好东西有木有！！） Each bit position can be accessed individually: for example, for a given bitset named foo, the expression foo[3] accesses its fourth bit, just like a regular array accesses its elements. But because no elemental type is a single bit in most C++ environments, the individual elements are accessed as special references type (seebitset::reference). //每个位都能被独立访问。（！！好东西有木有！！） Bitsets have the feature of being able to be constructed from and converted to both integer values and binary strings (see its constructor and membersto_ulong andto_string ). They can also be directly inserted and extracted from streams in binary format (see applicable operators). //这货还提供转化成其他类型的函数（！！好东西有木有！！） Thesize of a bitset is fixed at compile-time (determined by its template parameter). For a class that also optimizes for space allocation and allows for dynamic resizing, see the bool specialization ofvector (vector). Template parametersN Size of the bitset, in terms of number of bits. It is returned by member functionbitset::size.size_t is an unsigned integral type. //大小由位数决定，并且可以引用内置函数直接查询某一bitset的大小 Member typesreferenceReference-like type (public member class ) Member functions(constructor)Construct bitset (public member function ) applicable operatorsBitset operators (function ) Bit accessoperator[]Access bit (public member function ) //访问位集中的元素可以直接像访问数组一样完成 countCount bits set (public member function ) //计数有多少位 sizeReturn size (public member function ) //返回空间大小 testReturn bit value (public member function ) //返回…这什么东西？ anyTest if any bit is set (public member function ) //返回位集中是否有元素1 noneTest if no bit is set (public member function ) //返回位集是否全空 allTest if all bits are set (public member function ) //返回位集是否全满 Bit operationssetSet bits (public member function ) //设置位 resetReset bits (public member function ) //清空位集 flipFlip bits (public member function ) //位集元素置反 Bitset operationsto_stringConvert to string (public member function ) to_ulongConvert to unsigned long integer (public member function ) to_ullongConvert to unsigned long long (public member function ) Non-member function overloadsapplicable operatorsBitset operators (function ) Non-member class specializationshash Hash for bitset (class template specialization ) 主要操作测试123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;bitset&gt;using namespace std;int main()&#123; bitset&lt;1000&gt; a; a[100]=1; //直接赋值 cout &lt;&lt; a.count() &lt;&lt; endl; //计数1的个数 cout &lt;&lt; a.size() &lt;&lt; endl; //返回空间大小 cout &lt;&lt; a.test(1) &lt;&lt; endl; cout &lt;&lt; a.test(100) &lt;&lt; endl; //判断第i位是否为1 cout &lt;&lt; a.any() &lt;&lt; endl; //是否非空 cout &lt;&lt; a.none() &lt;&lt; endl; //是否全空 cout &lt;&lt; a.all() &lt;&lt; endl; //是否全满 a.set(2); //与赋值相同 cout &lt;&lt; a[2] &lt;&lt; endl; //直接访问，与test相同 a.reset(); //清空 cout &lt;&lt; a.count() &lt;&lt; endl; cout &lt;&lt; a.none() &lt;&lt; endl; a.flip(); //反置 cout &lt;&lt; a.count() &lt;&lt; endl; cout &lt;&lt; a.all() &lt;&lt; endl;&#125; 输出结果如下： 123456789101112110000110010110001]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>bitset</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACMICPC Asia Regional Guangzhou Online]]></title>
    <url>%2F2014%2F09%2F23%2F2014-09-23-The-2014-ACMICPC-Asia-Regional-Guangzhou-Online%2F</url>
    <content type="text"><![CDATA[【A】-_-/// 【B】线段树+位运算（感觉可出） 【C】地图BFS，找最长线 【D】地图BFS，加上各种复杂情况的最短路-_- 【E】-_-/// 【F】三分+圆与线段的交点，计算几何 【G】-_-/// 【H】线段树+树链剖分 【I】后缀数组+二分 【J】DFS搜索 这场网赛当时自己完成了的也就是两道地图题，过去好久了才想到还是该记录下来… C、Wang Xifeng’s Little PlotTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem Description《Dream of the Red Chamber》(also 《The Story of the Stone》) is one of the Four Great Classical Novels of Chinese literature, and it is commonly regarded as the best one. This novel was created in Qing Dynasty, by Cao Xueqin. But the last 40 chapters of the original version is missing, and that part of current version was written by Gao E. There is a heart breaking story saying that after Cao Xueqin died, Cao’s wife burned the last 40 chapter manuscript for heating because she was desperately poor. This story was proved a rumor a couple of days ago because someone found several pages of the original last 40 chapters written by Cao. In the novel, Wang Xifeng was in charge of Da Guan Yuan, where people of Jia family lived. It was mentioned in the newly recovered pages that Wang Xifeng used to arrange rooms for Jia Baoyu, Lin Daiyu, Xue Baochai and other teenagers. Because Jia Baoyu was the most important inheritor of Jia family, and Xue Baochai was beautiful and very capable , Wang Xifeng didn’t want Jia Baoyu to marry Xue Baochai, in case that Xue Baochai might take her place. So, Wang Xifeng wanted Baoyu’s room and Baochai’s room to be located at two ends of a road, and this road should be as long as possible. But Baoyu was very bad at directions, and he demanded that there could be at most one turn along the road from his room to Baochai’s room, and if there was a turn, that turn must be ninety degree. There is a map of Da Guan Yuan in the novel, and redists (In China English, one whose job is studying 《Dream of the Red Chamber》is call a “redist”) are always arguing about the location of Baoyu’s room and Baochai’s room. Now you can solve this big problem and then become a great redist. InputThe map of Da Guan Yuan is represented by a matrix of characters ‘.’ and ‘#’. A ‘.’ stands for a part of road, and a ‘#’ stands for other things which one cannot step onto. When standing on a ‘.’, one can go to adjacent ‘.’s through 8 directions: north, north-west, west, south-west, south, south-east,east and north-east. There are several test cases. For each case, the first line is an integer N(0&lt;N&lt;=100) ,meaning the map is a N × N matrix. Then the N × N matrix follows. The input ends with N = 0. OutputFor each test case, print the maximum length of the road which Wang Xifeng could find to locate Baoyu and Baochai’s rooms. A road’s length is the number of ‘.’s it includes. It’s guaranteed that for any test case, the maximum length is at least 2. Sample Input 3 #.###...#3 …##...#3 …###..#3 …##.…0 Sample Output 3 43 5 题意给出一张图，’.’表示路，’#’表示墙，要求找出一条最长的路，路径上只允许转弯一次。 分析本题首先应该看到虽然要求是可以向八个方向走，但是由于转弯只能是90°的直角弯，所以上下左右四个方向与另外四个方向是完全独立的。 所以直观的想法是分成两组进行BFS，从起点开始（起点应该为某一条路的尽头），记录下上一个点到达当前点的方向、是否转过弯，每一次向下一个点搜索的方向只有三个，记录沿途的最长距离即可。 这道题的代码写得太朴素了，方向这里写了一次复制了7遍，，，都不好意思贴出来了，，，-_-/// 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256/* ***********************************************MYID : Chen FanLANG : G++PROG : C1003************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int dx1[4]=&#123;-1,1,1,-1&#125;;int dy1[4]=&#123;-1,-1,1,1&#125;;int dx2[4]=&#123;-1,0,1,0&#125;;int dy2[4]=&#123;0,-1,0,1&#125;;int ans;bool ma[110][110];int num[110][110];bool check(int xx,int yy)&#123; int temp1=0,temp2=0; for (int i=0;i&lt;4;i++) &#123; temp1+=ma[xx+dx1[i]][yy+dy1[i]]; temp2+=ma[xx+dx2[i]][yy+dy2[i]]; &#125; if (temp1&gt;0||temp2&gt;0) return true; else return false;&#125;typedef struct nod&#123; int x,y,fx; bool zw;&#125; node;node q[10010];void doit(int xx,int yy)&#123; int head=1,tail=1; memset(num,0,sizeof(num)); q[head].x=xx; q[head].y=yy; q[head].fx=-1; q[head].zw=false; num[xx][yy]=1; while (head&lt;=tail) &#123; if (ma[q[head].x-1][q[head].y-1]&amp;&amp;num[q[head].x-1][q[head].y-1]==0&amp;&amp;(q[head].fx==-1||q[head].fx==0||q[head].fx==1||q[head].fx==2)) &#123; tail++; q[tail].x=q[head].x-1; q[tail].y=q[head].y-1; if (q[head].fx==0||q[head].fx==-1) &#123; q[tail].fx=0; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=0; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; if (ma[q[head].x-1][q[head].y+1]&amp;&amp;num[q[head].x-1][q[head].y+1]==0&amp;&amp;(q[head].fx==-1||q[head].fx==0||q[head].fx==1||q[head].fx==3)) &#123; tail++; q[tail].x=q[head].x-1; q[tail].y=q[head].y+1; if (q[head].fx==1||q[head].fx==-1) &#123; q[tail].fx=1; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=1; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; if (ma[q[head].x+1][q[head].y-1]&amp;&amp;num[q[head].x+1][q[head].y-1]==0&amp;&amp;(q[head].fx==-1||q[head].fx==0||q[head].fx==2||q[head].fx==3)) &#123; tail++; q[tail].x=q[head].x+1; q[tail].y=q[head].y-1; if (q[head].fx==2||q[head].fx==-1) &#123; q[tail].fx=2; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=2; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; if (ma[q[head].x+1][q[head].y+1]&amp;&amp;num[q[head].x+1][q[head].y+1]==0&amp;&amp;(q[head].fx==-1||q[head].fx==1||q[head].fx==2||q[head].fx==3)) &#123; tail++; q[tail].x=q[head].x+1; q[tail].y=q[head].y+1; if (q[head].fx==3||q[head].fx==-1) &#123; q[tail].fx=3; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=3; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; head++; &#125; head=1;tail=1; memset(num,0,sizeof(num)); num[xx][yy]=1; while (head&lt;=tail) &#123; if (ma[q[head].x-1][q[head].y]&amp;&amp;num[q[head].x-1][q[head].y]==0&amp;&amp;(q[head].fx==-1||q[head].fx==0||q[head].fx==1||q[head].fx==2)) &#123; tail++; q[tail].x=q[head].x-1; q[tail].y=q[head].y; if (q[head].fx==0||q[head].fx==-1) &#123; q[tail].fx=0; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=0; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; if (ma[q[head].x][q[head].y-1]&amp;&amp;num[q[head].x][q[head].y-1]==0&amp;&amp;(q[head].fx==-1||q[head].fx==0||q[head].fx==1||q[head].fx==3)) &#123; tail++; q[tail].x=q[head].x; q[tail].y=q[head].y-1; if (q[head].fx==1||q[head].fx==-1) &#123; q[tail].fx=1; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=1; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; if (ma[q[head].x][q[head].y+1]&amp;&amp;num[q[head].x][q[head].y+1]==0&amp;&amp;(q[head].fx==-1||q[head].fx==0||q[head].fx==2||q[head].fx==3)) &#123; tail++; q[tail].x=q[head].x; q[tail].y=q[head].y+1; if (q[head].fx==2||q[head].fx==-1) &#123; q[tail].fx=2; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=2; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; if (ma[q[head].x+1][q[head].y]&amp;&amp;num[q[head].x+1][q[head].y]==0&amp;&amp;(q[head].fx==-1||q[head].fx==1||q[head].fx==2||q[head].fx==3)) &#123; tail++; q[tail].x=q[head].x+1; q[tail].y=q[head].y; if (q[head].fx==3||q[head].fx==-1) &#123; q[tail].fx=3; q[tail].zw=q[head].zw; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else if (q[head].zw==false) &#123; q[tail].fx=3; q[tail].zw=true; num[q[tail].x][q[tail].y]=num[q[head].x][q[head].y]+1; if (ans&lt;num[q[tail].x][q[tail].y]) ans=num[q[tail].x][q[tail].y]; &#125; else tail--; &#125; head++; &#125;&#125;int main()&#123; //freopen("C1003.txt","r",stdin); int n; scanf("%d",&amp;n); while (n) &#123; getchar(); memset(ma,0,sizeof(ma)); for (int i=1;i&lt;=n;i++) &#123; for (int j=1;j&lt;=n;j++) &#123; char c; scanf("%c",&amp;c); if (c=='.') ma[i][j]=true; &#125; getchar(); &#125; ans=0; for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=n;j++) if (ma[i][j]&amp;&amp;check(i,j)) doit(i,j); printf("%d\n",ans); scanf("%d",&amp;n); &#125; return 0;&#125; 启发对于这种初看8个方向，然后相互没有关系的一定注意分开考虑。 D、Saving Tang MonkTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem Description《Journey to the West》(also 《Monkey》) is one of the Four Great Classical Novels of Chinese literature. It was written by Wu Cheng’en during the Ming Dynasty. In this novel, Monkey King Sun Wukong, pig Zhu Bajie and Sha Wujing, escorted Tang Monk to India to get sacred Buddhism texts. During the journey, Tang Monk was often captured by demons. Most of demons wanted to eat Tang Monk to achieve immortality, but some female demons just wanted to marry him because he was handsome. So, fighting demons and saving Monk Tang is the major job for Sun Wukong to do. Once, Tang Monk was captured by the demon White Bones. White Bones lived in a palace and she cuffed Tang Monk in a room. Sun Wukong managed to get into the palace. But to rescue Tang Monk, Sun Wukong might need to get some keys and kill some snakes in his way. The palace can be described as a matrix of characters. Each character stands for a room. In the matrix, ‘K’ represents the original position of Sun Wukong, ‘T’ represents the location of Tang Monk and ‘S’ stands for a room with a snake in it. Please note that there are only one ‘K’ and one ‘T’, and at most five snakes in the palace. And, ‘.’ means a clear room as well ‘#’ means a deadly room which Sun Wukong couldn’t get in. There may be some keys of different kinds scattered in the rooms, but there is at most one key in one room. There are at most 9 kinds of keys. A room with a key in it is represented by a digit(from ‘1’ to ‘9’). For example, ‘1’ means a room with a first kind key, ‘2’ means a room with a second kind key, ‘3’ means a room with a third kind key… etc. To save Tang Monk, Sun Wukong must get ALL kinds of keys(in other words, at least one key for each kind). For each step, Sun Wukong could move to the adjacent rooms(except deadly rooms) in 4 directions(north, west, south and east), and each step took him one minute. If he entered a room in which a living snake stayed, he must kill the snake. Killing a snake also took one minute. If Sun Wukong entered a room where there is a key of kind N, Sun would get that key if and only if he had already got keys of kind 1,kind 2 … and kind N-1. In other words, Sun Wukong must get a key of kind N before he could get a key of kind N+1 (N&gt;=1). If Sun Wukong got all keys he needed and entered the room in which Tang Monk was cuffed, the rescue mission is completed. If Sun Wukong didn’t get enough keys, he still could pass through Tang Monk’s room. Since Sun Wukong was a impatient monkey, he wanted to save Tang Monk as quickly as possible. Please figure out the minimum time Sun Wukong needed to rescue Tang Monk. InputThere are several test cases. For each case, the first line includes two integers N and M(0 &lt; N &lt;= 100, 0&lt;=M&lt;=9), meaning that the palace is a N×N matrix and Sun Wukong needed M kinds of keys(kind 1, kind 2, … kind M). Then the N × N matrix follows. The input ends with N = 0 and M = 0. OutputFor each test case, print the minimum time (in minutes) Sun Wukong needed to save Tang Monk. If it’s impossible for Sun Wukong to complete the mission, print “impossible”(no quotes). Sample Input 3 1K.S##11#T3 1K#T.S#1#.3 2K#T.S.21.0 0 Sample Output 5 impossible8 题意又是一道地图题，题目要求为从起点（悟空的位置）出发，按顺序通过m个点，最后到达终点（唐僧的位置），最后输出impossible或者全程的最短距离。 分析当时看到题目就傻了好久，需要处理的问题太多： 钥匙可能有多把，即地图上好多个1，好多个2…… 蛇的问题，刚开始看的时候还没觉得什么，以为标记一下经过的时候时间+1即可，后来发现有点不对，看了比赛时其他人的提问才知道原来我觉得不对的地方是真不对，有蛇的点经过一次之后蛇就会被杀死！！下次再经过就成了普通点！！ 首先解决第一个问题，BFS搜距离+DFS选择路径： 这张图的大小最大只有100*100，总点数最大是10000，考虑最坏情况，即图中的所有点中都有钥匙，9种钥匙均匀分布大约是（9998/9=1110），然后1000^9是……///-_-///……妈蛋，当时没考虑这么多，现在想想如果真的碰到这么大的数据量，我这么做真是早就爆了…… 我把所有钥匙相同的点都编为1组，然后用BFS搜出相邻两组所有点对之间的距离，最后从起点（第0组）开始，DFS找出到达终点（第m+1组）的路径。 接下来是第二个更加棘手的情况： 分析下面这张图： 12345K...........s...........T 从左上角到达右下角每一点的距离： 123450123412345235563456745678 发现一个重要规律！即如果能够不走有蛇的点，那么一定能找出一条同样短的路不经过有蛇点。也就是说，若是最短路中经过了有蛇点，那么这个点是必定要走的，否则另外找一条路花费的代价会比消灭这条蛇的1点时间更大，即例如出现下面这种情况。 12345K.........#S##......T.... 穿越蛇点的代价肯定比绕开走要小。然后看到题目特别指出最多只会出现5条蛇，于是就有了我下面的尝试算法： 将所有的蛇拿掉，做一次BFS+DFS操作，得到一个完成全程的最短距离ans； 放上一条蛇（把那个点标记为耗时2），再做一次BFS+DFS操作，得到一个完成全程的最短距离ans0。若ans0比ans更大，说明这个有蛇点是必须要经过的，记录一下，若ans0与ans相等则说明该点可以不经过，不影响结果； 拿掉之前的那条蛇，放上下一条蛇，并依此完成以上操作，判断每一个有蛇点是否都是必须经过的，不断记录即可。 最终答案为ans加上必须要经过的蛇点的个数 这里最为担心的是完成一次BFS+DFS操作需要的耗时问题，就像上面提到的，不用说极限情况，可能稍微接近一些极限的数据出来就会TLE，更不用说下面还要把这种操作再重复5次了。 幸运的是最后木有超…只能说出题的大大给的数据还没有到灭绝弱队的地步（哭） hdu这道题有耗时更少的，一会再去找找其他神牛的题解好了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175/* ***********************************************MYID : Chen FanLANG : G++PROG : D1004************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct nod&#123; int x,y;&#125; node;node lis[11][1000];node q[20000];int an[11][1000][1000];int ma[110][110],num[110][110];bool flag[110][110];bool state[110][1000][6];bool done;int ns;node ss[6];int n,m,ans;int dx[]=&#123;-1,0,1,0&#125;;int dy[]=&#123;0,-1,0,1&#125;;int doit(int sx,int sy,int ex,int ey)&#123; int head=1,tail=1,ret=0; q[head].x=sx; q[head].y=sy; memset(num,0,sizeof(num)); memset(flag,0,sizeof(flag)); flag[q[head].x][q[head].y]=true; bool done=false; while (head&lt;=tail) &#123; if (q[head].x==ex&amp;&amp;q[head].y==ey) &#123; done=true; ret=num[q[head].x][q[head].y]; break; &#125; for (int i=0;i&lt;4;i++) if (ma[q[head].x+dx[i]][q[head].y+dy[i]]&amp;&amp;(num[q[head].x+dx[i]][q[head].y+dy[i]]==0||num[q[head].x+dx[i]][q[head].y+dy[i]]&gt;num[q[head].x][q[head].y]+ma[q[head].x+dx[i]][q[head].y+dy[i]])) &#123; num[q[head].x+dx[i]][q[head].y+dy[i]]=num[q[head].x][q[head].y]+ma[q[head].x+dx[i]][q[head].y+dy[i]]; if (!flag[q[head].x+dx[i]][q[head].y+dy[i]]) &#123; tail++; q[tail].x=q[head].x+dx[i]; q[tail].y=q[head].y+dy[i]; flag[q[head].x+dx[i]][q[head].y+dy[i]]=true; &#125; &#125; flag[q[head].x][q[head].y]=false; head++; &#125; if (done) return ret; else return -1;&#125;void dfs(int s,int t,int tot)&#123; if (s==m+1) &#123; if (ans&gt;tot) &#123; ans=tot; done=true; &#125; return; &#125; for (int i=1;i&lt;=lis[s+1][0].x;i++) if (an[s][t][i]!=-1) &#123; dfs(s+1,i,tot+an[s][t][i]); &#125;&#125;int main()&#123; //freopen("D1004.txt","r",stdin); scanf("%d%d",&amp;n,&amp;m); while (!(n==0&amp;&amp;m==0)) &#123; getchar(); memset(ma,0,sizeof(ma)); memset(lis,0,sizeof(lis)); ns=0; for (int i=1;i&lt;=n;i++) &#123; for (int j=1;j&lt;=n;j++) &#123; char c; scanf("%c",&amp;c); switch(c) &#123; case 'K': lis[0][0].x=1; lis[0][1].x=i;lis[0][1].y=j; ma[i][j]=1; break; case 'T': lis[m+1][0].x=1; lis[m+1][1].x=i;lis[m+1][1].y=j; ma[i][j]=1; break; case 'S': ns++; ss[ns].x=i;ss[ns].y=j; ma[i][j]=1; break; case '.': ma[i][j]=1; break; case '#': break; default: ma[i][j]=1; lis[c-'0'][0].x++; lis[c-'0'][lis[c-'0'][0].x].x=i; lis[c-'0'][lis[c-'0'][0].x].y=j; &#125; &#125; getchar(); &#125; ans=0; memset(an,0,sizeof(an)); for (int i=0;i&lt;=m;i++) &#123; for (int j=1;j&lt;=lis[i][0].x;j++) for (int k=1;k&lt;=lis[i+1][0].x;k++) an[i][j][k]=doit(lis[i][j].x,lis[i][j].y,lis[i+1][k].x,lis[i+1][k].y); &#125; done=false; ans=2147483647; dfs(0,1,0); if (done) &#123; int ans0=ans; int temp=0; for (int p=1;p&lt;=ns;p++) &#123; if (p&gt;1) ma[ss[p-1].x][ss[p-1].y]=1; ans=0; ma[ss[p].x][ss[p].y]=2; memset(an,0,sizeof(an)); for (int i=0;i&lt;=m;i++) &#123; for (int j=1;j&lt;=lis[i][0].x;j++) for (int k=1;k&lt;=lis[i+1][0].x;k++) an[i][j][k]=doit(lis[i][j].x,lis[i][j].y,lis[i+1][k].x,lis[i+1][k].y); &#125; done=false; ans=2147483647; dfs(0,1,0); if (ans&gt;ans0) temp++; &#125; printf("%d\n",ans0+temp); &#125; else printf("impossible\n"); scanf("%d%d",&amp;n,&amp;m); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACMICPC Asia Regional Beijing Online]]></title>
    <url>%2F2014%2F09%2F23%2F2014-09-23-The-2014-ACMICPC-Asia-Regional-Beijing-Online%2F</url>
    <content type="text"><![CDATA[【A】极角排序+树状数组 【B】计算几何，凸包（队友已出） 【C】-_-///不懂 【D】数论，概率密度 【E】图的连通性+Floyed传递闭包+bitset 【F】贪心 【G】签到题 【H】区间维护+线段树+DFS序（可以看看） 【I】BFS地图题（当时好多人坑在摄像头上面了，现在有一点点思路，分层图，一会看看） 【J】-_-///// 貌似除了这两题巨坑的，剩下的都有能出的可能性 A、Always Cook MushroomTime Limit: 10000/5000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionMatt has a company, Always Cook Mushroom (ACM), which produces high-quality mushrooms. ACM has a large field to grow their mushrooms. The field can be considered as a 1000 * 1000 grid where mushrooms are grown in grid points numbered from (1, 1) to (1000, 1000). Because of humidity and sunshine, the productions in different grid points are not the same. Further, the production in the grid points (x, y) is (x + A)(y + B) where A, B are two constant. Matt,the owner of ACM has some queries where he wants to know the sum of the productions in a given scope(include the mushroom growing on the boundary). In each query, the scope Matt asks is a right angled triangle whose apexes are (0, 0), (p, 0), (p, q) 1&lt;=p, q&lt;=1000. As the employee of ACM, can you answer Matt’s queries? InputThe first line contains one integer T, indicating the number of test cases. For each test case, the first line contains two integers:A, B(0&lt;=A, B&lt;=1000). The second line contains one integer M(1&lt;=M&lt;=10^5), denoting the number of queries. In the following M lines, the i-th line contains three integers a, b, x (1&lt;=a, b&lt;=10^6, 1&lt;=x&lt;=1000), denoting one apex of the given right angled triangle is (x, 0) and the slope of its base is (a, b). It is guaranteed that the gird points in the given right angled triangle are all in valid area, numbered from (1, 1) to (1000, 1000). OutputFor each test case, output M + 1 lines. The first line contains “Case #x:”, where x is the case number (starting from 1) In the following M lines, the i-th line contains one integer, denoting the answer of the i-th query. Sample Input 2 0 03 3 5 82 4 71 2 31 23 3 5 82 4 71 2 3 Sample Output Case #1:1842170886Case #2:29012688200 题意给出一张最大1000*1000的图，给出一些询问，每次询问给出一个斜率和x，要求三角内的所有点的和。 分析题目意思比较裸，直观地想到这道题目的难度肯定不是在理解上，应该是数据比较大太裸的算法不可能卡过。 果然试了各种，唯一想到的树状数组也加上去了，还是一直TLE没能在比赛过程中出解。 赛后整理终于明白了，这道题目的关键在于如何把一张二维的图转化为一维来解决，本题给我的启发是：累加的题目如果要避免反复加引起的重复的话，树状数组是必须的，时间非常优秀，然而树状数组毕竟是一维的，必须想到办法解决二维压成一维之后的后效性等等问题才能发挥出最大的效果。 最后的算法： 二维转一维：将最大1000*1000个点按照斜率排序。对于每次的询问也是按照斜率排序。 想象有一根轴，从x轴的0°角开始逆时针扫整个平面，扫到的点按照x坐标加入树状数组，直到扫到与一条询问线重合，则对该次询问求sum(x)。这样就能避免了重复操作。把结果按照原始顺序重新输出即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU5032************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;long long c[1010];int outp[100010];long long ou[100010];typedef struct poin&#123; int x,y; long long v; double ang;&#125; point;point poi[1000010];int totp;typedef struct nod&#123; int a,b,x,src; double ang;&#125; node;node lis[100010];bool op1(point a,point b)&#123; if (a.ang==b.ang) return a.x&lt;b.x; else return a.ang&lt;b.ang;&#125;bool op2(node a,node b)&#123; if (a.ang==b.ang) return a.a&lt;b.a; else return a.ang&lt;b.ang;&#125;int lowbit(int s)&#123; return s&amp;-s;&#125; void update(int s,long long x) &#123; while (s&lt;=1000) &#123; c[s]+=x; s+=lowbit(s); &#125;&#125;long long sum(int s)&#123; long long t=0; while (s&gt;0) &#123; t+=c[s]; s-=lowbit(s); &#125; return t;&#125; int main()&#123; freopen("test.txt","r",stdin); int t; totp=0; for (int i=1;i&lt;=1000;i++) for (int j=1;j&lt;=1000;j++) &#123; totp++; poi[totp].x=i; poi[totp].y=j; poi[totp].ang=(double)j/i; &#125; sort(&amp;poi[1],&amp;poi[1+totp],op1); scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int A,B; scanf("%d%d",&amp;A,&amp;B); memset(c,0,sizeof(c)); for (int i=1;i&lt;=totp;i++) poi[i].v=(poi[i].x+A)*(poi[i].y+B); printf("Case #%d:\n",tt); int m; scanf("%d",&amp;m); for (int i=1;i&lt;=m;i++) &#123; scanf("%d%d%d",&amp;lis[i].a,&amp;lis[i].b,&amp;lis[i].x); lis[i].ang=(double)lis[i].b/lis[i].a; lis[i].src=i; &#125; sort(&amp;lis[1],&amp;lis[1+m],op2); for (int i=1;i&lt;=m;i++) outp[lis[i].src]=i; for (int i=1,j=1;i&lt;=m;i++) &#123; while (j&lt;=totp&amp;&amp;lis[i].ang-poi[j].ang&gt;=0) &#123; update(poi[j].x,poi[j].v); j++; &#125; ou[i]=sum(lis[i].x); &#125; for (int i=1;i&lt;=m;i++) printf("%lld\n",ou[outp[i]]); &#125; return 0;&#125; 启发既然已经想到了树状数组，就应该多想想树状数组的性质，本题是利用了极角的特性二维化一维，思维上非常地巧妙。 树状数组这一块另外再补补，尤其是多维的情况 B、BuildingTime Limit: 10000/5000 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Special Judge Problem DescriptionOnce upon a time Matt went to a small town. The town was so small and narrow that he can regard the town as a pivot. There were some skyscrapers in the town, each located at position xi with its height hi. All skyscrapers located in different place. The skyscrapers had no width, to make it simple. As the skyscrapers were so high, Matt could hardly see the sky.Given the position Matt was at, he wanted to know how large the angle range was where he could see the sky. Assume that Matt’s height is 0. It’s guaranteed that for each query, there is at least one building on both Matt’s left and right, and no building locate at his position. InputThe first line of the input contains an integer T, denoting the number of testcases. Then T test cases follow. Each test case begins with a number N(1&lt;=N&lt;=10^5), the number of buildings. In the following N lines, each line contains two numbers, xi(1&lt;=xi&lt;=10^7) and hi(1&lt;=hi&lt;=10^7). After that, there’s a number Q(1&lt;=Q&lt;=10^5) for the number of queries. In the following Q lines, each line contains one number qi, which is the position Matt was at. OutputFor each test case, first output one line “Case #x:”, where x is the case number (starting from 1). Then for each query, you should output the angle range Matt could see the sky in degrees. The relative error of the answer should be no more than 10^(-4). Sample Input 3 31 22 15 11 43 1 32 25 11 43 1 42 35 11 4 Sample Output Case #1:101.3099324740Case #2:90.0000000000Case #3:78.6900675260 说明不太擅长几何题，好在队友给力能解决，日后慢慢看……0.0，嘿嘿 http://www.cnblogs.com/AOQNRMGYXLMV/p/3987173.html E、ExplosionTime Limit: 6000/3000 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Total Problem DescriptionEveryone knows Matt enjoys playing games very much. Now, he is playing such a game. There are N rooms, each with one door. There are some keys(could be none) in each room corresponding to some doors among these N doors. Every key can open only one door. Matt has some bombs, each of which can destroy a door. He will uniformly choose a door that can not be opened with the keys in his hand to destroy when there are no doors that can be opened with keys in his hand. Now, he wants to ask you, what is the expected number of bombs he will use to open or destroy all the doors. Rooms are numbered from 1 to N. InputThe first line of the input contains an integer T, denoting the number of testcases. Then T test cases follow. In the first line of each test case, there is an integer N (N&lt;=1000) indicating the number of rooms. The following N lines corresponde to the rooms from 1 to N. Each line begins with an integer k (0&lt;=k&lt;=N) indicating the number of keys behind the door. Then k integers follow corresponding to the rooms these keys can open. OutputFor each test case, output one line “Case #x: y”, where x is the case number (starting from 1), y is the answer which should be rounded to 5 decimal places. Sample Input 2 31 21 31 13 00 0 Sample Output Case #1: 1.00000Case #2: 3.00000 题意有N个房间，每个房间都是被门锁上，而打开的方式有两种，找到这扇门的钥匙或者用炸弹炸开。每个房间中都可能存在一些钥匙，即打开这个房门之后可以获得其他一些房间的钥匙。本题求的是最终为了打开所有房门，所有点需要被炸开的数学期望值。 分析这里房间和钥匙的关系可以作为结点之间的连边条件，用于构造一张图。若1号房间有2号、3号房的钥匙，可用单向边连接1-&gt;2和1-&gt;3，表示如果能够进入1号房，那么2、3号房也能接着进入。最终将本题转化为传递闭包连通性的问题。 当时做题的时候想到的是用类似并查集的方法处理，但是想半天还是没想出方案来，最后用了BFS+DFS双向扩展，结果就TLE到死都没解决。 考虑我们已经通过某种方法得到了所有点对之间的到达关系，假设存在x个点都能够到达i号点，则为了到达i点，所有这x个房间需要被炸开的概率为1/x，因为任意炸开一个都能够到达i点，则为了到达i点需要的数学期望值为1*1/x=1/x。根据这种思路，容易看出来每一个点的数学期望值都是独立的。则可以单独求取到达每一个点需要的期望值，最终求和即可。 主要的时间复杂度都花在遍历得到所有点对上了，最基本的是用一个Floyed直接传递闭包，O(n^3) 然后从大神们的题解中发现了一种以前我从来没见过的新东西————位集。这货简直是黑科技……时空条件都非常优秀，直接用在Floyed上，复杂度可以降到O(n^3/64)（不要问我这个是怎么来的，，，我不知道） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU5036************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;bitset&gt;using namespace std;bitset&lt;1010&gt; a[1010];int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n; scanf("%d",&amp;n); for (int i=1;i&lt;=n;i++) &#123; a[i].reset(); a[i][i]=true; &#125; for (int i=1;i&lt;=n;i++) &#123; int m; scanf("%d",&amp;m); for (int j=1;j&lt;=m;j++) &#123; int k; scanf("%d",&amp;k); a[i][k]=true; &#125; &#125; for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=n;j++) if (a[j][i]) a[j]|=a[i]; double ans=0; for (int i=1;i&lt;=n;i++) &#123; int tot=0; for (int j=1;j&lt;=n;j++) if (a[j][i]) tot++; ans+=1.0/tot; &#125; printf("Case #%d: %.5lf\n",tt,ans); &#125; return 0;&#125; 有关bitset的用法，详见另外整理的一篇博文： F、FrogTime Limit: 3000/1500 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Problem DescriptionOnce upon a time, there is a little frog called Matt. One day, he came to a river. The river could be considered as an axis.Matt is standing on the left bank now (at position 0). He wants to cross the river, reach the right bank (at position M). But Matt could only jump for at most L units, for example from 0 to L. As the God of Nature, you must save this poor frog.There are N rocks lying in the river initially. The size of the rock is negligible. So it can be indicated by a point in the axis. Matt can jump to or from a rock as well as the bank. You don’t want to make the things that easy. So you will put some new rocks into the river such that Matt could jump over the river in maximal steps.And you don’t care the number of rocks you add since you are the God. Note that Matt is so clever that he always choose the optimal way after you put down all the rocks. InputThe first line contains only one integer T, which indicates the number of test cases. For each test case, the first line contains N, M, L (0&lt;=N&lt;=2*10^5,1&lt;=M&lt;=10^9, 1&lt;=L&lt;=10^9). And in the following N lines, each line contains one integer within (0, M) indicating the position of rock. OutputFor each test case, just output one line “Case #x: y”, where x is the case number (starting from 1) and y is the maximal number of steps Matt should jump. Sample Input 2 1 10 55 2 10 33 6 Sample Output Case #1: 2Case #2: 4 G、GradeTime Limit: 3000/1500 MS (Java/Others) Memory Limit: 262144/262144 K (Java/Others) Problem DescriptionTed is a employee of Always Cook Mushroom (ACM). His boss Matt gives him a pack of mushrooms and ask him to grade each mushroom according to its weight. Suppose the weight of a mushroom is w, then it’s grade s is s = 10000 - (100 - w)^2 What’s more, Ted also has to report the mode of the grade of these mushrooms. The mode is the value that appears most often. Mode may not be unique. If not all the value are the same but the frequencies of them are the same, there is no mode. InputThe first line of the input contains an integer T, denoting the number of testcases. Then T test cases follow. The first line of each test cases contains one integers N (1&lt;=N&lt;=10^6),denoting the number of the mushroom. The second line contains N integers, denoting the weight of each mushroom. The weight is greater than 0, and less than 200. OutputFor each test case, output 2 lines. The first line contains “Case #x:”, where x is the case number (starting from 1) The second line contains the mode of the grade of the given mushrooms. If there exists multiple modes, output them in ascending order. If there exists no mode, output “Bad Mushroom”. Sample Input 3 6100 100 100 99 98 1016 100 100 100 99 99 1016 100 100 98 99 99 97 Sample Output Case #1:10000Case #2:Bad MushroomCase #3:9999 10000 分析排序+判断，有些细节上的小问题要多注意一下]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACMICPC Asia Regional Xian Online]]></title>
    <url>%2F2014%2F09%2F15%2F2014-09-15-The-2014-ACMICPC-Asia-Regional-Xian-Online%2F</url>
    <content type="text"><![CDATA[【A】签到题 【B】后缀数组 【C】染色，DP（感觉可出） 【D】BFS搜索，有点麻烦 【E】博弈论，Nim博弈 【F】BFS状态搜索 【G】概率DP+状态压缩 【H】异或+构造 【I】矩阵快速幂（队友已出） 【J】树的分治 【K】类模拟退火的方向修正搜索、三分 A、Post RobotTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionDT is a big fan of digital products. He writes posts about technological products almost everyday in his blog. But there is such few comments of his posts that he feels depressed all the day. As his best friend and an excellent programmer, DT asked you to help make his blog look more popular. He is so warm that you have no idea how to refuse. But you are unwilling to read all of his boring posts word by word. So you decided to write a script to comment below his posts automatically. After observation, you found words “Apple” appear everywhere in his posts. After your counting, you concluded that “Apple”, “iPhone”, “iPod”, “iPad” are the most high-frequency words in his blog. Once one of these words were read by your smart script, it will make a comment “MAI MAI MAI!”, and go on reading the post. In order to make it more funny, you, as a fan of Sony, also want to make some comments about Sony. So you want to add a new rule to the script: make a comment “SONY DAFA IS GOOD!” when “Sony” appears. InputA blog article described above, which contains only printable characters(whose ASCII code is between 32 and 127), CR(ASCII code 13, ‘\r’ in C/C++), LF(ASCII code 10, ‘\n’ in C/C++), please process input until EOF. Note all characters are case sensitive. The size of the article does not exceed 8KB. OutputOutput should contains comments generated by your script, one per line. Sample Input Apple bananaiPad lemon ApplepiSony233Tim cook is doubi from AppleiPhoneipadiPhone30 is so biiiiiiig Microsoftmakes good App. Sample Output MAI MAI MAI! MAI MAI MAI! MAI MAI MAI! SONY DAFA IS GOOD! MAI MAI MAI! MAI MAI MAI! MAI MAI MAI! 分析买买买！！！ 直接扫描判断即可。 E、GameTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionHere is a game for two players. The rule of the game is described below: In the beginning of the game, there are a lot of piles of beads. Players take turns to play. Each turn, player choose a pile i and remove some (at least one) beads from it. Then he could do nothing or split pile i into two piles with a beads and b beads.(a,b &gt; 0 and a + b equals to the number of beads of pile i after removing) If after a player’s turn, there is no beads left, the player is the winner. Suppose that the two players are all very clever and they will use optimal game strategies. Your job is to tell whether the player who plays first can win the game. InputThere are multiple test cases. Please process till EOF. For each test case, the first line contains a postive integer n(n &lt; 105) means there are n piles of beads. The next line contains n postive integer, the i-th postive integer ai(ai &lt; 231) means there are ai beads in the i-th pile. OutputFor each test case, if the first player can win the game, ouput “Win” and if he can’t, ouput “Lose” Sample Input 1 12 1 13 1 2 3 Sample Output WinLoseLose 题意取石子的游戏：初始时有n堆，每人轮流进行取的过程，每次可以选择某一堆取任意个，然后选择把剩下的部分分成任意数量的两堆。最后一个取完所有东西的人获胜。 分析妈蛋！！！比赛前我根本没听说过Nim博弈好么！！！果然还是题目做的太少T_T，东西了解得太少T_T//////////////////////// 看着别人刷刷刷都过了，死活看不明白是怎么回事……哭晕在厕所啊 以下是两个博弈论知识的总结帖： http://blog.csdn.net/u012860063/article/details/21816635 http://blog.csdn.net/chao1983210400/article/details/10284693 启发……还是多做题吧，没见过只能怪自己没见过。 F、DiceTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionThere are 2 special dices on the table. On each face of the dice, a distinct number was written. Consider a1.a2,a3,a4,a5,a6 to be numbers written on top face, bottom face, left face, right face, front face and back face of dice A. Similarly, consider b1.b2,b3,b4,b5,b6 to be numbers on specific faces of dice B. It’s guaranteed that all numbers written on dices are integers no smaller than 1 and no more than 6 while ai ≠ aj and bi ≠ bj for all i ≠ j. Specially, sum of numbers on opposite faces may not be 7. At the beginning, the two dices may face different(which means there exist some i, ai ≠ bi). Ddy wants to make the two dices look the same from all directions(which means for all i, ai = bi) only by the following four rotation operations.(Please read the picture for more information) Now Ddy wants to calculate the minimal steps that he has to take to achieve his goal. InputThere are multiple test cases. Please process till EOF. For each case, the first line consists of six integers a1,a2,a3,a4,a5,a6, representing the numbers on dice A. The second line consists of six integers b1,b2,b3,b4,b5,b6, representing the numbers on dice B. OutputFor each test case, print a line with a number representing the answer. If there’s no way to make two dices exactly the same, output -1. Sample Input 1 2 3 4 5 61 2 3 4 5 61 2 3 4 5 61 2 5 6 4 31 2 3 4 5 61 4 2 5 3 6 Sample Output 0 3-1 题意给出四种筛子翻转的方案，给出两种筛子的状态，要求判断是否可以从一个状态操作到另一个。输出最少操作步数或者-1（不可能）。 分析看到题目的第一感觉就是这种题肯定没办法直接通过数学上的关系解决，每种状态是6个数字，最多也就是6^6=46k多一点种，而实际上筛子有严格的对应关系，所以实际的状态量要比这个还少一点。 于是采用BFS扩展状态即可，中间最重要的就是注意判重就好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144/* ***********************************************MYID : Chen FanLANG : G++PROG : F1006************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct nod&#123; int a,b,c,d,e,f;&#125; node;node move1(node a)&#123; node ret; ret.a=a.d;ret.b=a.c;ret.c=a.a; ret.d=a.b;ret.e=a.e;ret.f=a.f; return ret;&#125;node move2(node a)&#123; node ret; ret.a=a.c;ret.b=a.d;ret.c=a.b; ret.d=a.a;ret.e=a.e;ret.f=a.f; return ret;&#125;node move3(node a)&#123; node ret; ret.a=a.f;ret.b=a.e;ret.c=a.c; ret.d=a.d;ret.e=a.a;ret.f=a.b; return ret;&#125;node move4(node a)&#123; node ret; ret.a=a.e;ret.b=a.f;ret.c=a.c; ret.d=a.d;ret.e=a.b;ret.f=a.a; return ret;&#125;node q[50000];int num[50000];int head=1,tail=1;bool check(node a)&#123; for (int i=1;i&lt;=tail;i++) if (a.a==q[i].a&amp;&amp;a.b==q[i].b&amp;&amp;a.c==q[i].c&amp;&amp;a.d==q[i].d&amp;&amp;a.e==q[i].e&amp;&amp;a.f==q[i].f) return true; return false;&#125;int main()&#123; node a,b; while (scanf("%d",&amp;a.a)!=EOF) &#123; scanf("%d%d%d%d%d",&amp;a.b,&amp;a.c,&amp;a.d,&amp;a.e,&amp;a.f); scanf("%d%d%d%d%d%d",&amp;b.a,&amp;b.b,&amp;b.c,&amp;b.d,&amp;b.e,&amp;b.f); if (a.a==b.a&amp;&amp;a.b==b.b&amp;&amp;a.c==b.c&amp;&amp;a.d==b.d&amp;&amp;a.e==b.e&amp;&amp;a.f==b.f) &#123; printf("0\n"); &#125; else &#123; q[1]=a; num[1]=0; bool done=false; head=1;tail=1; while (head&lt;=tail) &#123; node temp=move1(q[head]); if (temp.a==b.a&amp;&amp;temp.b==b.b&amp;&amp;temp.c==b.c&amp;&amp;temp.d==b.d&amp;&amp;temp.e==b.e&amp;&amp;temp.f==b.f) &#123; printf("%d\n",num[head]+1); done=true; break; &#125; if (!check(temp)) &#123; tail++; q[tail]=temp; num[tail]=num[head]+1; &#125; temp=move2(q[head]); if (temp.a==b.a&amp;&amp;temp.b==b.b&amp;&amp;temp.c==b.c&amp;&amp;temp.d==b.d&amp;&amp;temp.e==b.e&amp;&amp;temp.f==b.f) &#123; printf("%d\n",num[head]+1); done=true; break; &#125; if (!check(temp)) &#123; tail++; q[tail]=temp; num[tail]=num[head]+1; &#125; temp=move3(q[head]); if (temp.a==b.a&amp;&amp;temp.b==b.b&amp;&amp;temp.c==b.c&amp;&amp;temp.d==b.d&amp;&amp;temp.e==b.e&amp;&amp;temp.f==b.f) &#123; printf("%d\n",num[head]+1); done=true; break; &#125; if (!check(temp)) &#123; tail++; q[tail]=temp; num[tail]=num[head]+1; &#125; temp=move4(q[head]); if (temp.a==b.a&amp;&amp;temp.b==b.b&amp;&amp;temp.c==b.c&amp;&amp;temp.d==b.d&amp;&amp;temp.e==b.e&amp;&amp;temp.f==b.f) &#123; printf("%d\n",num[head]+1); done=true; break; &#125; if (!check(temp)) &#123; tail++; q[tail]=temp; num[tail]=num[head]+1; &#125; head++; &#125; if (!done) printf("-1\n"); &#125; &#125; return 0;&#125; 代码写得也比较丑 启发当时初看题目的时候瞟了几眼直接就跳过了，-_-///事后感觉没什么可以做了才回来看的，然后才发现其实很简单。还是应该认真看看题，比赛的时候至少每道题都要看过去，说不定能找到一些突破点。 H、Number SequenceTime Limit: 4000/2000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Special Judge Problem DescriptionThere is a special number sequence which has n+1 integers. For each number in sequence, we have two rules: ai ∈ [0,n] ai ≠ aj( i ≠ j ) For sequence a and sequence b, the integrating degree t is defined as follows(“⊕” denotes exclusive or): t = (a0 ⊕ b0) + (a1 ⊕ b1) +···+ (an ⊕ bn) (sequence B should also satisfy the rules described above) Now give you a number n and the sequence a. You should calculate the maximum integrating degree t and print the sequence b. InputThere are multiple test cases. Please process till EOF. For each case, the first line contains an integer n(1 ≤ n ≤ 105), The second line contains a0,a1,a2,…,an. OutputFor each case, output two lines.The first line contains the maximum integrating degree t. The second line contains n+1 integers b0,b1,b2,…,bn. There is exactly one space between bi and bi+1(0 ≤ i ≤ n - 1). Don’t ouput any spaces after bn. Sample Input 4 2 0 1 4 3 Sample Output 201 0 2 3 4 题意题目给出一组A序列，t的值是等于B序列中的每一个对应的数与A序列异或之后的和。现在的要求就是求出能够使得t最大的B序列。 分析首先我真的不是特别明白这道题为什么会用到Special Judge？ ……按照我的想法，能使得t最大的B序列应该是唯一的，把所有数都化为二进制的形式，则为了使得Ai异或Bi的值最大，Bi的二进制数位要尽可能多地与Ai错开。 所以这样出来的数其实是一组两个的数，A序列是从0到n，B序列也是从0到n，且每个序列不出现重复的数字。对于每一个Ai，用一个等长的全1的二进制数去与之异或，就能得到对应的Bi，反过来对数Bi操作可以得到数Ai。其实就是构造出来之后使对应的Ai+Bi能等于一个全是1的二进制数。 如此就能构造出B序列了，下面是几个序列的对应关系，T这行表示异或结果 123456789101112131415161718192021222324n=7-------------------A 0 1 2 3 4 5 6 7B 7 6 5 4 3 2 1 0T 7 7 7 7 7 7 7 7-------------------n=8-------------------A 0 1 2 3 4 5 6 7 8B 0 6 5 4 3 2 1 8 7T 0 7 7 7 7 7 7 15 15-------------------n=9-------------------A 0 1 2 3 4 5 6 7 8 9B 1 0 5 4 3 2 9 8 7 6T 1 1 7 7 7 7 15 15 15 15------------------- 当然这里有个顺序的问题，构造的时候从最大（二进制最长的数）的开始向小的数进行，保证使最终总的异或值最大，并且后来构造出来Bi的数比Ai小，避免了异或构造出来的数大于n的情况（真的可能会有啊）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/* ***********************************************MYID : Chen FanLANG : G++PROG : H1008************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int pp[]=&#123;0,1,3,7,15,31,63,127,255,511,1023,2047,4095,8191,16383,32767,65535,131071&#125;;int a[100010],b[100010];int get(int s)&#123; int ret=0; while (s&gt;0) &#123; ret++; s/=2; &#125; return ret;&#125;int main()&#123; //freopen("H1008.txt","r",stdin); //freopen("H1008out.txt","w",stdout); int n; while (scanf("%d",&amp;n)!=EOF) &#123; memset(a,0,sizeof(a)); for (int i=n;i&gt;=0;i--) if (!a[i]) &#123; for (int j=i;j&gt;=pp[get(i)-1]+1;j--) &#123; int yh=j^pp[get(i)]; a[j]=yh; a[yh]=j; &#125; &#125; long long ans=0; int x; for (int i=0;i&lt;=n;i++) &#123; scanf("%d",&amp;x); //x=i; ans+=(long long )x^a[x]; b[i]=a[x]; &#125; printf("%lld\n",ans); printf("%d",b[0]); for (int i=1;i&lt;=n;i++) printf(" %d",b[i]); printf("\n"); &#125; return 0;&#125; 启发虽然当时还没想明白Special Judge是为什么（现在还是不明白…），不过以后碰到有思路的就直接上吧，说不定就对了呢。 K、EllipsoidTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Special Judge Problem DescriptionGiven a 3-dimension ellipsoid(椭球面) your task is to find the minimal distance between the original point (0,0,0) and points on the ellipsoid. The distance between two points (x1,y1,z1) and (x2,y2,z2) is defined as InputThere are multiple test cases. Please process till EOF. For each testcase, one line contains 6 real number a,b,c(0 &lt; a,b,c,&lt; 1),d,e,f(0 ≤ d,e,f &lt; 1), as described above. It is guaranteed that the input data forms a ellipsoid. All numbers are fit in double. OutputFor each test contains one line. Describes the minimal distance. Answer will be considered as correct if their absolute error is less than 10-5. Sample Input 1 0.04 0.01 0 0 0 Sample Output 1.0000000 题意题目给出一个椭圆面的空间方程，要求计算出原点与椭圆面之间的最短距离。 分析数学上的正解应该是解拉格朗日乘数法在附加条件下的多元函数极值。 网上给出的题解大多有两种：模拟退火、三分。这两种算法在一定程度上都有问题。 先说这个： 模拟退火一般的搜索都是完全朝着最优解去的，但是在某些情况下，这样就可能存在一个问题：可能当前得到的最优解只是局部的，无法得到全局的最优解。类似贪心算法的问题。 当然这样的情况不太常见。 然后模拟退火的核心是：以一定概率接受向着非最优解的方向移动，这里涉及到一个退火温度的概念。初始时温度较高，向非最优解方向移动的概率也较高，随着搜索层数的增加，可以认为离最优解越来越近了，温度逐渐下降，非最优解跳转的概率也会逐渐下降。 但是搜遍了网上关于本题的模拟退火代码，貌似都是一个版本出来的……-_-////// 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/* ***********************************************MYID : Chen FanLANG : G++PROG : HDU5017-Easy Simulated Annealing************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;using namespace std;double a,b,c,d,e,f;double r=0.99;double eps=1e-8;int dx[]=&#123;-1,-1,-1,0,0,1,1,1&#125;;int dy[]=&#123;-1,0,1,-1,1,-1,0,1&#125;;double cal(double x,double y)&#123; double A=c,B=e*x+d*y,C=a*x*x+b*y*y+f*x*y-1; double delta=B*B-4*A*C; if (delta&lt;0) return 1e60; double z1=(-B+sqrt(delta))/2/A,z2=(-B-sqrt(delta))/2/A; if (z1*z1&lt;z2*z2) return z1; else return z2;&#125;double dis(double x,double y,double z)&#123; return sqrt(x*x+y*y+z*z);&#125;double doit()&#123; double step=1; double x=0,y=0,z; while (step&gt;eps) &#123; z=cal(x,y); for (int i=0;i&lt;8;i++) &#123; double xx=x+dx[i]*step,yy=y+dy[i]*step; double zz=cal(xx,yy); if (zz&gt;1e30) continue; if (dis(xx,yy,zz)&lt;dis(x,y,z)) &#123; x=xx; y=yy; z=zz; &#125; &#125; step*=r; &#125; return dis(x,y,z);&#125;int main()&#123; while (scanf("%lf%lf%lf%lf%lf%lf",&amp;a,&amp;b,&amp;c,&amp;d,&amp;e,&amp;f)!=EOF) printf("%.8f\n",doit()); return 0;&#125; 个人认为这种搜索策略与真正的模拟退火算法还是有一点区别的，首先这里面根本没有涉及到向非最优解方向移动的情况。 我的理解是，这里模仿了模拟退火算法对温度控制的特性。由于是求解原点与椭球面的距离，这个结果必然是一个凸函数，则不断修正前进的方向并逐步缩小搜索范围，最终到达最优解。 首先指定一个步长与步长衰减速度（类退火速度），每次搜索完成一层，步长衰减一次，直到衰减程度在精度控制范围内。 三分二分用来求解线性函数的最优值，三分则是用来求解凸函数的最优值。 对于本题来说，基本想法是以x,y,z中的其中两个变量进行两次三分，而后计算出第三个变量，并以最终的距离作为判断函数。由于这里都是实数，两层三分需要迭代的次数比较难以控制，参数稍微修正得不对，精度与时间上就会出现比较大的问题，这两个要求的参数是对立的，精度高了耗时就长，时间控制了，可能最终结果又错了。 最后提交的是在超时边缘……中间出现了许多重复运算，这种还是不推荐使用了。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACM-ICPC Asia Regional Anshan Online]]></title>
    <url>%2F2014%2F09%2F13%2F2014-09-13-The-2014-ACM-ICPC-Asia-Regional-Anshan-Online%2F</url>
    <content type="text"><![CDATA[【A】无向图的双联通子图计数、DP+状态压缩 【B】计算几何（点的旋转） 【C】DP+状态压缩 【D】离散数学+DP （感觉可出） 【E】概率DP 【F】LCT模板题（-_-///LCT是啥！！！！） 【G】签到题 【H】染色+搜索 【I】阅读理解+记忆化搜索 【J】物理题，数论，高斯消元法（感觉可出） B、RotateTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Special Judge Problem DescriptionNoting is more interesting than rotation! Your little sister likes to rotate things. To put it easier to analyze, your sister makes n rotations. In the i-th time, she makes everything in the plane rotate counter-clockwisely around a point ai by a radian of pi. Now she promises that the total effect of her rotations is a single rotation around a point A by radian P (this means the sum of pi is not a multiplier of 2π). Of course, you should be able to figure out what is A and P :). InputThe first line contains an integer T, denoting the number of the test cases. For each test case, the first line contains an integer n denoting the number of the rotations. Then n lines follows, each containing 3 real numbers x, y and p, which means rotating around point (x, y) counter-clockwisely by a radian of p. We promise that the sum of all p’s is differed at least 0.1 from the nearest multiplier of 2π. T&lt;=100. 1&lt;=n&lt;=10. 0&lt;=x, y&lt;=100. $0&lt;=p&lt;=2\pi$. OutputFor each test case, print 3 real numbers x, y, p, indicating that the overall rotation is around (x, y) counter-clockwisely by a radian of p. Note that you should print p where $0&lt;=p&lt;2\pi$. Your answer will be considered correct if and only if for x, y and p, the absolute error is no larger than 1e-5. Sample Input 1 30 0 11 1 12 2 1 Sample Output 1.8088715944 0.1911284056 3.0000000000 题意对于每一组测试数据，给出一系列旋转命令，每次将整个平面空间绕着某一点逆时针旋转某一个弧度。而所有这些操作的最终结果可以等效为绕着A点旋转P弧度，题目的要求即求出这个坐标点A和弧度P。 分析当时看到这题解析几何，第一反应就是不想去管它了，因为手头没有趁手的模板可用，本来就数学不好，算起来太过耗时，还容易错，无奈水平有限，能做的题目不多，还是回来写这个了。 既然题目确定了所有这些操作都可以等效为一个，那我们也不需呀去管这个结论是怎么证明出来的了。基本的思想是用一条线段去指代这个平面，每次将线段的两端点都进行一次旋转操作，就相当于整条线段都旋转了，更进一步地就代表平面旋转了。之后分析原始坐标与结果坐标之间的关系即可得出等效结果。 一、原坐标与起始坐标相连的线段必然是以旋转中心为圆心的圆上的一条弦。则根据两个点的旋转结果，取线段中垂线算出交点即可求出圆心，即旋转中心； 二、然后就是旋转角度了，画个三角形余弦定理求解即可。 注意最开始一直WA是忽略了一个问题就是P的范围是0~2Pi，三角形余弦定理计算出的结果肯定是小于Pi的，就是最后必须判断一下旋转角度是否超过了Pi，超了则最终的输出结果应该是2Pi-P。这里要用到判断两点是否在一条直线的同一边。 没有好的模板……下面的代码看着各种乱啊： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/* ***********************************************MYID : Chen FanLANG : G++PROG : B1002************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cmath&gt;#define Pi 3.141592657typedef struct poi&#123; double x,y;&#125; point;using namespace std;point rotate(point v,point p,double angle)&#123; point ret=p; v.x-=p.x,v.y-=p.y; p.x=cos(angle); p.y=sin(angle); ret.x+=v.x*p.x-v.y*p.y; ret.y+=v.x*p.y+v.y*p.x; return ret;&#125;int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n; scanf("%d",&amp;n); point p1=&#123;20.123,6.678&#125;,p2=&#123;3.414,10.123&#125;; point p0=&#123;(p1.x+p2.x)/2,(p1.y+p2.y)/2&#125;; for (int i=1;i&lt;=n;i++) &#123; point now; double p; scanf("%lf%lf%lf",&amp;now.x,&amp;now.y,&amp;p); p1=rotate(p1,now,p); p2=rotate(p2,now,p); &#125; point p10; p10.x=(p1.x+20.123)/2; p10.y=(p1.y+6.678)/2; double k1=(p1.y-p10.y)/(p1.x-p10.x); k1=-1/k1; double b1=p10.y-k1*p10.x; point p20; p20.x=(p2.x+3.414)/2; p20.y=(p2.y+10.123)/2; double k2=(p2.y-p20.y)/(p2.x-p20.x); k2=-1/k2; double b2=p20.y-k2*p20.x; double xx=(b2-b1)/(k1-k2); double yy=k1*xx+b1; double bb=(xx-3.414)*(xx-3.414)+(yy-10.123)*(yy-10.123); double cc=(xx-p2.x)*(xx-p2.x)+(yy-p2.y)*(yy-p2.y); double aa=(p2.x-3.414)*(p2.x-3.414)+(p2.y-10.123)*(p2.y-10.123); double ct=(bb+cc-aa)/(2*sqrt(bb)*sqrt(cc)); point p3=&#123;2*xx-p0.x,2*yy-p0.y&#125;; point p4=&#123;(p1.x+p2.x)/2,(p1.y+p2.y)/2&#125;; double k3=(p3.y-p0.y)/(p3.x-p0.x); double b3=p3.y-k3*p3.x; point pp=&#123;xx,yy&#125;; point p5=rotate(p0,pp,1); if ((p4.x*k3+b3-p4.y)*(p5.x*k3+b3-p5.y)&gt;0) printf("%.10lf %.10lf %.10lf\n",xx,yy,acos(ct)); else printf("%.10lf %.10lf %.10lf\n",xx,yy,2*Pi-acos(ct)); &#125; return 0;&#125; 启发对于计算几何的东西，真的需要提前准备下模板，主要的几何思想要看数学能力。 E、WalkTime Limit: 30000/15000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionI used to think I could be anything, but now I know that I couldn’t do anything. So I started traveling. The nation looks like a connected bidirectional graph, and I am randomly walking on it. It means when I am at node i, I will travel to an adjacent node with the same probability in the next step. I will pick up the start node randomly (each node in the graph has the same probability.), and travel for d steps, noting that I may go through some nodes multiple times. If I miss some sights at a node, it will make me unhappy. So I wonder for each node, what is the probability that my path doesn’t contain it. InputThe first line contains an integer T, denoting the number of the test cases. For each test case, the first line contains 3 integers n, m and d, denoting the number of vertices, the number of edges and the number of steps respectively. Then m lines follows, each containing two integers a and b, denoting there is an edge between node a and node b. T&lt;=20, n&lt;=50, n-1&lt;=m&lt;=n*(n-1)/2, 1&lt;=d&lt;=10000. There is no self-loops or multiple edges in the graph, and the graph is connected. The nodes are indexed from 1. OutputFor each test cases, output n lines, the i-th line containing the desired probability for the i-th node. Your answer will be accepted if its absolute error doesn’t exceed 1e-5. Sample Input 2 5 10 1001 22 33 44 51 52 43 52 51 41 310 10 101 22 33 44 55 66 77 88 99 104 9 Sample Output 0.00000000000.00000000000.00000000000.00000000000.00000000000.69933179670.58642849520.44408608210.22758969910.42940745910.48510487420.48960188420.45250442500.34065674830.6421630037 题意随机随机随机！！…在一张无向图中，随机选定一个起点出发，之后的每一步都是等概率的。有一些点不能被到达，这里要求的就是每一个点不能被到达的概率。 分析概率DP的题目做得太少了，以至于当时最后的时间基本都花在这里了，还是没有什么想法。每次都是这样，比完之后才能想到最后的正解，唉。当时纠结的是怎么记录访问过的路径，然后判断每一次遍历完之后有哪些点是该次没有被访问过的。这个想法完全是不可行的。 既然是判断一个点不能到达的概率，则直接把这个点从图中去掉，依次递推其他所有点在走完d步之后到达的概率，结果的总和就是不能到达该点的概率。 概率DP的状态转移方程：f(i,j)=f(i-1,front[j])/num[front[j]] 求解的时候，依次枚举一个点，把这个点拿掉，然后递推DP结果，由于当前步的状态只与前一步有关，这里可以用滚动数组处理。 前向星是我的个人习惯，这里恰好需要一个计算的变量，前向星的结构恰好满足。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/* ***********************************************MYID : Chen FanLANG : G++PROG : E1005************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;typedef struct nod&#123; int a,b;&#125; node;node a[3000];int start[60],num[60],n,m,d,tot;bool ma[60][60];double ans[60],temp[2][60];bool op(node a,node b)&#123; if (a.a==b.a) return a.b&lt;b.b; else return a.a&lt;b.a;&#125;int main()&#123; freopen("E1005.txt","r",stdin); int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; scanf("%d%d%d",&amp;n,&amp;m,&amp;d); memset(ma,0,sizeof(ma)); int tail=0; for (int i=1;i&lt;=m;i++) &#123; int x,y; scanf("%d%d",&amp;x,&amp;y); if (!ma[x][y]) &#123; ma[x][y]=true; ma[y][x]=true; tail++; a[tail].a=x; a[tail].b=y; tail++; a[tail].a=y; a[tail].b=x; &#125; &#125; sort(&amp;a[1],&amp;a[tail+1],op); int o=0; memset(num,0,sizeof(num)); for (int i=1;i&lt;=tail;i++) &#123; if (o!=a[i].a) &#123; o=a[i].a; start[o]=i; &#125; num[o]++; &#125; memset(ans,0,sizeof(ans)); for (int k=1;k&lt;=n;k++) &#123; memset(temp,0,sizeof(temp)); int key=1; for (int i=1;i&lt;=n;i++) if (i!=k) temp[0][i]=1.0/n; for (int i=1;i&lt;=d;i++) &#123; memset(temp[key],0,sizeof(temp[key])); for (int j=1;j&lt;=n;j++) &#123; for (int o=0;o&lt;num[j];o++) if (a[start[j]+o].b!=k) temp[key][a[start[j]+o].b]+=temp[1-key][j]/(double)num[j]; &#125; key=1-key; &#125; key=1-key; for(int i=1;i&lt;=n;i++) if (i!=k) ans[k]+=temp[key][i]; &#125; for (int i=1;i&lt;=n;i++) printf("%.10lf\n",ans[i]); &#125; return 0;&#125; 启发题目中碰到一个点对前后状态影响较大，无法直接推算或者直接算需要涉及到大量复杂记录的情况，可以考虑先把这个点拿掉，完成之后再放上去，或者先把其变为普通点，完成后再恢复（后面有广州赛区的D题就是这种思想） G、Osu!Time Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Special Judge Problem DescriptionOsu! is a famous music game that attracts a lot of people. In osu!, there is a performance scoring system, which evaluates your performance. Each song you have played will have a score. And the system will sort all you scores in descending order. After that, the i-th song scored ai will add 0.95^(i-1)*ai to your total score. Now you are given the task to write a calculator for this system. InputThe first line contains an integer T, denoting the number of the test cases. For each test case, the first line contains an integer n, denoting the number of songs you have played. The second line contains n integers a1, a2, …, an separated by a single space, denoting the score of each song. T&lt;=20, n&lt;=50, 1&lt;=ai&lt;=500. OutputFor each test case, output one line for the answer.Your answers will be considered correct if its absolute error is smaller than 1e-5. Sample Input 1 2530 478 Sample Output 984.1000000000 题意排序+pow函数输出即可。]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 1068 Girls and Boys 二分图最大独立集（最大二分匹配）]]></title>
    <url>%2F2014%2F09%2F12%2F2014-09-12-HDU-1068-Girls-and-Boys%2F</url>
    <content type="text"><![CDATA[Girls and BoysTime Limit: 20000/10000 MS (Java/Others) Memory Limit: 65536/32768 K (Java/Others) Problem Descriptionthe second year of the university somebody started a study on the romantic relations between the students. The relation “romantically involved” is defined between one girl and one boy. For the study reasons it is necessary to find out the maximum set satisfying the condition: there are no two students in the set who have been “romantically involved”. The result of the program is the number of students in such a set. The input contains several data sets in text format. Each data set represents one set of subjects of the study, with the following description: the number of studentsthe description of each student, in the following formatstudent_identifier:(number_of_romantic_relations) student_identifier1 student_identifier2 student_identifier3 …orstudent_identifier:(0) The student_identifier is an integer number between 0 and n-1, for n subjects.For each given data set, the program should write to standard output a line containing the result. Sample Input 7 0: (3) 4 5 61: (2) 4 62: (0)3: (0)4: (2) 0 15: (1) 06: (2) 0 13 0: (2) 1 21: (1) 02: (1) 0 Sample Output 5 2 题意题目给定一些男女生之间相互的romantic关系，要求找出一个最大的集合，使得该集合中的所有男女生之间都不存在romantic关系。 分析一个二分图的最大独立集点数与最大二分匹配个数有直接的关系： 最大独立集点数 = 顶点数 - 最大二分匹配对数 故本题直接转化为求最大二分匹配即可，需要注意的是，题中给出的条件是1指向2，2也会指向1，所以最终算出来的匹配数其实是实际对数的两倍，最终被顶点数减去之前首先需要折半。 基础二分匹配练手题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192/*ID: Chen FanPROG: hdu1068LANG: G++*/#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;typedef struct nod&#123; int a,b;&#125; node;node a[1000];int result[1000],start[1000],num[1000];bool flag[1000];bool op(node a,node b)&#123; if (a.a==b.a) return a.b&lt;b.b; else return a.a&lt;b.a;&#125;bool find(int s)&#123; for (int i=0;i&lt;num[s];i++) &#123; int now=a[start[s]+i].b; if (!flag[now]) &#123; flag[now]=true; if (result[now]==-1||find(result[now])) &#123; result[now]=s; return true; &#125; &#125; &#125; return false;&#125; int main()&#123; int n; while (scanf("%d",&amp;n)!=EOF) &#123; int tail=0; for (int i=1;i&lt;=n;i++) &#123; int x,p; scanf("%d: (%d",&amp;x,&amp;p); getchar(); for (int j=1;j&lt;=p;j++) &#123; int y; scanf("%d",&amp;y); tail++; a[tail].a=x; a[tail].b=y; &#125; &#125; sort(&amp;a[1],&amp;a[tail+1],op); int o=-1; memset(num,0,sizeof(num)); for (int i=1;i&lt;=tail;i++) &#123; if (o!=a[i].a) &#123; o=a[i].a; start[o]=i; &#125; num[o]++; &#125; memset(result,-1,sizeof(result)); int ans=0; for (int i=0;i&lt;n;i++) &#123; memset(flag,0,sizeof(flag)); if (find(i)) ans++; &#125; printf("%d\n",n-ans/2); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>二分图匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 3367 Pseudoforest 最大生成树]]></title>
    <url>%2F2014%2F09%2F09%2F2014-09-09-HDU-3367-Pseudoforest%2F</url>
    <content type="text"><![CDATA[PseudoforestTime Limit: 10000/5000 MS (Java/Others) Memory Limit: 65536/65536 K (Java/Others) Problem DescriptionIn graph theory, a pseudoforest is an undirected graph in which every connected component has at most one cycle. The maximal pseudoforests of G are the pseudoforest subgraphs of G that are not contained within any larger pseudoforest of G. A pesudoforest is larger than another if and only if the total value of the edges is greater than another one’s. InputThe input consists of multiple test cases. The first line of each test case contains two integers, n(0 &lt; n &lt;= 10000), m(0 &lt;= m &lt;= 100000), which are the number of the vertexes and the number of the edges. The next m lines, each line consists of three integers, u, v, c, which means there is an edge with value c (0 &lt; c &lt;= 10000) between u and v. You can assume that there are no loop and no multiple edges. The last test case is followed by a line containing two zeros, which means the end of the input. OutputOutput the sum of the value of the edges of the maximum pesudoforest. Sample Input 3 30 1 11 2 12 0 14 50 1 11 2 12 3 13 0 10 2 20 0 Sample Output 3 5 题意给出一张图，求最大生成树。要求每一个连通块上只能有一个环。 分析对于Kruskal来说，最小/最大生成树只是改变一下排序顺序即可。这里需要另外注意添加的就是对环的判断了： 如果两个节点不在同一棵树内，且分别不成环，则可合并； 如果两个节点在同一棵树内，但是未成环，则加上这条边之后将成环； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;cstring&gt;using namespace std;typedef struct nod&#123; int x,y,c;&#125; node;node a[100010];bool op(node a,node b)&#123; return a.c&gt;b.c;&#125;int father[10010];bool flag[10010];void clean_father(int n)&#123; for (int i=0;i&lt;n;i++) father[i]=i;&#125; int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link(int x,int y)&#123; father[getfather(x)]=getfather(y);&#125; int main()&#123; int n,m; while (scanf("%d%d",&amp;n,&amp;m)) &#123; if (n==0&amp;&amp;m==0) break; for (int i=1;i&lt;=m;i++) scanf("%d%d%d",&amp;a[i].x,&amp;a[i].y,&amp;a[i].c); sort(&amp;a[1],&amp;a[m+1],op); clean_father(n); memset(flag,0,sizeof(flag)); int ans=0; for (int i=1;i&lt;=m;i++) if (getfather(a[i].x)!=getfather(a[i].y)) &#123; if (!(flag[getfather(a[i].x)]&amp;&amp;flag[getfather(a[i].y)])) &#123; if (flag[getfather(a[i].x)]||flag[getfather(a[i].y)]) &#123; flag[getfather(a[i].x)]=true; flag[getfather(a[i].y)]=true; &#125; link(a[i].x,a[i].y); ans+=a[i].c; &#125; &#125; else if (!flag[getfather(a[i].x)]) &#123; ans+=a[i].c; flag[getfather(a[i].x)]=true; &#125; printf("%d\n",ans); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The 2014 ACM-ICPC Asia Mudanjiang Regional First Round]]></title>
    <url>%2F2014%2F09%2F08%2F2014-09-08-The-2014-ACM-ICPC-Asia-Mudanjiang-Regional-First-Round%2F</url>
    <content type="text"><![CDATA[【A】签到题 【B】构造 【C】遍历+floodfill染色或并查集 【D】DP（背包）+状态压缩 （感觉可出） 【E】线段树 【F】图形题 搜索+状态压缩 【G】数论，某种不定方程……ZOJ上到现在只过了4个人，还找不到题解-_-/// 【H】回文数变种+dfs枚举构造 又是构造-_-/// 【I】字符串哈希+DP或者暴力 【J】直接模拟，字符串细节操作 A、The HimalayasTime Limit: 2 SecondsMemory Limit: 65536 KB DescriptionAs an artist, Bob usually need to travel around the world. He made a lot of sketch of scenery on his journey. A famous spot he have visited recently is the Himalayas. The Himalayas is a mountain range in South Asia separating the plains of the Indian subcontinent from the Qinghai-Tibet Plateau. The Himalayas include over a hundred mountains exceeding 7,200 meters in elevation. One day, Bob came up with an strange idea. He wanted to know the number of mountain peaks in his paintings. As his best friend, he turned to you for help. You are given a list of N height sampling values Hi. You should determine how many peaks are there. For all i which satisfies 2 &lt;= i &lt;= N - 1, Hi is defined as a peak if and only if Hi-1 &lt; Hi &gt; Hi+1. InputThere are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: The first line contains one integer N (1 &lt;= N &lt;= 50). The next line contains N integers Hi (1 &lt;= Hi &lt;= 8844). It is guaranteed that any two adjacent height sampling values will be different. OutputFor each test case, output the number of peaks. Sample Input 2 91 3 2 4 6 3 2 3 15 1 2 3 4 5 Sample Output 3 0 题意给出一系列地形的高度，要求找到山峰的个数 分析稍微判断一下即可。 B、A Volcanic IslandTime Limit: 2 SecondsMemory Limit: 65536 KBSpecial Judge DescriptionAn underwater volcano has erupted massively in somewhere of the deep Atlantis Ocean. This large eruption led to the birth of a new volcanic island, which had a shape of square. Near the island, there are N countries. All of them have claimed the sovereignty over the island. After a lot of multilateral negotiation and occasional armed conflicts, the N countries decided to divide the square volcanic island equally. They partitioned the island into N x N small equal-sized square chunks. Each country could get a connected region consists of exact N chunks. Two chunks A and B are called “connected” if they share an edge, or there exists another chunk C connected with both A and B. A group of chunks are called “connected region” if any two of these chunks are connected. Every country want a unique region. It means the N regions should be different with each other. Two regions are considered as the same if and only if one can transform into the other by an isometry (a combination of rigid motions, including translation, rotation and reflection). In a nutshell, your task is to divide a square island with N x N chunks into N connected regions with different shape. You also need to draw a map to color the regions of the map so that no two edge-adjacent regions have the same color. Most of the people in these countries believed that four different colors are enough. So you can mark these regions with at most four colors, red, green, blue and yellow. InputThere are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: There is only an integer N (1 &lt;= N &lt;= 100). OutputFor each test case, output a valid map described above. If there is no solution, output “No solution!” instead. Please note that only four colors (‘R’, ‘G’, ‘B’ and ‘Y’) can be used to drawing the map. ##Sample Input 2 25 Sample Output No solution! YYYGRYGGGRYGYYRBYYYRBBBBR 题意给出一张n*n的图，要求用4种颜色去染色，染色之后将图分成n个总数为n的颜色块。另外的一个重要要求就是这n个色块的形状不能出现重复。 分析本题采用的是special judge，所以需要自己构造出一种方案来对其进行染色。 赛后参考其他大神的想法，构造方案如下： 以7和8为例： 首先将其中的最后一列全部记为一种颜色，剩下的n*(n-1)部分用另外3种颜色去染： 前n/2种区块蛇形染色： 123456789101112131415161718192021n=7-------111111022222102233330xxx3330xxxxxx0xxxxxx0xxxxxx0-------n=8--------111111102222221022333330444433304444xxx0xxxxxxx0xxxxxxx0xxxxxxx0-------- 后n/2块一行一行从左向右将剩下部分填满: 1234567891011121314151617181920212223n=7-------111111 0222221 0223333 0 333 0444554444 0655555 0666666 0-------n=8--------1111111 02222221 02233333 04444333 04444 555 05555566 06666667 07777777 0-------- 以上n=7和n=8时最后对齐的方向与第一部分最后一行染色的方向相同。 接下来需要注意的就是第一块与最后一块的形状相同，于是作一下微调把它们变得不一样即可： 1234567891011121314151617181920n=7-------1111110222221022333304443330654444065555506666650-------n=8--------1111111022222210223333304444333044445550555556706666667067777770 我的做法是把最后一块色块在最后一行的其中一块与最后第二块色块在n-2行的其中一块进行互换，保证相同颜色这时候还是连在一起的就行。 测试时，发现n等于5和6时，用这种调整方案是不行的，因为格子数太少，会出现第一块与最后一块颜色相同然后重叠到一起的情况。对包括这个在内的几种情况进行一下特判即可。 赛后启发以后碰到这种需要自己进行构造的题目不要直接就放弃，感觉稍微花点时间还真是能想出来的，，，这种纠结程度就跟贪心似地，，，如果没别的题能做了，至少随便构造一个交一下看看呗。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/* ***********************************************MYID : Chen FanLANG : G++PROG : ZOJ3810************************************************ *//* 1111110 2222210 2233330 4443330 6544440 6555550 6666650 11111110 22222210 22333330 44443330 44445550 55555670 66666670 67777770*/#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;int a[110][110];int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n; scanf("%d",&amp;n); if (n==1) printf("Y\n"); else if (n==2||n==3||n==4) printf("No solution!\n"); else if (n==5) printf("YYYGR\nYGGGR\nYGYYR\nBYYYR\nBBBBR\n"); else if (n==6) printf("RRRRRY\nRGGGGY\nGGRRRY\nRRRGBY\nGGGGBY\nGBBBBY\n"); else &#123; memset(a,0,sizeof(a)); int x=1,y=1,fx=1; for (int i=1;i&lt;=n/2;i++) for (int j=1;j&lt;=n;j++) &#123; a[x][y]=i; y+=fx; if ((y==n&amp;&amp;fx==1)||(y==0&amp;&amp;fx==-1)) &#123; x++; fx=-fx; y+=fx; &#125; &#125; for (int i=n/2+1;i&lt;n;i++) for (int j=1;j&lt;=n;j++) &#123; a[x][y]=i; y+=fx; if ((y==n&amp;&amp;fx==1)||(y==0&amp;&amp;fx==-1)) &#123; x++; y=n-y+fx; &#125; &#125; if (fx==-1) &#123; a[n-2][1]=n-1; a[n][n-1]=n-2; &#125; else &#123; a[n-2][n-1]=n-1; a[n][1]=n-2; &#125; char words[]=&#123;'R','G','B'&#125;; for (int i=1;i&lt;=n;i++) &#123; for (int j=1;j&lt;=n;j++) if (a[i][j]==0) printf("Y"); else printf("%c",words[a[i][j]%3]); printf("\n"); &#125; &#125; &#125; return 0;&#125; C、Untrusted PatrolTime Limit: 3 SecondsMemory Limit: 65536 KB DescriptionEdward is a rich man. He owns a large factory for health drink production. As a matter of course, there is a large warehouse in the factory. To ensure the safety of drinks, Edward hired a security man to patrol the warehouse. The warehouse has N piles of drinks and M passageways connected them (warehouse is not big enough). When the evening comes, the security man will start to patrol the warehouse following a path to check all piles of drinks. Unfortunately, Edward is a suspicious man, so he sets sensors on K piles of the drinks. When the security man comes to check the drinks, the sensor will record a message. Because of the memory limit, the sensors can only record for the first time of the security man’s visit. After a peaceful evening, Edward gathered all messages ordered by recording time. He wants to know whether is possible that the security man has checked all piles of drinks. Can you help him? The security man may start to patrol at any piles of drinks. It is guaranteed that the sensors work properly. However, Edward thinks the security man may not works as expected. For example, he may digs through walls, climb over piles, use some black magic to teleport to anywhere and so on. InputThere are multiple test cases. The first line of input is an integer T indicates the number of test cases. For each test case: The first line contains three integers N (1 &lt;= N &lt;= 100000), M (1 &lt;= M &lt;= 200000) and K (1 &lt;= K &lt;= N). The next line contains K distinct integers indicating the indexes of piles (1-based) that have sensors installed. The following M lines, each line contains two integers Ai and Bi (1 &lt;= Ai, Bi &lt;= N) which indicates a bidirectional passageway connects piles Ai and Bi. Then, there is an integer L (1 &lt;= L &lt;= K) indicating the number of messages gathered from all sensors. The next line contains L distinct integers. These are the indexes of piles where the messages came from (each is among the K integers above), ordered by recording time. OutputFor each test case, output “Yes” if the security man worked normally and has checked all piles of drinks, or “No” if not. Sample Input 2 5 5 31 2 41 22 33 11 44 53 4 2 15 5 31 2 41 22 33 11 44 53 4 1 2 Sample Output NoYes 题意给出一张图，图中的某些结点上装着传感器，访问到装着传感器的节点时，就会被记录下来。现在给出一串传感器记录下来的时间访问序列，询问一个人是否能够按照序列的顺序遍历完整张图。 分析1.floodfill染色：当时看到这道题想到的只有暴力DFS，写出来之后发现由于floodfill染色的性质，每个点只是访问一次就够了，效率还是很优的。 基本想法是从时间序列第一个点开始，按时间顺序扩展每一个点，所有标记着传感器的点不可达。扩展完第一个点之后，判断一下下一个点是否能够到达，若不能到达即说明按顺序遍历不可能；否则继续扩展下一个点。 对每一个结点设置四种标记：无标记、已访问、不可访问、能够访问； 普通点在到达之前无标记，到达之后标记为已访问；传感器结点在到达之前标记为不可访问，到达之后标记为能够访问，搜过之后标记为已访问； 一、直接搜索L个传感器的序列，首先将所有拥有传感器的序列全部标记为不可访问； 二、从第一个开始，扩展遍历所有能到达的点，遍历时的点有两种状态：无标记（普通点）和不可/能够访问（带有传感器的点）。无标记点可以直接改为已访问，不可访问的点则标记为能够访问。遍历完之后将该传感器点标为已访问； 三、判断下一个传感器点的状况是不是能够访问，若是则表示可以从之前的传感器点不经过任何序列后面的点而到达，若标记是不可访问则表示不可能从之前的传感器点到达这里，那就直接输出NO了。 四、之后重复二、三过程，直到搜完所有L个点，判断整张图的连通性，即是不是所有点都能到达。 对于K和L，个人觉得描述的最后一句是有用的，就是L只会小于等于K，不一定要等于K。然后第一次没有加这个特判，结果WA了……加上之后AC，-_-////真的是我想多了吗 然后就是前向星存储。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/* ***********************************************MYID : Chen FanLANG : G++PROG : ZOJ3811************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef struct nod&#123; int x,y;&#125; node;node a[400010];int start[100010],num[100010];unsigned short flag[100010];int sen[100010];bool op(node a,node b)&#123; if (a.x==b.x) return a.y&lt;b.y; else return a.x&lt;b.x;&#125;void dfs(int s)&#123; flag[s]=3; for (int i=0;i&lt;num[s];i++) &#123; int next=a[start[s]+i].y; if (flag[next]==0) dfs(next); else if (flag[next]==1) flag[next]=2; &#125;&#125;int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; int n,m,k; scanf("%d%d%d",&amp;n,&amp;m,&amp;k); for (int i=1;i&lt;=k;i++) scanf("%d",&amp;sen[i]); for (int i=1;i&lt;=m;i++) &#123; int x,y; scanf("%d%d",&amp;x,&amp;y); a[i*2].x=x; a[i*2].y=y; a[i*2-1].x=y; a[i*2-1].y=x; &#125; int l; scanf("%d",&amp;l); memset(flag,0,sizeof(flag)); for (int i=1;i&lt;=l;i++) &#123; scanf("%d",&amp;sen[i]); flag[sen[i]]=1; &#125; if (l&lt;k) &#123; printf("No\n"); continue; &#125; m*=2; memset(start,0,sizeof(start)); memset(num,0,sizeof(num)); sort(&amp;a[1],&amp;a[m+1],op); int o=0; for (int i=1;i&lt;=m;i++) &#123; if (o!=a[i].x) &#123; o=a[i].x; start[o]=i; &#125; num[o]++; &#125; flag[sen[1]]=2; bool outp=true; for (int i=1;i&lt;=l;i++) if (flag[sen[i]]==2) dfs(sen[i]); else &#123; outp=false; break; &#125; if (outp) for (int i=1;i&lt;=n;i++) if (flag[i]!=3) &#123; outp=false; break; &#125; if (outp) printf("Yes\n"); else printf("No\n"); &#125; return 0;&#125; 2.并查集大体思路与上面类似，把1中的染色部分改成扩展一个可以任意到达的无限集即可。 启发题目中给出了明显的顺序的话，可以作为一个突破点。 H、Generalized Palindromic NumberTime Limit: 2 SecondsMemory Limit: 65536 KB DescriptionA number that will be the same when it is written forwards or backwards is known as a palindromic number. For example, 1234321 is a palindromic number. We call a number generalized palindromic number, if after merging all the consecutive same digits, the resulting number is a palindromic number. For example, 122111 is a generalized palindromic number. Because after merging, 122111 turns into 121 which is a palindromic number. Now you are given a positive integer N, please find the largest generalized palindromic number less than N. InputThere are multiple test cases. The first line of input contains an integer T (about 5000) indicating the number of test cases. For each test case: There is only one integer N (1 &lt;= N &lt;= 1018). OutputFor each test case, output the largest generalized palindromic number less than N. Sample Input 4 1212312241122 Sample Output 1112112211121 题意如果一个数把相邻数位相同的部分都合并之后它还是一个回文数，那就称其为广义回文数。现在给出一个n，要求找到小于n的最大的广义回文数。 分析开始时想从某种贪心的方法出发手动构造这样一个数出来，很可惜失败了，这里每次摆数字的判断有些麻烦。 赛后尝试用DFS进行构造： 每次从前后同时向中间构造，保证回文性。构造时先在高位摆上一个值，然后从低位向中间摆相同的数，中间注意剪枝判断。 注意摆值的时候上限与更高一位是不是与n相应位相同有关。如果前一位与原数不同，则可以从9开始摆，否则不能比原数大。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/* ***********************************************MYID : Chen FanLANG : G++PROG : ZOJ3816************************************************ */#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int tail,nn[30],num[30],ans[30];bool check(int a[],int b[],int l,int r) &#123; for (int i=r;i&gt;=l;i--) if (a[i]!=b[i]) return a[i]&gt;b[i]; return false;&#125;void dfs(int high,int low,bool flag)&#123; if (high&lt;low) &#123; if (check(nn,num,1,tail)&amp;&amp;check(num,ans,1,tail)) for (int i=1;i&lt;=tail;i++) ans[i]=num[i]; &#125; else &#123; int end; if (flag) end=nn[high]; else end=9; for (int i=end;i&gt;=0;i--) &#123; int now=low; if (check(ans,num,high+1,tail)) return; num[high]=i; if (num[high]!=num[high+1]) &#123; while (now&lt;=high) &#123; num[now]=num[high]; now++; dfs(high-1,now,flag&amp;&amp;i==end); &#125; &#125; else dfs(high-1,low,flag&amp;&amp;i==end); &#125; &#125;&#125;int main()&#123; int t; scanf("%d",&amp;t); for (int tt=1;tt&lt;=t;tt++) &#123; long long n; scanf("%llu",&amp;n); tail=0; while(n) &#123; tail++; nn[tail]=n%10; n/=10; &#125; memset(num,0,sizeof(num)); memset(ans,0,sizeof(ans)); dfs(tail,1,1); for (int i=tail;i&gt;=1;i--) printf("%d",ans[i]); printf("\n"); &#125; return 0;&#125; 启发实在不行可以暴搜啊！！！！！ J、Pretty PoemTime Limit: 2 SecondsMemory Limit: 65536 KB DescriptionPoetry is a form of literature that uses aesthetic and rhythmic qualities of language. There are many famous poets in the contemporary era. It is said that a few ACM-ICPC contestants can even write poetic code. Some poems has a strict rhyme scheme like “ABABA” or “ABABCAB”. For example, “niconiconi” is composed of a rhyme scheme “ABABA” with A = “ni” and B = “co”. More technically, we call a poem pretty if it can be decomposed into one of the following rhyme scheme: “ABABA” or “ABABCAB”. The symbol A, B and C are different continuous non-empty substrings of the poem. By the way, punctuation characters should be ignored when considering the rhyme scheme. You are given a line of poem, please determine whether it is pretty or not. InputThere are multiple test cases. The first line of input contains an integer T indicating the number of test cases. For each test case: There is a line of poem S (1 &lt;= length(S) &lt;= 50). S will only contains alphabet characters or punctuation characters. OutputFor each test case, output “Yes” if the poem is pretty, or “No” if not. Sample Input 3 niconiconi~ pettan,pettan,tsurupettanwafuwafu Sample Output YesYesNo 分析由于本题数据量最大只有50个字符，可以直接模拟枚举。 首先枚举AB，满足AB匹配之后，再判断AB长度的三倍与总长度之间的关系，分类讨论。 算法就是这么个算法，但是实现起来超容易出错…… 附上一组处理得比较全面的测试数据 8 xyxyxxyxyxyyxyxxxxyxxxxxxxxyxyxxxxxxxxxxxxxxxxxxxxxxxyzzxyzxyzzxyzxyzxyzzxyz 启发题目数据量才这么点。。。果断应该直接上的]]></content>
      <categories>
        <category>ACM-比赛</category>
      </categories>
      <tags>
        <tag>ACM-ICPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 1811 Rank of Tetris 拓扑排序+并查集]]></title>
    <url>%2F2014%2F09%2F07%2F2014-09-07-HDU-1811-Rank-of-Tetris%2F</url>
    <content type="text"><![CDATA[Rank of TetrisTime Limit: 1000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem Description自从Lele开发了Rating系统，他的Tetris事业更是如虎添翼，不久他遍把这个游戏推向了全球。 为了更好的符合那些爱好者的喜好，Lele又想了一个新点子：他将制作一个全球Tetris高手排行榜，定时更新，名堂要比福布斯富豪榜还响。关于如何排名，这个不用说都知道是根据Rating从高到低来排，如果两个人具有相同的Rating，那就按这几个人的RP从高到低来排。 终于，Lele要开始行动了，对N个人进行排名。为了方便起见，每个人都已经被编号，分别从0到N-1,并且编号越大，RP就越高。 同时Lele从狗仔队里取得一些（M个）关于Rating的信息。这些信息可能有三种情况，分别是”A &gt; B”,”A = B”,”A &lt; B”，分别表示A的Rating高于B,等于B,小于B。 现在Lele并不是让你来帮他制作这个高手榜，他只是想知道，根据这些信息是否能够确定出这个高手榜，是的话就输出”OK”。否则就请你判断出错的原因，到底是因为信息不完全（输出”UNCERTAIN”），还是因为这些信息中包含冲突（输出”CONFLICT”）。 注意，如果信息中同时包含冲突且信息不完全，就输出”CONFLICT”。 Input本题目包含多组测试，请处理到文件结束。 每组测试第一行包含两个整数N,M(0&lt;=N&lt;=10000,0&lt;=M&lt;=20000),分别表示要排名的人数以及得到的关系数。 接下来有M行，分别表示这些关系 Output对于每组测试，在一行里按题目要求输出 Sample Input 3 &gt; 1&lt; 2&gt; 24 = 2&gt; 3&gt; 0&gt; 13 &gt; 0&gt; 2&lt; 1 Sample Output OKCONFLICTUNCERTAIN 题意给定一些点对之间的关系（大于小于相等），判断是否发生冲突。 分析对于冲突判断，直观的想法就是拓扑排序。以大于号或者小于号方向作为拓扑序的方向，如果处理时出现违反拓扑序列的情况则可判断为冲突。 然后这道题还有另外一个问题就是相等情况的处理，如果直接按照拓扑排序对其进行分析，则可能发生错误。为了解决这个问题，可以用并查集进行预处理，将所有相等的点缩成1个点，然后进行后序的拓扑排序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int father[10001]; void clean_father(int n)&#123; for (int i=0;i&lt;n;i++) father[i]=i;&#125; int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link(int x,int y)&#123; father[getfather(x)]=getfather(y);&#125; int x[20001],y[20001],rd[10001],num[10001],start[10001];char z[20001];typedef struct &#123; int a,b;&#125; node;node a[20001];bool op(node a,node b)&#123; return a.a&lt;b.a;&#125;int stack[10001];int main()&#123; int n,m; while (scanf("%d%d",&amp;n,&amp;m)!=EOF) &#123; clean_father(n); for (int i=1;i&lt;=m;i++) &#123; scanf("%d %c %d",&amp;x[i],&amp;z[i],&amp;y[i]); if (z[i]=='=') link(x[i],y[i]); &#125; int tail=0; for (int i=0;i&lt;n;i++) rd[i]=-1; int tot=0; for (int i=0;i&lt;n;i++) &#123; rd[getfather(i)]=0; if (rd[i]==0) tot++; &#125; for (int i=1;i&lt;=m;i++) switch(z[i]) &#123; case '&gt;': tail++; a[tail].a=getfather(x[i]); a[tail].b=getfather(y[i]); rd[getfather(y[i])]++; break; case '&lt;': tail++; a[tail].a=getfather(y[i]); a[tail].b=getfather(x[i]); rd[getfather(x[i])]++; &#125; sort(&amp;a[1],&amp;a[tail+1],op); int o=-1; for (int i=0;i&lt;n;i++) num[i]=0; for (int i=1;i&lt;=tail;i++) &#123; if (a[i].a!=o) &#123; o=a[i].a; start[o]=i; &#125; num[o]++; &#125; int top=0; for (int i=0;i&lt;n;i++) if (rd[i]==0) &#123; top++; stack[top]=i; &#125; if (top==0) &#123; printf("CONFLICT\n"); continue; &#125; bool flag=false,done=false; while (top&gt;0&amp;&amp;(!done)) &#123; if (top&gt;1) flag=true; int now=stack[top]; top--;tot--; for (int i=0;i&lt;num[now];i++) &#123; rd[a[start[now]+i].b]--; if (rd[a[start[now]+i].b]==0) &#123; top++; stack[top]=a[start[now]+i].b; &#125; else if (rd[a[start[now]+i].b]&lt;0) &#123; done=true; break; &#125; &#125; num[now]=0; &#125; if (tot&gt;0||done) printf("CONFLICT\n"); else if (flag) printf("UNCERTAIN\n"); else printf("OK\n"); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>拓扑排序</tag>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 1589 Find The Most Comfortable Road 最小生成树+枚举]]></title>
    <url>%2F2014%2F09%2F07%2F2014-09-07-HDU-1589-Find-The-Most-Comfortable-Road%2F</url>
    <content type="text"><![CDATA[find the most comfortable roadTime Limit: 1000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionXX星有许多城市，城市之间通过一种奇怪的高速公路SARS(Super Air Roam Structure—超级空中漫游结构）进行交流，每条SARS都对行驶在上面的Flycar限制了固定的Speed，同时XX星人对 Flycar的“舒适度”有特殊要求，即乘坐过程中最高速度与最低速度的差越小乘坐越舒服 ,(理解为SARS的限速要求，flycar必须瞬间提速/降速，痛苦呀 ), 但XX星人对时间却没那么多要求。要你找出一条城市间的最舒适的路径。(SARS是双向的）。 Input 输入包括多个测试实例，每个实例包括： 第一行有2个正整数n (1&lt;n&lt;=200)和m (m&lt;=1000),表示有N个城市和M条SARS。 接下来的行是三个正整数StartCity,EndCity,speed,表示从表面上看StartCity到EndCity,限速为speedSARS。speed&lt;=1000000 然后是一个正整数Q（Q&lt;11),表示寻路的个数。接下来Q行每行有2个正整数Start,End,表示寻路的起终点。 Output每个寻路要求打印一行，仅输出一个非负整数表示最佳路线的舒适度最高速与最低速的差。如果起点和终点不能到达，那么输出-1。 Sample Input 4 2 23 44 14 21 32 Sample Output 1 0 题意给定一张无向有权图和一些询问，每一个询问都是一对起/终点，对于每一个询问，要求找到一条路能从起点到达终点，并且得到该条路上所有边权值中最大边与最小边的差，使得这个差值达到最小。最终的输出结果是这个最小差值。 分析考虑Kruskal的贪心过程：将边从小到大排序，不断添边的过程中用并查集判断端点的归属情况。 假设在MST的寻找过程中，一对询问的其中一个点已经加入集合，当找到另外一个点加入集合的时刻寻找就可以结束，此时能够保证最后这条加入的边是已有的边中最大的，因为更大的边还在后面。 所以可以不断枚举最小边，以指定的最小边为基础进行Kruskal最小生成树操作，这里可能有两种情况： 1、最小边恰好在起/终点的路径上，则找到的最后一条边与最小边的差值即为这次查找的结果； 2、最小边不在起/终点的路径上，没有关系，因为后序枚举中仍然能够找出来。 因为使用了贪心性质，这里不能保证这个算法是最优解，但是可以保证结果的正确性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;typedef struct &#123; int a,b,c;&#125; node;node a[1001];bool op(node a,node b)&#123; return a.c&lt;b.c;&#125;int father[201];void clean_father(int n)&#123; for (int i=1;i&lt;=n;i++) father[i]=i;&#125;int getfather(int x)&#123; if (father[x]!=x) father[x]=getfather(father[x]); return father[x];&#125;void link(int x,int y)&#123; father[getfather(x)]=getfather(y);&#125;int main()&#123; int n,m; while (scanf("%d%d",&amp;n,&amp;m)!=EOF) &#123; for (int i=1;i&lt;=m;i++) scanf("%d%d%d",&amp;a[i].a,&amp;a[i].b,&amp;a[i].c); sort(&amp;a[1],&amp;a[m+1],op); int q; scanf("%d",&amp;q); for (int i=1;i&lt;=q;i++) &#123; int t1,t2; scanf("%d%d",&amp;t1,&amp;t2); int minn,maxn,ans=2147483647; for (int j=1;j&lt;=m;j++) &#123; minn=2147483647; maxn=0; clean_father(n); for (int k=j;k&lt;=m;k++) if (getfather(a[k].a)!=getfather(a[k].b)) &#123; link(a[k].a,a[k].b); if (minn&gt;a[k].c) minn=a[k].c; if (maxn&lt;a[k].c) maxn=a[k].c; if (maxn-minn&gt;ans) break; if (getfather(t1)==getfather(t2)) &#123; if (ans&gt;maxn-minn) &#123; ans=maxn-minn; break; &#125; &#125; &#125; &#125; if (ans!=2147483647) printf("%d\n",ans); else printf("-1\n"); &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL中优先队列的使用]]></title>
    <url>%2F2014%2F09%2F06%2F2014-09-06-stl-queue%2F</url>
    <content type="text"><![CDATA[STL中有一个优先队列的容器可以使用。 头文件 queue 队列容器 vector 向量容器 操作 优先级队列支持的操作 调用 功能 q.empty() 如果队列为空，则返回true，否则返回false q.size() 返回队列中元素的个数 q.pop() 删除队首元素，但不返回其值 q.top() 返回具有最高优先级的元素值，但不删除该元素 q.push(item) 在基于优先级的适当位置插入新元素 对于Pascal留下来的手打堆习惯来说，其实对我用处不大，不过好像STL里面的复杂度更低，代码长度也能少点，以后尽量用STL好了。 测试如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;queue&gt;using namespace std;struct cmp&#123; bool operator()(int x,int y) &#123; return x&gt;y; &#125;&#125;;typedef struct nod&#123; int x,y; friend bool operator &lt; (nod a,nod b) &#123; return a.y&gt;b.y; &#125;&#125; node;int main()&#123; priority_queue&lt;int&gt;simple; priority_queue&lt;int,vector&lt;int&gt;,cmp&gt;define; priority_queue&lt;node&gt;heap; int a[10]=&#123;20,50,3202,20,503,12,56,62,50,80&#125;; printf("Simple Test:\n"); for (int i=0;i&lt;10;i++) simple.push(a[i]); for (int i=1;i&lt;=10;i++) &#123; int temp=simple.top(); simple.pop(); printf("%d\n",temp); &#125; printf("Define Test:\n"); for (int i=0;i&lt;10;i++) define.push(a[i]); for (int i=1;i&lt;=10;i++) &#123; int temp=define.top(); define.pop(); printf("%d\n",temp); &#125; printf("Heap Test:\n"); node b[10]=&#123;&#123;1,2&#125;,&#123;2,100&#125;,&#123;3,4&#125;,&#123;4,50&#125;,&#123;5,6&#125;,&#123;6,7&#125;,&#123;7,8&#125;,&#123;8,9&#125;,&#123;9,10&#125;,&#123;10,11&#125;&#125;; for (int i=0;i&lt;10;i++) heap.push(b[i]); for (int i=1;i&lt;=10;i++) &#123; node temp=heap.top(); heap.pop(); printf("%d %d\n",temp.x,temp.y); &#125; printf("%d",2&lt;1); return 0;&#125;]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>优先队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDU 2489 Minimal Ratio Tree 最小生成树+DFS]]></title>
    <url>%2F2014%2F09%2F06%2F2014-09-06-HDU-2489-Minimal-Ratio-Tree%2F</url>
    <content type="text"><![CDATA[Minimal Ratio TreeTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 32768/32768 K (Java/Others) Problem DescriptionFor a tree, which nodes and edges are all weighted, the ratio of it is calculated according to the following equation. Given a complete graph of n nodes with all nodes and edges weighted, your task is to find a tree, which is a sub-graph of the original graph, with m nodes and whose ratio is the smallest among all the trees of m nodes in the graph. InputInput contains multiple test cases. The first line of each test case contains two integers n (2&lt;=n&lt;=15) and m (2&lt;=m&lt;=n), which stands for the number of nodes in the graph and the number of nodes in the minimal ratio tree. Two zeros end the input. The next line contains n numbers which stand for the weight of each node. The following n lines contain a diagonally symmetrical n×n connectivity matrix with each element shows the weight of the edge connecting one node with another. Of course, the diagonal will be all 0, since there is no edge connecting a node with itself. All the weights of both nodes and edges (except for the ones on the diagonal of the matrix) are integers and in the range of [1, 100].The figure below illustrates the first test case in sample input. Node 1 and Node 3 form the minimal ratio tree. OutputFor each test case output one line contains a sequence of the m nodes which constructs the minimal ratio tree. Nodes should be arranged in ascending order. If there are several such sequences, pick the one which has the smallest node number; if there’s a tie, look at the second smallest node number, etc. Please note that the nodes are numbered from 1 . Sample Input 3 230 20 100 6 26 0 32 3 02 21 10 22 00 0 Sample Output 1 31 2 题意给出一张n个点的图，图中的每一个结点以及每一条边都有其权值，要求从中选出m个点，找到m-1条边将其连接，使得边权值与点权值的比值达到最小。 分析要使得比值最小，则点权值和尽可能地大同时边权值和尽可能地小。直接上考虑，边权值和尽可能小即对这m个点作最小生成树。 而题目给定的n不大，故可以用DFS搜出需要的m个点，然后对m个点进行最小生成树，中间注意判断和保存即可。 我用了一个dijkstra+优先队列的prim去找MST，这也是我第一次尝试使用STL的优先队列。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;#include&lt;queue&gt;using namespace std;bool flag[16],outp[16];int n,m,node[16];int ma[16][16];double mi;typedef struct heaptyp&#123; int num,key; friend bool operator &lt; (heaptyp a,heaptyp b) &#123; return a.num&gt;b.num; &#125;&#125; heaptype;void prim(int s,int tot)&#123; int i,j,now,ans; bool fla[16]; priority_queue&lt;heaptype&gt;heap; heaptype aaa; memset(fla,0,sizeof(fla)); fla[s]=true; ans=0; for (i=1;i&lt;=n;i++) if (flag[i]&amp;&amp;ma[s][i]) &#123; heaptype temp; temp.num=ma[s][i]; temp.key=i; heap.push(temp); aaa=heap.top(); &#125; for (j=1;j&lt;m;j++) &#123; heaptype h=heap.top(); heap.pop(); aaa=heap.top(); while (fla[h.key]) &#123; h=heap.top(); heap.pop(); &#125; now=h.key; fla[now]=true; ans+=h.num; for (i=1;i&lt;=n;i++) if (flag[i]&amp;&amp;ma[now][i]) if (!fla[i]) &#123; heaptype temp; temp.num=ma[now][i]; temp.key=i; heap.push(temp); aaa=heap.top(); &#125; &#125; double rat=(double)ans/tot; if (mi-rat&gt;0.0000001) &#123; mi=rat; for (int i=1;i&lt;=n;i++) outp[i]=fla[i]; &#125;&#125;void dfs(int now,int last,int tot)&#123; if (now==m) &#123; int i; for (i=1;i&lt;=n;i++) if (flag[i]) break; prim(i,tot); &#125; else &#123; now++; for (int i=last+1;i&lt;=n-m+now;i++) &#123; flag[i]=true; dfs(now,i,tot+node[i]); flag[i]=false; &#125; &#125;&#125;int main()&#123; scanf("%d%d",&amp;n,&amp;m); while (!(n==0&amp;&amp;m==0)) &#123; for (int i=1;i&lt;=n;i++) scanf("%d",&amp;node[i]); for (int i=1;i&lt;=n;i++) for (int j=1;j&lt;=n;j++) scanf("%d",&amp;ma[i][j]); mi=2147483647; memset(flag,0,sizeof(flag)); for (int i=1;i&lt;=n-m+1;i++) &#123; flag[i]=true; dfs(1,i,node[i]); flag[i]=false; &#125; int i; for (i=1;i&lt;=n;i++) if (outp[i]) &#123; printf("%d",i); break; &#125; for (int j=i+1;j&lt;=n;j++) if (outp[j]) printf(" %d",j); printf("\n"); scanf("%d%d",&amp;n,&amp;m); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM-训练</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈我院学术氛围之我见]]></title>
    <url>%2F2014%2F08%2F13%2F2014-08-13-xueshu%2F</url>
    <content type="text"><![CDATA[你的未来会是一张白纸，你想要变成什么样子呢？ 写在 长安大学ACM协会 成立之日 很高兴默默奋斗这么多年，我们学校自己的ACM组织终于成立了。 关于ACM学习的事情，其他几位ACM校队成员都给大家讲过不少了。我找了一下自己上学期写的一篇关于学校学术氛围的，算是在长大几年的心得体会吧，这个时候给刚进校的你们作为忠告好了，也是希望你们能在以后的学习生活中能够做得更好。有些事情对我们来说已经晚了，但是你们还能在问题出现之前提前警醒。 去年寒假的时候，有一位同学跟我闲聊，聊到我们学校缺少学术氛围的事，中间有不少比较尖锐的现实，然而我说我觉得他说的很对，别的学院我不知道，也不敢妄加品论，就光说我们自己的学院好了： 曾经，在来大学之前，我幻想的信息专业是这样的：学校里面有很多大神，各路大神都有自己擅长的方面，论坛上能有很多人讨论专业内容，几个学校的计算机系相互网络入侵对方……慢慢熟悉学校之后，我发现这一切确实都跟我原来幻想的很！不！一！样！。大概就是要什么没什么那种，然后最坑的是学信息的大一还要求不！准！带！电！脑！，这是让我们学什么呢？（虽然我默默地带了） 选择信息的，如果不是被调剂或者听别人介绍过来的，无论懂不懂电脑，至少都会对信息相关的内容很感兴趣。每个人都很想学新东西，然而我们学校目前的状况是高年级的大神不多，新进校的大一同学空有满腔热情，却苦于没有地方能够学习。久而久之，当初的兴趣也慢慢被磨蚀掉了。于是进入下一轮循环，等下一届新生到来之后，他们也没有办法再带什么，于是新老交替、最后状况却仍旧没有改变。 那位同学跟我聊到说，他曾经对一些能够自己在网上实现的小东西很感兴趣，关注过一些小比赛，例如微信开放平台等等。参赛者能看到西交、西电的，但是陕西省其他的学校几乎就没有人报过。其实在我们国家这样的教育体制下，每个学校所开的课程大致相似，在教学方面，每个学校所收的教师也都是硕博教授吧？可能名校师资力量确实要强很多，但从学习上来说，课堂教学又能相差多少呢？（课堂这块继续略过，里面水也很深。。。-_-///） 那么强校跟弱校的差距到底在哪？我想，或许学术氛围这四个字真的很重要。 我所见的现状环境不同，学生的思想也不同。因为没有他们那样的学术环境，所以我们把学习都局限在课堂上、课本上，大多数同学想要学东西，但是苦于没有人教，然后又没有自己学习的意识。但是恰恰是光靠课堂上学的那些，我们什么都做不了。人家大一大二就能自己做电赛、写手机软件、做微信平台，我们可能到大四毕业都做不了这些事。他们有活跃的社团，有专门研究软件开发的、有研究电竞的、有学做flash做游戏的、有研究网络攻防的、有研究无线加密的……只可惜很遗憾，在我们学校能看到的大部分社团都是人文类的。仅有的几个专业性社团最后也是因为人数太少，活动参与不积极，最终走向结束。我们学校计算机协会的前主席是我舍友，对于我所见到的现状，他也有过很多的思考，曾经也是想过希望能在他们协会内部多开点交流学习的版块，最后也是限于我们学校的各种因素而不了了之了。 大一刚入校的时候，对于学校组织的选择，我选择的是我们学院院科协的技术服务部。当初就是因为觉得只有在那里才算是能有个跟别人交流技术、学习技术的地方，才抛开了好多学长让我加学生会的建议。那一届科协的主席和部长的能力都很强，我大学三年过来所见到的科协也就是那时候才是巅峰。只可惜当时的主席、部长确实是有这方面的想法，最终也还是失败了。我们内部做过挺多次技术交流会，最终想要推广到全院、全校的时候，却还是遭遇了很大的困难。 这里再提到我们学院的几位辅导员，然而他们都并不是信息相关专业毕业出来的，对于如何管理学生，他们很有经验，但是对于我们信息学院学生真正需要的，他们却可能不是太了解。很遗憾我们学院每次在学校里面得的奖都是军训歌咏比赛、广播操比赛等等这类，而且辅导员看重、或者说他们有能力去促进的也只有这些，真正在信息相关的领域里却是做不出什么成就来。 最后我希望看到这里的同学都能够好好想想下面这个问题，既然我们现在跟大家上的都是一样的课，学的都是一样的东西，那么是什么能够让我们自己与众不同呢？ 没人带只能自己学咯？那还能怎么办？如果不想剩下时间都荒废掉，只有自己对自己想办法了。 剩下的还是留给大家自己思考吧。。。 我们在学校推广ACM，其实也是希望能够有更多的同学能够参与到自己思考、自己学习的道路上来。 确实很高兴看到我们学校的ACM社团终于要成立了，我校在ACM方面起步较晚，前面几届也是由老师和几个队员坚持下来的，三无组织维持了这么多年，今天终于有个像样的实体了。之后相信一定会越办越好！！！！！！！！！ 我们现在想做的也就是开拓者。我不敢说我们到底最终能够改变什么。只是到你们这届，我想说事情会不一样了。然后再下一届，我还在的话会继续带，你们那时候也已经培养出来自己的能力了。新生所能够接触到的环境也一定会更好！ 希望日后有一天，我曾经幻想过百花齐放的场景能在我们学院出现。]]></content>
      <categories>
        <category>Bachelor-of-Engineering</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[品牌笔记本预装windows的OEM分区问题（联想）]]></title>
    <url>%2F2014%2F07%2F20%2F2014-07-20-oem%2F</url>
    <content type="text"><![CDATA[我的Y480出厂预装的win7，现在过了好久了，系统早就格盘重装成win8.1了，但是分区表里面还有个OEM分区。里面存的应该是预装的系统备份，跟笔记本电源键旁边的恢复键直接绑定……不过系统既然早被我格完了，那个键已经很早没有用过了，想想决定还是把这个OEM分区删掉。 花了点时间把这事搞定了，之前是自己都不敢动这块硬盘，担心乱弄会出问题。 各大厂商设置的隐藏分区方法应该都类似，可以参照这个…或者，把正常分区隐藏起来也是可以的哦，隐藏之后，不知道这种方法的人应该是很难破解出来滴。 OEM分区其实只是做的隐藏分区，因为设置的分区不可读，所以在磁盘管理中系统分区表显示可用100%。 不需要别的，使用系统自带的diskpart工具即可以完成管理。 打开cmd，输入diskpart 输入？号可以查看帮助 list disk 显示当前系统上所有的磁盘信息 select disk 0 根据编号选择需要的硬盘…然而我的电脑上就这么一块 rescan 重新扫描配置信息 list partition 显示当前硬盘的分区信息 select partition 4 也是根据分区号选择，我的所有分区都是主分区，分区4的类型为OEM，即为本次要解决的目标分区 选择完分区之后 deatil partition 显示分区详细信息 分区类型4为fat32类型，7为NTFS类型，隐藏的OEM分区类型为11或12，设为11则显示为主分区 set id=7 将该分区类型改成7即可 改完之后隐藏分区就显示出来了，可以跟正常的一样使用。 然后我默默地把最后这一小块并到了D盘里面去，Hoho~]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>OEM</tag>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu11.04之后的runlevel定义]]></title>
    <url>%2F2014%2F06%2F21%2F2014-06-21-runlevel%2F</url>
    <content type="text"><![CDATA[Ubuntu是个不错的Linux发行版，至少当时我Linux启蒙的时候用的就是这个，也习惯了Debian的apt-get的软件库 然而有时候吧，我开Linux只是做做服务器的实验什么的，开机自动进图形界面反而会有一些费时。因此我想让Ubuntu启动之后直接进tty1的代码界面。 下面开始搞。 Debian的runlevel跟常见的一般Linux发行版有些与众不同，一般的定义为： 12345670 - Halt1 - Single2 - Full muti-user with display manager(GUI)3 - Full muti-user with display manager(GUI)4 - Full muti-user with display manager(GUI)5 - Full muti-user with display manager(GUI)6 - Reboot 系统默认启动进去是runlevel2 2~5的运行级别被设定成完全相同，我一般习惯于把GUI关掉，默认用CLI进行登陆，但是又希望需要图形界面的时候能够很快启动，于是就可以从这里下手。默认用一个不带GUI的运行模式启动，需要的时候再切换成带有GUI的模式进行工作。 GUI启动配置 Ubuntu11.04之后的版本GUI都使用Light Display Manager来进行管理，配置文件在： /etc/init/lightdm.conf 内容里面开头的一段，设置了在哪一运行级别下启动和停止： 1234567891011start on ((filesystem and runlevel [!06] and started dbus and plymouth-ready) or runlevel PREVLEVEL=S)stop on runlevel [016] 通过修改start on 中的内容，可以让其不在我们默认的运行级别中运行。 修改默认运行级别 默认运行级别的设置在这里： /etc/inittab 可能有默认没有这个文件的情况，貌似我装14.04的时候就是这样，新建一个就好。 在里面写上下面这句话，id后面的数字就是默认的启动级别。（注意最后的冒号不能少！！！） 1id:3:initdefault:]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>runlevel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win8/8.1下NetFramework3.5的安装]]></title>
    <url>%2F2014%2F06%2F21%2F2014-06-21-netframework%2F</url>
    <content type="text"><![CDATA[Win8/8.1自带了更高版本的NetFramework，但是经常性地会需要用到3.5。但是使用系统提示安装，或者直接下载安装包后安装，常常会遇见下载失败的错误，提示没有连上网络等等各种问题。 我常会在电脑上存着各种版本的系统镜像，这个时候就能够派上用场了。可以使用系统自带的dism工具提取镜像中的内容来进行安装。 一、解压镜像，或者win8/8.1直接装载镜像到一个新的盘。 二、管理员身份运行cmd，输入以下内容，注意修改source即可 1dism.exe /online /enable-feature /featurename:NetFX3 /Source:d:\sources\sxs]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>NetFramework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Wubi方式安装Ubuntu14.04出错的解决办法]]></title>
    <url>%2F2014%2F06%2F19%2F2014-06-19-wubi%2F</url>
    <content type="text"><![CDATA[主要用的笔记本上正常用的win8，但是有时候要切到linux下用，wubi的ubuntu对我来说就是再适合不过的东西了，也不需要另外分盘。 （其实主要的问题是我电脑上的MBR主分区已经到4个了，没法再分盘了） 电脑上最早装的12，后来换到13.10，再就是最近出了14.04，于是按照往常的方式wubi安装新的版本，但是14.04这个版本貌似在grub挂载文件系统的时候出现了权限问题，不知道后来的版本有没有改过来0.0（后来听说官方准备在以后的版本中都取消Wubi啦？Oh…不要啊）。。。总之导致的结果就是wubi的Ubuntu进系统时显示严重磁盘错误。 最开始以为是系统冲突还是怎么样，后来Google了下，原来出错的不止我一个。百般搜索终于在国外一个论坛上找到了解决方案。 首先要想办法进去正如之前看到的错误提示，Wubi进不去是因为挂载文件系统的时候权限不正常。 在Ubuntu的grub启动画面按e进入编辑模式，找到以“linux”开头的那行，将其中的ro rootflags=sync，改为rw rootflags=sync。 修改完毕后再按F10启动。 此时即可正常进入Ubuntu14.04，GUI也能正常调用出来。 修改grub的配置文件上面那一步只是正常地进去，接下来为了保证以后每次都能进，还需要再改一下grub的配置文件。 编辑这个文件： /etc/grub.d/10_lupin 用vi、emacs，GUI下可以用gedit，就是注意需要root权限。 找到第150行左右，吧”ro”改为”rw” 1234linux $&#123;rel_dirname&#125;/$&#123;basename&#125; root=$&#123;LINUX_HOST_DEVICE&#125; loop=$&#123;loop_file_relative&#125; ro $&#123;args&#125; //修改前linux $&#123;rel_dirname&#125;/$&#123;basename&#125; root=$&#123;LINUX_HOST_DEVICE&#125; loop=$&#123;loop_file_relative&#125; rw $&#123;args&#125; //修改后 保存，更新配置文件即可 1$ sudo update-grub]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>wubi</tag>
      </tags>
  </entry>
</search>
